# Takenoko Configuration File
# This file contains all possible parameters for training with default values and comments
# Remove the # at the beginning of lines to enable/configure parameters
# "⚠️" means that either relevant functionality is WIP (under construction) or has not been fully tested yet

# =============================================================================
# TARGET TRAINING
# =============================================================================

# "wan21" or "wan22" 
target_model = "wan22"  

# =============================================================================
# CHECKPOINT SETTINGS
# =============================================================================

# DiT checkpoint path
dit = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp16.safetensors"

# use scaled fp8 for DiT
fp8_scaled = true

# Use fp8 for base model
fp8_base = true

# FP8 format for quantization ("e4m3" for E4M3FN or "e5m2" for E5M2)
# E4M3FN (e4m3): Better precision, requires RTX 40 series or newer (default)
# E5M2 (e5m2): Compatible with RTX 3000 series, slightly lower precision
fp8_format = "e4m3"

# Percentile for FP8 scale calculation
# Uses percentile instead of max value to reduce impact of outliers
# Range: 0.0 to 1.0, recommended: 0.99 to 0.9999
# fp8_percentile = 0.999

# Exclude specific modules from FP8 quantization (works with ALL FP8 modes)
# Comma-separated list or array of module name patterns to preserve
# Useful for keeping critical layers in higher precision
# fp8_exclude_keys = ["norm", "patch_embedding", "text_embedding", "time_embedding", "time_projection", "head", "img_emb"]

# Scale input tensor format for scaled_mm ("e4m3" or "e5m2", None disables input scaling)
# Enables input tensor quantization for additional memory savings, may impact quality, only works with fp8_scaled = true
# scale_input_tensor = "e4m3"

# Upcast quantization to float32 when computing scales and FP8 weights (improves accuracy, small VRAM cost during optimization)
# Only works with fp8_scaled = true
upcast_quantization = false

# Upcast linear matmul to float32
# Only works with fp8_scaled = true
upcast_linear = false

# Exclude feedforward layers from scaled_mm optimization (useful for WAN models where scaled_mm degrades quality)
# May help to prevent quality degradation in feedforward layers, only works with fp8_scaled = true
exclude_ffn_from_scaled_mm = true

# Optional: force a uniform cast of DiT weights ("fp16" or "bf16").
# When null, preserves default behavior (checkpoint dtype or fp8 paths).
# Ignored when fp8_scaled = true or mixed_precision_transformer = true.
# dit_cast_dtype = "fp16"

# Use mixed precision for transformer (preserves per-tensor dtypes from checkpoint)
# true for better dtype precision when using fp8_scaled
mixed_precision_transformer = true

# Enable FP16 accumulation
fp16_accumulation = false

# VAE checkpoint path
vae = "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors"

# data type for VAE, default is float16
# vae_dtype = "float16" 

# Cache VAE on CPU
# vae_cache_cpu = true

# text encoder (T5) checkpoint path
t5 = "https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth"

# Use fp8 for Text Encoder model
# fp8_t5 = true

# Directory to cache downloaded models
# model_cache_dir = "cache/models"

# Enable memory tracing
trace_memory = true

# =============================================================================
# ENHANCED FP8 QUANTIZATION
# =============================================================================

# Enable enhanced FP8 quantization features (default: false)
# When enabled, provides additional quantization modes and better control
fp8_use_enhanced = true

# Enhanced FP8 quantization mode (ONLY when fp8_use_enhanced = true)
# "tensor": Per-tensor quantization (fastest, lowest memory)
# "channel": Per-channel quantization (balanced accuracy/memory)
# "block": Block-wise quantization (best compression, configurable)
fp8_quantization_mode = "block"

# Block size for block-wise quantization (ONLY when fp8_quantization_mode = "block")
# Smaller values = better compression, larger values = faster processing
# Must be divisible by layer input features, falls back to channel mode if not
fp8_block_size = 64

# Whether to fallback to channel quantization if block-wise fails
# (e.g., when tensor dimensions are not divisible by block size)
fp8_block_wise_fallback_to_channel = true

# =============================================================================
# ⚠️ TORCHAO FP8 INTEGRATION
# =============================================================================

# Enable TorchAO FP8 quantization (requires torchao installation, default: false)
# Alternative to built-in FP8 quantization, uses TorchAO library
# Provides per-channel symmetric quantization with automatic scale derivation
# Note: Takes precedence over fp8_use_enhanced if both are enabled
torchao_fp8_enabled = false

# TorchAO FP8 weight dtype (only when torchao_fp8_enabled = true)
# "e4m3fn": E4M3FN format (default, better precision)
# "e5m2": E5M2 format (broader range, slightly lower precision)
torchao_fp8_weight_dtype = "e4m3fn"

# Target modules for TorchAO quantization (optional)
# Only quantize modules matching these patterns, useful for selective quantization
torchao_fp8_target_modules = ["attn", "mlp", "proj"]

# Exclude modules from TorchAO quantization (optional)
# Skip quantization for modules matching these patterns
torchao_fp8_exclude_modules = ["final_layer", "patch_embedding"]

# =============================================================================
# TRAINING SETTINGS
# =============================================================================

# Training steps
max_train_steps = 2000

# Training epochs (overrides max_train_steps if specified)
# max_train_epochs = 10

# Max num workers for DataLoader (lower is less main RAM usage, faster epoch start and slower data loading)
max_data_loader_n_workers = 2

# Persistent DataLoader workers (useful for reduce time gap between epoch, but may use more memory)
persistent_data_loader_workers = true

# DataLoader pinned memory (performance). Default: false
# When true, DataLoader uses page-locked host memory to speed up CPU->GPU transfers.
# Recommended when GPU utilization is < 90% and CPU is the bottleneck.
data_loader_pin_memory = false

# DataLoader prefetch factor (performance). 0 (default) means "use PyTorch default". Typical values: 2-8.
# Number of batches to prefetch per worker (only used when max_data_loader_n_workers > 0).
data_loader_prefetch_factor = 0

# Bucket-level shuffling across datasets (performance)
# When true, iterates batches grouped by bucket key (resolution; plus frame_count for video)
# across all datasets. Usually improves throughput with mixed shapes.
# Trade-off: less step-to-step shape mixing (still shuffled within each epoch).
# Default: false (legacy behavior)
bucket_shuffle_across_datasets = false

# Random seed for training
seed = 20250811

# Enable gradient checkpointing
# Reduces activation memory by recomputing during backward; slower but lower VRAM
gradient_checkpointing = true

# Enable CPU offloading for gradient checkpointing activations
# Further reduces VRAM usage by offloading activations to CPU during gradient checkpointing
# Must be used together with gradient_checkpointing = true
# Particularly effective for video training or large latent sizes, but increases CPU-GPU transfer overhead
# Memory impact: VRAM ↓, RAM ↑, Training speed: 5-20% slower depending on model and batch size
gradient_checkpointing_cpu_offload = false

# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps = 1

# Mixed precision training (choices: "no", "fp16", "bf16")
mixed_precision = "fp16"

# Use broadcasted per-sample time embedding for Wan 2.2/FVDM (L=1 per sample),
# instead of per-token embeddings. Off by default for maximal expressiveness.
broadcast_time_embed = false

# Lean attention math (experimental). When true, Wan 2.2 blocks keep computations
# in the current compute dtype (bf16/fp16) to reduce large fp32 intermediates
# and memory use. Original numerics are preserved when false.
lean_attn_math = false

# When using lean attention math, control compute dtype policy:
# true  = default to fp32 compute unless force-fp16 is enabled
# false = default to input dtype (fp16/bf16) unless force-fp16 is enabled
lean_attention_fp32_default = false

# Lower precision attention (experimental). When true, Wan 2.2 blocks use lower precision
# for attention calculations to reduce large fp32 intermediates and memory use.
# Original numerics are preserved when false.
lower_precision_attention = false

# Use Wan 2.1 style modulation for Wan 2.2
simple_modulation = false

# Selective torch.compile of core modules for speed; safe no-op unless enabled.
# Mutually exclusive with dynamo_backend (keep dynamo_backend = "NO" to use this).
# Uses torch.compile with configurable backends (eager, inductor, etc.)
optimized_torch_compile = false

# Compile parameters for optimized_torch_compile. Format:
# compile_args = ["BACKEND", "MODE", DYNAMIC, "FULLGRAPH"]
# - BACKEND: "eager" (default, most compatible) or "inductor" (requires Triton + C++ compiler)
# - MODE: "default" | "max-autotune" | "reduce-overhead" (torch dependent)
# - DYNAMIC: true|false|"auto" (auto lets torch decide)
# - FULLGRAPH: "True"|"False" (string for clarity; case-insensitive)
# 
# Backend selection logic:
# 1. If "eager" is set, use eager backend
# 2. If "inductor" is set but Triton/C++ compiler not available, fallback to eager
# 3. If "inductor" is set and Triton/C++ compiler available, use inductor
compile_args = ["inductor", "default", "auto", "False"]

# Compute RoPE multipliers on-the-fly instead of using
# cached per-(F,H,W) precomputation. Off by default for speed.
rope_on_the_fly = false

# RoPE variant for positional embeddings: "default" (standard) or "comfy" (Comfy-style; enables advanced ND RoPE)
rope_func = "default"

# Use float32 precision for RoPE calculations instead of float64 (memory optimization, minimal accuracy loss)
rope_use_float32 = false

# =============================================================================
# OPTIMIZER SETTINGS
# =============================================================================

# Optimizer to use: AdamW (default), AdamW8bit, AdaFactor, RavenAdamW, RavenAdamW8bit. 
# You can also use any optimizer by specifying a fully-qualified class path 
# (e.g., 'torch.optim.AdamW', 'bitsandbytes.optim.AdEMAMix8bit').
# Resolved dynamically in optimizer manager with provided optimizer_args.
# Available optimizers:
# - "AdamW": Standard PyTorch AdamW
# - "AdamW8bit": bitsandbytes 8-bit AdamW (quantized GPU states)
# - "RavenAdamW": CPU-offloaded AdamW with BF16/FP32 states
# - "RavenAdamW8bit": CPU-offloaded 8-bit quantized AdamW
# - "AdaFactor": Memory-efficient adaptive optimizer with relative_step scheduling
# - "Muon", "NorMuon", "Riemannion": Second-order optimizers for matrix parameters (Riemannion supports paired LoRA updates)
# - "TemporalAdamW": AdamW with temporal gradient smoothing for video training
# - "Prodigy": Adaptive learning rate optimizer
# - "AdaMuon": Muon-style orthogonal optimizer with element-wise adaptivity for ≥2D params
# - "GaLoreAdamW", "GaLoreAdamW8bit", "GaLoreAdafactor": GaLore optimizers
# - "QGaLoreAdamW8bit": Q-GaLore optimizer (requires q-galore-torch)
# - "IVON": Bayesian optimizer that samples parameters each step (requires ivon_ess)
# - "SPlus": SPlus optimizer (kvfrans/splus)
# - "SOAP", "SophiaG", "Scion", etc. (see optimizer_manager.py)
optimizer_type = "adamw8bit"

# IVON effective sample size (ESS). Only used when optimizer_type = "ivon".
# If unset, Takenoko will auto-set it from the training dataset size.
# ivon_ess = 100000

# Additional arguments for optimizer (like "weight_decay=0.01 betas=0.9,0.999 ...")
# Format: key=value pairs as strings. Values are parsed as Python literals.
# optimizer_args = ["weight_decay=0.01", "betas=(0.9,0.999)", "log_muon_metrics=True"]
#
# Muon-specific arguments:
# - log_muon_metrics: Enable expensive metric logging (grad consistency, update ratio). Default: False.
#   Only supported by Muon, AdaMuon, NorMuon, ManifoldMuon, and MuonClip variants.
#   Setting to True adds CPU-GPU synchronization overhead but provides insights into training dynamics.
# - ns_steps: Number of Newton-Schulz iterations (default: 5).
# - nesterov: Enable Nesterov momentum (default: True).
# - weight_decay_type: Type of weight decay to apply (supported by Muon/AdaMuon/NorMuon).
#   Options: "default", "cautious" (updates only when grad & param agree), "scheduled" (scaled by lr/initial_lr), "cautious_scheduled".
#   Default: "default".
#
# AdaMuon-specific arguments:
# - beta2: Second moment coefficient (default: 0.95).
# - scale_factor: Scaling factor for updates (default: 0.2).
# - sign_stabilization: Use sign stabilization (default: True).
#
# NorMuon-specific arguments:
# - beta2: Second moment coefficient (default: 0.95).
#
# ManifoldMuon-specific arguments:
# - eta: Learning rate on the manifold (default: 0.1).
# - alpha: Regularization strength (default: 0.01).
# - dual_steps: Number of dual ascent steps (default: 100).
# - tolerance: Convergence tolerance for dual ascent (default: 1e-6).
#
# MuonClip-specific arguments:
# - tau: Clipping threshold for QK-Clip (default: 100.0).
#
# optimizer_args = ["log_muon_metrics=True", "ns_steps=5", "weight_decay_type='cautious'"]
#
# Example for GaLore
# optimizer_args = ["galore_rank=128", "galore_update_proj_gap=200", "galore_scale=0.25", "galore_proj_type='std'", "galore_apply_to='matrix_only'"]
# Optional: group GaLore params by layer or parameter for finer control ("none", "layer", "param")
# optimizer_args = ["galore_group_by='layer'"]
# Optional: override GaLore settings for specific parameter name patterns (fnmatch)
# optimizer_args = ["galore_group_by='layer'", "galore_group_overrides=[{'pattern':'transformer.blocks.*','rank':64},{'pattern':'transformer.final_layer.*','rank':32}]"]

# Q-GaLore target modules (required when optimizer_type = "q_galore_adamw8bit")
# Accepts a list of module name patterns or a single regex string.
# Special token "all-linear" targets every nn.Linear layer.
# q_galore_target_modules = ["attn", "mlp", "proj"]

# Q-GaLore int8 weight quantization for full fine-tuning (default: false)
# Replaces matching Linear modules with QGaLoreLinear to save memory.
# Requires optimizer_type = "q_galore_adamw8bit".
q_galore_weight_quant = false

# Q-GaLore int8 weight bits (must be 8)
q_galore_weight_bits = 8

# Q-GaLore int8 weight group size (must be > 0, typical: 256)
q_galore_weight_group_size = 256

# Q-GaLore stochastic rounding for int8 weights
q_galore_stochastic_round = false

# Learning rate
learning_rate = 2.0e-5

# Max gradient norm, 0 for no clipping
max_grad_norm = 1.0

# =============================================================================
# LEARNING RATE SCHEDULER
# =============================================================================

# Scheduler type
# - "constant": fixed LR for all steps (must have lr_warmup_steps = 0)
# - "constant_with_warmup": linear warmup to base LR, then hold constant
# - "linear": linear warmup, then linear decay to 0 over training steps
# - "cosine": linear warmup, then cosine decay to 0 over training steps
# - "cosine_with_restarts": warmup then cosine with restarts; uses lr_scheduler_num_cycles
# - "polynomial": warmup then polynomial decay; uses lr_scheduler_power
# - "inverse_sqrt": warmup then 1/sqrt(t) decay; uses lr_scheduler_timescale (defaults to lr_warmup_steps)
# - "cosine_with_min_lr": warmup then cosine decay to (min_lr_ratio * base LR); uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "warmup_stable_decay": warmup -> flat (stable) -> decay; requires lr_warmup_steps and lr_decay_steps; uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "piecewise_constant": step-wise schedule (from diffusers); requires lr_scheduler_args with step_rules
# - "adafactor:<initial_lr>": only valid with Adafactor optimizer (relative_step=True); no warmup/decay used
# All schedulers except "constant" require lr_warmup_steps to be provided (0 is allowed)
# "linear" and "cosine" ignore lr_decay_steps; "warmup_stable_decay" and some advanced schedules require lr_decay_steps > 0
lr_scheduler = "constant"

# Number of warmup steps (int) or fraction of total steps (float in 0..1)
# lr_warmup_steps and lr_decay_steps accept int (steps) or float (0<value<1) as a fraction of total optimizer steps (max_train_steps × num_processes)
lr_warmup_steps = 0

# Number of decay steps for schedulers that support it (int) or fraction (float in 0..1, ratio of train steps)
lr_decay_steps = 0

# Cycles: used by cosine_with_restarts; for cosine_with_min_lr and warmup_stable_decay, half of this value is used internally
lr_scheduler_num_cycles = 1

# Exponent for polynomial scheduler
lr_scheduler_power = 1.0

# Timescale for inverse_sqrt; if unset, defaults to lr_warmup_steps
# lr_scheduler_timescale = 0.0 

# Minimum LR ratio (final_LR = base_LR * ratio) for cosine_with_min_lr and warmup_stable_decay
# lr_scheduler_min_lr_ratio = 0.0 

# Use a custom scheduler class (overrides lr_scheduler above). Either a torch scheduler name (e.g. "OneCycleLR")
# or a fully-qualified import path (e.g. "torch.optim.lr_scheduler.StepLR"), kwargs come from lr_scheduler_args.
lr_scheduler_type = ""

# Extra kwargs for the chosen scheduler as a list of key=value strings (parsed via Python literal_eval)
# examples: lr_scheduler_args = 'T_max=100', lr_scheduler_args = 'step_rules=[(100,0.1),(300,0.01)]'
lr_scheduler_args = ''

# =============================================================================
# NETWORK SETTINGS
# =============================================================================

# Network module to train: "networks.lora_wan", "networks.riemann_lora_wan", "networks.control_lora_wan",
# "networks.reward_lora", "networks.vae_wan", "lycoris.kohya", "networks.t_lora", "networks.moc_lora",
# "networks.mhc_lora", "networks.singlora_wan", "networks.wan_finetune", "networks.slider_lora_wan",
# "networks.relora_wan"
# The module defines trainable adapters and is imported dynamically by the trainer.
network_module = "networks.lora_wan"

# network dimensions (depends on each network)
network_dim = 32

# alpha for LoRA weight scaling
network_alpha = 32

# pretrained weights for network
# network_weights = "path/to/network/weights.safetensors" 

# drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons)
# network_dropout = 0 

# additional arguments for network (key=value). Parsed into list and consumed by network module.
# For regular LoRA (networks.lora_wan), supported args include:
# - include_patterns / exclude_patterns: regex list of target module names
# - rank_dropout / module_dropout: dropout for LoRA ranks/modules
# - conv_dim / conv_alpha: Conv2d LoRA rank/alpha
# - loraplus_lr_ratio: LoRA+ lr scaling for lora_up
# - ggpo_sigma / ggpo_beta: LoRA-GGPO regularization
# - verbose: log selected/skippped modules
# Lycoris example is in wan21_lycoris.toml
# network_args = '' 

# LoRA+ (scaled lr on lora_up only):
# network_args = ["loraplus_lr_ratio=2.0"]

# LoRA-GGPO regularization (perturbs Linear LoRA weights during training):
# Parsed by `takenoko.create_args_from_config` into args.ggpo_sigma/ggpo_beta for the loss hooks.
# network_args = ["ggpo_sigma=0.03", "ggpo_beta=0.01"]
# RiemannLoRA (requires network_module = "networks.riemann_lora_wan"):
# - loi_enabled: enable gradient-informed initialization (default false)
# - loi_alpha: scale for LOI update (default 0.1)
# - loi_oversample: randomized SVD oversampling parameter p (default 2)
# - loi_power_iters: randomized SVD power iterations q (default 1)
# - loi_max_elements: skip LOI when out_dim * in_dim exceeds this (default 2000000)
# network_args = ["loi_enabled=true", "loi_alpha=0.1", "loi_oversample=2", "loi_power_iters=1", "loi_max_elements=2000000"]

# T-LoRA (timestep-gated LoRA)
# Activated when network_module = "networks.t_lora".
# Parameters:
# - tlora_trainer_type: "lora" (default) for vanilla T-LoRA, "ortho_lora" for orthogonal LoRA
# - tlora_min_rank: minimum active rank at highest noise (int ≥ 0)
# - tlora_alpha: curvature for r(t) = ((T - t)/T)^alpha * (max_rank - min_rank) + min_rank (float ≥ 0)
# - tlora_boundary_timestep: boundary timestep for WAN-Video style expert partitioning (int in [0,1000])
# Notes:
# - Use include_patterns to target only attention projections if desired.
#   Example include pattern: "include_patterns=['.*self_attn.*(q|k|v|to_out).*','.*cross_attn.*(q|k|v|to_out).*']"
# Example vanilla T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875"
# ]

# Orthogonal T-LoRA (optional, preserves original TLora behavior when disabled)
# Parameters:
# - sig_type: initialization singulars to use {"last", "principal", "middle"}; default "last"
# - ortho_from_layer: if true, initialize q/p/lambda from the original Linear weight SVD; otherwise use random base SVD
# - ortho_reg_lambda: global regularization weight for orthogonality penalties (applies to both p and q if specific lambdas are not set)
# - ortho_reg_lambda_p: regularization weight for p-layer orthogonality (||P^T P - I||_F^2)
# - ortho_reg_lambda_q: regularization weight for q-layer orthogonality (||Q Q^T - I||_F^2)
# Notes:
# - Orthogonal parametrization supports Linear-based LoRA adapters; Conv2d LoRA is skipped gracefully.
# - Timestep rank-mask and standard LoRA multiplier are still applied.
# Example orthogonal T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=ortho_lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875",
#   "sig_type=last",
#   "ortho_from_layer=true",
#   "ortho_reg_lambda_p=1e-3",
#   "ortho_reg_lambda_q=1e-3",
# ]

# MoC-LoRA (Mixture of Contexts for Long Video Generation)
# Activated when network_module = "networks.moc_lora".
# Parameters:
# - moc_chunk_size: chunk size for MoC attention (int ≥ 1024)
# - moc_top_k: top-k for MoC attention (int ≥ 1)
# - moc_progressive_sparsify: enable progressive sparsification for MoC attention (bool)
# - moc_enable_causality: enable causality for MoC attention (bool)
# - moc_context_dropout: dropout for MoC attention (float in [0.0, 1.0])
# - moc_implementation: implementation mode ("original" for 100% original accuracy, "optimized" for speed)
# - moc_max_layers: limit MoC to first N layers for memory control (int, optional)
# - moc_max_modules: limit MoC to first N modules per layer for memory control (int, optional)
# Example MoC-LoRA configuration:
# network_args = [
#   "moc_chunk_size=1280",
#   "moc_top_k=5",
#   "moc_progressive_sparsify=false",
#   "moc_enable_causality=true",
#   "moc_context_dropout=0.0",
#   "moc_implementation=optimized",
#   "moc_max_layers=2",
#   "moc_max_modules=4"
# ]

# mHC-LoRA (Manifold-Constrained Hyper-Connections style LoRA)
# Activated when network_module = "networks.mhc_lora".
# This approximates mHC by using multiple LoRA paths and a doubly-stochastic
# mixing matrix (Sinkhorn-Knopp) initialized near identity.
# Parameters (defaults shown):
# - mhc_num_paths: number of LoRA paths (int >= 1, typical 2 or 4)
# - mhc_sinkhorn_iters: Sinkhorn normalization iterations (int >= 1)
# - mhc_mixing_init: "identity" (default), "uniform", or "random"
# - mhc_mixing_strength: mix strength in [0,1] (0 = pure identity)
# - mhc_mixing_strength_end: end strength for linear schedule (float in [0,1], optional)
# - mhc_mixing_temperature: softmax temperature (>0, lower = sharper mixing)
# - mhc_mixing_temperature_end: end temperature for linear schedule (>0, optional)
# - mhc_mixing_schedule_steps: steps for linear schedule (int >= 0, 0 disables)
# - mhc_output_stream: which mixed stream to emit when output_mode="stream" (int in [0, mhc_num_paths-1])
# - mhc_output_mode: "stream" (default), "sum", or "mean" to aggregate all streams
# - mhc_nonneg_mixing: enforce non-negativity before Sinkhorn (bool)
# - mhc_dynamic_mixing: enable input-conditioned mixing logits (bool)
# - mhc_dynamic_hidden_dim: hidden dim for dynamic mixing MLP (int >= 0, 0 = linear)
# - mhc_dynamic_scale: scale for dynamic mixing logits (float >= 0)
# - mhc_dynamic_share: "none" (default) or "layer" to share dynamic MLP per block
# - mhc_timestep_mixing: scale mixing strength by timestep (bool)
# - mhc_timestep_max: max timestep for normalization (int > 0, default 1000)
# - mhc_timestep_gamma: exponent for timestep scaling (float >= 0)
# - mhc_timestep_strength_min: minimum mixing strength at high noise (float in [0,1])
# - mhc_path_scale_init: per-path scale init (float > 0)
# - mhc_path_scale_trainable: train per-path scales (bool)
# - mhc_path_dropout: drop whole paths during training (float in [0,1], optional)
# - mhc_freeze_mixing_steps: freeze mixing logits/MLP for first N steps (int >= 0)
# - mhc_identity_clamp_steps: clamp off-diagonal mixing for first N steps (int >= 0)
# - mhc_identity_clamp_max_offdiag: max off-diagonal weight during clamp (float in [0,1])
# - mhc_identity_reg_lambda: weight for identity regularization (float >= 0)
# - mhc_identity_reg_warmup_steps: decay regularization over this many steps (int >= 0)
# - mhc_identity_reg_power: decay power for identity regularization (float >= 0)
# - mhc_entropy_reg_lambda: weight for mixing entropy regularization (float >= 0)
# - mhc_entropy_reg_target: target entropy (float, default log(mhc_num_paths))
# mhc_num_paths = 2
# mhc_sinkhorn_iters = 20
# mhc_mixing_init = "identity"
# mhc_mixing_strength = 1.0
# mhc_mixing_strength_end = null
# mhc_mixing_temperature = 1.0
# mhc_mixing_temperature_end = null
# mhc_mixing_schedule_steps = 0
# mhc_output_stream = 0
# mhc_output_mode = "stream"
# mhc_nonneg_mixing = true
# mhc_dynamic_mixing = false
# mhc_dynamic_hidden_dim = 0
# mhc_dynamic_scale = 1.0
# mhc_dynamic_share = "none"
# mhc_timestep_mixing = false
# mhc_timestep_max = 1000
# mhc_timestep_gamma = 1.0
# mhc_timestep_strength_min = 0.0
# mhc_path_scale_init = 1.0
# mhc_path_scale_trainable = true
# mhc_path_dropout = null
# mhc_freeze_mixing_steps = 0
# mhc_identity_clamp_steps = 0
# mhc_identity_clamp_max_offdiag = 0.0
# mhc_identity_reg_lambda = 0.0
# mhc_identity_reg_warmup_steps = 0
# mhc_identity_reg_power = 1.0
# mhc_entropy_reg_lambda = 0.0
# mhc_entropy_reg_target = null
# Example mHC-LoRA configuration:
# network_args = [
#   "mhc_num_paths=2",
#   "mhc_sinkhorn_iters=20",
#   "mhc_mixing_init=identity",
#   "mhc_mixing_strength=1.0",
#   "mhc_mixing_strength_end=null",
#   "mhc_mixing_temperature=1.0",
#   "mhc_mixing_temperature_end=null",
#   "mhc_mixing_schedule_steps=0",
#   "mhc_output_stream=0",
#   "mhc_output_mode=stream",
#   "mhc_nonneg_mixing=true",
#   "mhc_dynamic_mixing=false",
#   "mhc_dynamic_hidden_dim=0",
#   "mhc_dynamic_scale=1.0",
#   "mhc_dynamic_share=none",
#   "mhc_timestep_mixing=false",
#   "mhc_timestep_max=1000",
#   "mhc_timestep_gamma=1.0",
#   "mhc_timestep_strength_min=0.0",
#   "mhc_path_scale_init=1.0",
#   "mhc_path_scale_trainable=true",
#   "mhc_path_dropout=null",
#   "mhc_freeze_mixing_steps=0",
#   "mhc_identity_clamp_steps=0",
#   "mhc_identity_clamp_max_offdiag=0.0",
#   "mhc_identity_reg_lambda=0.0",
#   "mhc_identity_reg_warmup_steps=0",
#   "mhc_identity_reg_power=1.0",
#   "mhc_entropy_reg_lambda=0.0",
#   "mhc_entropy_reg_target=null"
# ]

# SingLoRA-specific parameters
#network_args = [
#  "ramp_up_steps=1000",
#  "init_method=kaiming_uniform",
#  "use_rslora=false",
#  "dropout=null",
#]

# arbitrary comment string stored in metadata
# training_comment = "trained with Takenoko" 

# automatically determine dim (rank) from network_weights
# dim_from_weights = true 

# scale the weight of each key pair to help prevent overtraing via exploding gradients. (1 is a good starting point)
# scale_weight_norms = 1.0 

# network weights to merge into the model before training
# base_weights = "path/to/base1.safetensors"

# multiplier for base weights
# base_weights_multiplier = 1.0 

# frozen LoRA weights to apply during training only (not merged into the model)
# Accepts a single path or a list of paths. These LoRAs are applied to the model
# but never included in the optimizer or saved into the target LoRA.
# Requires: network_module compatible with LoRA weights (e.g., networks.lora_wan).
# frozen_network_weights = "path/to/frozen_lora.safetensors"
# frozen_network_weights = ["path/to/frozen_lora1.safetensors", "path/to/frozen_lora2.safetensors"]

# per-weight multiplier(s) for frozen_network_weights (float or list of floats)
# If a list is provided, it aligns by index with frozen_network_weights.
# frozen_network_weights_multiplier = 1.0
# frozen_network_weights_multiplier = [1.0, 0.75]

# use lycoris for inference
# lycoris = true 

# Weight for regularization loss (1.0 = normal, higher = stronger regularization)
# This applies to regularization datasets (is_reg=true) vs main dataset
# NOT related to masked training's "masked_prior_preservation_weight"
prior_loss_weight = 1.0

# =============================================================================
# ⚠️ RELORA TRAINING SETTINGS
# =============================================================================

# ReLoRA (periodic LoRA merge-and-reinit) is activated when network_module = "networks.relora_wan".
# ReLoRA requires lr_scheduler_type = "per_cycle_cosine" (cycle_steps=relora_cycle_length, warmup_steps=relora_restart_warmup_steps)
# or lr_scheduler_type = "relora_jagged_cosine" (first_warmup_steps=lr_warmup_steps, restart_warmup_steps=relora_restart_warmup_steps).
# Exactly one of relora_reset_optimizer_on_relora, relora_optimizer_random_pruning, relora_optimizer_magnitude_pruning must be enabled when relora_cycle_length > 0.
#
# relora_jagged_cosine also requires lr_scheduler_min_lr_ratio > 0 and lr_warmup_steps > 0.

# merge-and-reinit frequency in update steps (0 disables) (int >= 0)
relora_interval = 5000

# optimizer-reset frequency in update steps (0 disables) (int >= 0)
relora_cycle_length = 5000

# delay ReLoRA until this update step (int >= 0)
relora_start_step = 0

# offset applied to update step before modulo checks (int >= 0)
# set to 1 to offset reset cadence (step % reset_frequency == 1)
relora_adjust_step = 0

# warmup length to align with cycle restarts (int >= 0)
relora_restart_warmup_steps = 100

# reset optimizer states on ReLoRA cycles (bool)
relora_reset_optimizer_on_relora = true

# optional random pruning ratio for reset_optimizer_on_relora (float in [0,1])
# set to 0.999 for aggressive optimizer reset pruning
relora_reset_optimizer_prune_ratio = 0.0

# random pruning ratio for optimizer state (float in [0,1])
relora_optimizer_random_pruning = 0.0

# magnitude pruning ratio for optimizer state (float in [0,1])
relora_optimizer_magnitude_pruning = 0.0

# optimizer state tensors to reset (list of strings)
relora_optimizer_state_keys = ["exp_avg", "exp_avg_sq"]

# enable DeepSpeed ZeRO flat-state reset (bool)
relora_enable_deepspeed_zero_reset = false

# log reset count and optimizer zeroed ratio (bool)
relora_log_reset_metrics = false

# skip ReLoRA merge/reset until lr_warmup_steps elapses (bool)
relora_skip_reset_until_warmup = false

# =============================================================================
# ⚠️ SLIDER TRAINING SETTINGS
# =============================================================================

# Slider training for concept editing - enabled by using network_module = "networks.slider_lora_wan"

# Core slider parameters
# Strength of concept enhancement/suppression (default: 3.0)
slider_guidance_strength = 3.0

# Strength of anchor class preservation (default: 1.0)
slider_anchor_strength = 1.0

# Advanced guidance parameters (for fine-tuning)
# Base classifier-free guidance scale (default: 1.0)
slider_guidance_scale = 1.0

# Embedding-level guidance scaling (default: 1.0)
slider_guidance_embedding_scale = 1.0

# Separate scale for target predictions (default: 1.0)
slider_target_guidance_scale = 1.0

# Required prompt configuration
# Positive prompt representing desired concept direction (REQUIRED for slider training)
slider_positive_prompt = "beautiful, high quality, masterpiece"

# Negative prompt representing undesired concept direction (REQUIRED for slider training)
slider_negative_prompt = "ugly, low quality, blurry"

# Target class/concept being edited (REQUIRED for slider training)
slider_target_class = "portrait"

# Optional anchor class for concept preservation (helps prevent unwanted side effects)
slider_anchor_class = "person"

# Training parameters
# Learning rate multiplier for slider training (default: 1.0)
slider_learning_rate_multiplier = 1.0

# Cache text embeddings for efficiency (default: true)
slider_cache_embeddings = true

# T5 text encoder settings (follows Takenoko's pattern)
# Device for T5 encoder: "cpu" (saves VRAM) or "cuda" (default: "cpu")
slider_t5_device = "cpu"

# Cache embeddings during initialization (default: true)
slider_cache_on_init = true 

# =============================================================================
# FINE-TUNING SETTINGS
# =============================================================================

# Enable fine-tuning mode by setting network_module = "networks.wan_finetune"
# Requires significantly more VRAM than LoRA training

# Fraction of parameters to fine-tune (0.1 = 10%, 1.0 = 100%)
# Lower values reduce memory usage but may limit adaptation capability
fine_tune_ratio = 1

# Whether to also fine-tune the text encoder along with the transformer
# Significantly increases memory usage - only enable if you have sufficient VRAM (32GB+)
finetune_text_encoder = false

# Advanced optimizations for memory efficiency and performance
# These optimizations enable efficient training of large transformer models

# Enable full BF16 training including gradients (reduces memory usage ~50%)
# Requires mixed_precision = "bf16" to be set
# Provides better memory efficiency for large model fine-tuning
full_bf16 = false

# Automatically convert FP16 checkpoints to BF16 (finetune trainer only)
# When enabled:
# - Checks for existing bf16_* version in models/ directory
# - If not found, downloads and converts FP16→BF16 automatically  
# - Caches converted BF16 checkpoint for future runs
# - If original filename starts with "bf16_", uses as-is
use_or_convert_bf16 = true

# Enable direct checkpoint loading (skip base model)
# When enabled:
# - Skips base model loading and directly loads checkpoint weights
# - Requires checkpoint to be in the correct format
direct_checkpoint_loading = true

# Enable fused backward pass optimization for Adafactor optimizer
# Reduces memory peaks during backward pass by fusing operations
# Requires Adafactor optimizer and special fused optimization module
fused_backward_pass = false

# Enable memory-efficient checkpoint saving
# Uses direct GPU→disk writes to reduce memory peaks during save operations
# Recommended to keep enabled for fine-tuning large models
mem_eff_save = true

# Weight dynamics analysis frequency (0 = disabled)
# Set to 50-500 to monitor parameter evolution and training efficiency across model components
# Provides research insights into learning patterns, plasticity, and component specialization
verify_weight_dynamics_every_n_steps = 0

# Enable stochastic rounding for BF16 training (improves numerical stability)
# Requires mixed_precision = "bf16" to be effective
# Python implementation (default): Fast, works everywhere
use_stochastic_rounding = true

# CUDA implementation: Faster but requires compilation
# To use CUDA stochastic rounding:
# 1. Compile CUDA extension: cd src/utils/stochastic_rounding && python setup.py install  
# 2. Enable: use_stochastic_rounding_cuda = true
# NOTE: CUDA extension must be compiled on each machine individually!
# The setup.py is configured for compute capability 8.0 (RTX 30/40 series, A100).
# For other GPUs, modify setup.py:
#  - GTX 16/RTX 20 series: `-gencode=arch=compute_75,code=sm_75`
#  - RTX 30 series: `-gencode=arch=compute_86,code=sm_86` 
#  - RTX 40 series: `-gencode=arch=compute_89,code=sm_89`
use_stochastic_rounding_cuda = false

# =============================================================================
# PROFILING SETTINGS
# =============================================================================

# Enable PyTorch Profiler for performance analysis (CPU/GPU traces, memory usage)
profiling_enabled = false

# Number of steps to wait before starting profiling cycle (skip initial overhead)
profiling_skip_first = 10

# Number of steps to wait before starting profiling cycle (skip initial overhead)
profiling_wait_steps = 5

# Number of steps to warmup profiler
profiling_warmup_steps = 2

# Number of steps to record trace
profiling_active_steps = 3

# Number of times to repeat the cycle
profiling_repeat = 1

# Record shapes
profiling_record_shapes = true

# Record memory usage
profiling_profile_memory = true

# Record stack traces
profiling_with_stack = true

# Record FLOPs
profiling_with_flops = true

# =============================================================================
# TIMESTEP AND FLOW MATCHING SETTINGS
# =============================================================================

# Method to sample timesteps
# Available methods: sigma, uniform, sigmoid, shift, flux_shift, logsnr, qwen_shift, qinglong_flux, qinglong_qwen, logit_normal,
# bell_shaped, half_bell, lognorm_blend, lognorm_continuous_blend, enhanced_sigmoid, fopp, content, style, content_style_blend, mode_shift,
# temporal_pyramid, blockwise
timestep_sampling = "shift"

# Shift amount for discrete timestep sampling (used with 'shift' timestep_sampling)
discrete_flow_shift = 3.0

# Scale factor for sigmoid timestep sampling (used by: sigmoid, shift, flux_shift, qwen_shift, logit_normal, enhanced_sigmoid, qinglong_*)
sigmoid_scale = 1.0

# Bias for enhanced_sigmoid timestep sampling (only used when timestep_sampling = "enhanced_sigmoid")
sigmoid_bias = 0.0

# Enable dim-aware timestep shift for large video latents (default: false)
# When true, rescales timesteps by sqrt(latent_dim / dim_aware_shift_base) to reduce instability.
# Leave disabled unless training high-dimensional WanVideo LoRA; no effect on inference.
enable_dim_aware_time_shift = false

# Base divisor for dim-aware timestep shift. Effective shift = sqrt(latent_dim / dim_aware_shift_base).
# Higher values reduce the shift magnitude. Must be > 0.
dim_aware_shift_base = 4096.0

# Center of bell_shaped distribution in [0.0, 1.0]; 0.95 ~ timestep 950
# Only used when timestep_sampling = "bell_shaped"
bell_center = 0.5

# Std (spread) of bell_shaped distribution; smaller => sharper peak
# Only used when timestep_sampling = "bell_shaped"
bell_std = 0.2

# Alpha mix ratio for lognorm_blend and lognorm_continuous_blend (fraction of samples from LogNormal vs. linear)
# Only used when timestep_sampling = "lognorm_blend" or "lognorm_continuous_blend"
lognorm_blend_alpha = 0.75

# Blend ratio for content_style_blend (fraction of content vs style sampling)
# Only used when timestep_sampling = "content_style_blend"
# 0.0 = pure style (later timesteps), 1.0 = pure content (earlier timesteps)
content_style_blend_ratio = 0.5

# Weighting scheme for timestep distribution: logit_normal, mode, cosmap, sigma_sqrt, none.
# This key interacts with two independent things: sampling distribution (which timestep indices are drawn)
# and loss weighting (how much each drawn timestep contributes to the loss)
# Sampling distribution: only when timestep_sampling = "sigma".
# logit_normal (uses logit_mean/logit_std) or mode (uses mode_scale) shape the sampling density.
# none = uniform density. sigma_sqrt and cosmap do NOT influence sampling.
# For all other samplers, weighting_scheme does not change which timesteps are sampled.
# Loss weighting: for any sampler, ONLY sigma_sqrt or cosmap enable SD3-style per-index loss weights; other values imply unit weights.
weighting_scheme = "none"

# For logit-based transforms, sampling density when timestep_sampling="sigma" and weighting_scheme="logit_normal"
logit_mean = 0.0

# For logit-based transforms, mapping distribution when timestep_sampling="logsnr" (and the logsnr segment in qinglong_* variants)
logit_std = 1.0

# Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`
mode_scale = 1.29

# Time shift parameters for mode_shift timestep sampling
# time_shift_mu: shift parameter (usually >0, default 1.0)
time_shift_mu = 1.0

# time_shift_sigma: exponent parameter (default 1.0)
time_shift_sigma = 1.0

# set minimum time step for training (0~999, default is 0)
min_timestep = 0

# Set maximum time step for training (1~1000, default is 1000)
max_timestep = 1000

# Enable pre-computed timestep distribution (default: false)
# Uses a fixed, quantized distribution of t (reproducible) and takes precedence over num_timestep_buckets.
# Supported for most timestep_sampling methods; unsupported methods fall back to on-the-fly sampling.
# Distribution is sliced to [min_timestep, max_timestep] before sampling. Dataset-provided batch['timesteps']
# are ignored when enabled.
use_precomputed_timesteps = true

# Number of buckets for the precomputed distribution (default: 10000)
# Larger values = finer quantization of the target distribution. Key: 'precomputed_timestep_buckets'.
precomputed_timestep_buckets = 10000

# Optional: override the base area used by flux/qwen/qinglong precompute (auto-inferred from height/width/latents if unset)
# Helps make precomputed mid-shift match runtime behavior when image sizes are known ahead of time.
# precomputed_midshift_area = 1024.0

# Preserve distribution shape (default: false)
# If false: sampled t in [0,1] is linearly scaled into [min_timestep, max_timestep] (fast, but warps the shape).
# If true: rejection sampling preserves the original distribution shape inside the range (slower, shape-faithful).
# Applies after the sampling method (uniform/sigmoid/shift/flux_shift/logsnr/etc) or mapped presampled values.
# When dataset provides per-batch uniform t (see num_timestep_buckets), those are used first for mapping.
# With precomputed timesteps enabled, the global distribution is sliced to min/max first; constraints still apply.
preserve_distribution_shape = false

# Number of timestep buckets for training (default: None)
# Dataset-side per-epoch uniform stratification of u in [0,1] to improve coverage on small datasets.
# Used only when use_precomputed_timesteps = false. Samples are mapped through the chosen timestep_sampling,
# then min/max (and preserve_distribution_shape) are applied. Implemented in the dataset; batches may include
# a per-item 'timesteps' list. Ignored when use_precomputed_timesteps = true. Set >= 2 to enable.
# num_timestep_buckets = 10

# Skip extra in-range constraint
skip_extra_timestep_constraint = true

# Epsilon for extra in-range constraint
timestep_constraint_epsilon = 1e-5

# Round training timesteps to the nearest integer schedule step
round_training_timesteps = true

# Fast rejection sampling
fast_rejection_sampling = false

# When fast_rejection_sampling = true, control the candidate oversampling factor and max iterations
# rejection_overdraw_factor = 4.0

# When fast_rejection_sampling = true, control the max iterations
# rejection_max_iters = 10

# show timesteps: image, console
# Visualizes the actual distribution produced by the current sampling+constraints path. 
# show_timesteps = 'console' 

# Temporal pyramid sampling.
temporal_pyramid_num_stages = 3

# Stage weights (length = num_stages).
temporal_pyramid_stage_weights = [1.0, 1.0, 1.0]

# Optional custom stage boundaries in [0,1].
# Example: temporal_pyramid_stage_boundaries = [0.0, 0.4, 0.75, 1.0]
# temporal_pyramid_stage_boundaries = [0.0, 0.4, 0.75, 1.0]

# Temporal pyramid training (training-only).
enable_temporal_pyramid_training = false

# Temporal stride base and optional max stride.
temporal_pyramid_stride_base = 2

# Optional max stride.
temporal_pyramid_max_stride = 8

# Optional data-noise alignment (training-only).
enable_temporal_pyramid_data_noise_alignment = false

# Stage-wise targets (training-only).
enable_temporal_pyramid_stagewise_target = false

# Stage-wise target parameterization (training-only).
temporal_pyramid_gamma_sigma_mode = "flow"

# =============================================================================
# LOSS SETTINGS
# =============================================================================

# Base loss function type - expanded support for multiple loss functions
# Standard losses:
# - "mse": Standard Mean Squared Error loss (default)
# - "l1" or "mae": L1 (Mean Absolute Error) loss
# - "huber": Standard Huber loss with fixed delta (smooth_l1_loss)
# - "pure_huber": True Huber loss with configurable delta parameter
# - "pseudo_huber": Pseudo-Huber loss with fixed c parameter
# - "pseudo_huber_scheduled": Pseudo-Huber loss with adaptive c parameter scheduling
# 
# Advanced/Experimental losses:
# - "fourier": Frequency domain loss (requires additional parameters)
# - "dwt" or "wavelet": Discrete Wavelet Transform loss
# - "clustered_mse": Clustered MSE loss for better handling of outliers
# - "ew": EW (Error-Weighted) loss for adaptive weighting
# - "stepped": Stepped recovery loss for smoother flow dynamics (requires noise scheduler)
loss_type = "mse"

# Pseudo-Huber loss parameters (for loss_type = "pseudo_huber" or "pseudo_huber_scheduled")
# Pseudo-Huber c parameter (controls transition between L2 and L1 behavior)
# Smaller values (0.1-0.5) = more L1-like (robust to outliers)
# Larger values (1.0-2.0) = more L2-like (smooth gradients)
pseudo_huber_c = 0.5

# Scheduling type for adaptive c parameter (when loss_type = "pseudo_huber_scheduled")
# Options: "linear", "cosine", "exponential"
pseudo_huber_schedule_type = "linear"

# Minimum c value for scheduling (start of training)
pseudo_huber_c_min = 0.1

# Maximum c value for scheduling (end of training)
pseudo_huber_c_max = 1.0

# Huber loss parameters (for pure_huber loss type)
# Delta parameter for pure_huber loss type
# Controls the point where loss transitions from quadratic to linear
huber_delta = 1.0

# Fourier loss parameters (for loss_type = "fourier")
# Weight/strength of fourier loss component
fourier_weight = 0.05

# Fourier loss computation mode
# - "basic": Simple magnitude spectrum comparison
# - "weighted": Frequency-weighted comparison (default)
# - "multiscale": Multi-scale frequency analysis
# - "adaptive": Adaptive frequency weighting
fourier_mode = "weighted"

# Norm type for fourier loss computation
fourier_norm = "l2"  # "l1" or "l2"

# Dimensions for FFT computation (usually last 2 dims for 2D)
fourier_dims = [-2, -1]

# Numerical stability epsilon
fourier_eps = 1e-8

# Scale factors for multiscale fourier loss
fourier_multiscale_factors = [1, 2, 4]

# Threshold for adaptive fourier loss
fourier_adaptive_threshold = 0.1

# Alpha parameter for adaptive fourier loss
fourier_adaptive_alpha = 0.5

# High frequency weight for weighted fourier loss
fourier_high_freq_weight = 2.0

# Wavelet/DWT loss parameters (for loss_type = "dwt" or "wavelet")
# Wavelet type for DWT decomposition
# Common options: "haar", "db1", "db4", "db8", "bior2.2", "coif2"
wavelet_type = "haar"

# Number of decomposition levels
wavelet_levels = 1

# Border mode for wavelet transform
# Options: "zero", "symmetric", "periodization"
wavelet_mode = "zero"

# Clustered MSE loss parameters (for loss_type = "clustered_mse")
# Number of clusters for clustered MSE loss
clustered_mse_num_clusters = 8

# Weight for cluster-based loss component
clustered_mse_cluster_weight = 1.0

# EW loss parameters (for loss_type = "ew")
# Boundary shift parameter for exponential weighted loss
ew_boundary_shift = 0.0

# Stepped loss parameters (for loss_type = "stepped")
# Number of timestep indices to step forward in diffusion process
# Larger values = longer-term flow prediction, more smoothing effect
# Range: 10-100, recommended: 30-70 for video LoRA training
stepped_step_size = 50

# Multiplier for stepped loss magnitude, range: 0.1-20.0
# Values > 1.0 increase stepped loss weight, < 1.0 decrease it
stepped_multiplier = 10.0

# Explicit dimension-aware loss reduction for video tensors (B,C,F,H,W)
# mean([1,2,3,4]) for video vs mean([1,2,3]) for image
use_explicit_video_loss_reduction = false

# Custom loss target via noise_scheduler.get_loss_target() hook
# For schedulers with non-standard flow matching targets
enable_custom_loss_target = false

# =============================================================================
# ⚠️ MIXFLOW SETTINGS
# =============================================================================

# Enable MixFlow slowed interpolation mixture.
# When enabled, model input x_t is replaced by x_mt sampled at a slowed timestep mt
# while keeping supervision target unchanged for flow-matching objective.
# Default: false (safe/off).
enable_mixflow = false

# Slowed interpolation range coefficient gamma in [0.0, 1.0].
# mt is sampled as: mt = t + U(0, 1) * gamma * (1 - t_norm), where t_norm is
# the normalized timestep in [0, 1]. Larger gamma increases slowdown strength.
# Recommended start: 0.4.
# Dependency: only used when enable_mixflow=true.
mixflow_gamma = 0.4

# Optional Beta-style timestep resampling for closer MixFlow parity.
# When true, MixFlow remaps training timesteps with t = 1 - sqrt(u), u~U(0,1)
# before slowed interpolation is computed. This can change timestep distribution.
# Default: false (preserves existing timestep sampling behavior).
# Dependency: only used when enable_mixflow=true.
mixflow_beta_t_sampling = false

# Optional MixFlow parity transform after Beta timestep mapping:
# t <- shift * t / (1 + (shift - 1) * t), where t is normalized in [0, 1].
# This mirrors the appendix code path in the reference implementation.
# Default: false (disabled to preserve existing behavior).
# Dependency: requires enable_mixflow=true and mixflow_beta_t_sampling=true.
mixflow_time_dist_shift_enabled = false

# Positive shift factor for optional MixFlow time distribution transform.
# 1.0 means no-op; values > 1.0 bias toward larger normalized timesteps.
# Valid range: > 0.0
# Dependency: only used when mixflow_time_dist_shift_enabled=true.
mixflow_time_dist_shift = 1.0

# Optional info logging cadence for MixFlow stats.
# 0 disables periodic MixFlow logs.
# Positive value N logs approx every N global steps (main process only).
# Dependency: only used when enable_mixflow=true.
# Example: mixflow_log_every_n_steps = 100
mixflow_log_every_n_steps = 0

# =============================================================================
# ⚠️ DISPERSIVE LOSS SETTINGS
# =============================================================================

# Enable dispersive loss for improved generation quality and speed.
enable_dispersive_loss = false

# The lambda (λ) hyperparameter that controls the strength of the dispersive loss. The official PyTorch release uses 0.25.
dispersive_loss_lambda = 0.25

# Temperature parameter (> 0). Smaller values sharpen repulsion.
dispersive_loss_tau = 1.0

# None disables extraction, non-negative integer selects a block index, "last" picks the final block
dispersive_loss_target_block = "last"

# "l2_sq" (official), "l2_sq_legacy", or "cosine"
dispersive_loss_metric = "l2_sq"

# Optional: pool spatial tokens per frame before dispersion ("none" or "frame_mean")
dispersive_loss_pooling = "none"

# ============================================================================
# ⚠️ BLOCKWISE FLOW MATCHING SETTINGS
# ============================================================================

# Enable blockwise flow matching for training (no inference changes).
# Requires timestep_sampling = "blockwise".
enable_blockwise_flow_matching = false

# Number of temporal segments for blockwise sampling (>= 2).
bfm_num_segments = 6

# Segment sampling strategy: "stratified" forces per-batch segment coverage,
# "uniform" samples globally within [bfm_segment_min_t, bfm_segment_max_t].
bfm_segment_sampling = "stratified"

# Segment boundary min in normalized time [0.0, 1.0].
bfm_segment_min_t = 0.0

# Segment boundary max in normalized time [0.0, 1.0].
bfm_segment_max_t = 1.0

# Log per-segment losses during training (informational).
bfm_log_segment_losses = true

# Use segment endpoints to compute the blockwise FM interpolant + velocity target (Eq. 6-8).
# When true, targets come from segment endpoints with shared noise (training-only).
# Set false to keep the standard flow-matching target.
bfm_use_segment_objective = true

# Semantic Feature Guidance (SemFeat) alignment loss (training-only).
bfm_semfeat_enabled = false

# Pretrained encoder specification (EncoderManager format: "<type>-<arch>-<config>").
bfm_semfeat_encoder_name = "dinov2-vit-b14"

# Encoder input resolution (256 or 512; 512 only supported for DINOv2).
bfm_semfeat_resolution = 256

# Diffusion block index to capture features for alignment (0-based).
bfm_semfeat_alignment_depth = 8

# SemFeat alignment loss weight (>= 0).
bfm_semfeat_loss_weight = 0.05

# Spatial alignment for SemFeat token grids (recommended true).
bfm_semfeat_spatial_align = true

# Projection dimension for SemFeat alignment head (must be > 0).
bfm_semfeat_projection_dim = 1024

# Inject SemFeat conditioning tokens into text context (training-only).
bfm_semfeat_conditioning_enabled = false

# Conditioning strength multiplier (>= 0).
bfm_semfeat_conditioning_scale = 1.0

# Drop probability for SemFeat conditioning tokens in training [0, 1].
bfm_semfeat_conditioning_dropout = 0.0

# Add learned segment index tokens to context (training-only).
bfm_segment_conditioning_enabled = false

# Segment token strength multiplier (>= 0).
bfm_segment_conditioning_scale = 1.0

# Enable segment-specific velocity blocks (replicate DiT blocks per segment).
# Warning: changes inference behavior if enabled.
bfm_segment_blocks_enabled = false

# Segment block mode: "shared" reuses base blocks, "replicated" creates per-segment blocks.
bfm_segment_block_mode = "shared"

# Inject SemFeat tokens into the model time embedding (training/inference change).
bfm_semfeat_model_injection_enabled = false

# Scale for SemFeat time embedding injection (>= 0).
bfm_semfeat_model_injection_scale = 1.0

# Enable FRN residual approximation training (training-only).
bfm_frn_enabled = false

# FRN loss weight (>= 0).
bfm_frn_loss_weight = 0.1

# FRN hidden dimension (> 0).
bfm_frn_hidden_dim = 1024

# Enable BFM inference features (changes inference behavior; gated off by default).
bfm_inference_enabled = false

# Enable semantic feature conditioning during inference (requires bfm_inference_enabled).
bfm_inference_semfeat_enabled = false

# Refresh semantic features "per_segment" or "per_step" during inference.
bfm_inference_semfeat_refresh = "per_segment"

# Semantic feature strength multiplier for inference (>= 0).
bfm_inference_semfeat_scale = 1.0

# Enable segment token conditioning during inference (requires bfm_inference_enabled).
bfm_inference_segment_enabled = false

# Segment token strength multiplier for inference (>= 0).
bfm_inference_segment_scale = 1.0

# Use FRN-predicted semantic tokens during inference (requires bfm_frn_enabled).
bfm_inference_use_frn = false

# =============================================================================
# ⚠️ EQUILIBRIUM MATCHING SETTINGS
# =============================================================================

# Enable the experimental EqM training path (requires new objective and sampler).
enable_eqm_mode = false

# Prediction head type: "velocity", "score", or "noise".
eqm_prediction = "velocity"

# Transport path for EqM loss ("Linear", "GVP", or "VP").
eqm_path_type = "Linear"

# Weight applied to the EqM transport loss when enabled.
eqm_loss_weight = 1.0

# Optional transport weighting ("velocity" or "likelihood") applied to the EqM regression target.
# eqm_transport_weighting = "velocity"

# Optional epsilon offsets used by the transport planner.
# Train epsilon offset
# eqm_train_eps = 1e-3

# Sample epsilon offset
# eqm_sample_eps = 1e-3

# Enable EqM energy head (differentiable dot-product energy).
# eqm_energy_head = false

# Energy mode for EqM energy head, options: "dot", "l2", "mean"
# eqm_energy_mode = "dot" 

# Sampling parameters
# Step size for EqM sampling
# eqm_step_size = 0.0017

# Momentum for EqM sampling
# eqm_momentum = 0.0

# Sampler for EqM sampling, options: "gd", "ngd", "ode", "sde", or "ode_likelihood"
# eqm_sampler = "gd"   

# Enable adaptive sampler for EqM sampling
# eqm_use_adaptive_sampler = false

# Adaptive step size parameters for EqM sampling
# eqm_adaptive_step_min = 1e-5

# Maximum step size for EqM sampling
# eqm_adaptive_step_max = 0.01

# Growth factor for adaptive step size
# eqm_adaptive_growth = 1.05

# Shrink factor for adaptive step size
# eqm_adaptive_shrink = 0.5

# Restart patience for adaptive step size
# eqm_adaptive_restart_patience = 4

# Alignment threshold for adaptive step size
eqm_adaptive_alignment_threshold = 0.0

# ODE method for EqM sampling, options: "dopri5" (requires torchdiffeq), "heun", "euler", "rk4"
eqm_ode_method = "dopri5"  

# Number of integration steps for ODE samplers
eqm_ode_steps = 50

# Absolute tolerance for ODE samplers
eqm_ode_atol = 1e-6

# Relative tolerance for ODE samplers
eqm_ode_rtol = 1e-3

# Reverse the ODE trajectory
eqm_ode_reverse = false

# Absolute tolerance for ODE likelihood samplers
# eqm_ode_likelihood_atol = 1e-6

# Relative tolerance for ODE likelihood samplers
# eqm_ode_likelihood_rtol = 1e-3

# Number of Hutchinson trace samples for ODE likelihood samplers
eqm_ode_likelihood_trace_samples = 1

# SDE method for EqM sampling, options: "Euler", "Heun"
eqm_sde_method = "Euler"

# Number of SDE integration steps
eqm_sde_steps = 250

# SDE last step policy, options: "Mean", "Tweedie", "Euler", null
eqm_sde_last_step = "Mean"

# Step size applied when the final SDE correction is enabled
eqm_sde_last_step_size = 0.04

# SDE diffusion form, options: "SBDM", "SBDM-E"
eqm_sde_diffusion_form = "SBDM"

# SDE diffusion norm
eqm_sde_diffusion_norm = 1.0

# =============================================================================
# ⚠️ ADAPTIVE TIMESTEP SAMPLING
# =============================================================================

# Enable adaptive timestep sampling (default: false)
# Focuses training on timesteps that are most critical for video generation quality
enable_adaptive_timestep_sampling = false

# Core parameters
# How much to focus on important timesteps (1.0-5.0)
adaptive_focus_strength = 2.0

# Steps before analysis begins (100-2000)
adaptive_warmup_steps = 500

# Recent loss samples to analyze (100-5000)
adaptive_analysis_window = 1000

# Standard deviation multiplier for detection (0.5-3.0)
adaptive_importance_threshold = 1.5

# Steps between analysis updates (10-500)
adaptive_update_frequency = 100

# Timestep constraints
# Minimum important timesteps (10-500)
adaptive_min_timesteps = 50

# Maximum important timesteps (50-1000)
adaptive_max_timesteps = 200

# Video-specific features
# Enable video-specific categories
adaptive_video_specific = true

# Motion consistency weight (0.1-3.0)
adaptive_motion_weight = 1.0

# Detail preservation weight (0.1-3.0)
adaptive_detail_weight = 1.0

# Temporal coherence weight (0.1-3.0)
adaptive_temporal_weight = 1.0

# Enable Beta distribution sampling methodology (default: false)
# Uses paper's exact approach: neural network parameterized Beta distribution
adaptive_use_beta_sampler = false

# Feature selection size |S| for approximation (1-10)
adaptive_feature_selection_size = 3

# Sampler update frequency f_S (10-100)
adaptive_sampler_update_frequency = 100

# Enable separate neural network for timestep sampling (default: false)
adaptive_use_neural_sampler = false

# Initial Alpha parameter for Beta distribution (0.1-5.0)
adaptive_beta_alpha_init = 1.0

# Initial Beta parameter for Beta distribution (0.1-5.0)
adaptive_beta_beta_init = 1.0

# Neural sampler hidden layer size (16-256)
adaptive_neural_hidden_size = 64

# Enable loss-variance importance weighting (stable, default: true)
# This is our proven stable approach for production use
adaptive_use_importance_weighting = true

# Enable KL divergence RL learning (paper exact, default: false)
# This is the authors' exact methodology - more experimental
adaptive_use_kl_reward_learning = false

# Enable replay buffer for historical learning (default: false)
# Stores KL improvements across training steps for better feature selection
adaptive_use_replay_buffer = false

# Enable SelectKBest statistical feature selection (default: false)
# Requires scikit-learn, uses regression-based feature importance
adaptive_use_statistical_features = false

# Weight combination strategy ("fallback", "ensemble", "best")
# - fallback: Use first successful method (stable)
# - ensemble: Average all methods (balanced)
# - best: Choose method with highest focus (aggressive)
adaptive_weight_combination = "fallback"

# Replay buffer size for KL learning (10-1000)
adaptive_replay_buffer_size = 100

# RL learning rate for policy updates (1e-5 to 1e-3)
adaptive_rl_learning_rate = 1e-4

# Entropy coefficient for RL regularization (0.0-0.1)
adaptive_entropy_coefficient = 0.01

# KL reward update frequency (5-100)
adaptive_kl_update_frequency = 20

# Pure KL divergence-based feature selection for exact research replication (default: false)
# When enabled, automatically configures use_kl_reward_learning=true, use_beta_sampler=true, use_importance_weighting=false
adaptive_kl_exact_mode = false

# Enable comparative logging of KL vs importance approaches for research analysis (default: false)
# Logs performance comparison between different feature selection methods
adaptive_comparative_logging = false

# Enable full research mode (Algorithm 1 & 2) for exact research replication (default: false)
# When enabled, activates complete research methodology including policy gradient updates
# Requires diffusion object with q_posterior_mean_var and p_mean_var methods
adaptive_research_mode_enabled = false

# =============================================================================
# ⚠️ CONTRASTIVE FLOW MATCHING (ΔFM) SETTINGS
# =============================================================================

# Enable the enhanced contrastive objective for improved generation quality and speed.
enable_contrastive_flow_matching = false

# WanVideo LoRA cross-batch CFM regularizer (disabled by default)
# Adds a negative MSE between current targets and a batch-rolled target to encourage consistency.
enable_wanvideo_cfm = false

# Weighting: "uniform" or "linear" (linear scales by normalized timestep).
wanvideo_cfm_weighting = "uniform"

# Strength multiplier for the WanVideo cross-batch CFM regularizer.
# The loss computes a negative MSE between the current prediction/target and a batch-rolled target.
# wanvideo_cfm_lambda scales that term before adding it to the total loss.
# Default is 0.05, and it should be ≥ 0.
wanvideo_cfm_lambda = 0.05

# The lambda (λ) hyperparameter that controls the strength of the contrastive loss. The paper found 0.05 to be a robust value.
contrastive_flow_lambda = 0.05

# Enable class-conditioned negative sampling (recommended when labels are available)
# This ensures negative samples come from different classes, improving contrastive learning effectiveness
contrastive_flow_class_conditioning = true

# Skip contrastive loss computation on unconditional samples (useful for classifier-free guidance)
# This prevents unconditional samples from contributing to the contrastive objective
contrastive_flow_skip_unconditional = false

# Class index representing unconditional/null samples (required if skip_unconditional is true)
# Set this to the index used for unconditional generation in your dataset
# contrastive_flow_null_class_idx = 0

# =============================================================================
# ⚠️ CDC-FM (CARRE DU CHAMP FLOW MATCHING) SETTINGS
# =============================================================================

# Enable CDC-FM geometry-aware noise during training (LoRA trainer only).
# Works for both image and video latents; CDC caches are keyed by latent shape.
# Requires latent caching to be enabled (CDC uses cached latents to build k-NN geometry).
# When enabled, CDC caches are stored per latent file alongside the latent cache:
# image_0512x0768_latents.safetensors -> image_0512x0768_latents_cdc_1x96x96_<hash>.npz
# If FAISS is not installed, CDC preprocessing is skipped and training falls back to Gaussian noise.
enable_cdc_fm = false

# Number of neighbors for the k-NN graph (>= 2). Larger values increase compute and memory.
cdc_k_neighbors = 256

# Number of neighbors used to estimate bandwidth (>= 1).
cdc_k_bandwidth = 8

# CDC subspace dimension (>= 1). Controls the rank of the anisotropic noise.
cdc_d_cdc = 8

# Global CDC scaling factor (> 0).
cdc_gamma = 1.0

# Minimum bucket size for CDC computation. Buckets smaller than this fall back to Gaussian noise.
cdc_min_bucket_size = 16

# Force recompute CDC caches even if matching caches already exist.
cdc_force_recache = false

# =============================================================================
# ⚠️ INTERNAL GUIDANCE (IG) SETTINGS
# =============================================================================
# Enable Internal Guidance auxiliary supervision (training-only, default off).
enable_internal_guidance = false

# Internal Guidance mode:
# - "shift": use a shifted target objective during training.
# - "aux": auxiliary loss only (no target shift).
internal_guidance_mode = "shift"

# Guidance strength w used to shift the final target (> 0 when enabled).
internal_guidance_weight = 0.5

# Weight for the intermediate objective loss (>= 0).
internal_guidance_intermediate_weight = 1.0

# Transformer block index (0-based) to compute the intermediate prediction.
internal_guidance_target_block = 8

# Loss type for IG auxiliary supervision:
# - "l2": Mean squared error
# - "sml1": Smooth L1 with beta=0.05
# - "l1": Mean absolute error
internal_guidance_loss_type = "l2"

# =============================================================================
# ⚠️ SELF-TRANSCENDENCE SETTINGS
# =============================================================================

# Enable Self-Transcendence (training-only; uses internal feature guidance, no inference changes).
enable_self_transcendence = false

# Stage-1 VAE alignment loss weight (L2 between shallow features and VAE latents).
# Set to 0.0 to skip VAE alignment (stage-2 can still run).
self_transcendence_vae_loss_weight = 1.0

# Stage-2 self-guided feature loss weight (L2 alignment to CFG-guided deep features).
# Common starting points: 0.5 for SiT, 0.1 for LightningDiT.
self_transcendence_guidance_loss_weight = 0.5

# Feature-space CFG guidance scale for the self-guided representation.
# Common starting points: 30.0 for SiT, 10.0 for LightningDiT.
self_transcendence_guidance_scale = 30.0

# Guided layer index (1-based). Use -1 to auto-select half the transformer depth.
self_transcendence_guided_layer = -1

# Guiding layer index (1-based). Use -1 to auto-select two-thirds of the transformer depth.
self_transcendence_guiding_layer = -1

# Epoch schedule (preferred): warmup uses VAE alignment only; self-guided loss runs
# for guidance_epochs afterward and is then disabled.
self_transcendence_warmup_epochs = 40

# Number of epochs to apply self-guided loss after warmup.
self_transcendence_guidance_epochs = 20

# Optional schedule override in steps (use when max_train_epochs is unset).
# If either *_steps > 0, the step schedule takes precedence over epochs.
self_transcendence_warmup_steps = 0

# Optional schedule override in steps (use when max_train_epochs is unset).
self_transcendence_guidance_steps = 0

# MLP expansion ratio for the lightweight projection heads (>= 1).
self_transcendence_mlp_multiplier = 2

# Unconditional feature mode for CFG guidance in feature space:
# - "zero": use zeroed context embeddings.
# - "batch_null": use batch["t5_uncond"] if provided, else fall back to zeros.
self_transcendence_uncond_mode = "zero"

# =============================================================================
# ⚠️ REPA (REPRESENTATIONAL ALIGNMENT) SETTINGS
# =============================================================================

# Enable REPA for representational alignment during training
enable_repa = false

# Enable iREPA (improved REPA) for spatially focused alignment (LoRA training only, defaults off)
enable_irepa = false

# Enable VAE-REPA (VAE feature alignment from latent targets) for training acceleration.
# This is train-time only and does not affect LoRA inference behavior.
enable_vae_repa = false

# VAE-REPA alignment single fallback depth.
# Paper ablations favor early alignment (e.g., layer 2 on smaller backbones).
vae_repa_alignment_depth = 2

# VAE-REPA alignment multiple fallback depths.
# Optional explicit list for multi-layer alignment.
# vae_repa_alignment_depths = [2]

# Use paper-style automatic depth mapping by model scale:
# - small backbones -> depth 2
# - large backbones -> depth 8
# When enabled, this overrides manual vae_repa_alignment_depth(s).
vae_repa_auto_depth = false

# Weight for VAE-REPA alignment loss (lambda in L_total = L_denoise + lambda * L_align).
# Must be > 0 when enable_vae_repa = true.
vae_repa_loss_lambda = 1.0

# Smooth-L1 beta threshold for VAE-REPA feature alignment.
# Paper default: 0.05. Smaller values behave closer to L1.
vae_repa_loss_beta = 0.05

# Alignment objective for VAE-REPA:
# - "smooth_l1": paper default and recommended.
# - "cosine": cosine-distance objective.
# - "l1": absolute error objective.
# - "l2": mean-squared error objective.
vae_repa_alignment_loss = "smooth_l1"

# Hidden-width multiplier for the VAE-REPA MLP projector.
# Projector shape: hidden_dim -> hidden_dim * multiplier -> vae_feature_dim.
# Must be >= 1. Higher values increase capacity and train-time memory/compute.
vae_repa_projector_hidden_mult = 4

# Number of linear layers in the VAE-REPA projector MLP.
# Paper ablation best setting is 5-layer; set >= 2.
vae_repa_projector_layers = 5

# Optional explicit latent feature channel dimension for VAE targets.
# 0 = auto-infer from the active VAE backend; set a positive value to override.
vae_repa_target_dim = 0

# Apply alignment only for normalized timesteps in [min, max].
# Values use [0,1] normalized timestep space.
vae_repa_timestep_min = 0.2

# Max normalized timestep to apply alignment (0.0 to 1.0).
vae_repa_timestep_max = 0.8

# Spatial token alignment policy:
# - true: interpolate token grids when diffusion and latent token counts differ.
# - false: global-pool both sides before Smooth-L1 (cheaper but loses spatial detail).
vae_repa_spatial_align = true

# Video handling policy:
# - false: use first frame only for VAE-REPA targets (lower overhead).
# - true: align all frames when available (higher cost, stronger temporal supervision).
vae_repa_use_full_video = false

# Projection layer type for iREPA: "mlp" (baseline) or "conv" (iREPA default, preserves spatial structure)
# Only used when enable_irepa = true; defaults keep inference unchanged
irepa_projection_type = "conv"

# Convolution kernel size for iREPA projector (odd integers ≥ 3 recommended; 3 matches paper)
irepa_proj_kernel = 3

# Spatial normalization applied to encoder patch tokens before similarity ("none" or "zscore"; z-score is paper default)
irepa_spatial_norm = "zscore"

# Z-score scaling factor (gamma) for spatial normalization (> 0, 1.0 matches paper)
irepa_zscore_alpha = 1.0

# Visual encoder specification - supports both single and multi-encoder setups
# Single encoder: "dinov2-vit-b", "dinov2-vit-l", "dinov2-vit-g", "clip-vit-L", "mocov3-vit-b"
# Multi-encoder: "dinov2-vit-b,dinov2-vit-l", "dinov2-vit-b,clip-vit-L"
# Supported: DINOv2 (s/b/l/g), CLIP (B/L), MoCo-v3 (s/b/l), MAE (l), I-JEPA (h)
repa_encoder_name = "dinov2-vit-b"

# Diffusion layer alignment depth - single (8) or multi-layer ("8,12,16")
repa_alignment_depth = 8

# REPA loss weight (0.5 = paper default)
repa_loss_lambda = 0.1

# Similarity function ("cosine" recommended, "mse" alternative)
repa_similarity_fn = "cosine"

# Input resolution (256 or 512, 512 DINOv2-only)
repa_input_resolution = 256

# Ensemble mode ("individual" = separate heads, "concat" = concatenated features)
repa_ensemble_mode = "individual"

# Projection sharing (false = individual heads per encoder, true = shared head)
repa_shared_projection = false

# Spatial alignment via interpolation (default: true)
# When diffusion and encoder have different token counts:
# - true: Spatially interpolate features to match (preserves per-patch alignment)
# - false: Pool features globally (loses spatial information)
# Recommended: true for better alignment quality
repa_spatial_align = true

# =============================================================================
# ⚠️ REG (REPRESENTATION ENTANGLEMENT FOR GENERATION) SETTINGS
# =============================================================================

# Enable REG for class-token entanglement during training (default off).
enable_reg = false

# Visual encoder specification for REG (used to determine class-token size and targets).
# Single encoder: "dinov2-vit-b", "dinov2-vit-l", "dinov2-vit-g", "clip-vit-L"
reg_encoder_name = "dinov2-vit-b"

# Diffusion layer alignment depth (n = 4 for small models; n = 8 for others in the paper).
reg_alignment_depth = 8

# Alignment loss weight (lambda, > 0; reference default 0.5).
reg_proj_coeff = 0.5

# Class-token denoising loss weight (beta, > 0; reference default 0.03).
reg_cls_loss_weight = 0.03

# Similarity function for alignment ("cosine" recommended, "mse" alternative).
reg_similarity_fn = "cosine"

# Input resolution for encoder preprocessing (256 or 512; 512 DINOv2-only).
reg_input_resolution = 256

# Class-token embedding dimension (0 = infer from encoder; 768 matches DINOv2-B).
reg_cls_dim = 0

# Target type for class-token denoising loss: "flow" (noise - x) or "velocity".
# "flow" matches Takenoko's flow-matching objective; "velocity" follows Eq. 10.
reg_target_type = "flow"

# Spatial alignment via interpolation when token counts mismatch (recommended true).
reg_spatial_align = true

# =============================================================================
# ⚠️ LAYER SYNC (SELF ALIGNMENT) SETTINGS
# =============================================================================

# Enable LayerSync self-alignment regularizer.
enable_layer_sync = false

# Weight applied to the LayerSync projection loss (defaults to 0.2 when enabled).
# Compatibility: layersync_enabled/layersync_lambda are also accepted.
layer_sync_weight = 0.2

# Primary block pair for alignment (1-indexed, source < target; 0-based values accepted).
# Recommended: early semantic block guides a deeper one.
layer_sync_source_block = 8

# Target block for alignment (1-indexed, source < target; 0-based values accepted).
layer_sync_target_block = 16

# Optional list of additional (source, target) pairs, each 1-indexed with source < target.
# When provided, pairs are summed with equal weight. Example:
# layer_sync_pairs = [[4, 8], [12, 20]]

# Optional per-pair weights matching layer_sync_pairs. If omitted, all pairs weight=1.0.
# layer_sync_pair_weights = [0.5, 1.0]

# Detach guidance activations so gradients flow only through the source block.
layer_sync_detach_guidance = true

# Feature normalization mode for projection. "cosine" uses unit-normalized embeddings before dot product.
layer_sync_normalization = "cosine"

# =============================================================================
# ⚠️ CREPA (CROSS-FRAME REPRESENTATIONAL ALIGNMENT) SETTINGS
# =============================================================================

# Extends REPA with temporal neighbor alignment for better video consistency
# Recommended: true for video fine-tuning, false for image training
crepa_enabled = false

# Transformer block index to align from (0-based).
# Required when crepa_enabled = true.
crepa_block_index = 8

# Optional teacher block index for backbone-feature mode (0-based).
# If unset, defaults to crepa_block_index.
crepa_teacher_block_index = 16

# Weight for CREPA loss (default: 0.5). Set > 0 when enabled.
crepa_lambda = 0.5

# Adjacency range for temporal alignment (default: 1 => neighbors at t-1 and t+1).
# Higher values capture longer-term dependencies but increase compute cost.
crepa_adjacent_distance = 1

# Temperature for temporal weighting (default: 1.0).
# Controls how quickly alignment weight decays with distance.
crepa_adjacent_tau = 1.0

# Optional cumulative neighbors mode: use all offsets 1..distance instead of just +/- distance.
crepa_cumulative_neighbors = false

# Pretrained encoder to use for CREPA feature extraction (DINOv2 torch hub).
# Example: "dinov2_vitg14", "dinov2_vitb14", "dinov2_vits14".
crepa_encoder = "dinov2_vitg14"

# Input image size for CREPA encoder (default: 518 for DINOv2).
crepa_encoder_image_size = 518

# How many frames the external CREPA vision encoder processes in parallel.
# - <= 0: process all frames (B*T) at once (fastest, highest VRAM)
# - > 0: split frames into chunks of this size and run the encoder multiple times
# If this value is not a divisor of (B*T), the last chunk will be smaller.
# This only affects CREPA when crepa_use_backbone_features = false.
# Default: -1
crepa_encoder_frame_chunk_size = -1

# Spatial alignment strategy for CREPA:
# - true: interpolate patch tokens to align spatial grids
# - false: pool to global tokens when counts differ
crepa_spatial_align = true

# Enable backbone feature alignment instead of external encoder (saves VRAM).
# When true, CREPA aligns one transformer block to another and skips encoder loading.
crepa_use_backbone_features = false

# Drop the VAE encoder after loading to save memory when only decoding latents.
# Only safe if you never call VAE.encode during training (CREPA decode path is OK).
crepa_drop_vae_encoder = false

# Normalize CREPA loss by number of frames (default: true)
# - true: Mean over frames (consistent loss scale across video lengths)
# - false: Sum over frames (longer videos get stronger alignment signal)
# Recommended: true for stable training with varying video lengths
crepa_normalize_by_frames = true

# =============================================================================
# ⚠️ VIDEOREPA (TOKEN RELATION DISTILLATION) SETTINGS
# =============================================================================

# Enable VideoREPA token-relation distillation (default: false for behavior safety).
# VideoREPA aligns token-relation structure between DiT hidden states and frozen
# visual foundation model features. This path is train-time only and does not
# affect LoRA inference graphs.
# Compatibility:
# - Mutually exclusive with REPA/iREPA/CREPA/SARA/MOALIGN.
# - Requires pixel batches (handled automatically when enabled).
enable_videorepa = false

# Teacher encoder specification.
# REPA encoder manager backbones:
# - "dinov2-vit-b", "dinov2-vit-l", "mae-vit-l", "jepa-vit-h"
# Native video teachers (require videorepa_teacher_checkpoint):
# - "videomaev2-vit-b", "videomaev2-vit-l", "videomaev2-vit-g"
# - "vjepa-vit-l", "vjepa-vit-h", "vjepa-vit-g"
# - "vjepa2-vit-l" (loaded via torch.hub; does not use videorepa_teacher_checkpoint)
videorepa_encoder_name = "dinov2-vit-b"

# Teacher encoder preprocessing resolution.
# Valid values: 256, 512 (512 is supported only for DINOv2 encoders).
videorepa_input_resolution = 256

# Native teacher checkpoint path (required when videorepa_encoder_name starts with
# "videomaev2-" or "vjepa-"; ignored for "vjepa2-*" and REPA encoder-manager backbones).
# Example:
# videorepa_teacher_checkpoint = "models/VideoMAEv2/vit_b_k710_dl_from_giant.pth"
videorepa_teacher_checkpoint = ""

# Native video-teacher clip length expected by VideoMAEv2/VJEPA models.
videorepa_video_teacher_frames = 16

# Native video-teacher temporal tubelet size.
videorepa_video_teacher_tubelet_size = 2

# Native VJEPA patch size (ignored by VideoMAEv2 builders that define their own patch size).
videorepa_video_teacher_patch_size = 16

# Native video-teacher input frame size (square resize before teacher forward).
videorepa_video_teacher_image_size = 224

# VideoMAEv2 alignment resolution hint [height, width] used by the upstream
# checkpoint positional embedding interpolation path.
videorepa_video_teacher_align_resolution = [480, 720]

# For native video teachers, drop the first frame before teacher encoding.
# This mirrors the upstream VideoREPA training recipe.
videorepa_video_teacher_drop_first_frame = true

# VJEPA checkpoint key used when loading encoder weights.
videorepa_vjepa_checkpoint_key = "target_encoder"

# VJEPA architecture toggles (advanced; match checkpoint pretraining setup).
videorepa_vjepa_uniform_power = true

# Enable SDPA kernels for VJEPA attention (recommended true on modern PyTorch/CUDA).
videorepa_vjepa_use_sdpa = true

# Enable SiLU MLP activation variant in VJEPA blocks.
videorepa_vjepa_use_silu = false

# Use tight SiLU variant (only relevant when videorepa_vjepa_use_silu = true).
videorepa_vjepa_tight_silu = false

# Primary transformer block index (0-based) for VideoREPA alignment.
videorepa_alignment_depth = 18

# Optional multi-layer alignment depths (0-based). If set, overrides
# videorepa_alignment_depth. Example:
# videorepa_alignment_depths = [12, 18, 24]

# Global VideoREPA loss weight (lambda). Must be > 0 when enabled.
videorepa_loss_lambda = 0.5

# Relation objective mode:
# - "cosine_similarity": REPA-style cosine feature alignment ablation
# - "token_relation_distillation": frame-to-video relation distillation (paper-style default)
# - "token_relation_distillation_only_spatial": per-frame spatial relation only
# - "token_relation_distillation_only_temporal": cross-frame relation only
videorepa_relation_mode = "token_relation_distillation"

# Margin used in the ReLU(|S - T| - margin) relation penalty.
# Increase to soften alignment pressure when training is unstable.
videorepa_margin = 0.1

# Optional margin used by spatial-only / temporal-only TRD ablations.
# If omitted in a custom config, parser falls back to videorepa_margin.
videorepa_margin_matrix = 0.1

# Hidden width for the trainable diffusion-side projector MLP.
videorepa_projector_hidden_dim = 2048

# Target embedding dimension for alignment space.
# Set to the teacher feature width (768 for DINOv2-B / many ViT backbones).
videorepa_align_dim = 768

# Optional cap on per-frame spatial token count after interpolation.
# -1 disables token capping (full-resolution relation matrices).
# Use positive values to reduce VRAM / relation matrix cost.
videorepa_max_spatial_tokens = -1

# Spatial alignment policy when source/teacher token grids differ:
# - true: interpolate token grids to match
# - false: truncate to min token count
videorepa_spatial_align = true

# Temporal alignment policy when source/teacher frame counts differ:
# - true: interpolate source features along time to teacher frame count
# - false: truncate to shared frame count
videorepa_temporal_align = true

# Temporal-only TRD masking policy:
# - true: exclude same-frame pair blocks before averaging (paper-style behavior)
# - false: keep all temporal-pair entries
videorepa_temporal_exclude_same_frame = true

# Detach teacher tokens before relation loss (recommended true).
videorepa_detach_teacher = true

# Encoder forward chunk size over (batch * frames):
# - 0: process all frames at once (fastest, highest VRAM)
# - >0: process in chunks to reduce peak memory
videorepa_encoder_chunk_size = 0

# =============================================================================
# ⚠️ STRUCTURE FROM TRACKING (SAM2 LGF-KL DISTILLATION) SETTINGS
# =============================================================================

# Enable Structure-From-Tracking distillation.
# Distills structure-preserving motion priors from SAM2 teacher features into
# DiT hidden states using Local Gram Flow + KL alignment. This is training-only
# and does not modify downstream LoRA inference graphs.
# Compatibility:
# - Can be combined with REPA/iREPA/VideoREPA/SARA/MOALIGN/CREPA.
# - Combined modes increase memory/time and stack multiple alignment losses.
# - Requires pixel batches (loaded automatically when enabled).
enable_structure_from_tracking = false

# Teacher type. Currently supported:
# - "sam2": SAM2 vision encoder teacher loaded through transformers Sam2Model.
# - "sam3": SAM3 vision encoder teacher loaded through transformers Sam3Model.
sft_teacher_type = "sam2"

# Hugging Face model id used when sft_teacher_checkpoint is empty.
# Recommended defaults:
# - "facebook/sam2-hiera-small"
# - "facebook/sam2-hiera-base-plus"
# - "facebook/sam2-hiera-large"
# - For SAM3 (if available in your transformers version), set an appropriate
#   SAM3 model id supported by that release.
sft_teacher_model_id = "facebook/sam2-hiera-large"

# Optional local checkpoint/model directory for SAM2 teacher.
# If non-empty, this takes precedence over sft_teacher_model_id.
# Example:
# sft_teacher_checkpoint = "models/sam2-hiera-large"
sft_teacher_checkpoint = ""

# Teacher input resize (square pixels). Must be > 0.
# Higher values increase detail and VRAM cost.
sft_teacher_image_size = 512

# Frame chunk size for teacher forward passes over (batch * frames). Must be > 0.
# Lower values reduce VRAM at the cost of throughput.
sft_teacher_chunk_size = 8

# Optional cap on teacher frames per sample.
# - 0: use all frames
# - >0: uniformly subsample to this many frames before teacher extraction
sft_teacher_max_frames = 0

# Teacher autocast dtype:
# - "float16", "bfloat16", "float32"
sft_teacher_dtype = "float16"

# Strict paper-faithful mode (default off for behavior preservation).
# When true, SFT enforces stricter settings and fail-fast behavior instead of
# permissive fallbacks (tracker backend, mask prompting, KL+LGF setup).
sft_paper_strict_mode = false

# Teacher feature backend:
# - "vision_encoder": SAM2 vision-encoder tokens + optional causal memory (default)
# - "tracker_memory": stronger recurrent memory path for closer paper-style behavior
sft_teacher_backend = "vision_encoder"

# Tracker-memory backend options (used only when sft_teacher_backend="tracker_memory" and sft_teacher_type="sam2").
# These target SAM2 package checkpoints/config used for recurrent-memory extraction.
sft_tracker_memory_sam2_config = "sam2_hiera_l.yaml"

# Local SAM2 checkpoint file used by tracker-memory backend.
# Required when sft_teacher_backend="tracker_memory" unless
# sft_tracker_memory_download_checkpoint=true.
# Example:
# sft_tracker_memory_sam2_checkpoint = "checkpoints/sam2_hiera_large.pt"
sft_tracker_memory_sam2_checkpoint = "checkpoints/sam2_hiera_large.pt"

# If true and checkpoint is missing, attempt to download SAM2 tracker weights.
# If false, initialization fails when checkpoint path is missing/invalid.
sft_tracker_memory_download_checkpoint = false

# If true, fail fast when tracker-memory backend cannot initialize.
# If false, fallback to vision_encoder backend.
sft_tracker_memory_strict = false

# Build causal teacher-memory features with EMA aggregation across frames.
# Mirrors paper intent to transfer tracker memory-style temporal priors instead of
# per-frame independent features.
sft_use_causal_memory = true

# EMA decay for causal teacher-memory updates in [0, 1).
# Higher values emphasize longer-range temporal context.
sft_teacher_memory_decay = 0.9

# Enable mask-prompted teacher extraction when masks are available in the dataset.
# If no masks are present for a sample, SFT falls back to standard frame extraction.
sft_use_mask_prompting = false

# Mask prompt blend strength in [0, 1]:
# - 0.0: no masking effect
# - 1.0: full foreground-only emphasis
sft_mask_prompt_strength = 0.85

# Optional blur kernel (odd integer > 0) applied to mask prompts before blending.
sft_mask_prompt_blur_kernel = 5

# Enable optional GroundingDINO prompt-derived masks when dataset masks are missing.
# This approximates paper-style object-box prompting from text/caption context.
sft_use_groundingdino_prompts = false

# GroundingDINO checkpoint/model identifier (transformers-compatible).
sft_groundingdino_model_id = "IDEA-Research/grounding-dino-base"

# GroundingDINO thresholds in [0, 1].
sft_groundingdino_box_threshold = 0.35

# Text-score threshold in [0, 1] for GroundingDINO token/prompt matching.
# Lower values increase recall (more boxes, more noise), higher values are stricter.
sft_groundingdino_text_threshold = 0.25

# Prompt source:
# - "caption": use dataset caption text only
# - "item_key": use item filename/key text only
# - "caption_or_item_key": caption fallback to item key
sft_groundingdino_prompt_source = "caption_or_item_key"

# Detect on both first and last frame, then union boxes.
sft_groundingdino_apply_to_last_frame = true

# Optional: refine GroundingDINO boxes into SAM2 segmentation masks.
# Requires `sam2` Python package and valid SAM2 config/checkpoint paths.
sft_groundingdino_use_sam2_refine = false

# SAM2 config identifier/path used for GroundingDINO -> SAM2 mask refinement.
# Used only when sft_groundingdino_use_sam2_refine=true.
sft_groundingdino_sam2_config = "sam2_hiera_l.yaml"

# SAM2 checkpoint path used for GroundingDINO -> SAM2 mask refinement.
# Must be non-empty when sft_groundingdino_use_sam2_refine=true.
sft_groundingdino_sam2_checkpoint = "checkpoints/sam2_hiera_large.pt"

# If true and checkpoint is missing, attempt Hugging Face download.
sft_groundingdino_download_sam2_checkpoint = false

# Teacher feature cache mode:
# - "off": always run online teacher extraction
# - "read": load precomputed teacher features only
# - "write": always recompute and overwrite cache
# - "read_write": load when present, otherwise compute+write
sft_teacher_cache_mode = "off"

# Directory for SFT teacher feature cache files when cache mode is not "off".
# Example:
# sft_teacher_cache_dir = "cache/sft_teacher_features"
sft_teacher_cache_dir = ""

# Offline SFT teacher-cache precompute settings.
sft_teacher_cache_num_workers = 4

# Number of samples processed per cache write step. Must be > 0.
# Increase for throughput if VRAM allows; keep at 1 for lowest memory.
sft_teacher_cache_batch_size = 1

# Skip writing cache files that already exist.
# - true: reuse existing files (fast incremental updates)
# - false: recompute and overwrite existing entries
sft_teacher_cache_skip_existing = true

# Purge existing `*_sft_teacher.safetensors` files in cache dir before caching.
# Useful for clean rebuilds; usually combined with skip_existing=false.
sft_teacher_cache_purge = false

# Optional frame-start stride filter for offline precompute.
# - 1: process all clips
# - 20: paper-style "every 20 frames" cache sampling
sft_teacher_cache_frame_start_stride = 1

# Auto-run SFT teacher cache precompute before training starts.
# This keeps caching orchestration out of the entrypoint and runs as trainer preflight.
sft_teacher_cache_before_training = false

# Overwrite behavior for pre-train precompute:
# - false: keep existing cache files and skip recomputation for hits
# - true: purge existing teacher cache files and regenerate all entries
sft_teacher_cache_overwrite_existing = false

# Primary transformer block index (0-based) for SFT alignment.
# Paper default reports best results near a deep block (e.g. 25th in their setup).
sft_alignment_depth = 25

# Optional multi-layer alignment depths (0-based). If set, overrides
# sft_alignment_depth. Example:
# sft_alignment_depths = [19, 25]

# Global SFT loss weight (lambda). Must be > 0 when enabled.
sft_loss_lambda = 0.5

# LGF alignment loss type:
# - "kl": temperature-softmax distribution matching (paper default)
# - "l2": direct similarity regression (ablation mode)
sft_loss_mode = "kl"

# Teacher fusion strategy:
# - "lgf": fuse forward/backward in LGF similarity space (paper default)
# - "feature": fuse tokens before LGF (ablation mode)
sft_fusion_mode = "lgf"

# LGF neighborhood kernel size (odd integer >= 3). Paper uses 7x7.
sft_lgf_kernel_size = 7

# Temperature for LGF softmax distributions used in KL matching. Must be > 0.
sft_lgf_temperature = 0.1

# Fusion weight for forward/backward teacher LGF maps in [0, 1].
# 1.0 = forward only, 0.5 = balanced fusion.
sft_teacher_fusion_weight = 0.5

# Temporal interpolation factor hint for source feature alignment (>= 1).
# Keep at 4 for paper-style temporal alignment behavior.
sft_temporal_interp_factor = 4

# Temporal skip-mixer kernel (odd integer > 0) for interpolation branch.
# Paper-style temporal skip path uses a 3-step kernel.
sft_temporal_kernel_size = 3

# Use backward teacher pass (reverse video order and remap back) before fusion.
sft_enable_backward_teacher = true

# Detach teacher features before LGF loss (recommended true for stability).
sft_detach_teacher = true

# Hidden width of trainable diffusion-side projector MLP. Must be > 0.
# Legacy compatibility key for older configs; prefer sft_projector_dims below.
sft_projector_hidden_dim = 2048

# Apply GroupNorm in projector MLP blocks for stable token-feature alignment.
sft_projector_group_norm = true

# GroupNorm group count for projector hidden channels. Must be > 0.
sft_projector_gn_groups = 32

# Three-layer projector widths [d1, d2, d3]. Must contain exactly 3 positive ints.
# Paper-style setting for 768-channel DiT features is [512, 256, 256].
sft_projector_dims = [512, 256, 256]

# Optional cap on per-frame spatial token count after interpolation.
# -1 disables token capping.
sft_max_spatial_tokens = -1

# Token-grid alignment policy:
# - true: interpolate mismatched token grids
# - false: truncate to minimum token count
sft_spatial_align = true

# Temporal alignment policy:
# - true: interpolate source/teacher frame counts to match
# - false: truncate to shared frame count
sft_temporal_align = true

# =============================================================================
# ⚠️ MOALIGN (MOTION-CENTRIC RELATIONAL ALIGNMENT) SETTINGS
# =============================================================================
#
# - Requires video pixel batches (handled automatically when enable_moalign=true).
# - Mutually exclusive with REPA/SARA paths in parser validation:
#   enable_repa=false, enable_irepa=false, sara_enabled=false.

# Enable MOALIGN auxiliary loss (default: false for behavior preservation).
enable_moalign = false

# Visual encoder used as the motion-teacher backbone.
# Uses the same naming convention as REPA encoder manager.
moalign_encoder_name = "dinov2-vit-b"

# Transformer block index (0-based) whose hidden states are aligned.
# Paper setup uses block 18 for CogVideoX.
moalign_alignment_depth = 18

# Optional multi-layer alignment depths (0-based).
# - If set, overrides moalign_alignment_depth.
# - Use this to match paper-style multi-layer alignment more closely.
# Example: moalign_alignment_depths = [12, 18, 24]
# moalign_alignment_depths = [18]

# Global MOALIGN loss weight lambda (must be > 0 when enabled).
moalign_loss_lambda = 0.5

# Temporal weighting temperature tau for inter-frame relational loss (must be > 0).
# Lower values emphasize nearby frames; higher values flatten temporal weighting.
moalign_temporal_tau = 10.0

# Hidden width of MOALIGN projection heads (must be > 0).
moalign_projection_hidden_dim = 256

# Output embedding dimension for alignment space (must be > 0).
moalign_projection_out_dim = 64

# Maximum token count used for relation-matrix computation after alignment/interpolation.
# Higher values preserve more detail but increase memory/compute quadratically.
moalign_max_tokens = 256

# Motion target construction mode:
# - "delta": frame-to-frame encoder token differences (motion-centric default)
# - "encoder": raw encoder tokens (ablation-style fallback)
moalign_motion_target_mode = "delta"

# Relative weights for MOALIGN components (must be >= 0).
# Total MOALIGN term = lambda * (spatial_weight * L_spatial + temporal_weight * L_temporal)
moalign_spatial_weight = 1.0

# Temporal weight for MOALIGN
moalign_temporal_weight = 1.0

# If token grids differ between diffusion and teacher features:
# - true: interpolate token grids to preserve spatial correspondence
# - false: truncate to common token count
moalign_spatial_align = true

# Input resolution for encoder preprocessing (must be > 0).
moalign_input_resolution = 256

# Detach teacher-side motion targets after projection.
# - false: train MOALIGN target projector jointly (recommended default for this adaptation)
# - true: keep target projector fully frozen
moalign_detach_targets = false

# =============================================================================
# ⚠️ MOALIGN STAGE-1 TEACHER SETTINGS
# =============================================================================
#
# Stage-1 trains a motion teacher (projector + flow head) with optical-flow supervision.
# This is an optional pretraining step before LoRA Stage-2 alignment.

# Route training action to Stage-1 teacher pipeline (default: false).
# When true, `--train` dispatches Stage-1 trainer and exits after checkpoint save.
enable_moalign_stage1_training = false

# Use frozen Stage-1 projector as MOALIGN teacher target in Stage-2 training.
# Requires moalign_stage1_checkpoint to point to a valid Stage-1 checkpoint.
moalign_use_stage1_teacher = false

# Checkpoint path for Stage-1 teacher.
# - During Stage-1 training: empty string means auto path under output_dir.
# - During Stage-2 teacher usage: must be a valid existing checkpoint path.
moalign_stage1_checkpoint = ""

# Stage-1 training epochs (> 0).
moalign_stage1_num_epochs = 1

# Stage-1 max optimizer steps (> 0). Training stops at whichever comes first:
# epoch budget or max steps.
moalign_stage1_max_steps = 2000

# Stage-1 optimizer learning rate (> 0).
moalign_stage1_learning_rate = 0.0001

# Stage-1 AdamW weight decay (>= 0).
moalign_stage1_weight_decay = 0.0

# Stage-1 dataset batch size (> 0). This overrides runtime batch size for the Stage-1 action.
moalign_stage1_batch_size = 1

# Stage-1 dataloader workers (>= 0).
moalign_stage1_num_workers = 0

# Logging interval in optimizer steps (> 0).
moalign_stage1_log_interval = 20

# Intermediate checkpoint interval in optimizer steps (>= 0).
# Set to 0 to disable intermediate checkpoints.
moalign_stage1_save_interval = 500

# Gradient clipping norm for Stage-1 (>= 0).
# Set to 0 to disable clipping.
moalign_stage1_grad_clip_norm = 1.0

# Optional token-energy regularization weight for Stage-1 (>= 0).
# Paper-faithful setting is 0.0 (flow-supervision only).
moalign_stage1_token_reg_weight = 0.0

# Optical-flow target source for Stage-1:
# - "cache": require batch["optical_flow"] from cached flow tensors
# - "raft": always compute flow online with RAFT
# - "auto": use cache when present, otherwise RAFT fallback (recommended)
moalign_stage1_flow_source = "auto"

# If flow_source="auto", allow RAFT fallback when no cached flow is available.
moalign_stage1_allow_raft_fallback = true

# RAFT variant used when online flow is needed:
# - "raft_small": lower VRAM, faster
# - "raft_large": higher quality, heavier
moalign_stage1_raft_model = "raft_small"

# =============================================================================
# ⚠️ SELF-RESAMPLING SETTINGS
# =============================================================================
# Enable Self-Resampling-style history corruption during training.
# This feature is designed to reduce exposure bias by replacing history frames in noisy inputs
# with detached, shifted-logit-normal resamples. Inference behavior is unchanged.
# Dependency: effective only for LoRA training in video mode.
enable_self_resampling = false

# Warmup optimizer steps before enabling self-resampling (>= 0).
# Use teacher forcing during warmup to stabilize early convergence.
# Example: self_resampling_warmup_steps = 10000
self_resampling_warmup_steps = 0

# Per-step probability of applying self-resampling (0.0-1.0).
# 1.0 applies every step after warmup; lower values interleave clean-history steps.
self_resampling_apply_prob = 1.0

# Logit-normal shift factor for simulation timestep sampling (> 0).
# Shift is applied as: t <- s*t/(1 + (s-1)*t). Values < 1 bias toward lower noise.
# Paper-aligned default: 0.6.
self_resampling_shift = 0.6

# Lower clamp for sampled simulation timestep t (0.0 <= min < max <= 1.0).
# Prevents overly weak corruption near pure teacher forcing.
self_resampling_min_t = 0.05

# Upper clamp for sampled simulation timestep t (0.0 <= min < max <= 1.0).
# Prevents overly strong corruption that can destabilize semantics.
self_resampling_max_t = 0.95

# Blend ratio between original noisy history and detached resampled history (0.0-1.0).
# 1.0 fully replaces selected history frames; lower values mix with original noisy input.
self_resampling_blend = 1.0

# First temporal index eligible for history corruption (>= 0).
# Example: 0 means all history-capable frames can be replaced.
self_resampling_history_start_frame = 0

# Number of tail frames excluded from corruption (>= 0).
# Default 1 keeps the newest frame untouched while corrupting earlier history.
self_resampling_history_exclude_tail_frames = 1

# Enable autoregressive history accumulation in detached corruption (default: true).
# This propagates residual error across time (frame-to-frame) to better emulate
# inter-frame error accumulation described in the paper.
self_resampling_autoregressive_history = true

# Decay factor for accumulated history error (0.0-1.0).
# 0.0 = independent frame corruption only, 1.0 = strongest carry-over.
self_resampling_autoregressive_decay = 0.5

# Stability clip multiplier (> 0) for accumulated history residuals.
# Larger values allow stronger drift; smaller values are safer.
self_resampling_autoregressive_clip_multiplier = 3.0

# Enable exact high-cost model-in-the-loop autoregressive rollout for history
# corruption (default: false). This performs sequential frame denoising with
# no-grad DiT calls, closer to paper Eq. (5) behavior.
self_resampling_model_rollout = false

# Enable faster rollout predictor path when model rollout is active (default: true).
# When enabled, Takenoko uses a lightweight velocity-only forward for rollout and
# falls back to full call_dit path for unsupported enhancement combinations.
self_resampling_fast_rollout = true

# Enable cross-attention KV cache during fast rollout calls (default: true).
# This caches per-block text K/V projections across rollout predictor calls to
# reduce repeated context-side attention compute.
self_resampling_rollout_kv_cache = true

# Enable self-attention history KV-prefix cache during fast rollout (default: true).
# This reuses cached history-frame self-attention projections when rollout proceeds
# frame-by-frame at fixed timestep, and automatically resets cache when timestep changes.
self_resampling_rollout_self_attn_kv_cache = true

# Number of Euler rollout updates per history frame (> 0).
# 1 is fastest and matches paper's practical 1-step setting; higher values are
# more expensive and may better approximate multi-step integration.
self_resampling_model_rollout_steps = 1

# Optional: enable practical history routing with existing TREAD integration
# (default: false). This approximates paper Sec. 3.3 by dynamically selecting
# top-k history frames during routed layers.
enable_self_resampling_history_routing = false

# History routing mode used when enable_self_resampling_history_routing=true:
# - "frame_topk": dynamic frame-importance top-k selection (recommended)
# - "frame_stride": evenly spaced frame routing
# - "frame_contiguous": centered contiguous frame routing
self_resampling_history_routing_mode = "frame_topk"

# Number of history frames to keep for frame_topk routing (> 0).
self_resampling_history_routing_top_k = 5

# Expected history frame count used to derive keep/drop ratio (> 0).
# Example: top_k=5 and expected_frames=20 approximates 75% sparsity.
self_resampling_history_routing_expected_frames = 20

# Routed layer band for history routing.
self_resampling_history_routing_start_layer_idx = 2

# End layer index (inclusive) for routing band.
# Negative values are resolved from the end of the DiT block list; -2 means
# "second-to-last block", matching the default parser behavior.
self_resampling_history_routing_end_layer_idx = -2

# Anchor-frame keeps for frame_topk mode.
self_resampling_history_routing_always_keep_first_frame = true

# Keep the newest history frame as an always-routed anchor (default: false).
# Enable this if you want guaranteed short-term context retention.
self_resampling_history_routing_always_keep_last_frame = false

# Optional: enable token-wise/head-wise history routing directly in attention
# (default: false). This is higher-fidelity to paper Sec. 3.3 than TREAD-only
# frame routing, and is more expensive.
enable_self_resampling_attention_routing = false

# Top-k history frames used by token-wise attention routing (> 0).
self_resampling_attention_routing_top_k = 5

# Layer band where token-wise attention routing is active.
self_resampling_attention_routing_start_layer_idx = 2

# End layer index (inclusive) for token-wise routing.
# Negative values are resolved from the end of the DiT block list; -2 means
# "second-to-last block", mirroring history routing defaults.
self_resampling_attention_routing_end_layer_idx = -2

# Routing backend:
# - "exact": paper-closest token/head-wise routing (slower)
# - "kernel_frame_topk": kernel-backed approximation via frame-level top-k + FlashAttention (faster)
self_resampling_attention_routing_backend = "exact"

# Anchor-frame keeps for token-wise attention routing.
self_resampling_attention_routing_always_keep_first_frame = true

# Keep the newest history frame as an always-routed anchor (default: false).
# Turn on only when strict recency anchoring is preferred over adaptive top-k.
self_resampling_attention_routing_always_keep_last_frame = false

# Metric logging interval in optimizer steps (> 0).
# Controls cadence for `self_resampling/*` TensorBoard metrics.
self_resampling_log_interval = 50

# =============================================================================
# ⚠️ ERROR RECYCLING SETTINGS
# =============================================================================
# Enable Error-Recycling Fine-Tuning. Default: false.
# This injects buffered training errors into noisy inputs to improve stability
# and long-range consistency, while leaving inference unchanged.
enable_error_recycling = false

# Maximum number of error samples stored per timestep grid (>= 0).
# Larger buffers improve diversity but increase CPU RAM use. Default: 500.
error_buffer_k = 500

# Timestep grid size for buffer partitioning (> 0).
# Example: 25 => each grid spans 25 timesteps. Default: 25.
timestep_grid_size = 25

# Total number of timestep grids (> 0). Default: 50.
# If (num_grids * timestep_grid_size) < num_train_timesteps, the last grids
# will be reused by clamping.
num_grids = 50

# Buffer replacement strategy when full:
# - "random": replace a random sample (fastest)
# - "l2_similarity": replace the most similar (slowest)
# - "l2_batch": batch L2 replacement (balanced)
# - "fifo": first-in-first-out
buffer_replacement_strategy = "random"

# Warmup iterations for distributed buffer sync (>= 0). Default: 50.
# During warmup, buffers gather errors from all GPUs for faster bootstrapping.
buffer_warmup_iter = 50

# Modulate injected error intensity by +/- this factor (0.0-1.0).
# Example: 0.1 => scale in [0.9, 1.1]. Default: 0.0.
error_modulate_factor = 0.0

# Error mode selector:
# - 0: noise-only
# - 1: noise + latent + y (default)
# - 2: noise + latent (no y)
# - 3: y-only
# - 4: latent-only
error_setting = 1

# Probability to inject noise error (0.0-1.0).
noise_prob = 0.9

# Probability to inject latent error (0.0-1.0).
latent_prob = 0.9

# Probability to inject y-error (0.0-1.0).
y_prob = 0.9

# Probability to use clean inputs (no error injection). Default: 0.1.
clean_prob = 0.1

# For clean inputs, probability to still update buffers (0.0-1.0).
clean_buffer_update_prob = 0.1

# Number of frames for y-error injection (> 0).
y_error_num = 1

# Sample y-error from all grids (default: false).
y_error_sample_from_all_grids = false

# Optional timestep grid range for y-error sampling ("start,end").
# Example: y_error_sample_range = "0,10"
# y_error_sample_range = ""

# Use last y-error frame instead of random (default: false).
use_last_y_error = false

# Require scheduler-based error computation (self_corr steps) for buffer updates.
# - false: fall back to model_pred - target if scheduler doesn't support it.
# - true: skip buffer updates unless scheduler exposes required self_corr path.
error_recycling_require_scheduler_errors = false

# Build SVI Pro-style image_emb["y"] from cached anchors (train-only, default off).
# Requires cache_svi_y_anchor_latent=true during caching.
enable_svi_y_builder = false

# Motion source for SVI y construction:
# - "latent_last": use the last latent frame from the current batch
# - "zeros": use zero motion latents
# - "replay_buffer": use prior motion latents cached per item_key/base_key
svi_y_motion_source = "latent_last"

# Number of motion latents to splice in (>= 0). Default: 1.
svi_y_num_motion_latent = 1

# First-clip behavior when replay buffer has no prior motion:
# - "zeros": use zero motion (SVI Pro first clip behavior)
# - "current": use current batch motion as a fallback
svi_y_first_clip_mode = "zeros"

# Decode previous-clip motion latents to frames and re-encode the last frame
# (SVI 2.0 behavior). Requires svi_y_motion_source="replay_buffer" and a VAE
# available during training. Adds VRAM/time overhead (default: false).
svi_y_decode_last_frame = false

# Replay buffer size for previous motion latents (>= 0). Only used when
# svi_y_motion_source = "replay_buffer".
svi_y_replay_buffer_size = 256

# Replay buffer keying mode:
# - "item_key": use the full item_key string
# - "base_key": strip sliding window suffixes (e.g., _001-032)
svi_y_replay_key_mode = "item_key"

# Use sequence index lookup for replay even if batch order is shuffled.
# When true, the replay buffer stores motion by sequence index and fetches index-1,
# parsing the original item_key regardless of svi_y_replay_key_mode.
svi_y_replay_use_sequence_index = false

# Regex used to parse sequence key + index for replay (default matches _000-031).
# Example: svi_y_replay_sequence_pattern = "_(\\d+)-(\\d+)$"
svi_y_replay_sequence_pattern = "_(\\d+)-(\\d+)$"

# =============================================================================
# ⚠️ SEMANTICGEN LORA SETTINGS
# =============================================================================
# Enable SemanticGen-inspired semantic conditioning for LoRA training (train-only, default off).
enable_semanticgen_lora = false

# Semantic encoder model name (REPA-style encoders or HF models like Qwen-VL).
semantic_encoder_name = "dinov2-vit-b14"

# Semantic encoder backend:
# - "repa": use Takenoko's REPA encoder manager (DINO/CLIP/MAE/I-JEPA, downloads via torch.hub)
# - "hf": use HuggingFace AutoModel/AutoProcessor (supports Qwen-VL)
# - "qwen_vl": alias of "hf"
semantic_encoder_type = "repa"

# Encoder dtype ("float16", "bfloat16", "float32").
semantic_encoder_dtype = "float16"

# Input resolution for semantic encoder preprocessing (REPA encoders only).
semantic_encoder_resolution = 256

# Low-FPS semantic sampling (frames per second).
semantic_encoder_fps = 2.0

# Target FPS used for stride calculation
semantic_encoder_target_fps = 16.0

# Additional stride applied on top of FPS-based sampling (>= 1).
semantic_encoder_stride = 1

# Optional hard cap on number of semantic frames per clip (comment out to disable).
# semantic_encoder_frame_limit = 16

# Semantic embedding dimension expected from encoder (must match encoder output).
semantic_embed_dim = 1024

# Compression dimension for semantic MLP (must be <= semantic_embed_dim).
semantic_compress_dim = 256

# KL regularization weight for semantic compression (>= 0).
semantic_kl_weight = 0.1

# Optional noise applied to compressed semantic embeddings (>= 0).
semantic_noise_std = 0.0

# Semantic context injection mode:
# - "concat_text": append semantic tokens after text tokens
# - "concat_tokens": prepend semantic tokens before text tokens
semantic_context_mode = "concat_text"

# Global scale for semantic conditioning (>= 0).
semantic_context_scale = 1.0

# Drop semantic conditioning with this probability (0-1).
semantic_condition_dropout = 0.0

# Anneal semantic conditioning scale to semantic_condition_min_scale over these steps.
semantic_condition_anneal_steps = 0

# Semantic conditioning minimal scale.
semantic_condition_min_scale = 0.0

# Optional cache settings for semantic embeddings (default off).
semantic_cache_enabled = false

# semantic_cache_directory = "cache/semantic"
semantic_cache_require = false

# Semantic alignment loss (train-only) using diffusion block features.
semantic_align_enabled = false

# Weight for semantic alignment loss (>= 0). Higher values enforce stronger feature alignment.
semantic_align_lambda = 0.1

# Transformer block index to tap for alignment (0 = earliest block). Use a mid block for semantic structure.
semantic_align_block_index = 8

# Optional LR override for SemanticGen helper params (defaults to learning_rate).
# semanticgen_lr = 0.0001

# =============================================================================
# ⚠️ SARA (STRUCTURAL AND ADVERSARIAL REPRESENTATION ALIGNMENT) SETTINGS
# =============================================================================

# Enable SARA (Structural and Adversarial Representation Alignment) training
# SARA extends REPA with autocorrelation matching and adversarial discriminators
# to improve training efficiency and generation quality for diffusion models
sara_enabled = false

# Pretrained encoder to use for feature extraction
# Options: "dinov2_vitb14", "dinov2_vits14", "dinov2_vitl14", "dinov2_vitg14", "clip_vitb16", "clip_vitl14"
# DINOv2 models generally provide better structural features than CLIP
sara_encoder_name = "dinov2_vitb14"

# Number of transformer blocks in the diffusion model to align features from
# Lower values (4-8) align early features (structure), higher values align later features (semantics)
# Paper recommendation: 8 for balanced structural and semantic alignment
sara_alignment_depth = 8

# Weight for patch-wise feature alignment loss (REPA baseline component)
# Encourages diffusion model features to match pretrained encoder features locally
# Paper default: 0.5-1.0, higher values strengthen feature alignment
sara_patch_loss_weight = 0.5

# Weight for autocorrelation (structural) alignment loss
# Matches spatial correlation patterns between diffusion and encoder features
# Paper default: 0.5, captures long-range structural dependencies
sara_autocorr_loss_weight = 0.5

# Weight for adversarial discriminator loss
# Encourages diffusion features to be indistinguishable from encoder features
# Paper default: 0.05-0.1, too high can cause training instability
sara_adversarial_loss_weight = 0.05

# Normalize autocorrelation matrices before computing loss
# Recommended: true for stability and scale-invariance
sara_autocorr_normalize = true

# Use Frobenius norm for autocorrelation loss instead of element-wise MSE
# Frobenius norm provides more stable gradients for matrix differences
sara_autocorr_use_frobenius = true

# Enable adversarial discriminator training
# Set to false to use only patch and autocorrelation losses (no discriminator)
sara_adversarial_enabled = true

# Discriminator architecture: "resnet18" (deeper, more capacity) or "simple_cnn" (lightweight)
# ResNet18 is more powerful but slower; simple_cnn is faster and uses less memory
sara_discriminator_arch = "resnet18"

# Learning rate for discriminator optimizer (separate from main model LR)
# Typical range: 0.0001-0.0005, higher than generator for balanced adversarial training
sara_discriminator_lr = 0.0002

# Number of discriminator update steps per generator step
# Values > 1 give discriminator more training, improving feature quality at cost of speed
sara_discriminator_updates_per_step = 1

# Number of warmup steps before discriminator loss affects generator
# Allows generator to stabilize before adversarial training begins
sara_discriminator_warmup_steps = 500

# Similarity function for patch-wise alignment: "cosine", "mse", or "l1"
# Cosine is scale-invariant and works well for normalized features
# MSE/L1 are simpler but sensitive to feature magnitude
sara_similarity_fn = "cosine"

# Weight for gradient penalty on discriminator (regularization)
# Set to 0.0 to disable, typical values 0.1-10.0 for WGAN-GP style training
# Helps prevent discriminator gradient explosion
sara_gradient_penalty_weight = 0.0

# Enable feature matching loss (match intermediate discriminator features)
# Can improve training stability by providing additional alignment signal
# Adds computational overhead, recommended for difficult training scenarios
sara_feature_matching = false

# Scaling applied when feature matching is enabled. Kept small to avoid overpowering
# the primary adversarial objective.
sara_feature_matching_weight = 0.1

# Optional discriminator gradient clipping. Set to null to disable; values in the
# 0.5-5.0 range work well for stabilising adversarial updates.
# sara_discriminator_max_grad_norm = 1.0

# Optional StepLR schedule for the discriminator. Set scheduler_step to 0 to disable.
sara_discriminator_scheduler_step = 0

# When enabled, LR decays by scheduler_gamma every scheduler_step updates.
sara_discriminator_scheduler_gamma = 0.1

# Emit extended per-component metrics (discriminator LR, grad norms, logits).
# Helpful when debugging training instabilities.
sara_log_detailed_metrics = false

# Cache pretrained encoder outputs to avoid recomputation
# Significantly speeds up training when encoder is frozen
# Disable if encoder parameters are being updated
sara_cache_encoder_outputs = true

# Use mixed precision (fp16/bf16) for SARA computations
# Reduces memory usage and speeds up training with minimal quality impact
# Recommended: true unless you encounter numerical stability issues
sara_use_mixed_precision = true

# =============================================================================
# ⚠️ HASTE (HOLISTIC ALIGNMENT WITH STAGE-WISE TERMINATION) SETTINGS
# =============================================================================

# Enable HASTE alignment (train-time only, default off)
enable_haste = false

# Teacher encoder specification for HASTE (same format as REPA encoders)
# Examples: "dinov2-vit-b", "dinov2-vit-l"
haste_encoder_name = "dinov2-vit-b"

# Use DINOv2 teacher attention maps instead of token affinity proxy
# Requires a DINOv2 encoder; if unavailable, falls back to token affinity
haste_use_teacher_attention = true

# Offset applied to teacher attention layers (student layer i aligns to teacher i + offset)
# Defaults to 4 to match the HASTE reference mapping (student 4-7 -> teacher 8-11)
haste_teacher_attn_layer_offset = 4

# Limit the number of attention heads used for alignment (0 = use all heads)
haste_attn_head_limit = 12

# Optional random subset size for attention layer pairs per step.
# - 0: use the full [haste_attn_layer_start, haste_attn_layer_end) range
# - >0: sample this many layers uniformly each step (lower VRAM/compute)
haste_attn_pair_num = 0

# Attention distillation loss type:
# - "cross_entropy": original HASTE behavior (teacher probs vs student log-probs)
# - "kl_divergence": teacher->student KL
# - "mse": probability-space MSE
# - "l1": probability-space L1
haste_attn_loss_type = "cross_entropy"

# Enable CAMEO-style cycle-consistency masking for attention distillation.
# Applies query filtering using teacher argmax forward/backward consistency.
# Disabled by default for behavior preservation.
haste_use_cycle_consistency_mask = false

# Pixel-distance threshold for cycle-consistency masking (>= 0).
# - 0.0: auto threshold (patch_grid_side / 10)
# - >0: explicit threshold in patch-grid pixel units
haste_cycle_consistency_pixel_threshold = 0.0

# Minimum required valid query ratio [0,1] for cycle mask application.
# If the mask is too sparse, HASTE falls back to unmasked attention loss.
haste_cycle_consistency_min_valid_ratio = 0.05

# Run HASTE distillation calculations with autocast disabled (FP32-preferred).
# Useful for numerical stability in attention losses; train-time only.
haste_autocast_fp32_on_distill = false

# Diffusion layer alignment depth for projection loss (0-based block index)
haste_alignment_depth = 8

# Attention alignment layer range [start, end) (end is exclusive)
# Attention alignment start layer
haste_attn_layer_start = 10

# Attention alignment end layer
haste_attn_layer_end = 20

# Projection alignment loss weight (>= 0, 0.5 matches HASTE default)
haste_proj_coeff = 0.5

# Attention alignment loss weight (>= 0, 0.5 matches HASTE default)
haste_attn_coeff = 0.5

# Early-stop step when alignment losses are disabled (must be > 0)
haste_early_stop_step = 250000

# Input resolution for the teacher encoder (256 or 512; 512 requires DINOv2)    
haste_input_resolution = 256

# =============================================================================
# ⚠️ CONTRASTIVE ATTENTION SEPARATION SETTINGS
# =============================================================================

# Enable contrastive attention separation (default off).
# Uses cross-attention summaries to push concepts apart when multiple datasets
# are mixed in a single LoRA training run.
enable_contrastive_attention = false

# Weight for the contrastive attention loss (>= 0). Typical range: 0.01 - 0.5.
contrastive_attention_weight = 0.1

# Temperature for supervised contrastive loss (must be > 0).
# Lower = sharper separation; higher = softer separation.
contrastive_attention_temperature = 0.1

# Attention block range [start, end) to sample (end is exclusive).
# Keep the window small to reduce overhead.
contrastive_attention_layer_start = 10

# Attention block end layer
contrastive_attention_layer_end = 20

# Layer aggregation mode across sampled blocks ("mean" or "max").
contrastive_attention_layer_agg = "mean"

# Limit number of heads used (0 = all heads).
contrastive_attention_head_limit = 4

# Subsample query tokens to reduce memory/compute (must be > 0).
contrastive_attention_max_queries = 128

# Only compute this loss every N steps (must be > 0).
contrastive_attention_interval = 1

# Focus on specific text token indices per concept (optional).
# Requires knowing the text encoder token indices for your concept tokens.
# When enabled, attention is masked to these indices before contrastive loss.
contrastive_attention_focus_tokens = false

# Renormalize focused attention mass (recommended).
contrastive_attention_focus_renorm = true

# Weight queries by their attention to the focused tokens to derive a spatial
# mask-like summary (CLoRA-inspired).
contrastive_attention_spatial_focus = false

# Power applied to spatial focus weights (> 0). Larger = sharper focus.
contrastive_attention_spatial_focus_power = 1.0

# Mapping: concept_id -> [token indices].
# String example: "0:5,6,7;1:12,13"
# contrastive_attention_token_indices = "0:5,6,7;1:12,13"

# Penalize attention mass on non-target tokens (entropy-based).
# Encourages cleaner token separation when focus tokens are used.
contrastive_attention_diversity_weight = 0.0

# Encourage per-concept attention stability across steps (EMA consistency).
contrastive_attention_consistency_weight = 0.0

# EMA decay for consistency loss (0..1). Higher = slower updates.
contrastive_attention_consistency_decay = 0.9

# Enable FreeFuse-inspired subject-mask auxiliary losses from cross-attention
# query-token maps. Requires concept token index mapping.
enable_contrastive_attention_subject_masks = false

# Weight for per-query target/non-target overlap minimization (>= 0).
contrastive_attention_subject_overlap_weight = 0.05

# Weight for per-query concept assignment entropy minimization (>= 0).
contrastive_attention_subject_entropy_weight = 0.01

# Weight for EMA consistency of subject query masks across steps (>= 0).
contrastive_attention_subject_temporal_weight = 0.02

# EMA decay for subject query-mask consistency loss (0..1).
contrastive_attention_subject_mask_ema_decay = 0.9

# Minimum valid token count per concept in contrastive_attention_token_indices.
# Concepts with fewer valid indices contribute zero map mass.
contrastive_attention_subject_mask_min_token_count = 1

# Optional weight ramp schedule for contrastive attention loss.
# Set ramp_end > ramp_start to enable ramping.
contrastive_attention_weight_ramp_start = 0

# Optional weight ramp end step for contrastive attention loss.
contrastive_attention_weight_ramp_end = 0

# Optional weight ramp type for contrastive attention loss ("linear" or "cosine").
contrastive_attention_weight_ramp_type = "linear"

# Optional per-concept LoRA multiplier override (requires a single concept per batch).
# String example: "0:1.0;1:0.8"
# contrastive_attention_concept_multipliers = "0:1.0;1:0.8"

# Extra prompt passes to sample additional attention maps using shuffled/rolled
# batch contexts (adds forward passes; keep small).
contrastive_attention_extra_prompt_passes = 0

# Strategy for extra prompt passes: "shuffle" or "roll".
contrastive_attention_extra_prompt_strategy = "shuffle"

# Iterative latent update using contrastive attention (adds forward passes).
contrastive_attention_latent_update = false

# Number of inner steps for latent update (>= 0).
contrastive_attention_latent_update_steps = 0

# Step size for latent updates (> 0).
contrastive_attention_latent_update_step_size = 0.1

# Only run latent update every N steps (must be > 0).
contrastive_attention_latent_update_interval = 1

# ============================================================================= 
# ⚠️ SPRINT (SPARSE-DENSE RESIDUAL FUSION) SETTINGS
# ============================================================================= 

# Enable Sprint sparse-dense residual fusion for efficient training
enable_sprint = false

# Token drop ratio (fraction of tokens to drop in middle blocks)
# 0.75 = 75% dropping (keeps 25% of tokens)
# Lower values (0.5-0.6) for more conservative speedup with less risk
# Higher values (0.8-0.9) for maximum speedup (experimental)
sprint_token_drop_ratio = 0.75

# Number of encoder blocks (dense path, all tokens)
# If not specified (null), automatically calculated as 25% of total blocks
# For 32-block model: null = 8 blocks, manual override: 6-10 blocks
# Encoder captures local details and noise information
sprint_encoder_layers = 8

# Number of middle blocks (sparse path, dropped tokens)
# If not specified (null), automatically calculated as 50% of total blocks
# For 32-block model: null = 16 blocks, manual override: 12-20 blocks
# Middle blocks process global semantics with sparse tokens
sprint_middle_layers = 16

# Token sampling strategy for video (critical for temporal coherence)
# "temporal_coherent" (recommended): Samples full/partial frames, maintains motion coherence
# "spatial_coherent": Samples consistent spatial regions across frames, good for static scenes
# "uniform": Random uniform sampling, fastest but may cause temporal artifacts
sprint_sampling_strategy = "temporal_coherent"

# Path-drop learning probability
# During training, randomly replaces sparse path with MASK tokens at this probability
# Helps the model learn to handle missing sparse information
# 0.1 = 10% probability (default), 0.0 = disabled
sprint_path_drop_prob = 0.1

# Apply path-drop only to unconditional CFG branch during sampling (Sprint inference).
# Default false keeps Sprint training-only. Enable to mask sparse path on uncond branch when using Sprint at inference.
sprint_uncond_path_drop = false

# Use a learnable [MASK] token for restored / dropped positions (SPRINT)
# false (default): use zeros as MASK padding (most stable / minimal behavior change)
# true: learn a dedicated mask embedding
# Note: When enabled, path-drop replaces the sparse path with the learnable mask token.
sprint_use_learnable_mask_token = false

# Block partitioning strategy
# "percentage" (default): Flexible, scales with model size using encoder_ratio and middle_ratio
# "fixed": 2-N-2 partitioning (2 encoder, N middle, 2 decoder blocks)
# Note: Manual sprint_encoder_layers and sprint_middle_layers always override this
sprint_partitioning_strategy = "percentage"

# Encoder ratio for percentage-based partitioning (ignored if sprint_encoder_layers is set)
# 0.25 = 25% of blocks for encoder (e.g., 8 blocks for 32-layer model)
# Only used when partitioning_strategy = "percentage" and sprint_encoder_layers is not set
sprint_encoder_ratio = 0.25

# Middle ratio for percentage-based partitioning (ignored if sprint_middle_layers is set)
# 0.50 = 50% of blocks for middle (e.g., 16 blocks for 32-layer model)
# Only used when partitioning_strategy = "percentage" and sprint_middle_layers is not set
sprint_middle_ratio = 0.50

# Two-stage training: token-dropping pretraining → full-token fine-tuning
# Maximizes training efficiency while closing train-inference gap
# Presumably, 5-10% fine-tuning steps achieves full quality recovery
# Number of pretraining steps with token dropping
# Set to 0 to use entire training as pretraining (no separate finetune stage)
# Example: 10000 for long pretraining phase with maximum efficiency
sprint_pretrain_steps = 0

# Number of fine-tuning steps with full tokens (no dropping)
# Set to 0 to disable fine-tuning stage (entire training uses token dropping)
# Recommended: 5-10% of pretrain_steps (e.g., 1000 if pretrain=10000)
# Fine-tuning closes train-inference gap and recovers any quality loss
sprint_finetune_steps = 0

# Number of warmup steps (linear ramp from 0 → sprint_token_drop_ratio)
# Allows model to stabilize before aggressive token dropping begins
# Set to 0 to disable warmup and start with full drop ratio immediately
sprint_warmup_steps = 0

# Number of cooldown steps during transition from pretraining to fine-tuning
# Linear decay from sprint_token_drop_ratio → 0.0 over cooldown_steps
# Smooth transition prevents quality degradation from abrupt change
# Default 100 steps provides gentle transition
sprint_cooldown_steps = 100

# Enable Sprint diagnostic logging 
sprint_enable_diagnostics = false

# =============================================================================
# ⚠️ FOPP SETTINGS
# =============================================================================

# Schedule type for FoPP: linear, cosine
fopp_schedule_type = "cosine"

# Number of timesteps for FoPP
fopp_num_timesteps = 1000

# Beta start for FoPP
fopp_beta_start = 0.0001

# Beta end for FoPP
fopp_beta_end = 0.002

# Random seed for FoPP sampling
fopp_seed = 1997

# =============================================================================
# ⚠️ NABLA SETTINGS
# =============================================================================

# Enable Nabla sparse attention
nabla_sparse_attention = false

# nabla-0.6_sta-11-3-3, nabla-0.6_sta-11-5-5
nabla_sparse_algo = "nabla-0.9_sta-11-24-24"

# =============================================================================
# OPTIMIZATION SETTINGS
# =============================================================================

# Use SDPA for CrossAttention (requires PyTorch 2.0)
sdpa = true

# Use FlashAttention for CrossAttention, requires FlashAttention (uv pip install your-flash-attn.whl)
# flash_attn = true

# Use SageAttention, requires SageAttention
# sage_attn = true

# Use xformers for CrossAttention, requires xformers
# xformers = true

# Use FlashAttention 3 for CrossAttention
# flash3 = true

# Use split attention for attention calculation (split batch size=1, affects memory usage and speed)
# split_attn = false

# Number of blocks to swap in the model, max XXX
# blocks_to_swap = 20 

# Enable RamTorch Linear replacement (drop-in for selected Linear layers)
# When true, certain Linear layers are replaced with CPU-stored params that are
# transferred to GPU on-demand to reduce VRAM. See ramtorch_strength/min_features.
use_ramtorch_linear = false

# Device to use for RamTorch compute (e.g., "cuda" or "cuda:0").
# If unset, RamTorch will default to its internal device selection.
# ramtorch_device = "cuda"

# Fraction (0.0–1.0) of eligible Linear layers to replace with RamTorch.
# Useful for partial rollout and benchmarking. Deterministic selection per layer tag.
ramtorch_strength = 1.0

# Minimum parameter count (in_features * out_features) for a Linear to be eligible.
# Increase to target only large matrices (e.g., 1<<20 = 1048576 params).
ramtorch_min_features = 0

# Log every RamTorch replacement (verbose); default logs only the first
ramtorch_verbose = false

# Force RamTorch to run in float32 I/O passthrough for stability with mixed precision
# (keeps outputs in float32; recommended when training with fp16/bf16)
ramtorch_fp32_io = true

# =============================================================================
# ENHANCED OFFLOADING SETTINGS
# =============================================================================

# When enabled, provides advanced weight swapping with pinned memory and performance optimizations
offload_enhanced_enabled = false

# Use reusable pinned CPU buffers for block swapping
# Requires offload_enhanced_enabled = true
# Speeds up GPU/CPU transfers but increases Windows shared VRAM consumption
offload_pinned_memory_enabled = false

# Enable non-blocking memory transfers where possible
offload_non_blocking_transfers = true

# Target memory type for offloaded weights
# "auto": Prefer CPU (default behavior), honors offload_cpu_memory_priority
# "cpu": Force CPU RAM for offloaded weights (recommended for Windows shared VRAM limits)
# "shared_gpu": Keep offloaded weights in shared GPU memory (disables enhanced+pinned)
offload_target_memory_type = "auto"

# Prefer CPU memory in auto mode (has effect only when offload_target_memory_type = "auto")
offload_cpu_memory_priority = false

# Log timing for swapping operations (debugging/perf analysis)
offload_timing_enabled = false

# Verbose logging for offloading (may reduce performance)
offload_verbose_logging = false

# =============================================================================
# CUDA ENVIRONMENT & RUNTIME SETTINGS
# =============================================================================

# Enable PyTorch CUDA allocator tuning
cuda_allocator_enable = false

# Max split size in MB for CUDA allocator (only used when cuda_allocator_enable = true)
# cuda_allocator_max_split_size_mb = 512

# Enable expandable segments for CUDA allocator (only used when cuda_allocator_enable = true)
# cuda_allocator_expandable_segments = true

# Enable CUDA launch blocking for debugging (synchronous kernel launches)
# Set to true for better error reporting but slower execution
cuda_launch_blocking = false

# Enable unified memory (system RAM as GPU memory)
# Allows GPU to use system RAM when VRAM is exhausted
cuda_managed_force_device_alloc = false

# Specify which CUDA devices to use (e.g., "0" for first GPU, "0,1" for first two)
# Leave commented to use all available GPUs
# cuda_visible_devices = "0"

# CUDA memory fraction to allocate per process (0.0-1.0)
# 1.0 = use default PyTorch behavior (no artificial limit)
# Set to 0.95 to reserve 5% for system/other processes
# cuda_memory_fraction = 0.95

# Empty CUDA cache at startup to free fragmented memory
cuda_empty_cache = false

# Enable/disable Flash Scaled Dot Product Attention
# Sometimes helps with memory usage, may affect performance
# cuda_flash_sdp_enabled = false

# =============================================================================
# ⚠️ MEMORY OPTIMIZATION SETTINGS
# =============================================================================

# Training-safe memory optimization
# When enabled, activates memory-efficient safetensors loading and optional VRAM monitoring
safe_memory_optimization_enabled = false

# Enable memory-efficient SafeTensors loading using memory mapping
# Reduces RAM usage during LoRA weight loading without affecting training behavior
memory_opt_loading_enabled = true

# Enable VRAM/RAM monitoring with automatic garbage collection
# Helps prevent OOM errors during training by proactively managing memory usage
memory_opt_monitoring_enabled = true

# Garbage collection trigger threshold (0.0-1.0, default: 0.85)
# Triggers cleanup when GPU VRAM usage exceeds this ratio
memory_opt_gc_threshold_ratio = 0.85

# Trigger garbage collection every N training steps (default: 100)
# Higher values = less frequent cleanup, lower values = more frequent cleanup
memory_opt_gc_interval_steps = 100

# Monitor memory usage every N training steps (default: 50)
# Controls how often memory statistics are checked and logged
memory_opt_monitor_interval_steps = 50

# =============================================================================
# SAFETENSORS LOADING OPTIMIZATIONS
# =============================================================================

# Use numpy.memmap for large tensors when loading safetensors checkpoints.
# Improves peak memory usage and throughput for large models but may increase
# temporary disk I/O. Disabled by default to preserve previous behavior.
enable_memory_mapping = false

# Use numpy.fromfile for zero-copy loading instead of Python file reads.
# Reduces Python overhead when streaming weights from disk. Requires
# enable_memory_mapping for best results but can be toggled independently.
enable_zero_copy_loading = false

# Perform GPU transfers in a non-blocking manner when loading weights.
# Can overlap I/O and compute on CUDA devices but requires an explicit device
# synchronization before tensors are used. Automatically handled by Takenoko
# when enabled. Disabled by default.
enable_non_blocking_transfers = false

# Byte threshold controlling when memmap acceleration is considered. Increase
# to restrict memmap usage to very large tensors.
# memory_mapping_threshold = 10485760  # 10 MiB default

# =============================================================================
# MEMORY DIAGNOSTICS
# =============================================================================

# Enable comprehensive memory tracking and OOM diagnostics
# When enabled, tracks all memory allocations and provides detailed OOM error analysis
memory_tracking_enabled = false

# Maximum number of allocation records to keep in memory (default: 10000)
# Higher values = more detailed history, more memory overhead
memory_tracking_max_records = 10000

# Capture stack traces for allocations (expensive, only for debugging)
# Provides detailed call stack information for each allocation
memory_tracking_stack_traces = false

# Track individual tensor references and lifecycle
# Helps identify memory leaks and tensor usage patterns
memory_tracking_tensor_tracking = true

# Take automatic memory snapshots every N allocations (default: 100)
# Lower values = more frequent snapshots, higher overhead
memory_tracking_auto_snapshot_interval = 100

# Memory tracking verbosity level ("minimal", "standard", "debug")
# Controls how much detail is collected and displayed
memory_tracking_verbosity = "standard"

# Automatically export diagnostics when OOM errors occur
# Creates detailed JSON reports for post-mortem analysis
memory_tracking_export_on_oom = true

# Directory for exporting memory diagnostic files
memory_tracking_export_directory = "logs"

# =============================================================================
# WINDOWS SHARED GPU MEMORY DETECTION
# =============================================================================

# Enable Windows-specific shared GPU memory detection (Windows only)
# Default: false (lightweight async monitoring, minimal overhead)
enable_windows_vram_monitor = false

# Check interval in training steps (default: 50)
# How often to check for shared memory usage during training
# Uses PEAK memory values to catch spikes between checks - you won't miss transient overflows!
# Higher values = less overhead, lower values = more frequent reporting
# Recommended: 50-100 for minimal impact on training speed
windows_vram_check_interval = 50

# Log warning when shared memory usage exceeds this threshold in GB (default: 0.5)
# Only logs when shared memory usage is significant enough to impact performance
# Set to 0 to log any shared memory usage
windows_vram_warning_threshold_gb = 0.5

# Enable detailed shared memory usage logging (default: false)
# When enabled, logs detailed breakdown of VRAM vs shared memory
# Useful for debugging memory issues, adds minimal overhead
windows_vram_detailed_logging = false

# Automatically suggest optimizations when shared memory detected (default: true)
# Provides actionable recommendations like reducing batch size, enabling checkpointing, etc.
windows_vram_auto_suggest_optimizations = true

# =============================================================================
# DUAL MODEL TRAINING SETTINGS
# =============================================================================

# Enable dual model training
# Trains a single LoRA against two base DiTs (low/high noise) by swapping base weights on-the-fly
enable_dual_model_training = false

# DiT checkpoint path for high noise model
dit_high_noise = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp16.safetensors"

# Timestep boundary for dual model training (0.0 to 1.0)
# Determines the split between low/high noise regimes
timestep_boundary = 0.875

# Offload inactive DiT model to CPU
# Keeps only the active model on GPU; inactive state_dict resides on CPU to save VRAM
offload_inactive_dit = true

# Allow mixed block swap offload
allow_mixed_block_swap_offload = false

# Dual-mode timestep bucketing strategy (interaction of per-epoch timestep buckets with high/low boundary)
# "hybrid" (default): use presampled bucketed t if it matches the boundary side; otherwise resample on-demand
# "presampled": always use dataset-presampled uniform t as-is (may cross boundary)
# "on_demand": ignore presampled t; draw fresh uniform each retry until boundary side matches (up to max retries)
# "strict_clamp": minimally adjust a presampled t to be just inside the target boundary side
dual_timestep_bucket_strategy = "on_demand"

# Maximum retries for on-demand resampling
dual_timestep_bucket_max_retries = 100

# Epsilon for boundary matching
dual_timestep_bucket_eps = 1e-4

# =============================================================================
# ⚠️ CONTROLNET TRAINING SETTINGS
# =============================================================================

# Enable ControlNet training mode
enable_controlnet = false

# Weight for the ControlNet loss
controlnet_weight = 1.0

# Stride for the ControlNet loss
controlnet_stride = 1

# Max gradient norm for ControlNet
controlnet_max_grad_norm = 1.0

# ControlNet configuration
controlnet = {}

# =============================================================================
# ⚠️ CONTROL LORA TRAINING SETTINGS
# =============================================================================

# Enable control LoRA training mode
enable_control_lora = false

# Type of control LoRA: "tile", "canny", "depth", etc.
control_lora_type = "tile"

# Preprocessing method for control signal: "blur", "edge", "depth", etc.
control_preprocessing = "blur"

# Kernel size for Gaussian blur preprocessing
control_blur_kernel_size = 15

# Sigma for Gaussian blur preprocessing
control_blur_sigma = 4.0

# Scale factor for control signal strength
control_scale_factor = 1.0

# Multiplier to learning rate for the input patch_embedding layer if training control lora
input_lr_scale = 5.0

# Dimension index for concatenation of control signal
control_concatenation_dim = 0

# Suffix to append to media file name to get the control file
control_suffix = "_control"

# Add noise to the control latents, at a random strength up to this amount
control_inject_noise = 0.1

# Save control videos created on the fly to disk
save_control_videos = true

# Directory to save control videos for debugging
control_video_save_dir = "tmp/control_videos"

# =============================================================================
# ⚠️ REWARD LORA TRAINING SETTINGS
# =============================================================================

# Enable reward LoRA training mode
enable_reward_lora = false

# List of prompts to use for reward training
reward_prompts = [
  "a red spaceship flying in the sky",
  "a dog running in a park",
]

# Batch size for reward training
reward_train_batch_size = 1

# Height of the sample video for reward training
reward_train_sample_height = 256

# Width of the sample video for reward training
reward_train_sample_width = 256

# alias: reward_num_frames is also accepted
reward_video_length = 81

# Number of inference steps for reward training
reward_num_inference_steps = 50

# Guidance scale for reward training
reward_guidance_scale = 6.0

# Number of decoded latents for reward training
reward_num_decoded_latents = 1

# Number of steps to run validation on
reward_validation_steps = 10000

# Reward function to use for reward training
# Options include "HPSReward", "AestheticReward", "PickScoreReward", "MPSReward", "VCDReward"
reward_fn = "HPSReward"

# JSON string
# Example for VCDReward: reward_fn_kwargs = "{}" (or leave as-is; VCDReward ignores unknown keys)
reward_fn_kwargs = '{"version":"v2.1"}'

# VCD reward parameters (used when reward_fn = "VCDReward")
# Implements Video Consistency Distance over generated frames.

# Overall multiplier applied to the VCD reward loss term (>0).
reward_vcd_loss_scale = 1.0

# Include amplitude-domain Wasserstein term (global appearance consistency).
reward_vcd_use_amplitude = true

# Include phase-domain Wasserstein term (local structural consistency).
reward_vcd_use_phase = true

# Relative amplitude term weight (>=0).
reward_vcd_amplitude_weight = 1.0

# Relative phase term weight (>=0).
reward_vcd_phase_weight = 1.0

# Number of generated frames sampled for VCD (must be >=1, excludes conditioning frame).
reward_vcd_num_sampled_frames = 4

# Random frame sampling for VCD. false uses earliest frames.
reward_vcd_random_frame_sampling = true

# Apply temporal weighting (N-i+1)/N to sampled frames.
reward_vcd_use_temporal_weight = true

# VGG19 feature layers for VCD extraction.
# Accepts names (relu1_1..relu5_1) or integer indices.
reward_vcd_feature_layers = ["relu1_1", "relu2_1", "relu3_1", "relu4_1", "relu5_1"]

# Resize generated frames before VGG feature extraction.
reward_vcd_feature_resolution = 224

# Maximum FFT coefficients per layer used in Wasserstein computation (0 = all).
reward_vcd_max_coeffs = 16384

# Randomly sample coefficients when max_coeffs truncates full set.
reward_vcd_random_coeff_sampling = true

# Use pretrained VGG19 ImageNet weights for reward features.
reward_vcd_use_pretrained_vgg = true

# Conditioning source for VCD reward:
# - "first_generated_frame": use generated frame 0 as conditioning image
# - "provided_first_frame": reserved for future conditioning-frame injection
reward_vcd_conditioning_source = "first_generated_frame"

# Detach conditioning frame branch to avoid optimization collapse.
reward_vcd_detach_conditioning_frame = true

# Convert decoded frames from [-1,1] to [0,1] before VCD.
# For reward path decoded frames are already [0,1], so default is false.
reward_vcd_assume_neg_one_to_one = false

# Ensure reward training decodes enough temporal frames for VCD.
reward_vcd_force_min_decoded_frames = true

# Minimum decoded frames when reward_fn = "VCDReward" (must be >=2).
reward_vcd_min_decoded_frames = 2

# Enable backprop for reward training
reward_backprop = true

# Backprop strategy to use for reward training, choices: "last", "tail", "uniform", "random"
reward_backprop_strategy = "tail"

# Number of steps to backprop for reward training
reward_backprop_num_steps = 5

# List of steps to backprop for reward training
reward_backprop_step_list = [45, 46, 47, 48, 49]

# Random start step for reward training
reward_backprop_random_start_step = 0

# Random end step for reward training
reward_backprop_random_end_step = 50

# Stop backprop at the latent model input gradient
reward_stop_latent_model_input_gradient = false

# =============================================================================
# POLYLORA PIPELINE SETTINGS 
# =============================================================================

# Enable the PolyLoRA side pipeline (offline hypernetwork that maps style/identity frames to a LoRA).
# When false, no PolyLoRA code runs anywhere.
enable_polylora = false

# Path to a trained PolyLoRA checkpoint (.pt) produced by tools/polylora_train.py.
# Only required when enable_polylora = true.
# polylora_ckpt = "checkpoints/polylora_video.pt"

# Path to the target spec JSON produced by tools/polylora_collect_spec.py.
# Describes module names and A/B shapes expected by the hypernetwork and downstream LoRA merge.
# polylora_spec = "specs/polylora_video_spec.json"

# Encoder model name for generating style embeddings at inference time.
# Should match the encoder used to build training pairs; defaults to CLIP-L/336.
polylora_encoder = "openai/clip-vit-large-patch14-336"

# Encoder type: "clip" (image) or "video" (uses a video-capable HF processor/model).
polylora_encoder_type = "clip"

# Optional multi-encoder fusion for stronger embeddings (e.g., SigLIP2 + DINOv3 + Qwen-VL).
# Provide a list of model names; leave empty to use the single polylora_encoder above.
# polylora_encoders = ["openai/clip-vit-large-patch14-336", "google/siglip-so400m-patch14-384"]

# Fusion mode for multiple encoders: "mean" (average embeddings) or "concat" (concatenate then project).
polylora_fusion_mode = "mean"

# Head mode: "trunk" (shared trunk + per-target heads, current default) or "per_tensor" (strict per-tensor 2-layer heads).
polylora_head_mode = "trunk"

# Optional identity encoder (ArcFace/InsightFace) to fuse identity embeddings into the mapper.
polylora_use_identity = false

# Identity encoder model name (only used when polylora_use_identity = true).
polylora_identity_encoder = "antelopev2"

# Style + identity fusion mode: "style_only" (ignore identity), "mean", "gated" (learned gate), or "concat".
polylora_si_fusion_mode = "style_only"

# Enable a lightweight Perceiver-style mixer before the mapper (default off to match current behavior).
polylora_use_perceiver_frontend = false

# Emit an additional base LoRA branch (prefixed with "base.") alongside the main LoRA (defaults to off).
polylora_dual_lora_heads = false

# Use cached embeddings (recommended; default true). Set false to encode on-the-fly when building pairs.
polylora_use_cached_embeddings = true

# Optional sampler smoke command to run after prediction/merge.
# Can include placeholders: {lora} (predicted path), {merged} (merged path if merge target is provided).
# Example: polylora_smoke_command = "python tools/sample_with_lora.py --lora {lora} --config configs/quick_sample.toml"
# polylora_smoke_command = ""

# Optional base model checkpoint to merge predicted LoRA into (for convenience in smoke tests).
# polylora_merge_target = "models/base_model.pt"

# Embedding dimension emitted by the encoder (must align with PolyLoRA training).
# For CLIP-L/336 this is typically 768; set explicitly when enable_polylora = true.
# polylora_embed_dim = 768

# Directory containing per-LoRA frame folders (each named after the checkpoint stem).
# Used by the PolyLoRA interactive menu to build embedding/LoRA pairs without re-prompting.
# polylora_frames_root = "data/polylora_frames"

# Output directory for saved embedding/LoRA pairs.
# polylora_pairs_out = "data/polylora_pairs"

# Optional list of LoRA checkpoints used to derive the target spec and build pairs.
# polylora_lora_paths = ["loras/style1.safetensors", "loras/style2.safetensors"]

# Optional list of base LoRA checkpoints aligned with polylora_lora_paths (for dual-head/base branch training).
# Only used when polylora_dual_lora_heads = true; lengths and stems should match polylora_lora_paths.
# polylora_base_lora_paths = ["loras/style1_base.safetensors", "loras/style2_base.safetensors"]

# Heuristic attenuation factor applied to main LoRA when polylora_dual_lora_heads = true and no base list is provided.
polylora_base_attenuate = 0.5

# Optional list of frame paths for non-interactive predict sanity check.
# polylora_predict_frames = ["frames/style1/0.jpg", "frames/style1/1.jpg"]

# Output directory for saved embedding/LoRA pairs.
# polylora_predict_out = "predicted_lora.pt"

# Training hyperparameters for the hyper-LoRA network (used as defaults in the menu/CLI).
polylora_train_lr = 0.001

# Number of epochs to train the hyper-LoRA network.
polylora_train_epochs = 5

# Batch size for hyper-LoRA training.
polylora_train_batch_size = 4

# Weight decay for hyper-LoRA training.
polylora_train_weight_decay = 0.0

# Validation split for hyper-LoRA training (0 disables validation).
polylora_train_val_split = 0.1

# Enable AMP during hyper-LoRA training (auto-disables on CPU).
polylora_train_amp = true

# Gradient clip norm for hyper-LoRA training (<=0 disables).
polylora_train_grad_clip = 1.0

# Weight for the cosine loss term in hyper-LoRA training.
polylora_cosine_loss_weight = 0.0

# Optional loss weight for the base branch when polylora_dual_lora_heads = true.
polylora_base_loss_weight = 1.0

# Optional EMA tracking for the hyper-LoRA network (recommended for stability).
polylora_use_ema = false

# EMA decay rate (only used when polylora_use_ema = true).
polylora_ema_decay = 0.995

# Write a metadata JSON alongside the PolyLoRA checkpoint after training.
polylora_save_metadata = true

# Optional override path for metadata JSON; defaults to "{polylora_ckpt}.meta.json".
# polylora_metadata_out = "checkpoints/polylora_video.meta.json"

# Apply a predicted PolyLoRA on-the-fly to the LoRA network before training starts.
# Requires polylora_predict_frames + polylora_ckpt + polylora_spec + polylora_embed_dim.
polylora_live_apply = false

# Include the base branch when applying live PolyLoRA (only meaningful if dual heads are enabled).
polylora_live_include_base = false

# Optional: run a sampling command every N epochs using a predicted LoRA from the current hypernetwork.
# Placeholders: {lora} (predicted path), {merged} (merged path if merge target is provided), {epoch} (1-based epoch).
# polylora_sample_every_epochs = 0

# Command to run for sampling (e.g., to generate images or videos).
# polylora_sample_command = "python tools/polylora_sample.py --config {config} --lora {lora}"

# Directory to save sampled images or videos.
# polylora_sample_dir = "polylora_samples"

# Base model checkpoint to merge predicted LoRA into (for convenience in smoke tests).
# polylora_sample_merge_target = "models/base_model.pt"

# =============================================================================
# ⚠️ SRPO SETTINGS
# =============================================================================

# Enable SRPO preference training (Direct-Align algorithm with reward models)
# Trains LoRA to maximize reward scores (HPS, PickScore, Aesthetic) via online generation
# Use after supervised pretraining for preference alignment
enable_srpo_training = false

# Reward model selection
# Options: "hps" (Human Preference Score v2.1), "pickscore" (CLIP-based alignment), "aesthetic" (Aesthetic Predictor v2/v2.5)
srpo_reward_model_name = "hps"

# Data type for reward model inference
# Options: "float32" (most accurate), "bfloat16", "float16"
srpo_reward_model_dtype = "float32"

# Semantic Relative Preference (SRP) configuration
# SRP formula: r_srp = (1 + k) * r_positive - r_negative
# Uses relative difference between positive/negative prompt-conditioned rewards
# Controls strength of control word influence on reward
srpo_srp_control_weight = 1.0

# Positive control words for SRP (appended to prompts for positive rewards)
srpo_srp_positive_words = [
    "beautiful", "stunning", "gorgeous", "masterpiece",
    "professional", "high quality", "detailed", "elegant",
    "vibrant", "exquisite", "breathtaking", "magnificent",
    "pristine", "refined", "impeccable", "award-winning", "cinematic"
]

# Negative control words for SRP (appended to prompts for negative rewards)
srpo_srp_negative_words = [
    "ugly", "blurry", "low quality", "distorted",
    "amateur", "poor", "bad", "grainy",
    "noisy", "artifacts", "watermark", "text overlay",
    "oversaturated", "underexposed", "overexposed", "pixelated", "dull"
]

# Direct-Align algorithm parameters
# Sigma interpolation method for noise schedule
# Options: "linear" (uniform distribution), "cosine" (concentrated at mid-range)
srpo_sigma_interpolation_method = "linear"

# Minimum sigma value for noise interpolation (0.0 = clean)
srpo_sigma_interpolation_min = 0.0

# Maximum sigma value for noise interpolation (1.0 = pure noise)
srpo_sigma_interpolation_max = 1.0

# Number of Euler integration steps for online rollout and image recovery
srpo_num_inference_steps = 50

# Classifier-free guidance scale
# WAN does not use CFG during training
# This parameter is only preserved for config compatibility but has no effect
srpo_guidance_scale = 1.0

# Enable SD3-style time shift in sigma schedule (redistributes to focus on mid-range sigmas)
srpo_enable_sd3_time_shift = true

# SD3 time shift parameter (default: 3.0 from SD3 paper)
srpo_sd3_time_shift_value = 3.0

# Discount schedule for gradient backpropagation (denoise branch)
# Minimum discount applied to early denoising steps (0.0 = no gradient flow)
srpo_discount_denoise_min = 0.0

# Maximum discount applied to late denoising steps (1.0 = full gradient flow)
srpo_discount_denoise_max = 1.0

# Discount schedule for gradient backpropagation (inversion branch)
# Starting discount for early inversion steps (1.0 = full gradient flow)
srpo_discount_inversion_start = 1.0

# Ending discount for late inversion steps (0.0 = no gradient flow)
srpo_discount_inversion_end = 0.0

# SRPO training hyperparameters
# Number of videos per SRPO training iteration (lower values = less VRAM, slower training)
srpo_batch_size = 1

# Gradient accumulation steps for SRPO (simulates larger batch sizes)
srpo_gradient_accumulation_steps = 4

# Total number of SRPO training steps (typically 500-1000 after supervised pretraining)
srpo_num_training_steps = 500

# Validation configuration
# Prompts to use for SRPO validation (generates videos and computes rewards)
srpo_validation_prompts = [
    "a dog running in a sunny field",
    "a beautiful sunset over snow-capped mountains",
    "a futuristic city with flying cars at night",
]

# Run validation every N SRPO training steps
srpo_validation_frequency = 50

# Save validation outputs to disk (videos or images)
srpo_save_validation_videos = true

# Save validation outputs as PNG images instead of MP4 videos (faster, smaller file size)
srpo_save_validation_as_images = false

# Multi-frame reward computation
# By default, SRPO only evaluates first frame. These settings enable multi-frame evaluation.
# Options: "first" (frame 0 only), "uniform" (evenly spaced), "boundary" (first+last+middle), "all" (every frame)
srpo_reward_frame_strategy = "first"  

# Number of frames to evaluate (1 = fastest, more = better temporal coverage)
srpo_reward_num_frames = 1  

# How to combine multi-frame rewards: "mean", "min" (conservative), "max" (optimistic), "weighted" (prioritize later frames)
srpo_reward_aggregation = "mean"  

# Video-specific reward models (complement image-based rewards)
# These measure temporal properties that image rewards can't capture
# Enable video-specific rewards (temporal consistency, motion quality)
srpo_enable_video_rewards = false  

# Weight for frame-to-frame consistency (penalizes flickering)
srpo_temporal_consistency_weight = 0.0  # Weight for frame-to-frame consistency (penalizes flickering)

# Weight for optical flow smoothness (rewards coherent motion)
srpo_optical_flow_weight = 0.0  

# Weight for overall motion quality (balanced movement)
srpo_motion_quality_weight = 0.0  # Weight for overall motion quality (balanced movement)

# VAE spatial downsampling factor
srpo_vae_scale_factor = 8

# Number of channels in WAN latent space
srpo_latent_channels = 16

# =============================================================================
# ⚠️ DENSEDPO SETTINGS
# =============================================================================

# DenseDPO training (default off).
enable_densedpo_training = false

# Partial-noise mix ratio in [0,1].
densedpo_partial_noise_eta = 0.5

# Denoising steps for pair generation.
densedpo_num_inference_steps = 50

# Segment length in frames.
densedpo_segment_frames = 16

# DPO beta scaling.
densedpo_beta = 0.1

# Label source: "reward", "provided", or "vlm".
densedpo_label_source = "reward"

# Batch key for provided labels.
densedpo_segment_preference_key = "densedpo_segment_preferences"

# Reward model name.
densedpo_reward_model_name = "hps"

# Reward model data type.
densedpo_reward_model_dtype = "float32"

# Frame selection for reward.
densedpo_reward_frame_strategy = "first"

# Number of frames to evaluate (1 = fastest, more = better temporal coverage)
densedpo_reward_num_frames = 1

# How to combine multi-frame rewards: "mean", "min" (conservative), "max" (optimistic), "weighted" (prioritize later frames)
densedpo_reward_aggregation = "mean"

# VLM labeling (local-only).
densedpo_vlm_model_path = ""

# VLM data type.
densedpo_vlm_dtype = "bfloat16"

# VLM prompt.
densedpo_vlm_prompt = "Rate the visual quality and motion consistency of this short video clip on a scale of 1 to 10. Respond with a single number."

# VLM max new tokens.
densedpo_vlm_max_new_tokens = 8

# VLM temperature.
densedpo_vlm_temperature = 0.0

# VLM cache directory.
densedpo_vlm_cache_dir = ""

# VLM max frames.
densedpo_vlm_max_frames = 8

# ============================================================================= 
# ⚠️ OUTPUT PRESERVATION SETTINGS (DOP & BPP)
# ============================================================================= 

# Enable DOP (Differential Output Preservation) regularization. Preserves the base class knowledge
# by disabling the LoRA and replacing trigger words with the class prompt for a secondary loss term.
# Useful when training a concept against a broader class (e.g., "Alice" vs "woman").
diff_output_preservation = false

# The trigger word/phrase in captions to be replaced for DOP. This must exactly match the trigger in your dataset captions
diff_output_preservation_trigger_word = "tknk"

# Generic class prompt to replace the trigger word for DOP. This is what the trigger will be replaced with during preservation
diff_output_preservation_class = "a photo of a person"

# Multiplier for the DOP loss (default: 1.0). Higher values = stronger preservation effect
diff_output_preservation_multiplier = 1.0

# Enable BPP (Blank Prompt Preservation) regularization. Preserves unconditional knowledge by comparing the
# LoRA-enabled prediction on a blank prompt to the base model's blank prompt output. Recommended when targeting
# CFG-based inference so the unconditional branch remains generic and guidance can amplify the concept trigger.
# Cannot be enabled at the same time as diff_output_preservation.
blank_prompt_preservation = false

# Multiplier for the BPP loss (default: 1.0). Higher values strengthen blank prompt preservation. Set to 0.0 to disable
# the extra step while keeping the flag commented for documentation.
blank_prompt_preservation_multiplier = 1.0

# =============================================================================
# ⚠️ FLUXFLOW SETTINGS
# =============================================================================

# enable FLUXFLOW temporal augmentation during training
enable_fluxflow = false

# FLUXFLOW perturbation mode (choices: frame, block)
fluxflow_mode = "frame"

# Ratio of frames to shuffle in frame mode (0.0 to 1.0)
fluxflow_frame_perturb_ratio = 0.25

# Size of contiguous frame blocks in block mode
fluxflow_block_size = 4

# Beta: Ratio of blocks to reorder in block mode (0.0 to 1.0), NOT probability!
fluxflow_block_perturb_prob = 0.5

# Dimension index for frames/time in batched video tensor
fluxflow_frame_dim_in_batch = 2

# =============================================================================
# TEMPORAL FREQUENCY-DOMAIN CONSISTENCY SETTINGS
# =============================================================================

# Enable frequency-domain temporal consistency enhancement for video training
enable_frequency_domain_temporal_consistency = false

# Enable motion coherence loss using frequency domain analysis
freq_temporal_enable_motion_coherence = false

# Enable frequency-domain temporal loss between prediction and target
freq_temporal_enable_prediction_loss = false

# Low-frequency threshold for structural component extraction (0.0-1.0)
freq_temporal_low_threshold = 0.3

# High-frequency threshold for motion component extraction (0.0-1.0)
freq_temporal_high_threshold = 0.7

# Loss component weights
# Weight for structural consistency loss
freq_temporal_consistency_weight = 0.05    

# Weight for motion coherence loss  
freq_temporal_motion_weight = 0.05      

# Weight for freq-domain temporal loss
freq_temporal_prediction_weight = 0.08    

# Temporal processing parameters
# Maximum frame distance for consistency
freq_temporal_max_distance = 4            

# Weight decay for distant frames
freq_temporal_decay_factor = 0.8         

# Minimum frames needed to apply enhancement
freq_temporal_min_frames = 4                

# Motion variation threshold for coherence loss
freq_temporal_motion_threshold = 0.1   

# Frequency analysis settings
# Preserve DC (average brightness) component
freq_temporal_preserve_dc = true         

# Automatically adjust thresholds based on content
freq_temporal_adaptive_threshold = false           

# Range for adaptive threshold
freq_temporal_adaptive_range = [0.15, 0.35]  

# Loss scheduling - control when temporal consistency is applied
# Step to start applying temporal loss
freq_temporal_start_step = 0      

# Step to stop applying temporal loss (unset = never)
# freq_temporal_end_step = 1000  

# Number of warmup steps for gradual introduction
freq_temporal_warmup_steps = 100  

# Performance optimization settings
# Cache frequency masks for better performance
freq_temporal_enable_caching = true      

# Maximum number of cached frequency masks
freq_temporal_cache_size = 500                

# Process batch elements in parallel
freq_temporal_batch_parallel = true       

# Apply enhancement every N steps (1=every step)
freq_temporal_apply_every_n_steps = 1             

# Limit frames processed per batch for memory
freq_temporal_max_frames_per_batch = 16 

# Logging cadence for temporal consistency
# Console INFO log interval (steps)
freq_temporal_log_every_steps = 10

# TensorBoard metrics interval (steps)
freq_temporal_tb_log_every_steps = 10

# Advanced settings
# "linear", "exponential", or "uniform"
freq_temporal_weight_strategy = "exponential"  

# "mean", "sum", or "none"
freq_temporal_loss_reduction = "mean"              

# Apply to latent space vs pixel space
freq_temporal_apply_to_latent = true

# =============================================================================
# ⚠️ PHYSICS-GUIDED MOTION LOSS
# =============================================================================

# Enable physics-guided motion loss using spectral cues for video training.
# Adds a composite motion regularizer (translation/rotation/scaling) computed on
# predicted x0 in latent (or optional pixel) space. Default off to preserve baseline behavior.
enable_physics_guided_motion_loss = false

# Composite loss weight (>= 0). Higher values emphasize motion plausibility.
physics_motion_loss_weight = 0.05

# Low-pass cutoff ratio in frequency space (0.0 < value <= 0.5). Typical range: 0.2 to 0.35.
physics_motion_lowpass_ratio = 0.3

# Adaptive weighting temperature (tau). Lower values focus on the best-matching motion type; higher values mix components more evenly.
physics_motion_tau = 0.2

# Ridge regularization strength for translation plane fit (>= 0).
physics_motion_ridge_lambda = 0.0001

# Components to include in the composite loss. Valid entries: translation, rotation, scaling.
physics_motion_components = ["translation", "rotation", "scaling"]

# Use latent-space x0 prediction (true) or decode to pixels (false, requires VAE).
physics_motion_apply_to_latent = true

# x0 estimation source:
# - "flow": assumes flow-matching target (x0 ~= noise - model_pred).
# - "sigma": uses noisy input and sigma scaling (x0 ~= xt - sigma * model_pred).
physics_motion_x0_source = "flow"

# Radial bins for ring energy statistics (>= 8 recommended).
physics_motion_radial_bins = 32

# Maximum frames to process per batch (0 = no limit).
physics_motion_max_frames = 0

# =============================================================================
# ⚠️ DIFFERENTIAL GUIDANCE
# =============================================================================

# Enable differential guidance for training targets (experimental)
# Amplifies the difference between model prediction and target to potentially accelerate convergence
enable_differential_guidance = false

# Scale factor for differential guidance amplification
# Higher values = more aggressive overshoot. Default: 3.0
differential_guidance_scale = 3.0

# Optional: Start applying differential guidance after this training step
# Set to 0 to apply from the beginning
differential_guidance_start_step = 0

# Optional: Stop applying differential guidance after this training step
# Default: None (apply until end)
# differential_guidance_end_step = 1500

# =============================================================================
# ⚠️ TRANSITION TRAINING SETTINGS
# =============================================================================

# Enable TiM-inspired transition training pipeline (false keeps legacy diffusion loop).
transition_training_enabled = false

# Fraction of each batch forced to vanilla diffusion pairs (t == r).
transition_diffusion_ratio = 0.5

# Fraction of each batch clamped to r = t_min for one-step / consistency targets.
transition_consistency_ratio = 0.1

# Derivative estimator for transport targets ("dde" finite diff, "jvp" autograd, "auto" tries DDE then failover).
transition_derivative_mode = "dde"

# Estimator to use when derivative_mode="auto" raises ("dde", "jvp", or "none").
transition_derivative_failover = "jvp"

# Finite-difference step (delta-t) used by the DDE estimator.
transition_finite_difference_eps = 0.005

# Reserved for downstream hooks: interpret sampled times as "discrete" (1..1000) or "normalized" (0..1).
transition_delta_time_domain = "discrete"

# Transport family used to mix latents ("linear", "trigflow", or "vp").
transition_transport = "linear"

# Per-sample loss weighting schedule ("sqrt" uses 1/sqrt(delta-t), "constant", "tau_inverse").
transition_weight_schedule = "sqrt"

# Apply tan reparameterisation before weighting (emphasises late-time deltas).
transition_tangent_weighting = true

# Rescale weights by inverse per-sample loss inside each batch.
transition_adaptive_weighting = true

# Enable LoRA interval adapter to scale multipliers from delta-t features.
transition_lora_interval_modulation = false

# Weight for optional cosine directional loss (set >0 to request intermediate activations).
transition_directional_loss_weight = 0.0

# Maintain an EMA teacher and blend its prediction into the transport target.
transition_use_ema_teacher = false

# Fraction of teacher prediction to mix when enabled (0.0-1.0).
transition_teacher_mix = 0.2

# EMA decay applied when updating the teacher (0 < decay < 1).
transition_teacher_decay = 0.999

# When LoRA modulation is enabled, forward delta-t masks to networks that expose rank-mask hooks.
transition_delta_attention = false

# Width of the harmonic feature stack that drives LoRA multiplier modulation.
transition_delta_mlp_hidden = 64

# =============================================================================
# ⚠️ CONTEXT AS MEMORY SETTINGS
# =============================================================================

# Enable context-as-memory features for scene-consistent long video generation
# When enabled, stores previously generated frames as memory for improved consistency
ctxmem_enabled = false

# Basic context settings
# Number of context frames to use during generation
ctxmem_context_size = 4

# Maximum frames to keep in memory buffer
ctxmem_max_memory_frames = 100

# Update memory every N training steps
ctxmem_memory_update_frequency = 10

# Frame selection strategies:
# - "recent": Select most recent frames (fastest, good baseline)
# - "fov_overlap": Select based on FOV overlap with camera poses (paper's method)
# - "semantic": Select based on visual similarity (experimental)
# - "mixed": Combine recent and semantic selection (experimental)
ctxmem_frame_selection_strategy = "semantic"

# Similarity threshold for semantic selection (0.0 to 1.0)
ctxmem_semantic_similarity_threshold = 0.7

# FOV-based selection settings
# Enable FOV-based frame selection using camera poses
ctxmem_use_fov_selection = false

# Default field of view in degrees (from paper: 52.67)
ctxmem_default_fov = 52.67

# Minimum overlap threshold for FOV-based selection (0.0 to 1.0)
ctxmem_fov_overlap_threshold = 0.1

# Maximum camera distance for FOV overlap consideration (meters)
ctxmem_max_camera_distance = 10.0

# Context conditioning method:
# - "concatenation": Concatenate context frames along temporal dimension (recommended)
ctxmem_context_conditioning_method = "concatenation"

# Weight for context conditioning in loss computation
ctxmem_context_weight = 1.0

# Variable length context support (experimental)
ctxmem_use_variable_length_context = false

# Training enhancements
# Enable progressive context training (start without context, gradually enable)
ctxmem_progressive_context_training = true

# Context warmup steps (when progressive training is enabled)
ctxmem_context_warmup_steps = 500

# Randomly drop context frames during training for regularization
ctxmem_context_dropout_rate = 0.1

# Temporal consistency loss weight (0.0 to disable)
ctxmem_temporal_consistency_loss_weight = 0.1

# Performance optimizations
# Enable caching of frame embeddings for similarity computation
ctxmem_use_context_caching = true

# Enable asynchronous memory updates (experimental)
ctxmem_async_memory_updates = false

# =============================================================================
# MEMFLOW GUIDANCE SETTINGS
# =============================================================================
# Enable MemFlow-inspired rolling cache guidance loss during training.
# guidance is applied as an auxiliary loss only.
# Intended for video LoRA consistency; skip for image-only runs.
enable_memflow_guidance = false

# Weight for the guidance loss (0.0 disables even if enabled flag is true)
# Recommended range: 0.01 to 0.10 for LoRA tuning
memflow_guidance_weight = 0.05

# Memory bank size in frames (how many historical frames to retain)
# Must be >= 1. Higher values increase compute and memory in guidance path.
memflow_guidance_bank_size = 3

# Record interval in frames for bank updates (store every Nth frame)
# Must be >= 1. Example: 3 means keep frames 0,3,6,...
memflow_guidance_record_interval = 3

# Local attention window size in frames for rolling cache (set -1 for global)
# Must be -1 or >= 1. Smaller values emphasize short-term continuity.
memflow_guidance_local_attn_size = 8

# Top-k frame routing for bank selection (0 disables top-k filtering)
# Must be >= 0. Example: 3 keeps the 3 most relevant bank frames.
memflow_guidance_top_k = 3

# Minimum number of frames required to apply guidance (skip shorter clips)
memflow_guidance_min_frames = 2

# =============================================================================
# ⚠️ EQUIVDM CONSISTENT NOISE
# =============================================================================
# Enable EquiVDM-style temporally consistent noise for LoRA video training.
# This swaps the noise sampling to warped noise while keeping the denoising loss unchanged 
# Requires optical flow in the batch (default key "optical_flow") at latent
# resolution. Expected shape is [B, F-1, 2, H, W] with flow from t -> t+1.
# If flow is missing or mismatched, training falls back to iid noise.
enable_equivdm_consistent_noise = false

# Mix temporally consistent noise with independent noise (Eq. 4).
# beta in [0, 1]: 1.0 = only warped noise, 0.0 = iid noise.
# Paper default is 0.9.
equivdm_noise_beta = 0.9

# Flow source. "batch" uses precomputed flow from the dataloader.
equivdm_flow_source = "batch"

# Batch key that provides the flow tensor.
equivdm_flow_key = "optical_flow"

# Frame dimension index in latents (WAN video default is B,C,F,H,W -> 2).
equivdm_flow_frame_dim_in_batch = 2

# Flow resize mode when flow resolution doesn't match latent resolution.
# - "none": require exact match (default; falls back to iid noise if mismatch)
# - "bilinear": resize flow to latent H/W and scale vectors accordingly
# - "nearest": resize flow to latent H/W and scale vectors accordingly
# When using cache_optical_flow (pixel-space flow), "bilinear" is recommended.
equivdm_flow_resize_mode = "none"

# Align corners flag used only when equivdm_flow_resize_mode = "bilinear".
equivdm_flow_resize_align_corners = true

# Optional pixel-to-latent spatial stride. Use this when flow is already at
# latent resolution but still expressed in pixel units (e.g., pre-resized flow).
# Set to your VAE downsample stride (e.g., 8). <= 0 disables scaling.
equivdm_flow_spatial_stride = -1

# Warp mode used for consistent noise:
# - "grid": deterministic grid_sample warping (fast, default)
# - "stochastic": adds pixel jitter to approximate stochastic upsampling
# - "nte": multi-sample stochastic upsampling (approximate Noise Transport Equation)
equivdm_warp_mode = "grid"

# Jitter in pixels for stochastic warp mode (0.5 matches pixel-center jitter).
equivdm_warp_jitter = 0.5

# Number of stochastic samples for equivdm_warp_mode = "nte".
equivdm_nte_samples = 4

# Optional flow quality diagnostics.
equivdm_flow_quality_check = false

# Maximum allowed flow magnitude before warning; 0 uses auto threshold (0.5 * max(H, W)).
equivdm_flow_max_magnitude = 0.0

# Optional stride check for flow vs latent stride.
equivdm_flow_stride_check = false

# Expected stride for cached flow; <= 0 disables explicit check.
equivdm_flow_expected_stride = -1

# =============================================================================
# ⚠️ IMMISCIBLE DIFFUSION 
# =============================================================================
# Enable Immiscible Diffusion noise assignment for LoRA training.
# When enabled, each sample draws K Gaussian noise candidates and keeps the
# nearest candidate to the sample latent (per-sample KNN in latent space).
# This changes only training-time noise selection and does not alter inference.
# Default is false to preserve baseline behavior.
enable_immiscible_diffusion = false

# Assignment strategy:
# - "knn": per-sample KNN over K random candidates (fastest, robust default)
# - "linear_assignment": one noise per sample with global batch matching (Hungarian/greedy)
# - "linear_assignment_candidates": global matching over a larger pooled noise set
#   with pool size = batch_size * immiscible_assignment_pool_factor
# Default: "knn".
immiscible_mode = "knn"

# Number of Gaussian candidate noises (K) sampled per training sample.
# Used by mode "knn" and as fallback when enabled.
# Valid range: int >= 1.
# Higher values increase compute/memory but can strengthen immiscibility.
# Practical ranges:
# - 4 to 8 for low-overhead runs
# - 16+ for stronger assignment at higher cost
# Default: 4.
immiscible_candidate_count = 4

# Candidate pool multiplier used only by mode "linear_assignment_candidates".
# Pool size = batch_size * immiscible_assignment_pool_factor.
# Valid range: int >= 1.
# Recommended: 2 for moderate cost, 4+ for more aggressive matching.
# Default: 2.
immiscible_assignment_pool_factor = 2

# Compute dtype for KNN distance calculations.
# Also used for pairwise assignment distance matrices in linear-assignment modes.
# Valid options:
# - "float32": safest numerically, highest compute cost
# - "float16": faster/lighter on GPU, may reduce distance precision
# - "bfloat16": balanced precision/speed when supported
# This only affects distance computation, not model parameter dtype.
# Default: "float32".
immiscible_distance_dtype = "float32"

# If true, linear-assignment modes try SciPy Hungarian matching first.
# If SciPy is unavailable or fails, helper falls back to greedy matching.
# Ignored for mode "knn".
immiscible_use_scipy = true

# Fallback path when selected mode fails:
# - "knn": revert to KNN candidate selection
# - "random": revert to iid Gaussian noise (baseline behavior)
# Default: "knn".
immiscible_fallback_mode = "knn"

# =============================================================================
# ⚠️ CAT-LVDM CORRUPTION-AWARE TRAINING SETTINGS
# =============================================================================
# Enable CAT-LVDM structured corruption on conditioning embeddings (training-only).
# This applies BCNI/SACN noise injection and is designed to improve
# LoRA robustness without changing inference behavior. Default is off.
enable_catlvdm_corruption = false

# Noise type to apply when CAT-LVDM is enabled:
# - "bcni": Batch-Centered Noise Injection (perturbs along batch semantic axes)
# - "sacn": Spectrum-Aware Contextual Noise (perturbs along spectral modes)
# Defaults to "bcni"; recommended: use "bcni" for caption-rich datasets and
# "sacn" for class-labeled datasets
catlvdm_noise_type = "bcni"

# Noise strength rho in [0.0, 0.2].
# Paper ranges use 2.5% to 20% (0.025 to 0.2); 0.075 is a common mid-point.
# 0 disables noise even if enable_catlvdm_corruption = true.
catlvdm_noise_ratio = 0.075

# Which embeddings to corrupt. Currently supports:
# - "t5": apply to T5 conditioning embeddings (default)
# Keep as "t5" unless extending the helper.
catlvdm_apply_to = "t5"

# =============================================================================
# FVDM SETTINGS
# =============================================================================

# Enable Frame-Aware Video Diffusion Model (FVDM) training
enable_fvdm = false

# Probabilistic Timestep Sampling Strategy (PTSS) settings
# Fixed PTSS probability
fvdm_ptss_p = 0.2

# Adaptive PTSS: dynamically adjust probability during training
fvdm_adaptive_ptss = true

# Starting probability (higher for exploration) 
fvdm_ptss_initial = 0.3    

# Ending probability (lower for stability)  
fvdm_ptss_final = 0.1      

# Steps before adaptation begins
fvdm_ptss_warmup = 1000    

# FVDM training enhancements
# Weight for temporal consistency loss (0.0 = disabled)
# IMPROVED: Uses second-order motion derivatives with smooth L1 loss
# This penalizes sudden inconsistencies (flickering, artifacts) while preserving smooth motion
# Recommended values: 0.0 (disabled), 0.05-0.1 (mild), 0.2+ (strong)
fvdm_temporal_consistency_weight = 0.1

# Allow asynchronous PTSS sampling even when the temporal loss is enabled.
# Default false to guarantee deterministic, fully synchronous timesteps whenever the coherence loss is active.
fvdm_allow_async_with_temporal_loss = false

# Pin the first frame of each clip to share a common timestep (keeps a visual anchor even when async sampling is active).
fvdm_pin_first_frame = true

# Weight for frame diversity regularization (0.0 = disabled)
fvdm_frame_diversity_weight = 0.0         

# Use with AdaptiveTimestepManager if enabled
fvdm_integrate_adaptive_timesteps = false 

# FVDM evaluation and logging
# Compute and log temporal coherence metrics
fvdm_eval_temporal_metrics = true        

# Steps between FVDM metric evaluation and logging
fvdm_eval_frequency = 1000               

# Compare FVDM timesteps to baseline schedule in TensorBoard (only active when enable_fvdm = true)
log_timestep_distribution_compare_baseline = true

# Steps between comparison histogram logs (match log_timestep_distribution_interval for aligned charts)
log_timestep_distribution_compare_interval = 100

# Histogram bins for comparison charts (0 uses log_timestep_distribution_bins)
log_timestep_distribution_compare_bins = 100

# =============================================================================
# ⚠️ GLANCE DISTILLATION MODE
# =============================================================================

# Enable Glance-style fixed-timestep distillation inside the standard WAN loop.
# This does NOT switch pipelines; it only overrides timestep sampling.
glance_enabled = false

# Glance mode:
# - "slow" (default): use the slow 5-step schedule from Glance
# - "fast": use the fast 5-step schedule from Glance
# - "custom": use glance_timesteps (comma-separated list)
glance_mode = "slow"

# Custom fixed timesteps, only used when glance_mode = "custom".
# Values are in the same 1..1000 scale as other timestep settings and must
# fall within [min_timestep, max_timestep]. Leave blank to disable.
# Example: "1000,979.1915,957.5157,934.9171,911.3354"
glance_timesteps = ""

# =============================================================================
# ⚠️ RCM DISTILLATION PIPELINE SETTINGS
# =============================================================================

# Toggle the alternate RCM-style distillation trainer. When false the legacy WAN finetuner is used; set true to route UnifiedTrainer through distillation.
rcm_enabled = false

# Force the dispatcher to only run the rCM pipeline when enabled. Set false if you intend to call `dispatch_rcm_pipeline` manually (e.g., experimentation).
rcm_override_wan = true

# Accelerator selection for rCM: "auto" (mirror WAN accelerate settings) or "cpu" to run the distillation loop entirely on CPU for debugging.
rcm_accelerator_mode = "auto"

# rCM runner flavour. "distill" executes the generator/critic loop, while "policy_eval" (future) would run evaluation only.
rcm_trainer_variant = "distill"

# Optional hard ceiling on distillation steps. 0 defers to Takenoko's schedule derived from dataset length / `max_train_steps`.
rcm_max_steps = 0

# Precision override forwarded to the rCM accelerator; valid values mirror accelerate ("fp16", "bf16", "no").
rcm_mixed_precision = "bf16"

# Inline table of rCM-specific knobs (e.g. `synthetic_samples`, `rcm_sigma_min`, `text_tokenizer`). Values here land in `RCMConfig.extra_args` and gate optional behaviours.
rcm_extra_args = { }

# Enable this to keep the entire rCM loop on CPU regardless of accelerator settings—useful when debugging CUDA availability/build issues.
rcm_cpu_debug = false

# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Run validation every N steps (in addition to end-of-epoch validation)
validate_every_n_steps = 500

# Enable validation at the end of each epoch (default: false)
validate_on_epoch_end = true

# Run fast validation every N steps (optional, set to enable two-tier validation)
# Example: 50 = run fast validation every 50 steps
# Fast validation will NOT run on steps where regular validation runs
# validate_fast_every_n_steps = 50

# Fraction of validation dataset to use for fast validation (0.0-1.0)
# Example: 0.1 = use 10% of validation batches, 0.05 = use 5%
validation_fast_subset_fraction = 0.1

# Randomly sample batches for fast validation instead of using first N batches
# true = randomly select different batches each time (better coverage)
# false = always use first N batches (faster, deterministic)
validation_fast_random_subset = true

# Validation timesteps mode:
# - "fixed": use the list in validation_timesteps every time (default)
# - "random": draw validation_timesteps_count integers uniformly within [validation_timesteps_min, validation_timesteps_max]
# - "jitter": for each base timestep in validation_timesteps, draw a timestep within ±validation_timesteps_jitter, clamped to [validation_timesteps_min, validation_timesteps_max]
# - "training_distribution": use the same timestep sampling distribution as training (uniform, sigmoid, content, style, etc.)
validation_timesteps_mode = "fixed"

# Base list of timesteps for validation.
# - When validation_timesteps_mode = "fixed": use exactly this list every validation
# - When validation_timesteps_mode = "jitter": apply ±validation_timesteps_jitter around each base value
# - When validation_timesteps_mode = "random": ignored (random draws are used instead)
# - When validation_timesteps_mode = "training_distribution": ignored (uses training distribution instead)
# Example: '100,300,500,700,900'
validation_timesteps = "100,300,500,700,900"

# For mode == "random" or "training_distribution": number of unique timesteps to draw per validation
validation_timesteps_count = 4

# Global min clamp for random and jitter modes. If omitted, falls back to min_timestep or 0.
validation_timesteps_min = 0

# Global max clamp for random and jitter modes. If omitted, falls back to max_timestep or 1000.
validation_timesteps_max = 1000

# For mode == "jitter": radius to jitter each base timestep (in steps). Example: 50 means draw in [t-50, t+50].
validation_timesteps_jitter = 0

# Use unique noise per batch for proper validation (recommended: true for reliable metrics, false for legacy behavior)
use_unique_noise_per_batch = true

# Compute SSIM-based perceptual SNR on tiny subsample
enable_perceptual_snr = false

# Max samples per validation step for perceptual SNR
perceptual_snr_max_items = 4

# Temporal SSIM (adjacent-frame) validation
enable_temporal_ssim = false

# Per-step cap for temporal SSIM validation
temporal_ssim_max_items = 2

# Subsample frames for speed
temporal_ssim_frame_stride = 1

# LPIPS validation
enable_lpips = false

# Per-step cap for LPIPS validation
lpips_max_items = 2

# Network for LPIPS validation (vgg|alex|squeeze)
lpips_network = "vgg"

# Subsample frames for speed
lpips_frame_stride = 8

# Temporal LPIPS (adjacent-frame) validation
enable_temporal_lpips = false

# Per-step cap for temporal LPIPS validation
temporal_lpips_max_items = 2

# Subsample frames for speed
temporal_lpips_frame_stride = 2

# Flow-warped SSIM (RAFT)
enable_flow_warped_ssim = false

# Model for flow-warped SSIM validation (torchvision_raft_small|torchvision_raft_large)
flow_warped_ssim_model = "torchvision_raft_small"

# Per-step cap for flow-warped SSIM validation
flow_warped_ssim_max_items = 2

# Subsample frames for speed
flow_warped_ssim_frame_stride = 2

# FVD (Fréchet Video Distance)
enable_fvd = false

# Model for FVD validation (torchvision_r3d_18|reference_i3d)
# - torchvision_r3d_18: Fast approximation using R3D-18 (current default)
# - reference_i3d: Canonical implementation using I3D from TensorFlow Hub (more accurate but slower)
fvd_model = "torchvision_r3d_18"

# Per-step cap for FVD validation
fvd_max_items = 2

# Length of each FVD clip
fvd_clip_len = 16

# Subsample frames for speed
fvd_frame_stride = 2

# VMAF (requires ffmpeg with libvmaf installed)
enable_vmaf = false

# Path to VMAF model (optional, empty = ffmpeg default model)
vmaf_model_path = ""

# Per-step cap for VMAF validation
vmaf_max_items = 1

# Length of each VMAF clip
vmaf_clip_len = 16

# Subsample frames for speed
vmaf_frame_stride = 2

# Path to ffmpeg executable
vmaf_ffmpeg_path = "ffmpeg"

# =============================================================================
# ⚠️ OPTICAL FLOW LOSS SETTINGS
# =============================================================================

# Enable RAFT-based optical flow loss for motion stability (experimental)
enable_optical_flow_loss = false

# Weight for the optical flow loss term (set >0 to enable)
lambda_optical_flow = 0.01

# =============================================================================
# ⚠️ VIDEO CONSISTENCY DISTANCE (VCD) LOSS SETTINGS
# =============================================================================

# Enable Video Consistency Distance auxiliary loss (default: false).
# VCD is a training-only temporal consistency regularizer for I2V-style behavior.
# It compares the conditioning frame against generated frames in frequency space
# using amplitude and phase Wasserstein distances over shallow VGG19 features.
enable_video_consistency_distance = false

# Weight applied to the VCD loss term (>= 0). Typical range: 0.001 to 0.05.
# Set to 0.0 to keep the feature enabled but inactive.
vcd_loss_weight = 0.0

# Include amplitude-domain distance (global appearance attributes).
vcd_use_amplitude = true

# Include phase-domain distance (local structure and details).
vcd_use_phase = true

# Relative contribution for amplitude distance (>= 0).
vcd_amplitude_weight = 1.0

# Relative contribution for phase distance (>= 0).
vcd_phase_weight = 1.0

# Number of generated frames sampled per step for VCD computation (>= 1).
# Higher values improve coverage but increase training cost.
vcd_num_sampled_frames = 1

# Randomly sample generated frame indices each step.
# false uses deterministic earliest frames after conditioning.
vcd_random_frame_sampling = true

# Apply the paper's temporal weighting term (N - i + 1) / N.
# Helps discourage over-constraining late frames into still-image behavior.
vcd_use_temporal_weight = true

# First global step at which VCD is applied.
vcd_start_step = 0

# Optional last step for VCD (null = no end step).
# vcd_end_step = 20000

# Warmup steps for gradually ramping in VCD after vcd_start_step.
vcd_warmup_steps = 0

# Apply VCD every N steps (>= 1) for compute control.
vcd_apply_every_n_steps = 1

# VGG19 feature layers used for VCD feature extraction.
# Accepts layer names (relu1_1..relu5_1) or integer feature indices.
vcd_feature_layers = ["relu1_1", "relu2_1", "relu3_1", "relu4_1", "relu5_1"]

# Resize frames before feature extraction (pixels). Lower values reduce cost.
vcd_feature_resolution = 224

# Max flattened FFT coefficients used per layer per sample (0 = use all).
# Caps memory/compute for high-resolution feature maps.
vcd_max_coeffs = 16384

# Randomly subsample coefficients when vcd_max_coeffs truncates the full set.
vcd_random_coeff_sampling = true

# Use ImageNet-pretrained VGG19 weights for feature extraction.
# If unavailable at runtime, Takenoko falls back to untrained VGG with a warning.
vcd_use_pretrained_vgg = true

# Conditioning frame source:
# - "batch_first_frame": first frame from training batch pixels (preferred for I2V)
# - "pred_first_frame": first decoded predicted frame
vcd_conditioning_source = "batch_first_frame"

# Detach conditioning-frame features from gradient graph for stability/perf.
vcd_detach_conditioning_frame = true

# Convert inputs from [-1, 1] to [0, 1] before VGG/FFT.
vcd_assume_neg_one_to_one = true

# =============================================================================
# ⚠️ MASKED TRAINING SETTINGS
# =============================================================================

# Enable masked training with prior preservation
# Master toggle - MUST be true to enable the entire feature
# Note: Works with DOP (diff_output_preservation) - DOP's prior will be masked if both enabled
use_masked_training_with_prior = false

# Probability of removing mask during training (0.0 = never remove, 1.0 = always remove)
# Helps prevent overfitting to mask boundaries by randomly training on full images
unmasked_probability = 0.1

# Minimum weight for unmasked regions [0-1] 
# 0.1 = light masking (10% weight to unmasked), 0.01 = heavy masking (1% weight to unmasked)
unmasked_weight = 0.1

# Weight for preserving base model predictions in unmasked areas [0-inf] 
# This controls spatial masking within individual images/videos
# 0.0 = disabled, 0.3-0.8 = typical range, higher = stronger preservation of original model
masked_prior_preservation_weight = 0.0

# Normalize loss by masked area size (helps balance training across different mask sizes)
normalize_masked_area_loss = false

# Method for resizing masks to match loss dimensions ("area", "bilinear", "nearest")
# "area" provides cleanest boundaries, recommended for most use cases
mask_interpolation_mode = "area"

# Integration Parameters
# Enable prior prediction computation (required for prior preservation)
enable_prior_computation = true

# Method for computing prior predictions ("lora_disabled" recommended)
# "lora_disabled" = disable LoRA weights when computing base model predictions
prior_computation_method = "lora_disabled"

# Video-specific Parameters
# Weight for temporal consistency loss in videos [0-inf] (0 = disabled)
# 0.1-0.5 typical range, encourages smooth mask transitions between frames
temporal_consistency_weight = 0.0

# Temporal consistency computation method ("adjacent" or "all_pairs")
# "adjacent" = compare adjacent frames only (faster)
# "all_pairs" = compare all frame pairs (more thorough but slower)
frame_consistency_mode = "adjacent"

# =============================================================================
# TREAD SETTINGS
# =============================================================================

# Enable TREAD routing for training
enable_tread = false

# TREAD mode: "full", "frame_contiguous", "frame_stride", "frame_topk", "row_contiguous", "row_stride", "row_random", "spatial_auto"
# 'full' is used with tread_config_route parameters
# 'frame_contiguous'/'frame_stride'/'frame_topk' are used with 'tread' parameter for temporal routing
# 'row_contiguous'/'row_stride'/'row_random' are used with 'tread' parameter for spatial routing
# 'spatial_auto' automatically detects F=1 (rows) vs F>1 (frames) - hybrid auto-detection mode
tread_mode = "full"

# Each route is a semicolon-separated string of key=value pairs.
# Common keys: selection_ratio (0-1), start_layer_idx, end_layer_idx (negative allowed)
# Optional frame_topk keys: top_k_frames (>0), always_keep_first_frame (bool), always_keep_last_frame (bool)
tread_config_route1 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=-2"
# Example for frame_topk mode:
# tread_config_route1 = "selection_ratio=0.75; start_layer_idx=2; end_layer_idx=-2; top_k_frames=5; always_keep_first_frame=true"
# tread_config_route2 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=8"
# tread_config_route3 = "selection_ratio=0.25; start_layer_idx=9; end_layer_idx=11"
# tread_config_route4 = "selection_ratio=0.35; start_layer_idx=12; end_layer_idx=15"
# tread_config_route5 = "selection_ratio=0.25; start_layer_idx=16; end_layer_idx=23"
# tread_config_route6 = "selection_ratio=0.1; start_layer_idx=24; end_layer_idx=-2"

# Simplified routing blocks:
# For temporal routing: Use tread_mode = "frame_contiguous", "frame_stride", or "frame_topk"
# For spatial routing: Use tread_mode = "row_contiguous", "row_stride", or "row_random"
# keep_ratio: fraction of frames/rows to keep within the routed band
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.6 }

# Examples:
# Temporal routing (video frames):
# tread_mode = "frame_contiguous"
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.7 }

# Spatial routing (image rows):
# tread_mode = "row_contiguous"     # Keep center rows
# tread_mode = "row_stride"         # Keep evenly spaced rows
# tread_mode = "row_random"         # Keep randomly selected rows
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.8 }

# Hybrid auto-detection mode:
# tread_mode = "spatial_auto"       # F=1→rows, F>1→frames
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.7 }

# Auto-fallback for mixed datasets (when using row modes with video content)
# row_tread_auto_fallback = true   # Default: true (safe for mixed datasets)
row_tread_auto_fallback = true

# Enable strict sanity checks when slicing per-token time projections during
# frame-based routing (verifies index bounds, shapes, and dtypes). Off by default.
strict_e_slicing_checks = false

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

# directory to output trained model
output_dir = "output/wan22_lora"

# base name of trained model file 
output_name = "wan22_lora"

# save training state additionally (including optimizer states etc.) when saving model
save_state = true

# save checkpoint every N steps
save_every_n_steps = 500

# Save checkpoints before generating samples/validation (default: false, saves after)
# When true: checkpoint → sampling → validation
# When false: sampling → validation → checkpoint (current behavior)
# Useful if you want to prioritize checkpoint saving over potentially time-consuming sampling
# Default: false to maintain existing behavior
save_checkpoint_before_sampling = true

# save training state (including optimizer states etc.) on train end even if --save_state is not specified
# save_state_on_train_end = false 

# automatically resume training from latest checkpoint
auto_resume = true

# saved state to resume training
# resume = "path/to/resume/state"

# Resume from a specific step number (overrides initial_epoch and train_state.json)
# Useful for restarting training from an exact point across all epochs
# initial_step = 1000

# Resume from a specific epoch number (ignored if initial_step is set)
# Note: lr scheduler starts from step 0 unless --resume is also used
# initial_epoch = 5

# Skip data loading until reaching the initial step (default: false)
# When true: Uses accelerator.skip_first_batches for proper data ordering
# When false: Fast-forwards step counters without data skipping (faster but different data order)
# skip_until_initial_step = false

# fp16 training including gradients (temporary disabled)
# full_fp16 = false 

# bf16 training including gradients (temporary disabled)
# full_bf16 = false 

# save checkpoint every N epochs
# save_every_n_epochs = 1 

# save last N checkpoints when saving every N epochs (remove older checkpoints)
# save_last_n_epochs = 10 

# save last N checkpoints of state (overrides the value of --save_last_n_epochs)
# save_last_n_epochs_state = 3

# save training state (optimizer, scheduler, etc.) every N epochs, independent of checkpoint saving
# save_state_every_n_epochs = 1

# save training state (optimizer, scheduler, etc.) every N steps, independent of checkpoint saving  
# save_state_every_n_steps = 100 

# save checkpoints until N steps elapsed (remove older checkpoints if N steps elapsed)
# save_last_n_steps = 5000 

# save states until N steps elapsed (overrides --save_last_n_steps, overrides --save_last_n_steps)
# save_last_n_steps_state = 5000 

# allow resuming training with a different dataset (e.g., for domain adaptation or transfer learning)
# allow_dataset_change = true 

# reset epoch/step calculations when resuming with a different dataset (recommended for proper training behavior)
# reset_training_state = true 

# skip dataset compatibility validation entirely (advanced users only - use with caution)
# ignore_dataset_compatibility = true 

# =============================================================================
# METADATA SETTINGS
# =============================================================================

# embed the original config file content in safetensors metadata (default: True)
embed_config_in_metadata = true

# do not save metadata in output model
# no_metadata = true 

# title for model metadata (default is output_name)
# metadata_title = "" 

# author name for model metadata
# metadata_author = "" 

# description for model metadata
# metadata_description = "" 

# license for model metadata
# metadata_license = "" 

# tags for model metadata, separated by comma
# metadata_tags = "" 

# =============================================================================
# LOGGING SETTINGS
# =============================================================================

# Enhanced progress bar with additional metrics (learning rate, epoch, memory usage, etc.), set to false to use simple progress bar if you encounter any issues
enhanced_progress_bar = true

# Log training configuration
log_config = true

# Enable logging and output TensorBoard log to this directory
logging_dir = "logs/wan22_lora"

# Logging tool to use (only tensorboard is supported)
log_with = "tensorboard"

# Set logging level to DEBUG to see FluxFlow messages
logging_level = "INFO"

# Append small emoji hints to TensorBoard tags (e.g., loss 📉, throughput 📈)
tensorboard_append_direction_hints = true

# Add prefix for each log directory
# log_prefix = "" 

# Name of tracker to use for logging, default is script-specific default name
# log_tracker_name = "" 

# Path to tracker config file to use for logging
# log_tracker_config = "" 

# Automatically launch TensorBoard server on training start
launch_tensorboard_server = true

# Host for TensorBoard server
tensorboard_host = "127.0.0.1"

# Port for TensorBoard server  
tensorboard_port = 6006

# Enable auto-reload of logs (default: true)
tensorboard_auto_reload = true

# =============================================================================
# ADDITIONAL LOGGING SETTINGS
# =============================================================================

# Enable periodic logging of additional training loss diagnostics
log_extra_train_metrics = true

# Interval (in steps) to compute and log extra train metrics
train_metrics_interval = 50

# Log the exact dataset item(s) used for each training step.
# This prints entries from `batch["item_info"]` (ItemInfo.item_key, bucket size, frame count).
# Useful to correlate throughput spikes with specific sources/buckets, but can be very verbose.
# Default: false
log_batch_item_info = false

# How often to print item info (in global steps).
# - 1: every step
# - 10: every 10 steps
# Default: 1
log_batch_item_info_interval = 50

# Limit the number of items printed per step (batch can contain multiple items).
# Default: 8
log_batch_item_info_max_items = 10

# Enable gradient norm logging (logs total gradient norm before clipping)
# Creates TensorBoard metric: grad_norm
log_gradient_norm = false

# Enable detailed parameter statistics logging
# Logs per-parameter gradient norms and parameter norms for top N parameters
# Creates TensorBoard metrics: grad_norm/*, param_norm/*, param_stats/*
log_param_stats = false

# Interval (in steps) to log parameter statistics
param_stats_every_n_steps = 100

# Number of top parameters to track by norm
max_param_stats_logged = 20

# Enable per-source loss logging (attempts to detect video vs image sources)
# Creates TensorBoard metrics: loss/video, loss/image
log_per_source_loss = false

# Log LoRA weight value histograms to TensorBoard for training diagnostics.
# Helps detect weight explosion, collapse, or drift during training.
# Creates TensorBoard histograms: lora_weights/all_weights, lora_weights/down_weights, lora_weights/up_weights
# Creates TensorBoard scalars: lora_weights/mean, lora_weights/std, lora_weights/abs_mean, etc.
# Default: false
log_lora_weight_histograms = false

# Interval (in steps) between histogram logs. Histogram logging is more expensive than scalar logging.
# Scalar metrics (mean, std, etc.) are logged every 10 steps regardless of this setting.
# Default: 100
lora_weight_histogram_interval = 100

# Log lora_down and lora_up matrices separately in addition to combined histogram.
# Useful for debugging initialization or understanding which projection is changing more.
# Default: false
lora_weight_log_separate_matrices = false

# mHC mixing diagnostics
# Log scalar mixing stats every N steps (only when network_module = "networks.mhc_lora")
mhc_mix_log_interval = 100
# Log mixing weight histograms to TensorBoard every N steps (costly)
mhc_mix_histogram_interval = 500
# Warn if average mixing entropy drops below this threshold
mhc_mix_warn_entropy_min = 0.05
# Warn if average off-diagonal mixing exceeds this threshold
mhc_mix_warn_offdiag_max = 0.5

# Monitor activation values in DiT/transformer blocks during training.
# Helps detect activation explosion before it causes NaN/Inf errors.
# Creates TensorBoard scalars: activation/block_N/min, max, mean, std, abs_max
# Also tracks global aggregates: activation/global_abs_max, global_mean, global_std
log_activation_stats = false

# Interval (in steps) between activation stats logging.
# Activation tracking adds some overhead, so don't log too frequently.
# Default: 100
activation_stats_interval = 100

# Maximum number of transformer blocks to track (first N blocks).
# More blocks = more metrics but also more overhead.
# Default: 8
activation_stats_max_layers = 8

# Activation magnitude that triggers a warning in logs.
# If abs_max exceeds this, you'll see a warning message.
# Default: 1000.0
activation_stats_warn_threshold = 1000.0

# Activation magnitude that triggers a critical alert.
# Values this high often precede NaN/Inf explosions.
# Default: 10000.0
activation_stats_critical_threshold = 10000.0

# Log a scatter plot of per-sample loss vs timestep to TensorBoard
log_loss_scatterplot = true

# Interval (in steps) to log the scatter plot
log_loss_scatterplot_interval = 100

# Enable throughput metrics logging (samples per second, steps per second, runtime)
log_throughput_metrics = true

# Window size for throughput calculation (number of recent steps to average)
throughput_window_size = 100

# Enable automatic VRAM estimation validation after first training step
# Compares estimated VRAM vs actual peak usage
# Provides accuracy feedback to help calibrate expectations
log_vram_validation = false

# Progress bar postfix metrics mode:
# true  - alternate each step between showing timing (step_ms) and hardware metrics (peak/util)
# false - show both timing and hardware metrics every step
alternate_perf_postfix = false

# Enable advanced training diagnostics (gradient stability, convergence analysis, loss distributions)
# Provides comprehensive metrics similar to WAN 2.2 training reports
# DEFAULT: false (opt-in only, zero overhead when disabled)
enable_advanced_metrics = false

# Advanced metrics features to enable (optional, defaults to all features when enable_advanced_metrics = true)
# Available features: "gradient_stability", "convergence", "noise_split", "oscillation_bounds"
# "gradient_stability" - Track gradient norms with moving averages and threshold alerts
# "convergence" - Multi-scale R² convergence trend analysis (10/25/50/100 step windows)
# "noise_split" - HIGH/LOW noise loss split tracking (requires DualModelManager for WAN dual LoRA)
# "oscillation_bounds" - Track upper/lower loss bounds to detect training stability patterns
# Example: Enable selective features
advanced_metrics_features = ["gradient_stability", "convergence", "oscillation_bounds"]

# Maximum history length for advanced metrics (prevents unbounded memory growth)
# Older data is discarded when history exceeds this limit. DEFAULT: 10000 steps (~390 KB memory)
advanced_metrics_max_history = 10000

# Gradient stability watch threshold (alert when gradient norm exceeds this value)
# Used to detect gradient explosions or instability during training. DEFAULT: 0.5
gradient_watch_threshold = 0.5

# Gradient stability moving average window size (number of recent steps)
# Larger values provide smoother gradient stability metrics. DEFAULT: 10 steps
gradient_stability_window = 10

# Convergence analysis window sizes for multi-scale R² trend analysis
# Analyzes convergence at different time scales to detect short-term and long-term trends
# DEFAULT: [10, 25, 50, 100] steps
convergence_window_sizes = [10, 25, 50, 100]

# Diagnostic attention metrics, if enabled, compute lightweight cross-attention statistics (entropy, top-k mass, token focus)
# at a configurable interval and for a small number of layers to limit overhead.
# They DO NOT store attention maps; only scalars are logged.
enable_attention_metrics = false

# Interval to collect attention metrics (in steps)
attention_metrics_interval = 100

# Maximum number of layers to collect attention metrics from, per step, collect from at most this many cross-attn layers
attention_metrics_max_layers = 2

# Maximum number of queries to collect attention metrics from (subsample queries per head for bounded memory)
attention_metrics_max_queries = 32

# Top-k mass for attention concentration metric
attention_metrics_topk = 16

# Prefix for logged metric names
attention_metrics_log_prefix = "attn"

# Log a small attention heatmap image to TensorBoard 
# recomputes a tiny [queries x tokens] map from detached Q/K once per window.
attention_metrics_log_heatmap = false

# Max heads to average for the heatmap
attention_metrics_heatmap_max_heads = 2

# Max queries (rows) to include in heatmap (subsampled evenly)
attention_metrics_heatmap_max_queries = 32

# Prefix for heatmap tag in TensorBoard
attention_metrics_heatmap_log_prefix = "attn_hm"

# Attention heatmap colormap name (any Matplotlib colormap string, e.g., "magma", "viridis", "plasma")
attention_metrics_heatmap_cmap = "magma"

# Attention heatmap normalization: "log" for LogNorm, "linear" for no explicit normalization
attention_metrics_heatmap_norm = "log"

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmin_pct = 60.0

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmax_pct = 99.5

# Attention heatmap figure width in inches
attention_metrics_heatmap_fig_w = 6.0

# Attention heatmap figure height in inches
attention_metrics_heatmap_fig_h = 4.0

# Performance metrics verbosity level for model prediction and target analysis: minimal, standard, debug
performance_verbosity = "standard"

# Split SNR metrics into essential and other namespaces for clarity
snr_split_namespaces = true

# Split validation metrics into essential and detailed namespaces
val_split_namespaces = true

# EMA smoothing factor (0.9-0.99 typical). Higher = smoother, slower to react.
# ema_loss_beta = 0.98

# Number of steps to defer bias correction for early readability
# ema_loss_bias_warmup_steps = 100

# Enable latent quality analysis to check training data characteristics
# Runs once at startup to analyze cached latents and identify potential issues
latent_quality_analysis = false

# Threshold for acceptable mean deviation from 0.0 (absolute value)
# Images with |mean| > threshold will be flagged as potentially problematic
latent_mean_threshold = 0.16

# Threshold for acceptable standard deviation range around 1.0
# Valid std range: [1.0/threshold, threshold] - values outside trigger warnings
latent_std_threshold = 1.35

# Show visual analysis of worst latent using OpenCV (requires opencv-python)
# Displays red/blue heatmap showing positive/negative latent values
latent_quality_visualizer = false

# Log latent quality statistics to TensorBoard for monitoring
# Creates charts showing mean/std distributions and quality analysis results
latent_quality_tensorboard = true

# Enable video-specific temporal analysis (for video training datasets)
# Analyzes temporal consistency, scene transitions, and motion patterns
# Provides video-specific quality metrics beyond basic mean/std analysis
latent_quality_video_analysis = true

# =============================================================================
# MODEL WEIGHT EMA
# =============================================================================

# Track an exponential moving average of model weights for smoother evaluation/checkpoints.
# Disabled by default; only enable when you want EMA-based validation/saves.
enable_weight_ema = false

# EMA decay (0<value<1). Higher = smoother/less reactive. Common: 0.999-0.9999.
weight_ema_decay = 0.999

# Delay EMA updates until this global step (>=0). Useful when resuming or warming up.
weight_ema_start_step = 0

# Only include parameters with requires_grad=True (recommended for LoRA/finetune to reduce VRAM).
# Set to false to include all parameters.
weight_ema_trainable_only = true

# Swap EMA weights in for validation/sampling/checkpoint saving; training weights are restored automatically.
# Disable to keep raw weights for eval/saves even when EMA tracking is on.
weight_ema_use_for_eval = true

# Update EMA every N optimizer steps (>=1). Higher values reduce overhead at the cost of responsiveness.
weight_ema_update_interval = 1

# Where to keep EMA weights between updates: "accelerator" (faster, more VRAM) or "cpu" (saves VRAM, slower).
weight_ema_device = "accelerator"

# How to use EMA during evaluation:
# - "off": never swap EMA for eval/sampling (even if tracking)
# - "ema": use EMA weights for eval/sampling (default when enabled)
# - "compare": run validation twice (base and EMA) with separate metric tags
weight_ema_eval_mode = "ema"

# Save a separate EMA checkpoint alongside the base model (e.g., output_name-ema.safetensors).
weight_ema_save_separately = false

# =============================================================================
# TIMESTEP DISTRIBUTION LOGGING
# =============================================================================

# Timestep distribution logging
# Choose how to visualize timestep distributions in TensorBoard during training.
# "off":     disable logging
# "histogram": log TensorBoard histograms (fast; shows shape clearly)
# "chart":     log a static chart image (uses matplotlib if available)
# "both":      log both histogram and chart
log_timestep_distribution = "off"

# How often to log the live (used) timestep distribution during training (in steps)
log_timestep_distribution_interval = 100

# Number of bins for histogram/chart density
log_timestep_distribution_bins = 100

# Log an initial expected distribution at training start
# Uses precomputed buckets if enabled; otherwise simulates via the configured sampler
log_timestep_distribution_init = true

# Log an initial expected distribution at training start only once
log_timestep_distribution_init_once = true

# Number of samples to draw when simulating the initial distribution (when not using precomputed buckets)
log_timestep_distribution_samples = 20000

# Log a live (used) timestep distribution during training (in steps)
log_timestep_distribution_window = 10000

# Band ratios using configured edges, e.g., "850,900,950"
log_timestep_distribution_bands = "0,100,200,300,400,500,600,700,800,900,1000"

# Log the probability mass function (PMF) of the timestep distribution
log_timestep_distribution_pmf = true

# =============================================================================
# DDP SETTINGS
# =============================================================================

# DDP timeout (min, None for default of accelerate)
# ddp_timeout = 1 

# Enable gradient_as_bucket_view for DDP
# ddp_gradient_as_bucket_view = false 

# Enable static_graph for DDP
# ddp_static_graph = false 

# =============================================================================
# DYNAMO SETTINGS
# =============================================================================

# dynamo backend type (default is None)
dynamo_backend = "NO"

# Dynamo mode (choices: default, reduce-overhead, max-autotune)
dynamo_mode = "default"

# Use fullgraph mode for dynamo
# dynamo_fullgraph = false 

# Use dynamic mode for dynamo
# dynamo_dynamic = false # Use dynamic mode for dynamo

# =============================================================================
# ⚠️ VAE TRAINING SETTINGS
# =============================================================================

# VAE training mode: "full", "decoder_only", "encoder_only"
vae_training_mode = "full"

# Weight for KL divergence loss in VAE training
vae_kl_weight = 1e-6

# Reconstruction loss type: "mse", "l1", "huber"
vae_reconstruction_loss = "mse"

# Loss weights default to classic behaviour (MSE + KL). Increase others to blend terms.
vae_mse_weight = 1.0

# Mean absolute error (L1) reconstruction weight for sharper details.
vae_mae_weight = 0.0

# Learned perceptual metric weight. Requires `pip install lpips` and adds VRAM overhead.
vae_lpips_weight = 0.0

# Sobel edge consistency weight to encourage crisp edges.
vae_edge_weight = 0.0

# Rolling window (in steps) for the optional median loss balancer. 0 disables balancing.
vae_loss_balancer_window = 0

# Percentile used for robust median estimates when balancing (0-100].
vae_loss_balancer_percentile = 95

# Use latent mean instead of sampling in decoder-only mode (disables KL term by default).
vae_decoder_latent_mean = true

# =============================================================================
# ⚠️ ONLINE SELF-CORRECTION SETTINGS
# =============================================================================

# Enable lightweight online self-correction cache updates during training (non-invasive; off by default)
# When enabled, the trainer periodically generates short clips into output_dir/self_correction_cache
# and mixes them into training via a hybrid dataloader wrapper.
self_correction_enabled = false

# Do not start self-correction until after this many optimizer steps
self_correction_warmup_steps = 1000

# Regenerate correction clips every N optimizer steps (0 disables periodic updates)
self_correction_update_frequency = 1000

# Maximum number of correction clips to keep in the cache directory
self_correction_cache_size = 200

# Length (frames) of generated correction clips
self_correction_clip_len = 32

# Probability to draw a batch from the correction cache instead of the main dataset
self_correction_batch_ratio = 0.2

# Number of frames for generation
self_correction_sample_steps = 16

# Output width in pixels
self_correction_width = 256

# Output height in pixels 
self_correction_height = 256

# Classifier-free guidance scale (CFG)
self_correction_guidance_scale = 5.0

# Inline prompts for self-correction 
# Each table can include: text, width, height, frames, step, seed, guidance_scale, enum
# If not provided, captions from datasets are used.

[[self_correction_prompts]]
text = "a person walking on a beach at sunset"
frames = 33
step = 16
guidance_scale = 5.0

[[self_correction_prompts]]
text = "a busy downtown street with cars and pedestrians"
frames = 17
step = 12
guidance_scale = 4.5

# =============================================================================
# SAMPLING SETTINGS
# =============================================================================

# Generate sample images every N steps
sample_every_n_steps = 500

# Generate sample images before training
sample_at_first = true

# Generate sample images every N epochs (overwrites n_steps)
# sample_every_n_epochs = 1 

# =============================================================================
# SAMPLE PROMPTS SETTINGS
# =============================================================================

[[sample_prompts]]

# The main prompt text
text = """
Young woman with short blonde hair and grey eyes, tank top, denim shorts, \
in a cluttered open restaurant on a hill above amazing ocean beach
"""

# Output width in pixels
width = 384

# Output height in pixels
height = 384

# Number of frames for video generation
frames = 45

# Random seed for generation
seed = 20250801

# Step for generation
step = 20

# Classifier-free guidance scale (CFG)
cfg_scale = 6.0

# Optional EqM sampler overrides (only used when enable_eqm_mode = true)
# Step size for EqM sampling
# eqm_step_size = 0.0017

# Momentum for EqM sampling
# eqm_momentum = 0.0

# Sampler for EqM sampling, options: "gd", "ngd", "ode", "sde", or "ode_likelihood"
# eqm_sampler = "gd" 

# Enable adaptive sampler for EqM sampling
# eqm_use_adaptive_sampler = false

# Adaptive step size parameters for EqM sampling
# eqm_adaptive_step_min = 1e-5

# Maximum step size for EqM sampling
# eqm_adaptive_step_max = 0.01

# Growth factor for adaptive step size
# eqm_adaptive_growth = 1.05

# Shrink factor for adaptive step size
# eqm_adaptive_shrink = 0.5

# Restart patience for adaptive step size
# eqm_adaptive_restart_patience = 4

# Alignment threshold for adaptive step size
# eqm_adaptive_alignment_threshold = 0.0

# Weighting schedule for EqM sampling, options: "linear", "cosine", or "sigmoid"
# eqm_weighting_schedule = "linear"  

# Number of steps to apply weighting schedule
# eqm_weighting_steps = 1000

# Save NPZ files for EqM sampling
# eqm_save_npz = false

# Directory to save NPZ files for EqM sampling
# eqm_npz_dir = "output/eqm_npz"

# Maximum number of NPZ files to save
# eqm_npz_limit = 64

# =============================================================================
# LATENT CACHE SETTINGS
# =============================================================================

[datasets.latent_cache]

# cache features in VAE on CPU
vae_cache_cpu = true

# data type for VAE, default is float16
vae_dtype = "float16"

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
# batch_size = 4 

# number of workers for dataset, default is cpu count-1
# num_workers = 4 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# Cache SVI Pro anchor latents alongside latent caches (train-only feature).
# Enables image_emb["y"] construction during training; requires cache_latents.
cache_svi_y_anchor_latent = false

# Cache optical flow alongside latent caches for EquiVDM consistent noise.
# Generates *_flow.safetensors using RAFT during latent caching.
cache_optical_flow = false

# RAFT model to use when cache_optical_flow is enabled.
optical_flow_model = "raft_small"

# delete all existing latent cache files in cache_directory before caching      
# purge_before_run = true

# debug mode, choices: image, console, video
# debug_mode = "image" 

# console width, default is 80
# console_width = 80 

# console background color, default is black
# console_back = "black" 

# debug mode: not interactive, number of images to show for each dataset
# console_num_images = 1 

# =============================================================================
# TEXT ENCODER CACHE SETTINGS
# =============================================================================

[datasets.text_encoder_cache]

# use fp8 for Text Encoder model
fp8_t5 = true

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
batch_size = 16

# number of workers for dataset, default is cpu count-1
# num_workers = 4 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# delete all existing text encoder cache files in cache_directory before caching
# purge_before_run = true

# =============================================================================
# DATASET SETTINGS
# =============================================================================

[datasets.general]

# File extension for caption files
caption_extension = ".txt"

# Probability to drop entire captions (0.0-1.0). When >0, a random portion of items will have empty captions during
# caching/training. Defaults to 0.0 (disabled).
# caption_dropout_rate = 0.1

# Enable bucket-based training for different resolutions
enable_bucket = true

# Prevent upscaling of images to larger bucket sizes
bucket_no_upscale = true

# Preserve sequential clip ordering within batches (default: false).
# Recommended when using svi_y_motion_source = "replay_buffer".
sequence_batches = false

# Validate that cache item keys match sequence_batches_pattern when enabled.
# If any item key fails to match, training will abort early with a clear error.
sequence_batches_validate_names = false

# Log mismatched item keys without aborting (useful for diagnostics).
sequence_batches_report_names = false

# Regex used to group and order sequence batches (default matches _000-031 suffix).
# Example: sequence_batches_pattern = "_(\\d+)-(\\d+)$"
# sequence_batches_pattern = "_(\\d+)-(\\d+)$"

# Directory containing mask images for masked training
# When using masked training, ensure your dataset includes mask files alongside images/videos
# Masks should be binary: 0 = unmasked regions, 255 = masked regions
# For videos: masks can be per-frame or single mask applied to all frames       
# mask_path = "/path/to/mask/images"

# Optional concept metadata used for contrastive attention separation.
# concept_id defaults to dataset index if omitted.
# concept_id = 0

# concept_name is only for logging/debugging.
# concept_name = "concept_a"

# =============================================================================
# TRAINING DATASETS
# =============================================================================

# Image dataset
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "path/to/images"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/images/cache"

# Target resolution [width, height] (overrides general setting)
resolution = [512, 512]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Number of times to repeat this dataset per epoch (overrides general setting)  
num_repeats = 1

# Optional concept metadata (overrides general setting)
# concept_id = 0

# concept_name is only for logging/debugging.
# concept_name = "concept_a"

# Path to mask images for this dataset (overrides general setting)
# mask_path = "/path/to/mask/images" 

# Video dataset
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "path/to/videos"

# Path to cache directory
cache_directory = "path/to/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Optional concept metadata (overrides general setting)
# concept_id = 1

# concept_name is only for logging/debugging.
# concept_name = "concept_b"

# Supported frame extraction modes
# - head: take first window, requires setting target_frames. 
# - middle: take centered window, requires setting target_frames.
# - chunk: non-overlapping contiguous windows of length target_frame across the video, requires setting target_frames.
# - slide: sliding windows, requires setting target_frames and frame_stride (>=1), default is 1. Overlap (frames) = target_frame - frame_stride.
# - slide_end: sliding windows that guarantee the final window ends at the last frame. If the video is shorter than target_frame, produce a single full-length window. Requires target_frames and frame_stride (>=1). Overlap (frames) = target_frame - frame_stride.
# - uniform: fixed count of evenly spaced start positions, requires setting target_frames and frame_sample (>=1). If frame_sample == 1, it is auto-changed to "head" by the loader.
# - multiple_overlapping: minimally cover the video with windows (may overlap), end-aligned, requires setting target_frames.
# - adaptive: intelligently handles variable clip lengths - if shorter than target, takes whole clip; if longer, takes multiple fragments with possible overlap, max_frames caps the effective video length, requires setting target_frames.
# - uniform_adaptive: uniform sampling that accepts short videos - if shorter than target, takes whole clip; if longer, uses uniform sampling, max_frames caps the effective video length, requires setting target_frames and frame_sample.
# - epoch_slide: generates the same fragments as uniform_adaptive but only activates one fragment per video each epoch, cycling through frame_sample positions before repeating; useful for temporal coverage without multiplying dataset size.
# - full: use up to max_frames (rounded to N*4+1) starting at 0, requires setting target_frames to be non-empty.
# Uses max_frames to cap length (optional). If multiple target_frames are provided, one identical (0, use_frames) window is produced per entry; recommended target_frames = [1].
frame_extraction = "uniform"

# Extract different frame sequence lengths
target_frames = [1, 17, 33]

# Extract each target_frame sequence once
frame_sample = 1

# Stride between sliding windows; used only when frame_extraction = "slide"
# frame_stride = 1

# Cap total frames used by frame_extraction = "full"; final length is rounded to (4n+1)
# max_frames = 129

# When frame_extraction is adaptive/uniform_adaptive/epoch_slide and the video is shorter than target_frame,
# a fallback window is taken from the available frames. If that fallback would be shorter than this value,
# it is skipped (prevents tiny/1-frame fragments from entering training).
# Set to 0 to disable.
min_short_clip_frames = 5

# Source FPS
source_fps = 24

# Path to mask images/videos for this dataset
# mask_path = "/path/to/video/masks" 

# Regularization image dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "/path/to/your/regularization_images"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Marks regularization dataset
is_reg = true

# Regularization video dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "/path/to/your/regularization_videos"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# marks regularization dataset
is_reg = true

# =============================================================================
# VALIDATION DATASETS
# =============================================================================

# Image validation dataset
[[datasets.val]]

# Path to image directory (overrides general setting)
image_directory = "path/to/val/images"

# Path to cache directory
cache_directory = "path/to/val/images/cache"

# Target resolution [width, height]
resolution = [512, 512]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Video validation dataset
[[datasets.val]]

# Path to video directory (overrides general setting)
video_directory = "path/to/val/videos"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/val/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Use head sampling
frame_extraction = "head"

# Extract different frame sequence lengths
target_frames = [1, 25]

# Extract each target_frame sequence once
frame_sample = 1

