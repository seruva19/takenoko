# Takenoko Configuration File
# This file contains all possible parameters for training with default values and comments
# Remove the # at the beginning of lines to enable/configure parameters
# "⚠️" means the section is under construction and relevant functionality has not been fully tested yet

# =============================================================================
# TARGET TRAINING
# =============================================================================

# "wan21" or "wan22"
target_model = "wan21"

# =============================================================================
# CHECKPOINT SETTINGS
# =============================================================================

# DiT checkpoint path
dit = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp16.safetensors"

# use scaled fp8 for DiT
fp8_scaled = true

# Use fp8 for base model
fp8_base = true

# FP8 format for quantization ("e4m3" for E4M3FN or "e5m2" for E5M2)
# E4M3FN (e4m3): Better precision, requires RTX 40 series or newer (default)
# E5M2 (e5m2): Compatible with RTX 3000 series, slightly lower precision
# fp8_format = "e4m3"

# Scale input tensor format for scaled_mm ("e4m3" or "e5m2", None disables input scaling)
# Enables input tensor quantization for additional memory savings, may impact quality, only works with fp8_scaled = true
# scale_input_tensor = null

# Upcast quantization to float32 when computing scales and FP8 weights (improves accuracy, small VRAM cost during optimization)
# Only works with fp8_scaled = true
upcast_quantization = false

# Upcast linear matmul to float32
# Only works with fp8_scaled = true
upcast_linear = false

# Exclude feedforward layers from scaled_mm optimization (useful for WAN models where scaled_mm degrades quality)
# May help to prevent quality degradation in feedforward layers, only works with fp8_scaled = true
exclude_ffn_from_scaled_mm = true

# Optional: force a uniform cast of DiT weights ("fp16" or "bf16").
# When null, preserves default behavior (checkpoint dtype or fp8 paths).
# Ignored when fp8_scaled = true or mixed_precision_transformer = true.
# dit_cast_dtype = null

# Use mixed precision for transformer (preserves per-tensor dtypes from checkpoint)
# true for better dtype precision when using fp8_scaled
mixed_precision_transformer = true

# Enable FP16 accumulation
fp16_accumulation = false

# VAE checkpoint path
vae = "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors"

# data type for VAE, default is float16
# vae_dtype = "float16" 

# Cache VAE on CPU
# vae_cache_cpu = true

# text encoder (T5) checkpoint path
t5 = "https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth"

# Use fp8 for Text Encoder model
# fp8_t5 = true

# Directory to cache downloaded models
# model_cache_dir = "cache/models"

# Enable memory tracing
trace_memory = true

# =============================================================================
# TRAINING SETTINGS
# =============================================================================

# Training steps
max_train_steps = 2000

# Training epochs (overrides max_train_steps if specified)
# max_train_epochs = 10

# Max num workers for DataLoader (lower is less main RAM usage, faster epoch start and slower data loading)
max_data_loader_n_workers = 2

# Persistent DataLoader workers (useful for reduce time gap between epoch, but may use more memory)
persistent_data_loader_workers = true

# Random seed for training
seed = 20250811

# Enable gradient checkpointing
# Reduces activation memory by recomputing during backward; slower but lower VRAM
gradient_checkpointing = true

# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps = 1

# Mixed precision training (choices: "no", "fp16", "bf16")
mixed_precision = "fp16"

# Use broadcasted per-sample time embedding for Wan 2.2/FVDM (L=1 per sample),
# instead of per-token embeddings. Off by default for maximal expressiveness.
broadcast_time_embed = false

# Lean attention math (experimental). When true, Wan 2.2 blocks keep computations
# in the current compute dtype (bf16/fp16) to reduce large fp32 intermediates
# and memory use. Original numerics are preserved when false.
lean_attn_math = false

# When using lean attention math, control compute dtype policy:
# true  = default to fp32 compute unless force-fp16 is enabled
# false = default to input dtype (fp16/bf16) unless force-fp16 is enabled
lean_attention_fp32_default = false

# Lower precision attention (experimental). When true, Wan 2.2 blocks use lower precision
# for attention calculations to reduce large fp32 intermediates and memory use.
# Original numerics are preserved when false.
lower_precision_attention = false

# Use Wan 2.1 style modulation for Wan 2.2
simple_modulation = false

# Selective torch.compile of core modules for speed; safe no-op unless enabled.
# Mutually exclusive with dynamo_backend (keep dynamo_backend = "NO" to use this).
# Uses torch.compile with configurable backends (eager, inductor, etc.)
optimized_torch_compile = false

# Compile parameters for optimized_torch_compile. Format:
# compile_args = ["BACKEND", "MODE", DYNAMIC, "FULLGRAPH"]
# - BACKEND: "eager" (default, most compatible) or "inductor" (requires Triton + C++ compiler)
# - MODE: "default" | "max-autotune" | "reduce-overhead" (torch dependent)
# - DYNAMIC: true|false|"auto" (auto lets torch decide)
# - FULLGRAPH: "True"|"False" (string for clarity; case-insensitive)
# 
# Backend selection logic:
# 1. If "eager" is set, use eager backend
# 2. If "inductor" is set but Triton/C++ compiler not available, fallback to eager
# 3. If "inductor" is set and Triton/C++ compiler available, use inductor
compile_args = ["inductor", "default", "auto", "False"]

# Compute RoPE multipliers on-the-fly instead of using
# cached per-(F,H,W) precomputation. Off by default for speed.
rope_on_the_fly = false

# RoPE variant for positional embeddings: "default" (standard) or "comfy" (Comfy-style; enables advanced ND RoPE)
rope_func = "default"

# Use float32 precision for RoPE calculations instead of float64 (memory optimization, minimal accuracy loss)
rope_use_float32 = false

# =============================================================================
# OPTIMIZER SETTINGS
# =============================================================================

# Optimizer to use: AdamW (default), AdamW8bit, AdaFactor. Also, you can use any optimizer by specifying
# a fully-qualified class path (e.g., 'torch.optim.AdamW', 'bitsandbytes.optim.AdEMAMix8bit').
# Resolved dynamically in optimizer manager with provided optimizer_args.
optimizer_type = "adamw8bit"

# additional arguments for optimizer (like "weight_decay=0.01 betas=0.9,0.999 ...")
# optimizer_args = ["weight_decay=0.01", "betas=0.9,0.999"] 

# Learning rate
learning_rate = 2.0e-5

# Max gradient norm, 0 for no clipping
max_grad_norm = 1.0

# =============================================================================
# LEARNING RATE SCHEDULER
# =============================================================================

# Scheduler type
# - "constant": fixed LR for all steps (must have lr_warmup_steps = 0)
# - "constant_with_warmup": linear warmup to base LR, then hold constant
# - "linear": linear warmup, then linear decay to 0 over training steps
# - "cosine": linear warmup, then cosine decay to 0 over training steps
# - "cosine_with_restarts": warmup then cosine with restarts; uses lr_scheduler_num_cycles
# - "polynomial": warmup then polynomial decay; uses lr_scheduler_power
# - "inverse_sqrt": warmup then 1/sqrt(t) decay; uses lr_scheduler_timescale (defaults to lr_warmup_steps)
# - "cosine_with_min_lr": warmup then cosine decay to (min_lr_ratio * base LR); uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "warmup_stable_decay": warmup -> flat (stable) -> decay; requires lr_warmup_steps and lr_decay_steps; uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "piecewise_constant": step-wise schedule (from diffusers); requires lr_scheduler_args with step_rules
# - "adafactor:<initial_lr>": only valid with Adafactor optimizer (relative_step=True); no warmup/decay used
# All schedulers except "constant" require lr_warmup_steps to be provided (0 is allowed)
# "linear" and "cosine" ignore lr_decay_steps; "warmup_stable_decay" and some advanced schedules require lr_decay_steps > 0
lr_scheduler = "constant"

# Number of warmup steps (int) or fraction of total steps (float in 0..1)
# lr_warmup_steps and lr_decay_steps accept int (steps) or float (0<value<1) as a fraction of total optimizer steps (max_train_steps × num_processes)
lr_warmup_steps = 0

# Number of decay steps for schedulers that support it (int) or fraction (float in 0..1, ratio of train steps)
lr_decay_steps = 0

# Cycles: used by cosine_with_restarts; for cosine_with_min_lr and warmup_stable_decay, half of this value is used internally
lr_scheduler_num_cycles = 1

# Exponent for polynomial scheduler
lr_scheduler_power = 1.0

# Timescale for inverse_sqrt; if unset, defaults to lr_warmup_steps
# lr_scheduler_timescale = 0.0 

# Minimum LR ratio (final_LR = base_LR * ratio) for cosine_with_min_lr and warmup_stable_decay
# lr_scheduler_min_lr_ratio = 0.0 

# Use a custom scheduler class (overrides lr_scheduler above). Either a torch scheduler name (e.g. "OneCycleLR")
# or a fully-qualified import path (e.g. "torch.optim.lr_scheduler.StepLR"), kwargs come from lr_scheduler_args.
lr_scheduler_type = ""

# Extra kwargs for the chosen scheduler as a list of key=value strings (parsed via Python literal_eval)
# examples: lr_scheduler_args = 'T_max=100', lr_scheduler_args = 'step_rules=[(100,0.1),(300,0.01)]'
lr_scheduler_args = ''

# =============================================================================
# NETWORK SETTINGS
# =============================================================================

# Network module to train: "networks.lora_wan", "networks.control_lora_wan", "networks.reward_lora", "networks.vae_wan", "lycoris.kohya", "networks.t_lora", "networks.moc_lora", "networks.singlora_wan"
# The module defines trainable adapters and is imported dynamically by the trainer.
network_module = "networks.lora_wan"

# network dimensions (depends on each network)
network_dim = 32

# alpha for LoRA weight scaling
network_alpha = 32

# pretrained weights for network
# network_weights = "path/to/network/weights.safetensors" 

# drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons)
# network_dropout = 0 

# additional arguments for network (key=value). Parsed into list and consumed by network module.
# Lycoris example is in wan21_lycoris.toml
# network_args = '' 

# LoRA+ (scaled lr on lora_up only):
# network_args = ["loraplus_lr_ratio=2.0"]

# LoRA-GGPO regularization (perturbs Linear LoRA weights during training):
# Parsed by `takenoko.create_args_from_config` into args.ggpo_sigma/ggpo_beta for the loss hooks.
# network_args = ["ggpo_sigma=0.03", "ggpo_beta=0.01"]

# T-LoRA (timestep-gated LoRA)
# Activated when network_module = "networks.t_lora".
# Parameters:
# - tlora_trainer_type: "lora" (default) for vanilla T-LoRA, "ortho_lora" for orthogonal LoRA
# - tlora_min_rank: minimum active rank at highest noise (int ≥ 0)
# - tlora_alpha: curvature for r(t) = ((T - t)/T)^alpha * (max_rank - min_rank) + min_rank (float ≥ 0)
# - tlora_boundary_timestep: boundary timestep for WAN-Video style expert partitioning (int in [0,1000])
# Notes:
# - Use include_patterns to target only attention projections if desired.
#   Example include pattern: "include_patterns=['.*self_attn.*(q|k|v|to_out).*','.*cross_attn.*(q|k|v|to_out).*']"
# Example vanilla T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875"
# ]

# Orthogonal T-LoRA (optional, preserves original TLora behavior when disabled)
# Parameters:
# - sig_type: initialization singulars to use {"last", "principal", "middle"}; default "last"
# - ortho_from_layer: if true, initialize q/p/lambda from the original Linear weight SVD; otherwise use random base SVD
# - ortho_reg_lambda: global regularization weight for orthogonality penalties (applies to both p and q if specific lambdas are not set)
# - ortho_reg_lambda_p: regularization weight for p-layer orthogonality (||P^T P - I||_F^2)
# - ortho_reg_lambda_q: regularization weight for q-layer orthogonality (||Q Q^T - I||_F^2)
# Notes:
# - Orthogonal parametrization supports Linear-based LoRA adapters; Conv2d LoRA is skipped gracefully.
# - Timestep rank-mask and standard LoRA multiplier are still applied.
# Example orthogonal T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=ortho_lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875",
#   "sig_type=last",
#   "ortho_from_layer=true",
#   "ortho_reg_lambda_p=1e-3",
#   "ortho_reg_lambda_q=1e-3",
# ]

# MoC-LoRA (Mixture of Contexts for Long Video Generation)
# Activated when network_module = "networks.moc_lora".
# Parameters:
# - moc_chunk_size: chunk size for MoC attention (int ≥ 1024)
# - moc_top_k: top-k for MoC attention (int ≥ 1)
# - moc_progressive_sparsify: enable progressive sparsification for MoC attention (bool)
# - moc_enable_causality: enable causality for MoC attention (bool)
# - moc_context_dropout: dropout for MoC attention (float in [0.0, 1.0])
# - moc_implementation: implementation mode ("original" for 100% original accuracy, "optimized" for speed)
# - moc_max_layers: limit MoC to first N layers for memory control (int, optional)
# - moc_max_modules: limit MoC to first N modules per layer for memory control (int, optional)
# Example MoC-LoRA configuration:
# network_args = [
#   "moc_chunk_size=1280",
#   "moc_top_k=5",
#   "moc_progressive_sparsify=false",
#   "moc_enable_causality=true",
#   "moc_context_dropout=0.0",
#   "moc_implementation=optimized",
#   "moc_max_layers=2",
#   "moc_max_modules=4"
# ]

# SingLoRA-specific parameters
#network_args = [
#  "ramp_up_steps=1000",
#  "init_method=kaiming_uniform",
#  "use_rslora=false",
#  "dropout=null",
#]

# arbitrary comment string stored in metadata
# training_comment = "trained with Takenoko" 

# automatically determine dim (rank) from network_weights
# dim_from_weights = true 

# show detailed information about trainable LoRA parameters and network structure
# verbose_network = true 

# scale the weight of each key pair to help prevent overtraing via exploding gradients. (1 is a good starting point)
# scale_weight_norms = 1.0 

# network weights to merge into the model before training
# base_weights = "path/to/base1.safetensors" 

# multiplier for base weights
# base_weights_multiplier = 1.0 

# use lycoris for inference
# lycoris = true 

# Weight for regularization loss (1.0 = normal, higher = stronger regularization)
# This applies to regularization datasets (is_reg=true) vs main dataset
# NOT related to masked training's "masked_prior_preservation_weight"
prior_loss_weight = 1.0

# =============================================================================
# TIMESTEP AND FLOW MATCHING SETTINGS
# =============================================================================

# Method to sample timesteps
# Available methods: sigma, uniform, sigmoid, shift, flux_shift, logsnr, qwen_shift, qinglong_flux, qinglong_qwen, logit_normal,
# bell_shaped, half_bell, lognorm_blend, lognorm_continuous_blend, enhanced_sigmoid, fopp, content, style, content_style_blend
timestep_sampling = "shift"

# Shift amount for discrete timestep sampling (used with 'shift' timestep_sampling)
discrete_flow_shift = 3.0

# Scale factor for sigmoid timestep sampling (used by: sigmoid, shift, flux_shift, qwen_shift, logit_normal, enhanced_sigmoid, qinglong_*)
sigmoid_scale = 1.0

# Bias for enhanced_sigmoid timestep sampling (only used when timestep_sampling = "enhanced_sigmoid")
sigmoid_bias = 0.0

# Center of bell_shaped distribution in [0.0, 1.0]; 0.95 ~ timestep 950
# Only used when timestep_sampling = "bell_shaped"
bell_center = 0.5

# Std (spread) of bell_shaped distribution; smaller => sharper peak
# Only used when timestep_sampling = "bell_shaped"
bell_std = 0.2

# Alpha mix ratio for lognorm_blend and lognorm_continuous_blend (fraction of samples from LogNormal vs. linear)
# Only used when timestep_sampling = "lognorm_blend" or "lognorm_continuous_blend"
lognorm_blend_alpha = 0.75

# Blend ratio for content_style_blend (fraction of content vs style sampling)
# Only used when timestep_sampling = "content_style_blend"
# 0.0 = pure style (later timesteps), 1.0 = pure content (earlier timesteps)
content_style_blend_ratio = 0.5

# Weighting scheme for timestep distribution: logit_normal, mode, cosmap, sigma_sqrt, none.
# This key interacts with two independent things: sampling distribution (which timestep indices are drawn)
# and loss weighting (how much each drawn timestep contributes to the loss)
# Sampling distribution: only when timestep_sampling = "sigma".
# logit_normal (uses logit_mean/logit_std) or mode (uses mode_scale) shape the sampling density.
# none = uniform density. sigma_sqrt and cosmap do NOT influence sampling.
# For all other samplers, weighting_scheme does not change which timesteps are sampled.
# Loss weighting: for any sampler, ONLY sigma_sqrt or cosmap enable SD3-style per-index loss weights; other values imply unit weights.
weighting_scheme = "none"

# For logit-based transforms, sampling density when timestep_sampling="sigma" and weighting_scheme="logit_normal"
logit_mean = 0.0

# For logit-based transforms, mapping distribution when timestep_sampling="logsnr" (and the logsnr segment in qinglong_* variants)
logit_std = 1.0

# Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`
mode_scale = 1.29

# set minimum time step for training (0~999, default is 0)
min_timestep = 0

# Set maximum time step for training (1~1000, default is 1000)
max_timestep = 1000

# Enable pre-computed timestep distribution (default: false)
# Uses a fixed, quantized distribution of t (reproducible) and takes precedence over num_timestep_buckets.
# Supported for most timestep_sampling methods; unsupported methods fall back to on-the-fly sampling.
# Distribution is sliced to [min_timestep, max_timestep] before sampling. Dataset-provided batch['timesteps']
# are ignored when enabled.
use_precomputed_timesteps = true

# Number of buckets for the precomputed distribution (default: 10000)
# Larger values = finer quantization of the target distribution. Key: 'precomputed_timestep_buckets'.
precomputed_timestep_buckets = 10000

# Optional: override the base area used by flux/qwen/qinglong precompute (auto-inferred from height/width/latents if unset)
# Helps make precomputed mid-shift match runtime behavior when image sizes are known ahead of time.
# precomputed_midshift_area = 1024.0

# Preserve distribution shape (default: false)
# If false: sampled t in [0,1] is linearly scaled into [min_timestep, max_timestep] (fast, but warps the shape).
# If true: rejection sampling preserves the original distribution shape inside the range (slower, shape-faithful).
# Applies after the sampling method (uniform/sigmoid/shift/flux_shift/logsnr/etc) or mapped presampled values.
# When dataset provides per-batch uniform t (see num_timestep_buckets), those are used first for mapping.
# With precomputed timesteps enabled, the global distribution is sliced to min/max first; constraints still apply.
preserve_distribution_shape = false

# Number of timestep buckets for training (default: None)
# Dataset-side per-epoch uniform stratification of u in [0,1] to improve coverage on small datasets.
# Used only when use_precomputed_timesteps = false. Samples are mapped through the chosen timestep_sampling,
# then min/max (and preserve_distribution_shape) are applied. Implemented in the dataset; batches may include
# a per-item 'timesteps' list. Ignored when use_precomputed_timesteps = true. Set >= 2 to enable.
# num_timestep_buckets = 10

# Skip extra in-range constraint
skip_extra_timestep_constraint = true

# Epsilon for extra in-range constraint
timestep_constraint_epsilon = 1e-5

# Round training timesteps to the nearest integer schedule step
round_training_timesteps = true

# Fast rejection sampling
fast_rejection_sampling = false

# When fast_rejection_sampling = true, control the candidate oversampling factor and max iterations
# rejection_overdraw_factor = 4.0

# When fast_rejection_sampling = true, control the max iterations
# rejection_max_iters = 10

# show timesteps: image, console
# Visualizes the actual distribution produced by the current sampling+constraints path. 
# show_timesteps = 'console' 

# =============================================================================
# LOSS SETTINGS
# =============================================================================

# Base loss function type: "mse", "l1", "huber", "pseudo_huber", "pseudo_huber_scheduled"
# - "mse": Standard Mean Squared Error loss (default)
# - "l1": L1 (Mean Absolute Error) loss
# - "huber": Standard Huber loss with fixed delta
# - "pseudo_huber": Pseudo-Huber loss with fixed c parameter
# - "pseudo_huber_scheduled": Pseudo-Huber loss with adaptive c parameter scheduling
loss_type = "mse"

# Pseudo-Huber c parameter (controls transition between L2 and L1 behavior)
# Smaller values (0.1-0.5) = more L1-like (robust to outliers)
# Larger values (1.0-2.0) = more L2-like (smooth gradients)
pseudo_huber_c = 0.5

# Scheduling type for adaptive c parameter (when loss_type = "pseudo_huber_scheduled")
# Options: "linear", "cosine", "exponential"
pseudo_huber_schedule_type = "linear"

# Minimum c value for scheduling (start of training)
pseudo_huber_c_min = 0.1

# Maximum c value for scheduling (end of training)
pseudo_huber_c_max = 1.0

# =============================================================================
# ⚠️ DISPERSIVE LOSS SETTINGS
# =============================================================================

# Enable dispersive loss for improved generation quality and speed.
enable_dispersive_loss = false

# The lambda (λ) hyperparameter that controls the strength of the dispersive loss. The paper found 0.05 to be a robust value.
dispersive_loss_lambda = 0.05

# Temperature parameter (> 0). Smaller values sharpen repulsion.
dispersive_loss_tau = 0.5

# None disables extraction, non-negative integer selects a block index
dispersive_loss_target_block = 2

# "l2_sq" or "cosine"
dispersive_loss_metric = "l2_sq"

# Optional: pool spatial tokens per frame before dispersion ("none" or "frame_mean")
dispersive_loss_pooling = "none"

# =============================================================================
# ⚠️ ADAPTIVE TIMESTEP SAMPLING
# =============================================================================

# Enable adaptive timestep sampling (default: false)
# Focuses training on timesteps that are most critical for video generation quality
enable_adaptive_timestep_sampling = false

# Core parameters
# How much to focus on important timesteps (1.0-5.0)
adaptive_focus_strength = 2.0

# Steps before analysis begins (100-2000)
adaptive_warmup_steps = 500

# Recent loss samples to analyze (100-5000)
adaptive_analysis_window = 1000

# Standard deviation multiplier for detection (0.5-3.0)
adaptive_importance_threshold = 1.5

# Steps between analysis updates (10-500)
adaptive_update_frequency = 100

# Timestep constraints
# Minimum important timesteps (10-500)
adaptive_min_timesteps = 50

# Maximum important timesteps (50-1000)
adaptive_max_timesteps = 200

# Video-specific features
# Enable video-specific categories
adaptive_video_specific = true

# Motion consistency weight (0.1-3.0)
adaptive_motion_weight = 1.0

# Detail preservation weight (0.1-3.0)
adaptive_detail_weight = 1.0

# Temporal coherence weight (0.1-3.0)
adaptive_temporal_weight = 1.0

# Enable Beta distribution sampling methodology (default: false)
# Uses paper's exact approach: neural network parameterized Beta distribution
adaptive_use_beta_sampler = false

# Feature selection size |S| for approximation (1-10)
adaptive_feature_selection_size = 3

# Sampler update frequency f_S (10-100)
adaptive_sampler_update_frequency = 100

# Enable separate neural network for timestep sampling (default: false)
adaptive_use_neural_sampler = false

# Initial Alpha parameter for Beta distribution (0.1-5.0)
adaptive_beta_alpha_init = 1.0

# Initial Beta parameter for Beta distribution (0.1-5.0)
adaptive_beta_beta_init = 1.0

# Neural sampler hidden layer size (16-256)
adaptive_neural_hidden_size = 64

# Enable loss-variance importance weighting (stable, default: true)
# This is our proven stable approach for production use
adaptive_use_importance_weighting = true

# Enable KL divergence RL learning (paper exact, default: false)
# This is the authors' exact methodology - more experimental
adaptive_use_kl_reward_learning = false

# Enable replay buffer for historical learning (default: false)
# Stores KL improvements across training steps for better feature selection
adaptive_use_replay_buffer = false

# Enable SelectKBest statistical feature selection (default: false)
# Requires scikit-learn, uses regression-based feature importance
adaptive_use_statistical_features = false

# Weight combination strategy ("fallback", "ensemble", "best")
# - fallback: Use first successful method (stable)
# - ensemble: Average all methods (balanced)
# - best: Choose method with highest focus (aggressive)
adaptive_weight_combination = "fallback"

# Replay buffer size for KL learning (10-1000)
adaptive_replay_buffer_size = 100

# RL learning rate for policy updates (1e-5 to 1e-3)
adaptive_rl_learning_rate = 1e-4

# Entropy coefficient for RL regularization (0.0-0.1)
adaptive_entropy_coefficient = 0.01

# KL reward update frequency (5-100)
adaptive_kl_update_frequency = 20

# Pure KL divergence-based feature selection for exact research replication (default: false)
# When enabled, automatically configures use_kl_reward_learning=true, use_beta_sampler=true, use_importance_weighting=false
adaptive_kl_exact_mode = false

# Enable comparative logging of KL vs importance approaches for research analysis (default: false)
# Logs performance comparison between different feature selection methods
adaptive_comparative_logging = false

# Enable full research mode (Algorithm 1 & 2) for exact research replication (default: false)
# When enabled, activates complete research methodology including policy gradient updates
# Requires diffusion object with q_posterior_mean_var and p_mean_var methods
adaptive_research_mode_enabled = false

# =============================================================================
# ⚠️ CONTRASTIVE FLOW MATCHING (ΔFM) SETTINGS
# =============================================================================

# Enable the enhanced contrastive objective for improved generation quality and speed.
enable_contrastive_flow_matching = false

# The lambda (λ) hyperparameter that controls the strength of the contrastive loss. The paper found 0.05 to be a robust value.
contrastive_flow_lambda = 0.05

# Enable class-conditioned negative sampling (recommended when labels are available)
# This ensures negative samples come from different classes, improving contrastive learning effectiveness
contrastive_flow_class_conditioning = true

# Skip contrastive loss computation on unconditional samples (useful for classifier-free guidance)
# This prevents unconditional samples from contributing to the contrastive objective
contrastive_flow_skip_unconditional = false

# Class index representing unconditional/null samples (required if skip_unconditional is true)
# Set this to the index used for unconditional generation in your dataset
# contrastive_flow_null_class_idx = null

# =============================================================================
# ⚠️ REPA (Representation Alignment) SETTINGS
# =============================================================================

# Set to false to disable
enable_repa = false

# Name of the pretrained visual encoder from the `timm` library. 
# DINOv2 models: "dinov2_vitb14", "dinov2_vitl14", "dinov2_vitg14".
# CLIP models: "clip_vitb32", "clip_vitl14".
# EfficientNet models: "efficientnet_b0", "efficientnet_b1".
# ResNet models: "resnet50", "resnet101".
repa_encoder_name = "dinov2_vitb14"

# The depth (layer index) of the diffusion transformer to align. The paper finds that aligning early layers (e.g., 8) is most effective.
repa_alignment_depth = 8

# The lambda (λ) hyperparameter that controls the strength of the REPA loss. The paper found 0.5 to be a robust value.
repa_loss_lambda = 0.5

# The similarity function to use for alignment. "cosine" is recommended. Options: "cosine", "nt_xent"
repa_similarity_fn = "cosine"

# =============================================================================
# ⚠️ FOPP SETTINGS
# =============================================================================

# Schedule type for FoPP: linear, cosine
fopp_schedule_type = "cosine"

# Number of timesteps for FoPP
fopp_num_timesteps = 1000

# Beta start for FoPP
fopp_beta_start = 0.0001

# Beta end for FoPP
fopp_beta_end = 0.002

# Random seed for FoPP sampling
fopp_seed = 1997

# =============================================================================
# ⚠️ NABLA SETTINGS
# =============================================================================

# Enable Nabla sparse attention
nabla_sparse_attention = false

# nabla-0.6_sta-11-3-3, nabla-0.6_sta-11-5-5
nabla_sparse_algo = "nabla-0.9_sta-11-24-24"

# =============================================================================
# OPTIMIZATION SETTINGS
# =============================================================================

# Use SDPA for CrossAttention (requires PyTorch 2.0)
sdpa = true

# Use FlashAttention for CrossAttention, requires FlashAttention (uv pip install your-flash-attn.whl)
# flash_attn = true 

# Use SageAttention, requires SageAttention
# sage_attn = true 

# Use xformers for CrossAttention, requires xformers
# xformers = true 

# Use FlashAttention 3 for CrossAttention
# flash3 = true 

# Use split attention for attention calculation (split batch size=1, affects memory usage and speed)
# split_attn = true 

# Number of blocks to swap in the model, max XXX
# blocks_to_swap = 20 

# Enable PyTorch CUDA allocator tuning
cuda_allocator_enable = true

# Max split size in MB for CUDA allocator
cuda_allocator_max_split_size_mb = 64

# Enable expandable segments for CUDA allocator
cuda_allocator_expandable_segments = false

# =============================================================================
# DUAL MODEL TRAINING SETTINGS
# =============================================================================

# Enable dual model training
# Trains a single LoRA against two base DiTs (low/high noise) by swapping base weights on-the-fly
enable_dual_model_training = false

# DiT checkpoint path for high noise model
dit_high_noise = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp16.safetensors"

# Timestep boundary for dual model training (0.0 to 1.0)
# Determines the split between low/high noise regimes
timestep_boundary = 0.875

# Offload inactive DiT model to CPU
# Keeps only the active model on GPU; inactive state_dict resides on CPU to save VRAM
offload_inactive_dit = true

# Allow mixed block swap offload
allow_mixed_block_swap_offload = false

# Dual-mode timestep bucketing strategy (interaction of per-epoch timestep buckets with high/low boundary)
# "hybrid" (default): use presampled bucketed t if it matches the boundary side; otherwise resample on-demand
# "presampled": always use dataset-presampled uniform t as-is (may cross boundary)
# "on_demand": ignore presampled t; draw fresh uniform each retry until boundary side matches (up to max retries)
# "strict_clamp": minimally adjust a presampled t to be just inside the target boundary side
dual_timestep_bucket_strategy = "on_demand"

# Maximum retries for on-demand resampling
dual_timestep_bucket_max_retries = 100

# Epsilon for boundary matching
dual_timestep_bucket_eps = 1e-4

# =============================================================================
# ⚠️ CONTROLNET TRAINING SETTINGS
# =============================================================================

# Enable ControlNet training mode
enable_controlnet = false

# Weight for the ControlNet loss
controlnet_weight = 1.0

# Stride for the ControlNet loss
controlnet_stride = 1

# Max gradient norm for ControlNet
controlnet_max_grad_norm = 1.0

# ControlNet configuration
controlnet = {}

# =============================================================================
# ⚠️ CONTROL LORA TRAINING SETTINGS
# =============================================================================

# Enable control LoRA training mode
enable_control_lora = false

# Type of control LoRA: "tile", "canny", "depth", etc.
control_lora_type = "tile"

# Preprocessing method for control signal: "blur", "edge", "depth", etc.
control_preprocessing = "blur"

# Kernel size for Gaussian blur preprocessing
control_blur_kernel_size = 15

# Sigma for Gaussian blur preprocessing
control_blur_sigma = 4.0

# Scale factor for control signal strength
control_scale_factor = 1.0

# Multiplier to learning rate for the input patch_embedding layer if training control lora
input_lr_scale = 5.0

# Dimension index for concatenation of control signal
control_concatenation_dim = 0

# Suffix to append to media file name to get the control file
control_suffix = "_control"

# Add noise to the control latents, at a random strength up to this amount
control_inject_noise = 0.1

# Save control videos created on the fly to disk
save_control_videos = true

# Directory to save control videos for debugging
control_video_save_dir = "tmp/control_videos"

# =============================================================================
# ⚠️ REWARD LORA TRAINING SETTINGS
# =============================================================================

# Enable reward LoRA training mode
enable_reward_lora = false

# List of prompts to use for reward training
reward_prompts = [
  "a red spaceship flying in the sky",
  "a dog running in a park",
]

# Batch size for reward training
reward_train_batch_size = 1

# Height of the sample video for reward training
reward_train_sample_height = 256

# Width of the sample video for reward training
reward_train_sample_width = 256

# alias: reward_num_frames is also accepted
reward_video_length = 81

# Number of inference steps for reward training
reward_num_inference_steps = 50

# Guidance scale for reward training
reward_guidance_scale = 6.0

# Number of decoded latents for reward training
reward_num_decoded_latents = 1

# Number of steps to run validation on
reward_validation_steps = 10000

# Reward function to use for reward training
reward_fn = "HPSReward"

# JSON string
reward_fn_kwargs = '{"version":"v2.1"}'

# Enable backprop for reward training
reward_backprop = true

# Backprop strategy to use for reward training, choices: "last", "tail", "uniform", "random"
reward_backprop_strategy = "tail"

# Number of steps to backprop for reward training
reward_backprop_num_steps = 5

# List of steps to backprop for reward training
reward_backprop_step_list = [45, 46, 47, 48, 49]

# Random start step for reward training
reward_backprop_random_start_step = 0

# Random end step for reward training
reward_backprop_random_end_step = 50

# Stop backprop at the latent model input gradient
reward_stop_latent_model_input_gradient = false

# =============================================================================
# ⚠️ DOP (DIFFERENTIAL OUTPUT PRESERVATION) SETTINGS
# =============================================================================

# Enable DOP regularization
diff_output_preservation = false

# The trigger word/phrase in captions to be replaced for DOP. This must exactly match the trigger in your dataset captions
diff_output_preservation_trigger_word = "ohwx"

# Generic class prompt to replace the trigger word for DOP. This is what the trigger will be replaced with during preservation
diff_output_preservation_class = "a photo of a person"

# Multiplier for the DOP loss (default: 1.0). Higher values = stronger preservation effect
diff_output_preservation_multiplier = 1.0

# =============================================================================
# ⚠️ FLUXFLOW SETTINGS
# =============================================================================

# enable FLUXFLOW temporal augmentation during training
enable_fluxflow = false

# FLUXFLOW perturbation mode (choices: frame, block)
fluxflow_mode = "frame"

# Ratio of frames to shuffle in frame mode (0.0 to 1.0)
fluxflow_frame_perturb_ratio = 0.25

# Size of contiguous frame blocks in block mode
fluxflow_block_size = 4

# Beta: Ratio of blocks to reorder in block mode (0.0 to 1.0), NOT probability!
fluxflow_block_perturb_prob = 0.5

# Dimension index for frames/time in batched video tensor
fluxflow_frame_dim_in_batch = 2

# =============================================================================
# ⚠️ CONTEXT AS MEMORY SETTINGS
# =============================================================================

# Enable context-as-memory features for scene-consistent long video generation
# When enabled, stores previously generated frames as memory for improved consistency
ctxmem_enabled = false

# Basic context settings
# Number of context frames to use during generation
ctxmem_context_size = 4

# Maximum frames to keep in memory buffer
ctxmem_max_memory_frames = 100

# Update memory every N training steps
ctxmem_memory_update_frequency = 10

# Frame selection strategies:
# - "recent": Select most recent frames (fastest, good baseline)
# - "fov_overlap": Select based on FOV overlap with camera poses (paper's method)
# - "semantic": Select based on visual similarity (experimental)
# - "mixed": Combine recent and semantic selection (experimental)
ctxmem_frame_selection_strategy = "semantic"

# Similarity threshold for semantic selection (0.0 to 1.0)
ctxmem_semantic_similarity_threshold = 0.7

# FOV-based selection settings
# Enable FOV-based frame selection using camera poses
ctxmem_use_fov_selection = false

# Default field of view in degrees (from paper: 52.67)
ctxmem_default_fov = 52.67

# Minimum overlap threshold for FOV-based selection (0.0 to 1.0)
ctxmem_fov_overlap_threshold = 0.1

# Maximum camera distance for FOV overlap consideration (meters)
ctxmem_max_camera_distance = 10.0

# Context conditioning method:
# - "concatenation": Concatenate context frames along temporal dimension (recommended)
ctxmem_context_conditioning_method = "concatenation"

# Weight for context conditioning in loss computation
ctxmem_context_weight = 1.0

# Variable length context support (experimental)
ctxmem_use_variable_length_context = false

# Training enhancements
# Enable progressive context training (start without context, gradually enable)
ctxmem_progressive_context_training = true

# Context warmup steps (when progressive training is enabled)
ctxmem_context_warmup_steps = 500

# Randomly drop context frames during training for regularization
ctxmem_context_dropout_rate = 0.1

# Temporal consistency loss weight (0.0 to disable)
ctxmem_temporal_consistency_loss_weight = 0.1

# Performance optimizations
# Enable caching of frame embeddings for similarity computation
ctxmem_use_context_caching = true

# Enable asynchronous memory updates (experimental)
ctxmem_async_memory_updates = false

# =============================================================================
# ⚠️ FVDM SETTINGS
# =============================================================================

# Enable Frame-Aware Video Diffusion Model (FVDM) training
enable_fvdm = false

# Probabilistic Timestep Sampling Strategy (PTSS) settings
# Fixed PTSS probability
fvdm_ptss_p = 0.2

# Adaptive PTSS: dynamically adjust probability during training
fvdm_adaptive_ptss = true

# Starting probability (higher for exploration) 
fvdm_ptss_initial = 0.3    

# Ending probability (lower for stability)  
fvdm_ptss_final = 0.1      

# Steps before adaptation begins
fvdm_ptss_warmup = 1000    

# FVDM training enhancements
# Weight for temporal consistency loss (0.0 = disabled)
fvdm_temporal_consistency_weight = 0.1    

# Weight for frame diversity regularization (0.0 = disabled)
fvdm_frame_diversity_weight = 0.0         

# Use with AdaptiveTimestepManager if enabled
fvdm_integrate_adaptive_timesteps = false 

# FVDM evaluation and logging
# Compute and log temporal coherence metrics
fvdm_eval_temporal_metrics = true        

# Steps between FVDM metric evaluation and logging
fvdm_eval_frequency = 1000               

# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Run validation every N steps (in addition to end-of-epoch validation)
validate_every_n_steps = 500

# Enable validation at the end of each epoch (default: false)
validate_on_epoch_end = true

# Validation timesteps mode:
# - "fixed": use the list in validation_timesteps every time (default)
# - "random": draw validation_timesteps_count integers uniformly within [validation_timesteps_min, validation_timesteps_max]
# - "jitter": for each base timestep in validation_timesteps, draw a timestep within ±validation_timesteps_jitter, clamped to [validation_timesteps_min, validation_timesteps_max]
# - "training_distribution": use the same timestep sampling distribution as training (uniform, sigmoid, content, style, etc.)
validation_timesteps_mode = "fixed"

# Base list of timesteps for validation.
# - When validation_timesteps_mode = "fixed": use exactly this list every validation
# - When validation_timesteps_mode = "jitter": apply ±validation_timesteps_jitter around each base value
# - When validation_timesteps_mode = "random": ignored (random draws are used instead)
# - When validation_timesteps_mode = "training_distribution": ignored (uses training distribution instead)
# Example: '100,300,500,700,900'
validation_timesteps = "100,300,500,700,900"

# For mode == "random" or "training_distribution": number of unique timesteps to draw per validation
validation_timesteps_count = 4

# Global min clamp for random and jitter modes. If omitted, falls back to min_timestep or 0.
validation_timesteps_min = 0

# Global max clamp for random and jitter modes. If omitted, falls back to max_timestep or 1000.
validation_timesteps_max = 1000

# For mode == "jitter": radius to jitter each base timestep (in steps). Example: 50 means draw in [t-50, t+50].
validation_timesteps_jitter = 0

# Use unique noise per batch for proper validation (recommended: true for reliable metrics, false for legacy behavior)
use_unique_noise_per_batch = true

# Compute SSIM-based perceptual SNR on tiny subsample
enable_perceptual_snr = false

# Max samples per validation step for perceptual SNR
perceptual_snr_max_items = 4

# Temporal SSIM (adjacent-frame) validation
enable_temporal_ssim = false

# Per-step cap for temporal SSIM validation
temporal_ssim_max_items = 2

# Subsample frames for speed
temporal_ssim_frame_stride = 1

# LPIPS validation
enable_lpips = false

# Per-step cap for LPIPS validation
lpips_max_items = 2

# Network for LPIPS validation (vgg|alex|squeeze)
lpips_network = "vgg"

# Subsample frames for speed
lpips_frame_stride = 8

# Temporal LPIPS (adjacent-frame) validation
enable_temporal_lpips = false

# Per-step cap for temporal LPIPS validation
temporal_lpips_max_items = 2

# Subsample frames for speed
temporal_lpips_frame_stride = 2

# Flow-warped SSIM (RAFT)
enable_flow_warped_ssim = false

# Model for flow-warped SSIM validation (torchvision_raft_small|torchvision_raft_large)
flow_warped_ssim_model = "torchvision_raft_small"

# Per-step cap for flow-warped SSIM validation
flow_warped_ssim_max_items = 2

# Subsample frames for speed
flow_warped_ssim_frame_stride = 2

# FVD (Fréchet Video Distance)
enable_fvd = false

# Model for FVD validation (torchvision_r3d_18|reference_i3d)
# - torchvision_r3d_18: Fast approximation using R3D-18 (current default)
# - reference_i3d: Canonical implementation using I3D from TensorFlow Hub (more accurate but slower)
fvd_model = "torchvision_r3d_18"

# Per-step cap for FVD validation
fvd_max_items = 2

# Length of each FVD clip
fvd_clip_len = 16

# Subsample frames for speed
fvd_frame_stride = 2

# VMAF (requires ffmpeg with libvmaf installed)
enable_vmaf = false

# Path to VMAF model (optional, empty = ffmpeg default model)
vmaf_model_path = ""

# Per-step cap for VMAF validation
vmaf_max_items = 1

# Length of each VMAF clip
vmaf_clip_len = 16

# Subsample frames for speed
vmaf_frame_stride = 2

# Path to ffmpeg executable
vmaf_ffmpeg_path = "ffmpeg"

# =============================================================================
# ⚠️ OPTICAL FLOW LOSS SETTINGS
# =============================================================================

# Enable RAFT-based optical flow loss for motion stability (experimental)
enable_optical_flow_loss = false

# Weight for the optical flow loss term (set >0 to enable)
lambda_optical_flow = 0.01

# =============================================================================
# ⚠️ MASKED TRAINING SETTINGS
# =============================================================================

# Enable masked training with prior preservation
# Master toggle - MUST be true to enable the entire feature
# Note: Works with DOP (diff_output_preservation) - DOP's prior will be masked if both enabled
use_masked_training_with_prior = false

# Probability of removing mask during training (0.0 = never remove, 1.0 = always remove)
# Helps prevent overfitting to mask boundaries by randomly training on full images
unmasked_probability = 0.1

# Minimum weight for unmasked regions [0-1] 
# 0.1 = light masking (10% weight to unmasked), 0.01 = heavy masking (1% weight to unmasked)
unmasked_weight = 0.1

# Weight for preserving base model predictions in unmasked areas [0-inf] 
# This controls spatial masking within individual images/videos
# 0.0 = disabled, 0.3-0.8 = typical range, higher = stronger preservation of original model
masked_prior_preservation_weight = 0.0

# Normalize loss by masked area size (helps balance training across different mask sizes)
normalize_masked_area_loss = false

# Method for resizing masks to match loss dimensions ("area", "bilinear", "nearest")
# "area" provides cleanest boundaries, recommended for most use cases
mask_interpolation_mode = "area"

# Integration Parameters
# Enable prior prediction computation (required for prior preservation)
enable_prior_computation = true

# Method for computing prior predictions ("lora_disabled" recommended)
# "lora_disabled" = disable LoRA weights when computing base model predictions
prior_computation_method = "lora_disabled"

# Video-specific Parameters
# Weight for temporal consistency loss in videos [0-inf] (0 = disabled)
# 0.1-0.5 typical range, encourages smooth mask transitions between frames
temporal_consistency_weight = 0.0

# Temporal consistency computation method ("adjacent" or "all_pairs")
# "adjacent" = compare adjacent frames only (faster)
# "all_pairs" = compare all frame pairs (more thorough but slower)
frame_consistency_mode = "adjacent"

# =============================================================================
# TREAD SETTINGS
# =============================================================================

# Enable TREAD routing for training
enable_tread = false

# TREAD mode: "full", "frame_contiguous", "frame_stride"
# 'full' is used with tread_config_route parameters
# 'frame_contiguous'/'frame_stride' are used with 'tread' parameter
tread_mode = "full"

# Each route is a semicolon-separated string of key=value pairs. Keys: selection_ratio (0-1), start_layer_idx, end_layer_idx (negative allowed)
tread_config_route1 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=-2"
# tread_config_route2 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=8"
# tread_config_route3 = "selection_ratio=0.25; start_layer_idx=9; end_layer_idx=11"
# tread_config_route4 = "selection_ratio=0.35; start_layer_idx=12; end_layer_idx=15"
# tread_config_route5 = "selection_ratio=0.25; start_layer_idx=16; end_layer_idx=23"
# tread_config_route6 = "selection_ratio=0.1; start_layer_idx=24; end_layer_idx=-2"

# Simplified frame routing block:
# Use tread_mode to select contiguous vs stride
# keep_ratio: fraction of frames to keep within the routed band
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.6 }

# Enable strict sanity checks when slicing per-token time projections during
# frame-based routing (verifies index bounds, shapes, and dtypes). Off by default.
strict_e_slicing_checks = false

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

# directory to output trained model
output_dir = "output/wan21_lora"

# base name of trained model file 
output_name = "wan21_lora"

# save training state additionally (including optimizer states etc.) when saving model
save_state = true

# save checkpoint every N steps
save_every_n_steps = 50

# save training state (including optimizer states etc.) on train end even if --save_state is not specified
# save_state_on_train_end = false 

# automatically resume training from latest checkpoint
auto_resume = true

# saved state to resume training
# resume = "path/to/resume/state" 

# fp16 training including gradients (temporary disabled)
# full_fp16 = false 

# bf16 training including gradients (temporary disabled)
# full_bf16 = false 

# save checkpoint every N epochs
# save_every_n_epochs = 1 

# save last N checkpoints when saving every N epochs (remove older checkpoints)
# save_last_n_epochs = 10 

# save last N checkpoints of state (overrides the value of --save_last_n_epochs)
# save_last_n_epochs_state = 3 

# save checkpoints until N steps elapsed (remove older checkpoints if N steps elapsed)
# save_last_n_steps = 5000 

# save states until N steps elapsed (overrides --save_last_n_steps, overrides --save_last_n_steps)
# save_last_n_steps_state = 5000 

# allow resuming training with a different dataset (e.g., for domain adaptation or transfer learning)
# allow_dataset_change = true 

# reset epoch/step calculations when resuming with a different dataset (recommended for proper training behavior)
# reset_training_state = true 

# skip dataset compatibility validation entirely (advanced users only - use with caution)
# ignore_dataset_compatibility = true 

# =============================================================================
# METADATA SETTINGS
# =============================================================================

# embed the original config file content in safetensors metadata (default: True)
embed_config_in_metadata = true

# do not save metadata in output model
# no_metadata = true 

# title for model metadata (default is output_name)
# metadata_title = "" 

# author name for model metadata
# metadata_author = "" 

# description for model metadata
# metadata_description = "" 

# license for model metadata
# metadata_license = "" 

# tags for model metadata, separated by comma
# metadata_tags = "" 

# =============================================================================
# LOGGING SETTINGS
# =============================================================================

# Enhanced progress bar with additional metrics (learning rate, epoch, memory usage, etc.), set to false to use simple progress bar if you encounter any issues
enhanced_progress_bar = true

# Log training configuration
log_config = true

# Enable logging and output TensorBoard log to this directory
logging_dir = "logs/wan21_lora"

# Logging tool to use (only tensorboard is supported)
log_with = "tensorboard"

# Set logging level to DEBUG to see FluxFlow messages
logging_level = "INFO"

# Append small emoji hints to TensorBoard tags (e.g., loss 📉, throughput 📈)
tensorboard_append_direction_hints = true

# Enable periodic logging of additional training loss diagnostics
log_extra_train_metrics = true

# Interval (in steps) to compute and log extra train metrics
train_metrics_interval = 50

# Log a scatter plot of per-sample loss vs timestep to TensorBoard
log_loss_scatterplot = true

# Interval (in steps) to log the scatter plot
log_loss_scatterplot_interval = 100

# Enable throughput metrics logging (samples per second, steps per second, runtime)
log_throughput_metrics = true

# Window size for throughput calculation (number of recent steps to average)
throughput_window_size = 100

# Progress bar postfix metrics mode:
# true  - alternate each step between showing timing (step_ms) and hardware metrics (peak/util)
# false - show both timing and hardware metrics every step
alternate_perf_postfix = true

# Diagnostic attention metrics, if enabled, compute lightweight cross-attention statistics (entropy, top-k mass, token focus)
# at a configurable interval and for a small number of layers to limit overhead.
# They DO NOT store attention maps; only scalars are logged.
enable_attention_metrics = false

# Interval to collect attention metrics (in steps)
attention_metrics_interval = 100

# Maximum number of layers to collect attention metrics from, per step, collect from at most this many cross-attn layers
attention_metrics_max_layers = 2

# Maximum number of queries to collect attention metrics from (subsample queries per head for bounded memory)
attention_metrics_max_queries = 32

# Top-k mass for attention concentration metric
attention_metrics_topk = 16

# Prefix for logged metric names
attention_metrics_log_prefix = "attn"

# Log a small attention heatmap image to TensorBoard 
# recomputes a tiny [queries x tokens] map from detached Q/K once per window.
attention_metrics_log_heatmap = false

# Max heads to average for the heatmap
attention_metrics_heatmap_max_heads = 2

# Max queries (rows) to include in heatmap (subsampled evenly)
attention_metrics_heatmap_max_queries = 32

# Prefix for heatmap tag in TensorBoard
attention_metrics_heatmap_log_prefix = "attn_hm"

# Attention heatmap colormap name (any Matplotlib colormap string, e.g., "magma", "viridis", "plasma")
attention_metrics_heatmap_cmap = "magma"

# Attention heatmap normalization: "log" for LogNorm, "linear" for no explicit normalization
attention_metrics_heatmap_norm = "log"

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmin_pct = 60.0

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmax_pct = 99.5

# Attention heatmap figure width in inches
attention_metrics_heatmap_fig_w = 6.0

# Attention heatmap figure height in inches
attention_metrics_heatmap_fig_h = 4.0

# Performance metrics verbosity level for model prediction and target analysis: minimal, standard, debug
performance_verbosity = "standard"

# Split SNR metrics into essential and other namespaces for clarity
snr_split_namespaces = true

# Split validation metrics into essential and detailed namespaces
val_split_namespaces = true

# EMA smoothing factor (0.9-0.99 typical). Higher = smoother, slower to react.
# ema_loss_beta = 0.98

# Number of steps to defer bias correction for early readability
# ema_loss_bias_warmup_steps = 100

# =============================================================================
# TIMESTEP DISTRIBUTION LOGGING
# =============================================================================

# Add prefix for each log directory
# log_prefix = "" 

# Name of tracker to use for logging, default is script-specific default name
# log_tracker_name = "" 

# Path to tracker config file to use for logging
# log_tracker_config = "" 

# WandB support has been removed. The following options are deprecated and ignored.
# wandb_run_name = ""
# wandb_api_key = ""

# Automatically launch TensorBoard server on training start
launch_tensorboard_server = true

# Host for TensorBoard server
tensorboard_host = "127.0.0.1"

# Port for TensorBoard server  
tensorboard_port = 6006

# Enable auto-reload of logs (default: true)
tensorboard_auto_reload = true

# Timestep distribution logging
# Choose how to visualize timestep distributions in TensorBoard during training.
# "off":     disable logging
# "histogram": log TensorBoard histograms (fast; shows shape clearly)
# "chart":     log a static chart image (uses matplotlib if available)
# "both":      log both histogram and chart
log_timestep_distribution = "off"

# How often to log the live (used) timestep distribution during training (in steps)
log_timestep_distribution_interval = 100

# Number of bins for histogram/chart density
log_timestep_distribution_bins = 100

# Log an initial expected distribution at training start
# Uses precomputed buckets if enabled; otherwise simulates via the configured sampler
log_timestep_distribution_init = true

# Log an initial expected distribution at training start only once
log_timestep_distribution_init_once = true

# Number of samples to draw when simulating the initial distribution (when not using precomputed buckets)
log_timestep_distribution_samples = 20000

# Log a live (used) timestep distribution during training (in steps)
log_timestep_distribution_window = 10000

# Band ratios using configured edges, e.g., "850,900,950"
log_timestep_distribution_bands = "0,100,200,300,400,500,600,700,800,900,1000"

# Log the probability mass function (PMF) of the timestep distribution
log_timestep_distribution_pmf = true

# =============================================================================
# DDP SETTINGS
# =============================================================================

# DDP timeout (min, None for default of accelerate)
# ddp_timeout = null 

# Enable gradient_as_bucket_view for DDP
# ddp_gradient_as_bucket_view = false # Enable gradient_as_bucket_view for DDP

# Enable static_graph for DDP
# ddp_static_graph = false 

# =============================================================================
# DYNAMO SETTINGS
# =============================================================================

# dynamo backend type (default is None)
dynamo_backend = "NO"

# Dynamo mode (choices: default, reduce-overhead, max-autotune)
dynamo_mode = "default"

# Use fullgraph mode for dynamo
# dynamo_fullgraph = false 

# Use dynamic mode for dynamo
# dynamo_dynamic = false # Use dynamic mode for dynamo

# =============================================================================
# ⚠️ VAE TRAINING SETTINGS
# =============================================================================

# VAE training mode: "full", "decoder_only", "encoder_only"
vae_training_mode = "full"

# Weight for KL divergence loss in VAE training
vae_kl_weight = 1e-6

# Reconstruction loss type: "mse", "l1", "huber"
vae_reconstruction_loss = "mse"

# =============================================================================
# ⚠️ ONLINE SELF-CORRECTION SETTINGS
# =============================================================================

# Enable lightweight online self-correction cache updates during training (non-invasive; off by default)
# When enabled, the trainer periodically generates short clips into output_dir/self_correction_cache
# and mixes them into training via a hybrid dataloader wrapper.
self_correction_enabled = false

# Do not start self-correction until after this many optimizer steps
self_correction_warmup_steps = 1000

# Regenerate correction clips every N optimizer steps (0 disables periodic updates)
self_correction_update_frequency = 1000

# Maximum number of correction clips to keep in the cache directory
self_correction_cache_size = 200

# Length (frames) of generated correction clips
self_correction_clip_len = 32

# Probability to draw a batch from the correction cache instead of the main dataset
self_correction_batch_ratio = 0.2

# Number of frames for generation
self_correction_sample_steps = 16

# Output width in pixels
self_correction_width = 256

# Output height in pixels 
self_correction_height = 256

# Classifier-free guidance scale (CFG)
self_correction_guidance_scale = 5.0

# Inline prompts for self-correction 
# Each table can include: text, width, height, frames, step, seed, guidance_scale, enum
# If not provided, captions from datasets are used.

[[self_correction_prompts]]
text = "a person walking on a beach at sunset"
frames = 33
step = 16
guidance_scale = 5.0

[[self_correction_prompts]]
text = "a busy downtown street with cars and pedestrians"
frames = 17
step = 12
guidance_scale = 4.5

# =============================================================================
# SAMPLING SETTINGS
# =============================================================================

# Generate sample images every N steps
sample_every_n_steps = 500

# Generate sample images before training
sample_at_first = true

# Generate sample images every N epochs (overwrites n_steps)
# sample_every_n_epochs = 1 

# =============================================================================
# SAMPLE PROMPTS SETTINGS
# =============================================================================

[[sample_prompts]]

# The main prompt text
text = """
Young woman with short blonde hair and grey eyes, tank top, denim shorts, \
in a cluttered open restaurant on a hill above amazing ocean beach
"""

# Output width in pixels
width = 384

# Output height in pixels
height = 384

# Number of frames for video generation
frames = 45

# Random seed for generation
seed = 20250801

# Step for generation
step = 20

# Classifier-free guidance scale (CFG)
cfg_scale = 6.0

# =============================================================================
# LATENT CACHE SETTINGS
# =============================================================================

[datasets.latent_cache]

# cache features in VAE on CPU
vae_cache_cpu = true

# data type for VAE, default is float16
vae_dtype = "float16"

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
# batch_size = null 

# number of workers for dataset, default is cpu count-1
# num_workers = null 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# delete all existing latent cache files in cache_directory before caching
# purge_before_run = true

# debug mode, choices: image, console, video
# debug_mode = "image" 

# console width, default is 80
# console_width = 80 

# console background color, default is black
# console_back = "black" 

# debug mode: not interactive, number of images to show for each dataset
# console_num_images = 1 

# =============================================================================
# TEXT ENCODER CACHE SETTINGS
# =============================================================================

[datasets.text_encoder_cache]

# use fp8 for Text Encoder model
fp8_t5 = true

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
batch_size = 16

# number of workers for dataset, default is cpu count-1
# num_workers = null 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# delete all existing text encoder cache files in cache_directory before caching
# purge_before_run = true

# =============================================================================
# DATASET SETTINGS
# =============================================================================

[datasets.general]

# File extension for caption files
caption_extension = ".txt"

# Probability to drop entire captions (0.0-1.0). When >0, a random portion of items will have empty captions during
# caching/training. Defaults to 0.0 (disabled).
# caption_dropout_rate = 0.1

# Enable bucket-based training for different resolutions
enable_bucket = true

# Prevent upscaling of images to larger bucket sizes
bucket_no_upscale = true

# Directory containing mask images for masked training
# When using masked training, ensure your dataset includes mask files alongside images/videos
# Masks should be binary: 0 = unmasked regions, 255 = masked regions
# For videos: masks can be per-frame or single mask applied to all frames
# mask_path = "/path/to/mask/images" 

# =============================================================================
# TRAINING DATASETS
# =============================================================================

# Image dataset
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "path/to/images"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/images/cache"

# Target resolution [width, height] (overrides general setting)
resolution = [512, 512]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Path to mask images for this dataset (overrides general setting)
# mask_path = "/path/to/mask/images" 

# Video dataset
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "path/to/videos"

# Path to cache directory
cache_directory = "path/to/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Supported frame extraction modes
# - head: take first window, requires setting target_frames. 
# - middle: take centered window, requires setting target_frames.
# - chunk: non-overlapping contiguous windows of length target_frame across the video, requires setting target_frames.
# - slide: sliding windows, requires setting target_frames and frame_stride (>=1), default is 1. Overlap (frames) = target_frame - frame_stride.
# - slide_end: sliding windows that guarantee the final window ends at the last frame; if the video is shorter than target_frame, produce a single full-length window. Requires target_frames and frame_stride (>=1). Overlap (frames) = target_frame - frame_stride.
# - uniform: fixed count of evenly spaced start positions, requires setting target_frames and frame_sample (>=1). If frame_sample == 1, it is auto-changed to "head" by the loader.
# - multiple_overlapping: minimally cover the video with windows (may overlap), end-aligned, requires setting target_frames.
# - full: use up to max_frames (rounded to N*4+1) starting at 0, requires setting target_frames to be non-empty; Uses max_frames to cap length (optional). If multiple target_frames are provided, one identical (0, use_frames) window is produced per entry; recommended target_frames = [1].
frame_extraction = "uniform"

# Extract different frame sequence lengths
target_frames = [1, 17, 33]

# Extract each target_frame sequence once
frame_sample = 1

# Stride between sliding windows; used only when frame_extraction = "slide"
# frame_stride = 1

# Cap total frames used by frame_extraction = "full"; final length is rounded to (4n+1)
# max_frames = 129

# Source FPS
source_fps = 24

# Path to mask images/videos for this dataset
# mask_path = "/path/to/video/masks" 

# Regularization image dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "/path/to/your/regularization_images"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Marks regularization dataset
is_reg = true

# Regularization video dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "/path/to/your/regularization_videos"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# marks regularization dataset
is_reg = true

# =============================================================================
# VALIDATION DATASETS
# =============================================================================

# Image validation dataset
[[datasets.val]]

# Path to image directory (overrides general setting)
image_directory = "path/to/val/images"

# Path to cache directory
cache_directory = "path/to/val/images/cache"

# Target resolution [width, height]
resolution = [512, 512]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Video validation dataset
[[datasets.val]]

# Path to video directory (overrides general setting)
video_directory = "path/to/val/videos"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/val/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Use head sampling
frame_extraction = "head"

# Extract different frame sequence lengths
target_frames = [1, 25]

# Extract each target_frame sequence once
frame_sample = 1
