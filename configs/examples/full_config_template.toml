# Takenoko Configuration File
# This file contains all possible parameters for training with default values and comments
# Remove the # at the beginning of lines to enable/configure parameters
# "⚠️" means that either relevant functionality is not ready (under construction) or has not been fully tested yet

# =============================================================================
# TARGET TRAINING
# =============================================================================

# "wan21" or "wan22" 
target_model = "wan22"  

# =============================================================================
# CHECKPOINT SETTINGS
# =============================================================================

# DiT checkpoint path
dit = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp16.safetensors"

# use scaled fp8 for DiT
fp8_scaled = true

# Use fp8 for base model
fp8_base = true

# FP8 format for quantization ("e4m3" for E4M3FN or "e5m2" for E5M2)
# E4M3FN (e4m3): Better precision, requires RTX 40 series or newer (default)
# E5M2 (e5m2): Compatible with RTX 3000 series, slightly lower precision
fp8_format = "e4m3"

# Percentile for FP8 scale calculation
# Uses percentile instead of max value to reduce impact of outliers
# Range: 0.0 to 1.0, recommended: 0.99 to 0.9999
# fp8_percentile = 0.999

# Exclude specific modules from FP8 quantization (works with ALL FP8 modes)
# Comma-separated list or array of module name patterns to preserve
# Useful for keeping critical layers in higher precision
# fp8_exclude_keys = ["norm", "patch_embedding", "text_embedding", "time_embedding", "time_projection", "head", "img_emb"]

# Scale input tensor format for scaled_mm ("e4m3" or "e5m2", None disables input scaling)
# Enables input tensor quantization for additional memory savings, may impact quality, only works with fp8_scaled = true
# scale_input_tensor = "e4m3"

# Upcast quantization to float32 when computing scales and FP8 weights (improves accuracy, small VRAM cost during optimization)
# Only works with fp8_scaled = true
upcast_quantization = false

# Upcast linear matmul to float32
# Only works with fp8_scaled = true
upcast_linear = false

# Exclude feedforward layers from scaled_mm optimization (useful for WAN models where scaled_mm degrades quality)
# May help to prevent quality degradation in feedforward layers, only works with fp8_scaled = true
exclude_ffn_from_scaled_mm = true

# Optional: force a uniform cast of DiT weights ("fp16" or "bf16").
# When null, preserves default behavior (checkpoint dtype or fp8 paths).
# Ignored when fp8_scaled = true or mixed_precision_transformer = true.
# dit_cast_dtype = "fp16"

# Use mixed precision for transformer (preserves per-tensor dtypes from checkpoint)
# true for better dtype precision when using fp8_scaled
mixed_precision_transformer = true

# Enable FP16 accumulation
fp16_accumulation = false

# VAE checkpoint path
vae = "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors"

# data type for VAE, default is float16
# vae_dtype = "float16" 

# Cache VAE on CPU
# vae_cache_cpu = true

# text encoder (T5) checkpoint path
t5 = "https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth"

# Use fp8 for Text Encoder model
# fp8_t5 = true

# Directory to cache downloaded models
# model_cache_dir = "cache/models"

# Enable memory tracing
trace_memory = true

# =============================================================================
# ENHANCED FP8 QUANTIZATION
# =============================================================================

# Enable enhanced FP8 quantization features (default: false)
# When enabled, provides additional quantization modes and better control
fp8_use_enhanced = true

# Enhanced FP8 quantization mode (ONLY when fp8_use_enhanced = true)
# "tensor": Per-tensor quantization (fastest, lowest memory)
# "channel": Per-channel quantization (balanced accuracy/memory)
# "block": Block-wise quantization (best compression, configurable)
fp8_quantization_mode = "block"

# Block size for block-wise quantization (ONLY when fp8_quantization_mode = "block")
# Smaller values = better compression, larger values = faster processing
# Must be divisible by layer input features, falls back to channel mode if not
fp8_block_size = 64

# Whether to fallback to channel quantization if block-wise fails
# (e.g., when tensor dimensions are not divisible by block size)
fp8_block_wise_fallback_to_channel = true

# =============================================================================
# ⚠️ TORCHAO FP8 INTEGRATION
# =============================================================================

# Enable TorchAO FP8 quantization (requires torchao installation, default: false)
# Alternative to built-in FP8 quantization, uses TorchAO library
# Provides per-channel symmetric quantization with automatic scale derivation
# Note: Takes precedence over fp8_use_enhanced if both are enabled
torchao_fp8_enabled = false

# TorchAO FP8 weight dtype (only when torchao_fp8_enabled = true)
# "e4m3fn": E4M3FN format (default, better precision)
# "e5m2": E5M2 format (broader range, slightly lower precision)
torchao_fp8_weight_dtype = "e4m3fn"

# Target modules for TorchAO quantization (optional)
# Only quantize modules matching these patterns, useful for selective quantization
torchao_fp8_target_modules = ["attn", "mlp", "proj"]

# Exclude modules from TorchAO quantization (optional)
# Skip quantization for modules matching these patterns
torchao_fp8_exclude_modules = ["final_layer", "patch_embedding"]

# =============================================================================
# TRAINING SETTINGS
# =============================================================================

# Training steps
max_train_steps = 2000

# Training epochs (overrides max_train_steps if specified)
# max_train_epochs = 10

# Max num workers for DataLoader (lower is less main RAM usage, faster epoch start and slower data loading)
max_data_loader_n_workers = 2

# Persistent DataLoader workers (useful for reduce time gap between epoch, but may use more memory)
persistent_data_loader_workers = true

# Random seed for training
seed = 20250811

# Enable gradient checkpointing
# Reduces activation memory by recomputing during backward; slower but lower VRAM
gradient_checkpointing = true

# Enable CPU offloading for gradient checkpointing activations
# Further reduces VRAM usage by offloading activations to CPU during gradient checkpointing
# Must be used together with gradient_checkpointing = true
# Particularly effective for video training or large latent sizes, but increases CPU-GPU transfer overhead
# Memory impact: VRAM ↓, RAM ↑, Training speed: 5-20% slower depending on model and batch size
gradient_checkpointing_cpu_offload = false

# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps = 1

# Mixed precision training (choices: "no", "fp16", "bf16")
mixed_precision = "fp16"

# Use broadcasted per-sample time embedding for Wan 2.2/FVDM (L=1 per sample),
# instead of per-token embeddings. Off by default for maximal expressiveness.
broadcast_time_embed = false

# Lean attention math (experimental). When true, Wan 2.2 blocks keep computations
# in the current compute dtype (bf16/fp16) to reduce large fp32 intermediates
# and memory use. Original numerics are preserved when false.
lean_attn_math = false

# When using lean attention math, control compute dtype policy:
# true  = default to fp32 compute unless force-fp16 is enabled
# false = default to input dtype (fp16/bf16) unless force-fp16 is enabled
lean_attention_fp32_default = false

# Lower precision attention (experimental). When true, Wan 2.2 blocks use lower precision
# for attention calculations to reduce large fp32 intermediates and memory use.
# Original numerics are preserved when false.
lower_precision_attention = false

# Use Wan 2.1 style modulation for Wan 2.2
simple_modulation = false

# Selective torch.compile of core modules for speed; safe no-op unless enabled.
# Mutually exclusive with dynamo_backend (keep dynamo_backend = "NO" to use this).
# Uses torch.compile with configurable backends (eager, inductor, etc.)
optimized_torch_compile = false

# Compile parameters for optimized_torch_compile. Format:
# compile_args = ["BACKEND", "MODE", DYNAMIC, "FULLGRAPH"]
# - BACKEND: "eager" (default, most compatible) or "inductor" (requires Triton + C++ compiler)
# - MODE: "default" | "max-autotune" | "reduce-overhead" (torch dependent)
# - DYNAMIC: true|false|"auto" (auto lets torch decide)
# - FULLGRAPH: "True"|"False" (string for clarity; case-insensitive)
# 
# Backend selection logic:
# 1. If "eager" is set, use eager backend
# 2. If "inductor" is set but Triton/C++ compiler not available, fallback to eager
# 3. If "inductor" is set and Triton/C++ compiler available, use inductor
compile_args = ["inductor", "default", "auto", "False"]

# Compute RoPE multipliers on-the-fly instead of using
# cached per-(F,H,W) precomputation. Off by default for speed.
rope_on_the_fly = false

# RoPE variant for positional embeddings: "default" (standard) or "comfy" (Comfy-style; enables advanced ND RoPE)
rope_func = "default"

# Use float32 precision for RoPE calculations instead of float64 (memory optimization, minimal accuracy loss)
rope_use_float32 = false

# =============================================================================
# OPTIMIZER SETTINGS
# =============================================================================

# Optimizer to use: AdamW (default), AdamW8bit, AdaFactor, RavenAdamW. 
# You can also use any optimizer by specifying a fully-qualified class path 
# (e.g., 'torch.optim.AdamW', 'bitsandbytes.optim.AdEMAMix8bit').
# Resolved dynamically in optimizer manager with provided optimizer_args.
# Available optimizers:
# - "AdamW": Standard PyTorch AdamW (full precision, GPU states)
# - "AdamW8bit": bitsandbytes 8-bit AdamW (quantized states)
# - "RavenAdamW": CPU-offloaded AdamW with reusable GPU buffers
# - "AdaFactor": Memory-efficient adaptive optimizer with relative_step scheduling
# - "Muon", "NorMuon": Second-order optimizers for matrix parameters
# - "TemporalAdamW": AdamW with temporal gradient smoothing for video training
# - "Prodigy": Adaptive learning rate optimizer
# - "SOAP", "SophiaG", "Scion", etc. (see optimizer_manager.py)
optimizer_type = "adamw8bit"

# Additional arguments for optimizer (like "weight_decay=0.01 betas=0.9,0.999 ...")
# Format: key=value pairs as strings. Values are parsed as Python literals.
# optimizer_args = ["weight_decay=0.01", "betas=(0.9,0.999)"]

# Learning rate
learning_rate = 2.0e-5

# Max gradient norm, 0 for no clipping
max_grad_norm = 1.0

# =============================================================================
# LEARNING RATE SCHEDULER
# =============================================================================

# Scheduler type
# - "constant": fixed LR for all steps (must have lr_warmup_steps = 0)
# - "constant_with_warmup": linear warmup to base LR, then hold constant
# - "linear": linear warmup, then linear decay to 0 over training steps
# - "cosine": linear warmup, then cosine decay to 0 over training steps
# - "cosine_with_restarts": warmup then cosine with restarts; uses lr_scheduler_num_cycles
# - "polynomial": warmup then polynomial decay; uses lr_scheduler_power
# - "inverse_sqrt": warmup then 1/sqrt(t) decay; uses lr_scheduler_timescale (defaults to lr_warmup_steps)
# - "cosine_with_min_lr": warmup then cosine decay to (min_lr_ratio * base LR); uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "warmup_stable_decay": warmup -> flat (stable) -> decay; requires lr_warmup_steps and lr_decay_steps; uses lr_scheduler_min_lr_ratio and half of lr_scheduler_num_cycles
# - "piecewise_constant": step-wise schedule (from diffusers); requires lr_scheduler_args with step_rules
# - "adafactor:<initial_lr>": only valid with Adafactor optimizer (relative_step=True); no warmup/decay used
# All schedulers except "constant" require lr_warmup_steps to be provided (0 is allowed)
# "linear" and "cosine" ignore lr_decay_steps; "warmup_stable_decay" and some advanced schedules require lr_decay_steps > 0
lr_scheduler = "constant"

# Number of warmup steps (int) or fraction of total steps (float in 0..1)
# lr_warmup_steps and lr_decay_steps accept int (steps) or float (0<value<1) as a fraction of total optimizer steps (max_train_steps × num_processes)
lr_warmup_steps = 0

# Number of decay steps for schedulers that support it (int) or fraction (float in 0..1, ratio of train steps)
lr_decay_steps = 0

# Cycles: used by cosine_with_restarts; for cosine_with_min_lr and warmup_stable_decay, half of this value is used internally
lr_scheduler_num_cycles = 1

# Exponent for polynomial scheduler
lr_scheduler_power = 1.0

# Timescale for inverse_sqrt; if unset, defaults to lr_warmup_steps
# lr_scheduler_timescale = 0.0 

# Minimum LR ratio (final_LR = base_LR * ratio) for cosine_with_min_lr and warmup_stable_decay
# lr_scheduler_min_lr_ratio = 0.0 

# Use a custom scheduler class (overrides lr_scheduler above). Either a torch scheduler name (e.g. "OneCycleLR")
# or a fully-qualified import path (e.g. "torch.optim.lr_scheduler.StepLR"), kwargs come from lr_scheduler_args.
lr_scheduler_type = ""

# Extra kwargs for the chosen scheduler as a list of key=value strings (parsed via Python literal_eval)
# examples: lr_scheduler_args = 'T_max=100', lr_scheduler_args = 'step_rules=[(100,0.1),(300,0.01)]'
lr_scheduler_args = ''

# =============================================================================
# NETWORK SETTINGS
# =============================================================================

# Network module to train: "networks.lora_wan", "networks.control_lora_wan", "networks.reward_lora", "networks.vae_wan",
# "lycoris.kohya", "networks.t_lora", "networks.moc_lora", "networks.singlora_wan", "networks.wan_finetune", "networks.slider_lora_wan"
# The module defines trainable adapters and is imported dynamically by the trainer.
network_module = "networks.lora_wan"

# network dimensions (depends on each network)
network_dim = 32

# alpha for LoRA weight scaling
network_alpha = 32

# pretrained weights for network
# network_weights = "path/to/network/weights.safetensors" 

# drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons)
# network_dropout = 0 

# additional arguments for network (key=value). Parsed into list and consumed by network module.
# Lycoris example is in wan21_lycoris.toml
# network_args = '' 

# LoRA+ (scaled lr on lora_up only):
# network_args = ["loraplus_lr_ratio=2.0"]

# LoRA-GGPO regularization (perturbs Linear LoRA weights during training):
# Parsed by `takenoko.create_args_from_config` into args.ggpo_sigma/ggpo_beta for the loss hooks.
# network_args = ["ggpo_sigma=0.03", "ggpo_beta=0.01"]

# T-LoRA (timestep-gated LoRA)
# Activated when network_module = "networks.t_lora".
# Parameters:
# - tlora_trainer_type: "lora" (default) for vanilla T-LoRA, "ortho_lora" for orthogonal LoRA
# - tlora_min_rank: minimum active rank at highest noise (int ≥ 0)
# - tlora_alpha: curvature for r(t) = ((T - t)/T)^alpha * (max_rank - min_rank) + min_rank (float ≥ 0)
# - tlora_boundary_timestep: boundary timestep for WAN-Video style expert partitioning (int in [0,1000])
# Notes:
# - Use include_patterns to target only attention projections if desired.
#   Example include pattern: "include_patterns=['.*self_attn.*(q|k|v|to_out).*','.*cross_attn.*(q|k|v|to_out).*']"
# Example vanilla T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875"
# ]

# Orthogonal T-LoRA (optional, preserves original TLora behavior when disabled)
# Parameters:
# - sig_type: initialization singulars to use {"last", "principal", "middle"}; default "last"
# - ortho_from_layer: if true, initialize q/p/lambda from the original Linear weight SVD; otherwise use random base SVD
# - ortho_reg_lambda: global regularization weight for orthogonality penalties (applies to both p and q if specific lambdas are not set)
# - ortho_reg_lambda_p: regularization weight for p-layer orthogonality (||P^T P - I||_F^2)
# - ortho_reg_lambda_q: regularization weight for q-layer orthogonality (||Q Q^T - I||_F^2)
# Notes:
# - Orthogonal parametrization supports Linear-based LoRA adapters; Conv2d LoRA is skipped gracefully.
# - Timestep rank-mask and standard LoRA multiplier are still applied.
# Example orthogonal T-LoRA configuration:
# network_args = [
#   "tlora_trainer_type=ortho_lora",
#   "tlora_min_rank=1",
#   "tlora_alpha=1.0",
#   "tlora_boundary_timestep=875",
#   "sig_type=last",
#   "ortho_from_layer=true",
#   "ortho_reg_lambda_p=1e-3",
#   "ortho_reg_lambda_q=1e-3",
# ]

# MoC-LoRA (Mixture of Contexts for Long Video Generation)
# Activated when network_module = "networks.moc_lora".
# Parameters:
# - moc_chunk_size: chunk size for MoC attention (int ≥ 1024)
# - moc_top_k: top-k for MoC attention (int ≥ 1)
# - moc_progressive_sparsify: enable progressive sparsification for MoC attention (bool)
# - moc_enable_causality: enable causality for MoC attention (bool)
# - moc_context_dropout: dropout for MoC attention (float in [0.0, 1.0])
# - moc_implementation: implementation mode ("original" for 100% original accuracy, "optimized" for speed)
# - moc_max_layers: limit MoC to first N layers for memory control (int, optional)
# - moc_max_modules: limit MoC to first N modules per layer for memory control (int, optional)
# Example MoC-LoRA configuration:
# network_args = [
#   "moc_chunk_size=1280",
#   "moc_top_k=5",
#   "moc_progressive_sparsify=false",
#   "moc_enable_causality=true",
#   "moc_context_dropout=0.0",
#   "moc_implementation=optimized",
#   "moc_max_layers=2",
#   "moc_max_modules=4"
# ]

# SingLoRA-specific parameters
#network_args = [
#  "ramp_up_steps=1000",
#  "init_method=kaiming_uniform",
#  "use_rslora=false",
#  "dropout=null",
#]

# arbitrary comment string stored in metadata
# training_comment = "trained with Takenoko" 

# automatically determine dim (rank) from network_weights
# dim_from_weights = true 

# show detailed information about trainable LoRA parameters and network structure
# verbose_network = true 

# scale the weight of each key pair to help prevent overtraing via exploding gradients. (1 is a good starting point)
# scale_weight_norms = 1.0 

# network weights to merge into the model before training
# base_weights = "path/to/base1.safetensors" 

# multiplier for base weights
# base_weights_multiplier = 1.0 

# use lycoris for inference
# lycoris = true 

# Weight for regularization loss (1.0 = normal, higher = stronger regularization)
# This applies to regularization datasets (is_reg=true) vs main dataset
# NOT related to masked training's "masked_prior_preservation_weight"
prior_loss_weight = 1.0

# =============================================================================
# ⚠️ SLIDER TRAINING SETTINGS
# =============================================================================

# Slider training for concept editing - enabled by using network_module = "networks.slider_lora_wan"

# Core slider parameters
# Strength of concept enhancement/suppression (default: 3.0)
slider_guidance_strength = 3.0

# Strength of anchor class preservation (default: 1.0)
slider_anchor_strength = 1.0

# Advanced guidance parameters (for fine-tuning)
# Base classifier-free guidance scale (default: 1.0)
slider_guidance_scale = 1.0

# Embedding-level guidance scaling (default: 1.0)
slider_guidance_embedding_scale = 1.0

# Separate scale for target predictions (default: 1.0)
slider_target_guidance_scale = 1.0

# Required prompt configuration
# Positive prompt representing desired concept direction (REQUIRED for slider training)
slider_positive_prompt = "beautiful, high quality, masterpiece"

# Negative prompt representing undesired concept direction (REQUIRED for slider training)
slider_negative_prompt = "ugly, low quality, blurry"

# Target class/concept being edited (REQUIRED for slider training)
slider_target_class = "portrait"

# Optional anchor class for concept preservation (helps prevent unwanted side effects)
slider_anchor_class = "person"

# Training parameters
# Learning rate multiplier for slider training (default: 1.0)
slider_learning_rate_multiplier = 1.0

# Cache text embeddings for efficiency (default: true)
slider_cache_embeddings = true

# T5 text encoder settings (follows Takenoko's pattern)
# Device for T5 encoder: "cpu" (saves VRAM) or "cuda" (default: "cpu")
slider_t5_device = "cpu"

# Cache embeddings during initialization (default: true)
slider_cache_on_init = true 

# =============================================================================
# FINE-TUNING SETTINGS
# =============================================================================

# Enable fine-tuning mode by setting network_module = "networks.wan_finetune"
# Requires significantly more VRAM than LoRA training

# Fraction of parameters to fine-tune (0.1 = 10%, 1.0 = 100%)
# Lower values reduce memory usage but may limit adaptation capability
fine_tune_ratio = 1

# Whether to also fine-tune the text encoder along with the transformer
# Significantly increases memory usage - only enable if you have sufficient VRAM (32GB+)
finetune_text_encoder = false

# Advanced optimizations for memory efficiency and performance
# These optimizations enable efficient training of large transformer models

# Enable full BF16 training including gradients (reduces memory usage ~50%)
# Requires mixed_precision = "bf16" to be set
# Provides better memory efficiency for large model fine-tuning
full_bf16 = false

# Automatically convert FP16 checkpoints to BF16 (finetune trainer only)
# When enabled:
# - Checks for existing bf16_* version in models/ directory
# - If not found, downloads and converts FP16→BF16 automatically  
# - Caches converted BF16 checkpoint for future runs
# - If original filename starts with "bf16_", uses as-is
use_or_convert_bf16 = true

# Enable direct checkpoint loading (skip base model)
# When enabled:
# - Skips base model loading and directly loads checkpoint weights
# - Requires checkpoint to be in the correct format
direct_checkpoint_loading = true

# Enable fused backward pass optimization for Adafactor optimizer
# Reduces memory peaks during backward pass by fusing operations
# Requires Adafactor optimizer and special fused optimization module
fused_backward_pass = false

# Enable memory-efficient checkpoint saving
# Uses direct GPU→disk writes to reduce memory peaks during save operations
# Recommended to keep enabled for fine-tuning large models
mem_eff_save = true

# Weight dynamics analysis frequency (0 = disabled)
# Set to 50-500 to monitor parameter evolution and training efficiency across model components
# Provides research insights into learning patterns, plasticity, and component specialization
verify_weight_dynamics_every_n_steps = 0

# Enable stochastic rounding for BF16 training (improves numerical stability)
# Requires mixed_precision = "bf16" to be effective
# Python implementation (default): Fast, works everywhere
use_stochastic_rounding = true

# CUDA implementation: Faster but requires compilation
# To use CUDA stochastic rounding:
# 1. Compile CUDA extension: cd src/utils/stochastic_rounding && python setup.py install  
# 2. Enable: use_stochastic_rounding_cuda = true
# NOTE: CUDA extension must be compiled on each machine individually!
# The setup.py is configured for compute capability 8.0 (RTX 30/40 series, A100).
# For other GPUs, modify setup.py:
#  - GTX 16/RTX 20 series: `-gencode=arch=compute_75,code=sm_75`
#  - RTX 30 series: `-gencode=arch=compute_86,code=sm_86` 
#  - RTX 40 series: `-gencode=arch=compute_89,code=sm_89`
use_stochastic_rounding_cuda = false

# =============================================================================
# TIMESTEP AND FLOW MATCHING SETTINGS
# =============================================================================

# Method to sample timesteps
# Available methods: sigma, uniform, sigmoid, shift, flux_shift, logsnr, qwen_shift, qinglong_flux, qinglong_qwen, logit_normal,
# bell_shaped, half_bell, lognorm_blend, lognorm_continuous_blend, enhanced_sigmoid, fopp, content, style, content_style_blend, mode_shift
timestep_sampling = "shift"

# Shift amount for discrete timestep sampling (used with 'shift' timestep_sampling)
discrete_flow_shift = 3.0

# Scale factor for sigmoid timestep sampling (used by: sigmoid, shift, flux_shift, qwen_shift, logit_normal, enhanced_sigmoid, qinglong_*)
sigmoid_scale = 1.0

# Bias for enhanced_sigmoid timestep sampling (only used when timestep_sampling = "enhanced_sigmoid")
sigmoid_bias = 0.0

# Center of bell_shaped distribution in [0.0, 1.0]; 0.95 ~ timestep 950
# Only used when timestep_sampling = "bell_shaped"
bell_center = 0.5

# Std (spread) of bell_shaped distribution; smaller => sharper peak
# Only used when timestep_sampling = "bell_shaped"
bell_std = 0.2

# Alpha mix ratio for lognorm_blend and lognorm_continuous_blend (fraction of samples from LogNormal vs. linear)
# Only used when timestep_sampling = "lognorm_blend" or "lognorm_continuous_blend"
lognorm_blend_alpha = 0.75

# Blend ratio for content_style_blend (fraction of content vs style sampling)
# Only used when timestep_sampling = "content_style_blend"
# 0.0 = pure style (later timesteps), 1.0 = pure content (earlier timesteps)
content_style_blend_ratio = 0.5

# Weighting scheme for timestep distribution: logit_normal, mode, cosmap, sigma_sqrt, none.
# This key interacts with two independent things: sampling distribution (which timestep indices are drawn)
# and loss weighting (how much each drawn timestep contributes to the loss)
# Sampling distribution: only when timestep_sampling = "sigma".
# logit_normal (uses logit_mean/logit_std) or mode (uses mode_scale) shape the sampling density.
# none = uniform density. sigma_sqrt and cosmap do NOT influence sampling.
# For all other samplers, weighting_scheme does not change which timesteps are sampled.
# Loss weighting: for any sampler, ONLY sigma_sqrt or cosmap enable SD3-style per-index loss weights; other values imply unit weights.
weighting_scheme = "none"

# For logit-based transforms, sampling density when timestep_sampling="sigma" and weighting_scheme="logit_normal"
logit_mean = 0.0

# For logit-based transforms, mapping distribution when timestep_sampling="logsnr" (and the logsnr segment in qinglong_* variants)
logit_std = 1.0

# Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme`
mode_scale = 1.29

# Time shift parameters for mode_shift timestep sampling
# time_shift_mu: shift parameter (usually >0, default 1.0)
time_shift_mu = 1.0

# time_shift_sigma: exponent parameter (default 1.0)
time_shift_sigma = 1.0

# set minimum time step for training (0~999, default is 0)
min_timestep = 0

# Set maximum time step for training (1~1000, default is 1000)
max_timestep = 1000

# Enable pre-computed timestep distribution (default: false)
# Uses a fixed, quantized distribution of t (reproducible) and takes precedence over num_timestep_buckets.
# Supported for most timestep_sampling methods; unsupported methods fall back to on-the-fly sampling.
# Distribution is sliced to [min_timestep, max_timestep] before sampling. Dataset-provided batch['timesteps']
# are ignored when enabled.
use_precomputed_timesteps = true

# Number of buckets for the precomputed distribution (default: 10000)
# Larger values = finer quantization of the target distribution. Key: 'precomputed_timestep_buckets'.
precomputed_timestep_buckets = 10000

# Optional: override the base area used by flux/qwen/qinglong precompute (auto-inferred from height/width/latents if unset)
# Helps make precomputed mid-shift match runtime behavior when image sizes are known ahead of time.
# precomputed_midshift_area = 1024.0

# Preserve distribution shape (default: false)
# If false: sampled t in [0,1] is linearly scaled into [min_timestep, max_timestep] (fast, but warps the shape).
# If true: rejection sampling preserves the original distribution shape inside the range (slower, shape-faithful).
# Applies after the sampling method (uniform/sigmoid/shift/flux_shift/logsnr/etc) or mapped presampled values.
# When dataset provides per-batch uniform t (see num_timestep_buckets), those are used first for mapping.
# With precomputed timesteps enabled, the global distribution is sliced to min/max first; constraints still apply.
preserve_distribution_shape = false

# Number of timestep buckets for training (default: None)
# Dataset-side per-epoch uniform stratification of u in [0,1] to improve coverage on small datasets.
# Used only when use_precomputed_timesteps = false. Samples are mapped through the chosen timestep_sampling,
# then min/max (and preserve_distribution_shape) are applied. Implemented in the dataset; batches may include
# a per-item 'timesteps' list. Ignored when use_precomputed_timesteps = true. Set >= 2 to enable.
# num_timestep_buckets = 10

# Skip extra in-range constraint
skip_extra_timestep_constraint = true

# Epsilon for extra in-range constraint
timestep_constraint_epsilon = 1e-5

# Round training timesteps to the nearest integer schedule step
round_training_timesteps = true

# Fast rejection sampling
fast_rejection_sampling = false

# When fast_rejection_sampling = true, control the candidate oversampling factor and max iterations
# rejection_overdraw_factor = 4.0

# When fast_rejection_sampling = true, control the max iterations
# rejection_max_iters = 10

# show timesteps: image, console
# Visualizes the actual distribution produced by the current sampling+constraints path. 
# show_timesteps = 'console' 

# =============================================================================
# LOSS SETTINGS
# =============================================================================

# Base loss function type - expanded support for multiple loss functions
# Standard losses:
# - "mse": Standard Mean Squared Error loss (default)
# - "l1" or "mae": L1 (Mean Absolute Error) loss
# - "huber": Standard Huber loss with fixed delta (smooth_l1_loss)
# - "pure_huber": True Huber loss with configurable delta parameter
# - "pseudo_huber": Pseudo-Huber loss with fixed c parameter
# - "pseudo_huber_scheduled": Pseudo-Huber loss with adaptive c parameter scheduling
# 
# Advanced/Experimental losses:
# - "fourier": Frequency domain loss (requires additional parameters)
# - "dwt" or "wavelet": Discrete Wavelet Transform loss
# - "clustered_mse": Clustered MSE loss for better handling of outliers
# - "ew": EW (Error-Weighted) loss for adaptive weighting
# - "stepped": Stepped recovery loss for smoother flow dynamics (requires noise scheduler)
loss_type = "mse"

# Pseudo-Huber loss parameters (for loss_type = "pseudo_huber" or "pseudo_huber_scheduled")
# Pseudo-Huber c parameter (controls transition between L2 and L1 behavior)
# Smaller values (0.1-0.5) = more L1-like (robust to outliers)
# Larger values (1.0-2.0) = more L2-like (smooth gradients)
pseudo_huber_c = 0.5

# Scheduling type for adaptive c parameter (when loss_type = "pseudo_huber_scheduled")
# Options: "linear", "cosine", "exponential"
pseudo_huber_schedule_type = "linear"

# Minimum c value for scheduling (start of training)
pseudo_huber_c_min = 0.1

# Maximum c value for scheduling (end of training)
pseudo_huber_c_max = 1.0

# Huber loss parameters (for pure_huber loss type)
# Delta parameter for pure_huber loss type
# Controls the point where loss transitions from quadratic to linear
huber_delta = 1.0

# Fourier loss parameters (for loss_type = "fourier")
# Weight/strength of fourier loss component
fourier_weight = 0.05

# Fourier loss computation mode
# - "basic": Simple magnitude spectrum comparison
# - "weighted": Frequency-weighted comparison (default)
# - "multiscale": Multi-scale frequency analysis
# - "adaptive": Adaptive frequency weighting
fourier_mode = "weighted"

# Norm type for fourier loss computation
fourier_norm = "l2"  # "l1" or "l2"

# Dimensions for FFT computation (usually last 2 dims for 2D)
fourier_dims = [-2, -1]

# Numerical stability epsilon
fourier_eps = 1e-8

# Scale factors for multiscale fourier loss
fourier_multiscale_factors = [1, 2, 4]

# Threshold for adaptive fourier loss
fourier_adaptive_threshold = 0.1

# Alpha parameter for adaptive fourier loss
fourier_adaptive_alpha = 0.5

# High frequency weight for weighted fourier loss
fourier_high_freq_weight = 2.0

# Wavelet/DWT loss parameters (for loss_type = "dwt" or "wavelet")
# Wavelet type for DWT decomposition
# Common options: "haar", "db1", "db4", "db8", "bior2.2", "coif2"
wavelet_type = "haar"

# Number of decomposition levels
wavelet_levels = 1

# Border mode for wavelet transform
# Options: "zero", "symmetric", "periodization"
wavelet_mode = "zero"

# Clustered MSE loss parameters (for loss_type = "clustered_mse")
# Number of clusters for clustered MSE loss
clustered_mse_num_clusters = 8

# Weight for cluster-based loss component
clustered_mse_cluster_weight = 1.0

# EW loss parameters (for loss_type = "ew")
# Boundary shift parameter for exponential weighted loss
ew_boundary_shift = 0.0

# Stepped loss parameters (for loss_type = "stepped")
# Number of timestep indices to step forward in diffusion process
# Larger values = longer-term flow prediction, more smoothing effect
# Range: 10-100, recommended: 30-70 for video LoRA training
stepped_step_size = 50

# Multiplier for stepped loss magnitude, range: 0.1-20.0
# Values > 1.0 increase stepped loss weight, < 1.0 decrease it
stepped_multiplier = 10.0

# Explicit dimension-aware loss reduction for video tensors (B,C,F,H,W)
# mean([1,2,3,4]) for video vs mean([1,2,3]) for image
use_explicit_video_loss_reduction = false

# Custom loss target via noise_scheduler.get_loss_target() hook
# For schedulers with non-standard flow matching targets
enable_custom_loss_target = false

# =============================================================================
# ⚠️ DISPERSIVE LOSS SETTINGS
# =============================================================================

# Enable dispersive loss for improved generation quality and speed.
enable_dispersive_loss = false

# The lambda (λ) hyperparameter that controls the strength of the dispersive loss. The official PyTorch release uses 0.25.
dispersive_loss_lambda = 0.25

# Temperature parameter (> 0). Smaller values sharpen repulsion.
dispersive_loss_tau = 1.0

# None disables extraction, non-negative integer selects a block index, "last" picks the final block
dispersive_loss_target_block = "last"

# "l2_sq" (official), "l2_sq_legacy", or "cosine"
dispersive_loss_metric = "l2_sq"

# Optional: pool spatial tokens per frame before dispersion ("none" or "frame_mean")
dispersive_loss_pooling = "none"

# =============================================================================
# ⚠️ EQUILIBRIUM MATCHING SETTINGS
# =============================================================================

# Enable the experimental EqM training path (requires new objective and sampler).
enable_eqm_mode = false

# Prediction head type: "velocity", "score", or "noise".
eqm_prediction = "velocity"

# Transport path for EqM loss ("Linear", "GVP", or "VP").
eqm_path_type = "Linear"

# Weight applied to the EqM transport loss when enabled.
eqm_loss_weight = 1.0

# Optional transport weighting ("velocity" or "likelihood") applied to the EqM regression target.
# eqm_transport_weighting = "velocity"

# Optional epsilon offsets used by the transport planner.
# Train epsilon offset
# eqm_train_eps = 1e-3

# Sample epsilon offset
# eqm_sample_eps = 1e-3

# Enable EqM energy head (differentiable dot-product energy).
# eqm_energy_head = false

# Energy mode for EqM energy head, options: "dot", "l2", "mean"
# eqm_energy_mode = "dot" 

# Sampling parameters
# Step size for EqM sampling
# eqm_step_size = 0.0017

# Momentum for EqM sampling
# eqm_momentum = 0.0

# Sampler for EqM sampling, options: "gd", "ngd", "ode", "sde", or "ode_likelihood"
# eqm_sampler = "gd"   

# Enable adaptive sampler for EqM sampling
# eqm_use_adaptive_sampler = false

# Adaptive step size parameters for EqM sampling
# eqm_adaptive_step_min = 1e-5

# Maximum step size for EqM sampling
# eqm_adaptive_step_max = 0.01

# Growth factor for adaptive step size
# eqm_adaptive_growth = 1.05

# Shrink factor for adaptive step size
# eqm_adaptive_shrink = 0.5

# Restart patience for adaptive step size
# eqm_adaptive_restart_patience = 4

# Alignment threshold for adaptive step size
eqm_adaptive_alignment_threshold = 0.0

# ODE method for EqM sampling, options: "dopri5" (requires torchdiffeq), "heun", "euler", "rk4"
eqm_ode_method = "dopri5"  

# Number of integration steps for ODE samplers
eqm_ode_steps = 50

# Absolute tolerance for ODE samplers
eqm_ode_atol = 1e-6

# Relative tolerance for ODE samplers
eqm_ode_rtol = 1e-3

# Reverse the ODE trajectory
eqm_ode_reverse = false

# Absolute tolerance for ODE likelihood samplers
# eqm_ode_likelihood_atol = 1e-6

# Relative tolerance for ODE likelihood samplers
# eqm_ode_likelihood_rtol = 1e-3

# Number of Hutchinson trace samples for ODE likelihood samplers
eqm_ode_likelihood_trace_samples = 1

# SDE method for EqM sampling, options: "Euler", "Heun"
eqm_sde_method = "Euler"

# Number of SDE integration steps
eqm_sde_steps = 250

# SDE last step policy, options: "Mean", "Tweedie", "Euler", null
eqm_sde_last_step = "Mean"

# Step size applied when the final SDE correction is enabled
eqm_sde_last_step_size = 0.04

# SDE diffusion form, options: "SBDM", "SBDM-E"
eqm_sde_diffusion_form = "SBDM"

# SDE diffusion norm
eqm_sde_diffusion_norm = 1.0

# =============================================================================
# ⚠️ ADAPTIVE TIMESTEP SAMPLING
# =============================================================================

# Enable adaptive timestep sampling (default: false)
# Focuses training on timesteps that are most critical for video generation quality
enable_adaptive_timestep_sampling = false

# Core parameters
# How much to focus on important timesteps (1.0-5.0)
adaptive_focus_strength = 2.0

# Steps before analysis begins (100-2000)
adaptive_warmup_steps = 500

# Recent loss samples to analyze (100-5000)
adaptive_analysis_window = 1000

# Standard deviation multiplier for detection (0.5-3.0)
adaptive_importance_threshold = 1.5

# Steps between analysis updates (10-500)
adaptive_update_frequency = 100

# Timestep constraints
# Minimum important timesteps (10-500)
adaptive_min_timesteps = 50

# Maximum important timesteps (50-1000)
adaptive_max_timesteps = 200

# Video-specific features
# Enable video-specific categories
adaptive_video_specific = true

# Motion consistency weight (0.1-3.0)
adaptive_motion_weight = 1.0

# Detail preservation weight (0.1-3.0)
adaptive_detail_weight = 1.0

# Temporal coherence weight (0.1-3.0)
adaptive_temporal_weight = 1.0

# Enable Beta distribution sampling methodology (default: false)
# Uses paper's exact approach: neural network parameterized Beta distribution
adaptive_use_beta_sampler = false

# Feature selection size |S| for approximation (1-10)
adaptive_feature_selection_size = 3

# Sampler update frequency f_S (10-100)
adaptive_sampler_update_frequency = 100

# Enable separate neural network for timestep sampling (default: false)
adaptive_use_neural_sampler = false

# Initial Alpha parameter for Beta distribution (0.1-5.0)
adaptive_beta_alpha_init = 1.0

# Initial Beta parameter for Beta distribution (0.1-5.0)
adaptive_beta_beta_init = 1.0

# Neural sampler hidden layer size (16-256)
adaptive_neural_hidden_size = 64

# Enable loss-variance importance weighting (stable, default: true)
# This is our proven stable approach for production use
adaptive_use_importance_weighting = true

# Enable KL divergence RL learning (paper exact, default: false)
# This is the authors' exact methodology - more experimental
adaptive_use_kl_reward_learning = false

# Enable replay buffer for historical learning (default: false)
# Stores KL improvements across training steps for better feature selection
adaptive_use_replay_buffer = false

# Enable SelectKBest statistical feature selection (default: false)
# Requires scikit-learn, uses regression-based feature importance
adaptive_use_statistical_features = false

# Weight combination strategy ("fallback", "ensemble", "best")
# - fallback: Use first successful method (stable)
# - ensemble: Average all methods (balanced)
# - best: Choose method with highest focus (aggressive)
adaptive_weight_combination = "fallback"

# Replay buffer size for KL learning (10-1000)
adaptive_replay_buffer_size = 100

# RL learning rate for policy updates (1e-5 to 1e-3)
adaptive_rl_learning_rate = 1e-4

# Entropy coefficient for RL regularization (0.0-0.1)
adaptive_entropy_coefficient = 0.01

# KL reward update frequency (5-100)
adaptive_kl_update_frequency = 20

# Pure KL divergence-based feature selection for exact research replication (default: false)
# When enabled, automatically configures use_kl_reward_learning=true, use_beta_sampler=true, use_importance_weighting=false
adaptive_kl_exact_mode = false

# Enable comparative logging of KL vs importance approaches for research analysis (default: false)
# Logs performance comparison between different feature selection methods
adaptive_comparative_logging = false

# Enable full research mode (Algorithm 1 & 2) for exact research replication (default: false)
# When enabled, activates complete research methodology including policy gradient updates
# Requires diffusion object with q_posterior_mean_var and p_mean_var methods
adaptive_research_mode_enabled = false

# =============================================================================
# ⚠️ CONTRASTIVE FLOW MATCHING (ΔFM) SETTINGS
# =============================================================================

# Enable the enhanced contrastive objective for improved generation quality and speed.
enable_contrastive_flow_matching = false

# The lambda (λ) hyperparameter that controls the strength of the contrastive loss. The paper found 0.05 to be a robust value.
contrastive_flow_lambda = 0.05

# Enable class-conditioned negative sampling (recommended when labels are available)
# This ensures negative samples come from different classes, improving contrastive learning effectiveness
contrastive_flow_class_conditioning = true

# Skip contrastive loss computation on unconditional samples (useful for classifier-free guidance)
# This prevents unconditional samples from contributing to the contrastive objective
contrastive_flow_skip_unconditional = false

# Class index representing unconditional/null samples (required if skip_unconditional is true)
# Set this to the index used for unconditional generation in your dataset
# contrastive_flow_null_class_idx = 0

# =============================================================================
# ⚠️ REPA (REPRESENTATIONAL ALIGNMENT) SETTINGS
# =============================================================================

# Enable REPA for representational alignment during training
enable_repa = false

# Visual encoder specification - supports both single and multi-encoder setups
# Single encoder: "dinov2-vit-b", "dinov2-vit-l", "dinov2-vit-g", "clip-vit-L", "mocov3-vit-b"
# Multi-encoder: "dinov2-vit-b,dinov2-vit-l", "dinov2-vit-b,clip-vit-L"
# Supported: DINOv2 (s/b/l/g), CLIP (B/L), MoCo-v3 (s/b/l), MAE (l), I-JEPA (h)
repa_encoder_name = "dinov2-vit-b"

# Diffusion layer alignment depth - single (8) or multi-layer ("8,12,16")
repa_alignment_depth = 8

# REPA loss weight (0.5 = paper default)
repa_loss_lambda = 0.1

# Similarity function ("cosine" recommended, "mse" alternative)
repa_similarity_fn = "cosine"

# Input resolution (256 or 512, 512 DINOv2-only)
repa_input_resolution = 256

# Ensemble mode ("individual" = separate heads, "concat" = concatenated features)
repa_ensemble_mode = "individual"

# Projection sharing (false = individual heads per encoder, true = shared head)
repa_shared_projection = false

# =============================================================================
# ⚠️ SARA (STRUCTURAL AND ADVERSARIAL REPRESENTATION ALIGNMENT) SETTINGS
# =============================================================================

# Enable SARA (Structural and Adversarial Representation Alignment) training
# SARA extends REPA with autocorrelation matching and adversarial discriminators
# to improve training efficiency and generation quality for diffusion models
sara_enabled = false

# Pretrained encoder to use for feature extraction
# Options: "dinov2_vitb14", "dinov2_vits14", "dinov2_vitl14", "dinov2_vitg14", "clip_vitb16", "clip_vitl14"
# DINOv2 models generally provide better structural features than CLIP
sara_encoder_name = "dinov2_vitb14"

# Number of transformer blocks in the diffusion model to align features from
# Lower values (4-8) align early features (structure), higher values align later features (semantics)
# Paper recommendation: 8 for balanced structural and semantic alignment
sara_alignment_depth = 8

# Weight for patch-wise feature alignment loss (REPA baseline component)
# Encourages diffusion model features to match pretrained encoder features locally
# Paper default: 0.5-1.0, higher values strengthen feature alignment
sara_patch_loss_weight = 0.5

# Weight for autocorrelation (structural) alignment loss
# Matches spatial correlation patterns between diffusion and encoder features
# Paper default: 0.5, captures long-range structural dependencies
sara_autocorr_loss_weight = 0.5

# Weight for adversarial discriminator loss
# Encourages diffusion features to be indistinguishable from encoder features
# Paper default: 0.05-0.1, too high can cause training instability
sara_adversarial_loss_weight = 0.05

# Normalize autocorrelation matrices before computing loss
# Recommended: true for stability and scale-invariance
sara_autocorr_normalize = true

# Use Frobenius norm for autocorrelation loss instead of element-wise MSE
# Frobenius norm provides more stable gradients for matrix differences
sara_autocorr_use_frobenius = true

# Enable adversarial discriminator training
# Set to false to use only patch and autocorrelation losses (no discriminator)
sara_adversarial_enabled = true

# Discriminator architecture: "resnet18" (deeper, more capacity) or "simple_cnn" (lightweight)
# ResNet18 is more powerful but slower; simple_cnn is faster and uses less memory
sara_discriminator_arch = "resnet18"

# Learning rate for discriminator optimizer (separate from main model LR)
# Typical range: 0.0001-0.0005, higher than generator for balanced adversarial training
sara_discriminator_lr = 0.0002

# Number of discriminator update steps per generator step
# Values > 1 give discriminator more training, improving feature quality at cost of speed
sara_discriminator_updates_per_step = 1

# Number of warmup steps before discriminator loss affects generator
# Allows generator to stabilize before adversarial training begins
sara_discriminator_warmup_steps = 500

# Similarity function for patch-wise alignment: "cosine", "mse", or "l1"
# Cosine is scale-invariant and works well for normalized features
# MSE/L1 are simpler but sensitive to feature magnitude
sara_similarity_fn = "cosine"

# Weight for gradient penalty on discriminator (regularization)
# Set to 0.0 to disable, typical values 0.1-10.0 for WGAN-GP style training
# Helps prevent discriminator gradient explosion
sara_gradient_penalty_weight = 0.0

# Enable feature matching loss (match intermediate discriminator features)
# Can improve training stability by providing additional alignment signal
# Adds computational overhead, recommended for difficult training scenarios
sara_feature_matching = false

# Scaling applied when feature matching is enabled. Kept small to avoid overpowering
# the primary adversarial objective.
sara_feature_matching_weight = 0.1

# Optional discriminator gradient clipping. Set to null to disable; values in the
# 0.5-5.0 range work well for stabilising adversarial updates.
# sara_discriminator_max_grad_norm = 1.0

# Optional StepLR schedule for the discriminator. Set scheduler_step to 0 to disable.
sara_discriminator_scheduler_step = 0

# When enabled, LR decays by scheduler_gamma every scheduler_step updates.
sara_discriminator_scheduler_gamma = 0.1

# Emit extended per-component metrics (discriminator LR, grad norms, logits).
# Helpful when debugging training instabilities.
sara_log_detailed_metrics = false

# Cache pretrained encoder outputs to avoid recomputation
# Significantly speeds up training when encoder is frozen
# Disable if encoder parameters are being updated
sara_cache_encoder_outputs = true

# Use mixed precision (fp16/bf16) for SARA computations
# Reduces memory usage and speeds up training with minimal quality impact
# Recommended: true unless you encounter numerical stability issues
sara_use_mixed_precision = true

# =============================================================================
# ⚠️ SPRINT (SPARSE-DENSE RESIDUAL FUSION) SETTINGS
# =============================================================================

# Enable Sprint sparse-dense residual fusion for efficient training
enable_sprint = false

# Token drop ratio (fraction of tokens to drop in middle blocks)
# 0.75 = 75% dropping (keeps 25% of tokens)
# Lower values (0.5-0.6) for more conservative speedup with less risk
# Higher values (0.8-0.9) for maximum speedup (experimental)
sprint_token_drop_ratio = 0.75

# Number of encoder blocks (dense path, all tokens)
# If not specified (null), automatically calculated as 25% of total blocks
# For 32-block model: null = 8 blocks, manual override: 6-10 blocks
# Encoder captures local details and noise information
sprint_encoder_layers = 8

# Number of middle blocks (sparse path, dropped tokens)
# If not specified (null), automatically calculated as 50% of total blocks
# For 32-block model: null = 16 blocks, manual override: 12-20 blocks
# Middle blocks process global semantics with sparse tokens
sprint_middle_layers = 16

# Token sampling strategy for video (critical for temporal coherence)
# "temporal_coherent" (recommended): Samples full/partial frames, maintains motion coherence
# "spatial_coherent": Samples consistent spatial regions across frames, good for static scenes
# "uniform": Random uniform sampling, fastest but may cause temporal artifacts
sprint_sampling_strategy = "temporal_coherent"

# Path-drop learning probability
# During training, randomly replaces sparse path with MASK tokens at this probability
# Helps the model learn to handle missing sparse information
# 0.1 = 10% probability (default), 0.0 = disabled
sprint_path_drop_prob = 0.1

# Block partitioning strategy
# "percentage" (default): Flexible, scales with model size using encoder_ratio and middle_ratio
# "fixed": 2-N-2 partitioning (2 encoder, N middle, 2 decoder blocks)
# Note: Manual sprint_encoder_layers and sprint_middle_layers always override this
sprint_partitioning_strategy = "percentage"

# Encoder ratio for percentage-based partitioning (ignored if sprint_encoder_layers is set)
# 0.25 = 25% of blocks for encoder (e.g., 8 blocks for 32-layer model)
# Only used when partitioning_strategy = "percentage" and sprint_encoder_layers is not set
sprint_encoder_ratio = 0.25

# Middle ratio for percentage-based partitioning (ignored if sprint_middle_layers is set)
# 0.50 = 50% of blocks for middle (e.g., 16 blocks for 32-layer model)
# Only used when partitioning_strategy = "percentage" and sprint_middle_layers is not set
sprint_middle_ratio = 0.50

# Two-stage training: token-dropping pretraining → full-token fine-tuning
# Maximizes training efficiency while closing train-inference gap
# Presumably, 5-10% fine-tuning steps achieves full quality recovery
# Number of pretraining steps with token dropping
# Set to 0 to use entire training as pretraining (no separate finetune stage)
# Example: 10000 for long pretraining phase with maximum efficiency
sprint_pretrain_steps = 0

# Number of fine-tuning steps with full tokens (no dropping)
# Set to 0 to disable fine-tuning stage (entire training uses token dropping)
# Recommended: 5-10% of pretrain_steps (e.g., 1000 if pretrain=10000)
# Fine-tuning closes train-inference gap and recovers any quality loss
sprint_finetune_steps = 0

# Number of warmup steps (linear ramp from 0 → sprint_token_drop_ratio)
# Allows model to stabilize before aggressive token dropping begins
# Set to 0 to disable warmup and start with full drop ratio immediately
sprint_warmup_steps = 0

# Number of cooldown steps during transition from pretraining to fine-tuning
# Linear decay from sprint_token_drop_ratio → 0.0 over cooldown_steps
# Smooth transition prevents quality degradation from abrupt change
# Default 100 steps provides gentle transition
sprint_cooldown_steps = 100

# Enable Sprint diagnostic logging 
sprint_enable_diagnostics = false

# =============================================================================
# ⚠️ FOPP SETTINGS
# =============================================================================

# Schedule type for FoPP: linear, cosine
fopp_schedule_type = "cosine"

# Number of timesteps for FoPP
fopp_num_timesteps = 1000

# Beta start for FoPP
fopp_beta_start = 0.0001

# Beta end for FoPP
fopp_beta_end = 0.002

# Random seed for FoPP sampling
fopp_seed = 1997

# =============================================================================
# ⚠️ NABLA SETTINGS
# =============================================================================

# Enable Nabla sparse attention
nabla_sparse_attention = false

# nabla-0.6_sta-11-3-3, nabla-0.6_sta-11-5-5
nabla_sparse_algo = "nabla-0.9_sta-11-24-24"

# =============================================================================
# OPTIMIZATION SETTINGS
# =============================================================================

# Use SDPA for CrossAttention (requires PyTorch 2.0)
sdpa = true

# Use FlashAttention for CrossAttention, requires FlashAttention (uv pip install your-flash-attn.whl)
# flash_attn = true

# Use SageAttention, requires SageAttention
# sage_attn = true

# Use xformers for CrossAttention, requires xformers
# xformers = true

# Use FlashAttention 3 for CrossAttention
# flash3 = true

# Use split attention for attention calculation (split batch size=1, affects memory usage and speed)
# split_attn = false

# Number of blocks to swap in the model, max XXX
# blocks_to_swap = 20 

# Enable RamTorch Linear replacement (drop-in for selected Linear layers)
# When true, certain Linear layers are replaced with CPU-stored params that are
# transferred to GPU on-demand to reduce VRAM. See ramtorch_strength/min_features.
use_ramtorch_linear = false

# Device to use for RamTorch compute (e.g., "cuda" or "cuda:0").
# If unset, RamTorch will default to its internal device selection.
# ramtorch_device = "cuda"

# Fraction (0.0–1.0) of eligible Linear layers to replace with RamTorch.
# Useful for partial rollout and benchmarking. Deterministic selection per layer tag.
ramtorch_strength = 1.0

# Minimum parameter count (in_features * out_features) for a Linear to be eligible.
# Increase to target only large matrices (e.g., 1<<20 = 1048576 params).
ramtorch_min_features = 0

# Log every RamTorch replacement (verbose); default logs only the first
ramtorch_verbose = false

# Force RamTorch to run in float32 I/O passthrough for stability with mixed precision
# (keeps outputs in float32; recommended when training with fp16/bf16)
ramtorch_fp32_io = true

# =============================================================================
# ENHANCED OFFLOADING SETTINGS
# =============================================================================

# When enabled, provides advanced weight swapping with pinned memory and performance optimizations
offload_enhanced_enabled = false

# Use pinned memory staging buffers
# Requires offload_enhanced_enabled = true
# Enables persistent staging buffers for 96% shared VRAM reduction
offload_pinned_memory_enabled = false

# Enable non-blocking memory transfers where possible
offload_non_blocking_transfers = true

# Target memory type for offloaded weights
# "auto": Prefer CPU (default behavior), honors offload_cpu_memory_priority
# "cpu": Force CPU RAM for offloaded weights (recommended for Windows shared VRAM limits)
# "shared_gpu": Keep offloaded weights in shared GPU memory (disables enhanced+pinned)
offload_target_memory_type = "auto"

# Prefer CPU memory in auto mode (has effect only when offload_target_memory_type = "auto")
offload_cpu_memory_priority = false

# Log timing for swapping operations (debugging/perf analysis)
offload_timing_enabled = false

# Verbose logging for offloading (may reduce performance)
offload_verbose_logging = false

# =============================================================================
# CUDA ENVIRONMENT & RUNTIME SETTINGS
# =============================================================================

# Enable PyTorch CUDA allocator tuning
cuda_allocator_enable = false

# Max split size in MB for CUDA allocator (only used when cuda_allocator_enable = true)
# cuda_allocator_max_split_size_mb = 512

# Enable expandable segments for CUDA allocator (only used when cuda_allocator_enable = true)
# cuda_allocator_expandable_segments = true

# Enable CUDA launch blocking for debugging (synchronous kernel launches)
# Set to true for better error reporting but slower execution
cuda_launch_blocking = false

# Enable unified memory (system RAM as GPU memory)
# Allows GPU to use system RAM when VRAM is exhausted
cuda_managed_force_device_alloc = false

# Specify which CUDA devices to use (e.g., "0" for first GPU, "0,1" for first two)
# Leave commented to use all available GPUs
# cuda_visible_devices = "0"

# CUDA memory fraction to allocate per process (0.0-1.0)
# 1.0 = use default PyTorch behavior (no artificial limit)
# Set to 0.95 to reserve 5% for system/other processes
# cuda_memory_fraction = 0.95

# Empty CUDA cache at startup to free fragmented memory
cuda_empty_cache = false

# Enable/disable Flash Scaled Dot Product Attention
# Sometimes helps with memory usage, may affect performance
# cuda_flash_sdp_enabled = false

# =============================================================================
# ⚠️ MEMORY OPTIMIZATION SETTINGS
# =============================================================================

# Training-safe memory optimization
# When enabled, activates memory-efficient safetensors loading and optional VRAM monitoring
safe_memory_optimization_enabled = false

# Enable memory-efficient SafeTensors loading using memory mapping
# Reduces RAM usage during LoRA weight loading without affecting training behavior
memory_opt_loading_enabled = true

# Enable VRAM/RAM monitoring with automatic garbage collection
# Helps prevent OOM errors during training by proactively managing memory usage
memory_opt_monitoring_enabled = true

# Garbage collection trigger threshold (0.0-1.0, default: 0.85)
# Triggers cleanup when GPU VRAM usage exceeds this ratio
memory_opt_gc_threshold_ratio = 0.85

# Trigger garbage collection every N training steps (default: 100)
# Higher values = less frequent cleanup, lower values = more frequent cleanup
memory_opt_gc_interval_steps = 100

# Monitor memory usage every N training steps (default: 50)
# Controls how often memory statistics are checked and logged
memory_opt_monitor_interval_steps = 50

# =============================================================================
# SAFETENSORS LOADING OPTIMIZATIONS
# =============================================================================

# Use numpy.memmap for large tensors when loading safetensors checkpoints.
# Improves peak memory usage and throughput for large models but may increase
# temporary disk I/O. Disabled by default to preserve previous behavior.
enable_memory_mapping = false

# Use numpy.fromfile for zero-copy loading instead of Python file reads.
# Reduces Python overhead when streaming weights from disk. Requires
# enable_memory_mapping for best results but can be toggled independently.
enable_zero_copy_loading = false

# Perform GPU transfers in a non-blocking manner when loading weights.
# Can overlap I/O and compute on CUDA devices but requires an explicit device
# synchronization before tensors are used. Automatically handled by Takenoko
# when enabled. Disabled by default.
enable_non_blocking_transfers = false

# Byte threshold controlling when memmap acceleration is considered. Increase
# to restrict memmap usage to very large tensors.
# memory_mapping_threshold = 10485760  # 10 MiB default

# =============================================================================
# MEMORY DIAGNOSTICS
# =============================================================================

# Enable comprehensive memory tracking and OOM diagnostics
# When enabled, tracks all memory allocations and provides detailed OOM error analysis
memory_tracking_enabled = false

# Maximum number of allocation records to keep in memory (default: 10000)
# Higher values = more detailed history, more memory overhead
memory_tracking_max_records = 10000

# Capture stack traces for allocations (expensive, only for debugging)
# Provides detailed call stack information for each allocation
memory_tracking_stack_traces = false

# Track individual tensor references and lifecycle
# Helps identify memory leaks and tensor usage patterns
memory_tracking_tensor_tracking = true

# Take automatic memory snapshots every N allocations (default: 100)
# Lower values = more frequent snapshots, higher overhead
memory_tracking_auto_snapshot_interval = 100

# Memory tracking verbosity level ("minimal", "standard", "debug")
# Controls how much detail is collected and displayed
memory_tracking_verbosity = "standard"

# Automatically export diagnostics when OOM errors occur
# Creates detailed JSON reports for post-mortem analysis
memory_tracking_export_on_oom = true

# Directory for exporting memory diagnostic files
memory_tracking_export_directory = "logs"

# =============================================================================
# DUAL MODEL TRAINING SETTINGS
# =============================================================================

# Enable dual model training
# Trains a single LoRA against two base DiTs (low/high noise) by swapping base weights on-the-fly
enable_dual_model_training = false

# DiT checkpoint path for high noise model
dit_high_noise = "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp16.safetensors"

# Timestep boundary for dual model training (0.0 to 1.0)
# Determines the split between low/high noise regimes
timestep_boundary = 0.875

# Offload inactive DiT model to CPU
# Keeps only the active model on GPU; inactive state_dict resides on CPU to save VRAM
offload_inactive_dit = true

# Allow mixed block swap offload
allow_mixed_block_swap_offload = false

# Dual-mode timestep bucketing strategy (interaction of per-epoch timestep buckets with high/low boundary)
# "hybrid" (default): use presampled bucketed t if it matches the boundary side; otherwise resample on-demand
# "presampled": always use dataset-presampled uniform t as-is (may cross boundary)
# "on_demand": ignore presampled t; draw fresh uniform each retry until boundary side matches (up to max retries)
# "strict_clamp": minimally adjust a presampled t to be just inside the target boundary side
dual_timestep_bucket_strategy = "on_demand"

# Maximum retries for on-demand resampling
dual_timestep_bucket_max_retries = 100

# Epsilon for boundary matching
dual_timestep_bucket_eps = 1e-4

# =============================================================================
# ⚠️ CONTROLNET TRAINING SETTINGS
# =============================================================================

# Enable ControlNet training mode
enable_controlnet = false

# Weight for the ControlNet loss
controlnet_weight = 1.0

# Stride for the ControlNet loss
controlnet_stride = 1

# Max gradient norm for ControlNet
controlnet_max_grad_norm = 1.0

# ControlNet configuration
controlnet = {}

# =============================================================================
# ⚠️ CONTROL LORA TRAINING SETTINGS
# =============================================================================

# Enable control LoRA training mode
enable_control_lora = false

# Type of control LoRA: "tile", "canny", "depth", etc.
control_lora_type = "tile"

# Preprocessing method for control signal: "blur", "edge", "depth", etc.
control_preprocessing = "blur"

# Kernel size for Gaussian blur preprocessing
control_blur_kernel_size = 15

# Sigma for Gaussian blur preprocessing
control_blur_sigma = 4.0

# Scale factor for control signal strength
control_scale_factor = 1.0

# Multiplier to learning rate for the input patch_embedding layer if training control lora
input_lr_scale = 5.0

# Dimension index for concatenation of control signal
control_concatenation_dim = 0

# Suffix to append to media file name to get the control file
control_suffix = "_control"

# Add noise to the control latents, at a random strength up to this amount
control_inject_noise = 0.1

# Save control videos created on the fly to disk
save_control_videos = true

# Directory to save control videos for debugging
control_video_save_dir = "tmp/control_videos"

# =============================================================================
# ⚠️ REWARD LORA TRAINING SETTINGS
# =============================================================================

# Enable reward LoRA training mode
enable_reward_lora = false

# List of prompts to use for reward training
reward_prompts = [
  "a red spaceship flying in the sky",
  "a dog running in a park",
]

# Batch size for reward training
reward_train_batch_size = 1

# Height of the sample video for reward training
reward_train_sample_height = 256

# Width of the sample video for reward training
reward_train_sample_width = 256

# alias: reward_num_frames is also accepted
reward_video_length = 81

# Number of inference steps for reward training
reward_num_inference_steps = 50

# Guidance scale for reward training
reward_guidance_scale = 6.0

# Number of decoded latents for reward training
reward_num_decoded_latents = 1

# Number of steps to run validation on
reward_validation_steps = 10000

# Reward function to use for reward training
reward_fn = "HPSReward"

# JSON string
reward_fn_kwargs = '{"version":"v2.1"}'

# Enable backprop for reward training
reward_backprop = true

# Backprop strategy to use for reward training, choices: "last", "tail", "uniform", "random"
reward_backprop_strategy = "tail"

# Number of steps to backprop for reward training
reward_backprop_num_steps = 5

# List of steps to backprop for reward training
reward_backprop_step_list = [45, 46, 47, 48, 49]

# Random start step for reward training
reward_backprop_random_start_step = 0

# Random end step for reward training
reward_backprop_random_end_step = 50

# Stop backprop at the latent model input gradient
reward_stop_latent_model_input_gradient = false

# =============================================================================
# ⚠️ SRPO SETTINGS
# =============================================================================

# Enable SRPO preference training (Direct-Align algorithm with reward models)
# Trains LoRA to maximize reward scores (HPS, PickScore, Aesthetic) via online generation
# Use after supervised pretraining for preference alignment
enable_srpo_training = false

# Reward model selection
# Options: "hps" (Human Preference Score v2.1), "pickscore" (CLIP-based alignment), "aesthetic" (Aesthetic Predictor v2/v2.5)
srpo_reward_model_name = "hps"

# Data type for reward model inference
# Options: "float32" (most accurate), "bfloat16", "float16"
srpo_reward_model_dtype = "float32"

# Semantic Relative Preference (SRP) configuration
# SRP formula: r_srp = (1 + k) * r_positive - r_negative
# Uses relative difference between positive/negative prompt-conditioned rewards
# Controls strength of control word influence on reward
srpo_srp_control_weight = 1.0

# Positive control words for SRP (appended to prompts for positive rewards)
srpo_srp_positive_words = [
    "beautiful", "stunning", "gorgeous", "masterpiece",
    "professional", "high quality", "detailed", "elegant",
    "vibrant", "exquisite", "breathtaking", "magnificent",
    "pristine", "refined", "impeccable", "award-winning", "cinematic"
]

# Negative control words for SRP (appended to prompts for negative rewards)
srpo_srp_negative_words = [
    "ugly", "blurry", "low quality", "distorted",
    "amateur", "poor", "bad", "grainy",
    "noisy", "artifacts", "watermark", "text overlay",
    "oversaturated", "underexposed", "overexposed", "pixelated", "dull"
]

# Direct-Align algorithm parameters
# Sigma interpolation method for noise schedule
# Options: "linear" (uniform distribution), "cosine" (concentrated at mid-range)
srpo_sigma_interpolation_method = "linear"

# Minimum sigma value for noise interpolation (0.0 = clean)
srpo_sigma_interpolation_min = 0.0

# Maximum sigma value for noise interpolation (1.0 = pure noise)
srpo_sigma_interpolation_max = 1.0

# Number of Euler integration steps for online rollout and image recovery
srpo_num_inference_steps = 50

# Classifier-free guidance scale
# WAN does not use CFG during training
# This parameter is only preserved for config compatibility but has no effect
srpo_guidance_scale = 1.0

# Enable SD3-style time shift in sigma schedule (redistributes to focus on mid-range sigmas)
srpo_enable_sd3_time_shift = true

# SD3 time shift parameter (default: 3.0 from SD3 paper)
srpo_sd3_time_shift_value = 3.0

# Discount schedule for gradient backpropagation (denoise branch)
# Minimum discount applied to early denoising steps (0.0 = no gradient flow)
srpo_discount_denoise_min = 0.0

# Maximum discount applied to late denoising steps (1.0 = full gradient flow)
srpo_discount_denoise_max = 1.0

# Discount schedule for gradient backpropagation (inversion branch)
# Starting discount for early inversion steps (1.0 = full gradient flow)
srpo_discount_inversion_start = 1.0

# Ending discount for late inversion steps (0.0 = no gradient flow)
srpo_discount_inversion_end = 0.0

# SRPO training hyperparameters
# Number of videos per SRPO training iteration (lower values = less VRAM, slower training)
srpo_batch_size = 1

# Gradient accumulation steps for SRPO (simulates larger batch sizes)
srpo_gradient_accumulation_steps = 4

# Total number of SRPO training steps (typically 500-1000 after supervised pretraining)
srpo_num_training_steps = 500

# Validation configuration
# Prompts to use for SRPO validation (generates videos and computes rewards)
srpo_validation_prompts = [
    "a dog running in a sunny field",
    "a beautiful sunset over snow-capped mountains",
    "a futuristic city with flying cars at night",
]

# Run validation every N SRPO training steps
srpo_validation_frequency = 50

# Save validation outputs to disk (videos or images)
srpo_save_validation_videos = true

# Save validation outputs as PNG images instead of MP4 videos (faster, smaller file size)
srpo_save_validation_as_images = false

# Multi-frame reward computation
# By default, SRPO only evaluates first frame. These settings enable multi-frame evaluation.
# Options: "first" (frame 0 only), "uniform" (evenly spaced), "boundary" (first+last+middle), "all" (every frame)
srpo_reward_frame_strategy = "first"  

# Number of frames to evaluate (1 = fastest, more = better temporal coverage)
srpo_reward_num_frames = 1  

# How to combine multi-frame rewards: "mean", "min" (conservative), "max" (optimistic), "weighted" (prioritize later frames)
srpo_reward_aggregation = "mean"  

# Video-specific reward models (complement image-based rewards)
# These measure temporal properties that image rewards can't capture
# Enable video-specific rewards (temporal consistency, motion quality)
srpo_enable_video_rewards = false  

# Weight for frame-to-frame consistency (penalizes flickering)
srpo_temporal_consistency_weight = 0.0  # Weight for frame-to-frame consistency (penalizes flickering)

# Weight for optical flow smoothness (rewards coherent motion)
srpo_optical_flow_weight = 0.0  

# Weight for overall motion quality (balanced movement)
srpo_motion_quality_weight = 0.0  # Weight for overall motion quality (balanced movement)

# VAE spatial downsampling factor
srpo_vae_scale_factor = 8

# Number of channels in WAN latent space
srpo_latent_channels = 16

# =============================================================================
# ⚠️ DOP (DIFFERENTIAL OUTPUT PRESERVATION) SETTINGS
# =============================================================================

# Enable DOP regularization
diff_output_preservation = false

# The trigger word/phrase in captions to be replaced for DOP. This must exactly match the trigger in your dataset captions
diff_output_preservation_trigger_word = "ohwx"

# Generic class prompt to replace the trigger word for DOP. This is what the trigger will be replaced with during preservation
diff_output_preservation_class = "a photo of a person"

# Multiplier for the DOP loss (default: 1.0). Higher values = stronger preservation effect
diff_output_preservation_multiplier = 1.0

# =============================================================================
# ⚠️ FLUXFLOW SETTINGS
# =============================================================================

# enable FLUXFLOW temporal augmentation during training
enable_fluxflow = false

# FLUXFLOW perturbation mode (choices: frame, block)
fluxflow_mode = "frame"

# Ratio of frames to shuffle in frame mode (0.0 to 1.0)
fluxflow_frame_perturb_ratio = 0.25

# Size of contiguous frame blocks in block mode
fluxflow_block_size = 4

# Beta: Ratio of blocks to reorder in block mode (0.0 to 1.0), NOT probability!
fluxflow_block_perturb_prob = 0.5

# Dimension index for frames/time in batched video tensor
fluxflow_frame_dim_in_batch = 2

# =============================================================================
# TEMPORAL FREQUENCY-DOMAIN CONSISTENCY SETTINGS
# =============================================================================

# Enable frequency-domain temporal consistency enhancement for video training
enable_frequency_domain_temporal_consistency = false

# Enable motion coherence loss using frequency domain analysis
freq_temporal_enable_motion_coherence = false

# Enable frequency-domain temporal loss between prediction and target
freq_temporal_enable_prediction_loss = false

# Low-frequency threshold for structural component extraction (0.0-1.0)
freq_temporal_low_threshold = 0.3

# High-frequency threshold for motion component extraction (0.0-1.0)
freq_temporal_high_threshold = 0.7

# Loss component weights
# Weight for structural consistency loss
freq_temporal_consistency_weight = 0.05    

# Weight for motion coherence loss  
freq_temporal_motion_weight = 0.05      

# Weight for freq-domain temporal loss
freq_temporal_prediction_weight = 0.08    

# Temporal processing parameters
# Maximum frame distance for consistency
freq_temporal_max_distance = 4            

# Weight decay for distant frames
freq_temporal_decay_factor = 0.8         

# Minimum frames needed to apply enhancement
freq_temporal_min_frames = 4                

# Motion variation threshold for coherence loss
freq_temporal_motion_threshold = 0.1   

# Frequency analysis settings
# Preserve DC (average brightness) component
freq_temporal_preserve_dc = true         

# Automatically adjust thresholds based on content
freq_temporal_adaptive_threshold = false           

# Range for adaptive threshold
freq_temporal_adaptive_range = [0.15, 0.35]  

# Loss scheduling - control when temporal consistency is applied
# Step to start applying temporal loss
freq_temporal_start_step = 0      

# Step to stop applying temporal loss (unset = never)
# freq_temporal_end_step = 1000  

# Number of warmup steps for gradual introduction
freq_temporal_warmup_steps = 100  

# Performance optimization settings
# Cache frequency masks for better performance
freq_temporal_enable_caching = true      

# Maximum number of cached frequency masks
freq_temporal_cache_size = 500                

# Process batch elements in parallel
freq_temporal_batch_parallel = true       

# Apply enhancement every N steps (1=every step)
freq_temporal_apply_every_n_steps = 1             

# Limit frames processed per batch for memory
freq_temporal_max_frames_per_batch = 16 

# Logging cadence for temporal consistency
# Console INFO log interval (steps)
freq_temporal_log_every_steps = 10

# TensorBoard metrics interval (steps)
freq_temporal_tb_log_every_steps = 10

# Advanced settings
# "linear", "exponential", or "uniform"
freq_temporal_weight_strategy = "exponential"  

# "mean", "sum", or "none"
freq_temporal_loss_reduction = "mean"              

# Apply to latent space vs pixel space
freq_temporal_apply_to_latent = true         

# =============================================================================
# ⚠️ TRANSITION TRAINING SETTINGS
# =============================================================================

# Enable TiM-inspired transition training pipeline (false keeps legacy diffusion loop).
transition_training_enabled = false

# Fraction of each batch forced to vanilla diffusion pairs (t == r).
transition_diffusion_ratio = 0.5

# Fraction of each batch clamped to r = t_min for one-step / consistency targets.
transition_consistency_ratio = 0.1

# Derivative estimator for transport targets ("dde" finite diff, "jvp" autograd, "auto" tries DDE then failover).
transition_derivative_mode = "dde"

# Estimator to use when derivative_mode="auto" raises ("dde", "jvp", or "none").
transition_derivative_failover = "jvp"

# Finite-difference step (delta-t) used by the DDE estimator.
transition_finite_difference_eps = 0.005

# Reserved for downstream hooks: interpret sampled times as "discrete" (1..1000) or "normalized" (0..1).
transition_delta_time_domain = "discrete"

# Transport family used to mix latents ("linear", "trigflow", or "vp").
transition_transport = "linear"

# Per-sample loss weighting schedule ("sqrt" uses 1/sqrt(delta-t), "constant", "tau_inverse").
transition_weight_schedule = "sqrt"

# Apply tan reparameterisation before weighting (emphasises late-time deltas).
transition_tangent_weighting = true

# Rescale weights by inverse per-sample loss inside each batch.
transition_adaptive_weighting = true

# Enable LoRA interval adapter to scale multipliers from delta-t features.
transition_lora_interval_modulation = false

# Weight for optional cosine directional loss (set >0 to request intermediate activations).
transition_directional_loss_weight = 0.0

# Maintain an EMA teacher and blend its prediction into the transport target.
transition_use_ema_teacher = false

# Fraction of teacher prediction to mix when enabled (0.0-1.0).
transition_teacher_mix = 0.2

# EMA decay applied when updating the teacher (0 < decay < 1).
transition_teacher_decay = 0.999

# When LoRA modulation is enabled, forward delta-t masks to networks that expose rank-mask hooks.
transition_delta_attention = false

# Width of the harmonic feature stack that drives LoRA multiplier modulation.
transition_delta_mlp_hidden = 64

# =============================================================================
# ⚠️ CONTEXT AS MEMORY SETTINGS
# =============================================================================

# Enable context-as-memory features for scene-consistent long video generation
# When enabled, stores previously generated frames as memory for improved consistency
ctxmem_enabled = false

# Basic context settings
# Number of context frames to use during generation
ctxmem_context_size = 4

# Maximum frames to keep in memory buffer
ctxmem_max_memory_frames = 100

# Update memory every N training steps
ctxmem_memory_update_frequency = 10

# Frame selection strategies:
# - "recent": Select most recent frames (fastest, good baseline)
# - "fov_overlap": Select based on FOV overlap with camera poses (paper's method)
# - "semantic": Select based on visual similarity (experimental)
# - "mixed": Combine recent and semantic selection (experimental)
ctxmem_frame_selection_strategy = "semantic"

# Similarity threshold for semantic selection (0.0 to 1.0)
ctxmem_semantic_similarity_threshold = 0.7

# FOV-based selection settings
# Enable FOV-based frame selection using camera poses
ctxmem_use_fov_selection = false

# Default field of view in degrees (from paper: 52.67)
ctxmem_default_fov = 52.67

# Minimum overlap threshold for FOV-based selection (0.0 to 1.0)
ctxmem_fov_overlap_threshold = 0.1

# Maximum camera distance for FOV overlap consideration (meters)
ctxmem_max_camera_distance = 10.0

# Context conditioning method:
# - "concatenation": Concatenate context frames along temporal dimension (recommended)
ctxmem_context_conditioning_method = "concatenation"

# Weight for context conditioning in loss computation
ctxmem_context_weight = 1.0

# Variable length context support (experimental)
ctxmem_use_variable_length_context = false

# Training enhancements
# Enable progressive context training (start without context, gradually enable)
ctxmem_progressive_context_training = true

# Context warmup steps (when progressive training is enabled)
ctxmem_context_warmup_steps = 500

# Randomly drop context frames during training for regularization
ctxmem_context_dropout_rate = 0.1

# Temporal consistency loss weight (0.0 to disable)
ctxmem_temporal_consistency_loss_weight = 0.1

# Performance optimizations
# Enable caching of frame embeddings for similarity computation
ctxmem_use_context_caching = true

# Enable asynchronous memory updates (experimental)
ctxmem_async_memory_updates = false

# =============================================================================
# ⚠️ FVDM SETTINGS
# =============================================================================

# Enable Frame-Aware Video Diffusion Model (FVDM) training
enable_fvdm = false

# Probabilistic Timestep Sampling Strategy (PTSS) settings
# Fixed PTSS probability
fvdm_ptss_p = 0.2

# Adaptive PTSS: dynamically adjust probability during training
fvdm_adaptive_ptss = true

# Starting probability (higher for exploration) 
fvdm_ptss_initial = 0.3    

# Ending probability (lower for stability)  
fvdm_ptss_final = 0.1      

# Steps before adaptation begins
fvdm_ptss_warmup = 1000    

# FVDM training enhancements
# Weight for temporal consistency loss (0.0 = disabled)
# IMPROVED: Uses second-order motion derivatives with smooth L1 loss
# This penalizes sudden inconsistencies (flickering, artifacts) while preserving smooth motion
# Recommended values: 0.0 (disabled), 0.05-0.1 (mild), 0.2+ (strong)
fvdm_temporal_consistency_weight = 0.1

# Weight for frame diversity regularization (0.0 = disabled)
fvdm_frame_diversity_weight = 0.0         

# Use with AdaptiveTimestepManager if enabled
fvdm_integrate_adaptive_timesteps = false 

# FVDM evaluation and logging
# Compute and log temporal coherence metrics
fvdm_eval_temporal_metrics = true        

# Steps between FVDM metric evaluation and logging
fvdm_eval_frequency = 1000               

# =============================================================================
# ⚠️ RCM DISTILLATION PIPELINE SETTINGS
# =============================================================================

# Toggle the alternate RCM-style distillation trainer. When false the legacy WAN finetuner is used; set true to route UnifiedTrainer through distillation.
rcm_enabled = false

# Force the dispatcher to only run the rCM pipeline when enabled. Set false if you intend to call `dispatch_rcm_pipeline` manually (e.g., experimentation).
rcm_override_wan = true

# Accelerator selection for rCM: "auto" (mirror WAN accelerate settings) or "cpu" to run the distillation loop entirely on CPU for debugging.
rcm_accelerator_mode = "auto"

# rCM runner flavour. "distill" executes the generator/critic loop, while "policy_eval" (future) would run evaluation only.
rcm_trainer_variant = "distill"

# Optional hard ceiling on distillation steps. 0 defers to Takenoko's schedule derived from dataset length / `max_train_steps`.
rcm_max_steps = 0

# Precision override forwarded to the rCM accelerator; valid values mirror accelerate ("fp16", "bf16", "no").
rcm_mixed_precision = "bf16"

# Inline table of rCM-specific knobs (e.g. `synthetic_samples`, `rcm_sigma_min`, `text_tokenizer`). Values here land in `RCMConfig.extra_args` and gate optional behaviours.
rcm_extra_args = { }

# Enable this to keep the entire rCM loop on CPU regardless of accelerator settings—useful when debugging CUDA availability/build issues.
rcm_cpu_debug = false

# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Run validation every N steps (in addition to end-of-epoch validation)
validate_every_n_steps = 500

# Enable validation at the end of each epoch (default: false)
validate_on_epoch_end = true

# Validation timesteps mode:
# - "fixed": use the list in validation_timesteps every time (default)
# - "random": draw validation_timesteps_count integers uniformly within [validation_timesteps_min, validation_timesteps_max]
# - "jitter": for each base timestep in validation_timesteps, draw a timestep within ±validation_timesteps_jitter, clamped to [validation_timesteps_min, validation_timesteps_max]
# - "training_distribution": use the same timestep sampling distribution as training (uniform, sigmoid, content, style, etc.)
validation_timesteps_mode = "fixed"

# Base list of timesteps for validation.
# - When validation_timesteps_mode = "fixed": use exactly this list every validation
# - When validation_timesteps_mode = "jitter": apply ±validation_timesteps_jitter around each base value
# - When validation_timesteps_mode = "random": ignored (random draws are used instead)
# - When validation_timesteps_mode = "training_distribution": ignored (uses training distribution instead)
# Example: '100,300,500,700,900'
validation_timesteps = "100,300,500,700,900"

# For mode == "random" or "training_distribution": number of unique timesteps to draw per validation
validation_timesteps_count = 4

# Global min clamp for random and jitter modes. If omitted, falls back to min_timestep or 0.
validation_timesteps_min = 0

# Global max clamp for random and jitter modes. If omitted, falls back to max_timestep or 1000.
validation_timesteps_max = 1000

# For mode == "jitter": radius to jitter each base timestep (in steps). Example: 50 means draw in [t-50, t+50].
validation_timesteps_jitter = 0

# Use unique noise per batch for proper validation (recommended: true for reliable metrics, false for legacy behavior)
use_unique_noise_per_batch = true

# Compute SSIM-based perceptual SNR on tiny subsample
enable_perceptual_snr = false

# Max samples per validation step for perceptual SNR
perceptual_snr_max_items = 4

# Temporal SSIM (adjacent-frame) validation
enable_temporal_ssim = false

# Per-step cap for temporal SSIM validation
temporal_ssim_max_items = 2

# Subsample frames for speed
temporal_ssim_frame_stride = 1

# LPIPS validation
enable_lpips = false

# Per-step cap for LPIPS validation
lpips_max_items = 2

# Network for LPIPS validation (vgg|alex|squeeze)
lpips_network = "vgg"

# Subsample frames for speed
lpips_frame_stride = 8

# Temporal LPIPS (adjacent-frame) validation
enable_temporal_lpips = false

# Per-step cap for temporal LPIPS validation
temporal_lpips_max_items = 2

# Subsample frames for speed
temporal_lpips_frame_stride = 2

# Flow-warped SSIM (RAFT)
enable_flow_warped_ssim = false

# Model for flow-warped SSIM validation (torchvision_raft_small|torchvision_raft_large)
flow_warped_ssim_model = "torchvision_raft_small"

# Per-step cap for flow-warped SSIM validation
flow_warped_ssim_max_items = 2

# Subsample frames for speed
flow_warped_ssim_frame_stride = 2

# FVD (Fréchet Video Distance)
enable_fvd = false

# Model for FVD validation (torchvision_r3d_18|reference_i3d)
# - torchvision_r3d_18: Fast approximation using R3D-18 (current default)
# - reference_i3d: Canonical implementation using I3D from TensorFlow Hub (more accurate but slower)
fvd_model = "torchvision_r3d_18"

# Per-step cap for FVD validation
fvd_max_items = 2

# Length of each FVD clip
fvd_clip_len = 16

# Subsample frames for speed
fvd_frame_stride = 2

# VMAF (requires ffmpeg with libvmaf installed)
enable_vmaf = false

# Path to VMAF model (optional, empty = ffmpeg default model)
vmaf_model_path = ""

# Per-step cap for VMAF validation
vmaf_max_items = 1

# Length of each VMAF clip
vmaf_clip_len = 16

# Subsample frames for speed
vmaf_frame_stride = 2

# Path to ffmpeg executable
vmaf_ffmpeg_path = "ffmpeg"

# =============================================================================
# ⚠️ OPTICAL FLOW LOSS SETTINGS
# =============================================================================

# Enable RAFT-based optical flow loss for motion stability (experimental)
enable_optical_flow_loss = false

# Weight for the optical flow loss term (set >0 to enable)
lambda_optical_flow = 0.01

# =============================================================================
# ⚠️ MASKED TRAINING SETTINGS
# =============================================================================

# Enable masked training with prior preservation
# Master toggle - MUST be true to enable the entire feature
# Note: Works with DOP (diff_output_preservation) - DOP's prior will be masked if both enabled
use_masked_training_with_prior = false

# Probability of removing mask during training (0.0 = never remove, 1.0 = always remove)
# Helps prevent overfitting to mask boundaries by randomly training on full images
unmasked_probability = 0.1

# Minimum weight for unmasked regions [0-1] 
# 0.1 = light masking (10% weight to unmasked), 0.01 = heavy masking (1% weight to unmasked)
unmasked_weight = 0.1

# Weight for preserving base model predictions in unmasked areas [0-inf] 
# This controls spatial masking within individual images/videos
# 0.0 = disabled, 0.3-0.8 = typical range, higher = stronger preservation of original model
masked_prior_preservation_weight = 0.0

# Normalize loss by masked area size (helps balance training across different mask sizes)
normalize_masked_area_loss = false

# Method for resizing masks to match loss dimensions ("area", "bilinear", "nearest")
# "area" provides cleanest boundaries, recommended for most use cases
mask_interpolation_mode = "area"

# Integration Parameters
# Enable prior prediction computation (required for prior preservation)
enable_prior_computation = true

# Method for computing prior predictions ("lora_disabled" recommended)
# "lora_disabled" = disable LoRA weights when computing base model predictions
prior_computation_method = "lora_disabled"

# Video-specific Parameters
# Weight for temporal consistency loss in videos [0-inf] (0 = disabled)
# 0.1-0.5 typical range, encourages smooth mask transitions between frames
temporal_consistency_weight = 0.0

# Temporal consistency computation method ("adjacent" or "all_pairs")
# "adjacent" = compare adjacent frames only (faster)
# "all_pairs" = compare all frame pairs (more thorough but slower)
frame_consistency_mode = "adjacent"

# =============================================================================
# TREAD SETTINGS
# =============================================================================

# Enable TREAD routing for training
enable_tread = false

# TREAD mode: "full", "frame_contiguous", "frame_stride", "row_contiguous", "row_stride", "row_random", "spatial_auto"
# 'full' is used with tread_config_route parameters
# 'frame_contiguous'/'frame_stride' are used with 'tread' parameter for temporal routing
# 'row_contiguous'/'row_stride'/'row_random' are used with 'tread' parameter for spatial routing
# 'spatial_auto' automatically detects F=1 (rows) vs F>1 (frames) - hybrid auto-detection mode
tread_mode = "full"

# Each route is a semicolon-separated string of key=value pairs. Keys: selection_ratio (0-1), start_layer_idx, end_layer_idx (negative allowed)
tread_config_route1 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=-2"
# tread_config_route2 = "selection_ratio=0.1; start_layer_idx=2; end_layer_idx=8"
# tread_config_route3 = "selection_ratio=0.25; start_layer_idx=9; end_layer_idx=11"
# tread_config_route4 = "selection_ratio=0.35; start_layer_idx=12; end_layer_idx=15"
# tread_config_route5 = "selection_ratio=0.25; start_layer_idx=16; end_layer_idx=23"
# tread_config_route6 = "selection_ratio=0.1; start_layer_idx=24; end_layer_idx=-2"

# Simplified routing blocks:
# For temporal routing: Use tread_mode = "frame_contiguous" or "frame_stride"
# For spatial routing: Use tread_mode = "row_contiguous", "row_stride", or "row_random"
# keep_ratio: fraction of frames/rows to keep within the routed band
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.6 }

# Examples:
# Temporal routing (video frames):
# tread_mode = "frame_contiguous"
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.7 }

# Spatial routing (image rows):
# tread_mode = "row_contiguous"     # Keep center rows
# tread_mode = "row_stride"         # Keep evenly spaced rows
# tread_mode = "row_random"         # Keep randomly selected rows
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.8 }

# Hybrid auto-detection mode:
# tread_mode = "spatial_auto"       # F=1→rows, F>1→frames
# tread = { start_layer = 2, end_layer = 36, keep_ratio = 0.7 }

# Auto-fallback for mixed datasets (when using row modes with video content)
# row_tread_auto_fallback = true   # Default: true (safe for mixed datasets)
row_tread_auto_fallback = true

# Enable strict sanity checks when slicing per-token time projections during
# frame-based routing (verifies index bounds, shapes, and dtypes). Off by default.
strict_e_slicing_checks = false

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

# directory to output trained model
output_dir = "output/wan22_lora"

# base name of trained model file 
output_name = "wan22_lora"

# save training state additionally (including optimizer states etc.) when saving model
save_state = true

# save checkpoint every N steps
save_every_n_steps = 500

# Save checkpoints before generating samples/validation (default: false, saves after)
# When true: checkpoint → sampling → validation
# When false: sampling → validation → checkpoint (current behavior)
# Useful if you want to prioritize checkpoint saving over potentially time-consuming sampling
# Default: false to maintain existing behavior
save_checkpoint_before_sampling = true

# save training state (including optimizer states etc.) on train end even if --save_state is not specified
# save_state_on_train_end = false 

# automatically resume training from latest checkpoint
auto_resume = true

# saved state to resume training
# resume = "path/to/resume/state"

# Resume from a specific step number (overrides initial_epoch and train_state.json)
# Useful for restarting training from an exact point across all epochs
# initial_step = 1000

# Resume from a specific epoch number (ignored if initial_step is set)
# Note: lr scheduler starts from step 0 unless --resume is also used
# initial_epoch = 5

# Skip data loading until reaching the initial step (default: false)
# When true: Uses accelerator.skip_first_batches for proper data ordering
# When false: Fast-forwards step counters without data skipping (faster but different data order)
# skip_until_initial_step = false

# fp16 training including gradients (temporary disabled)
# full_fp16 = false 

# bf16 training including gradients (temporary disabled)
# full_bf16 = false 

# save checkpoint every N epochs
# save_every_n_epochs = 1 

# save last N checkpoints when saving every N epochs (remove older checkpoints)
# save_last_n_epochs = 10 

# save last N checkpoints of state (overrides the value of --save_last_n_epochs)
# save_last_n_epochs_state = 3

# save training state (optimizer, scheduler, etc.) every N epochs, independent of checkpoint saving
# save_state_every_n_epochs = 1

# save training state (optimizer, scheduler, etc.) every N steps, independent of checkpoint saving  
# save_state_every_n_steps = 100 

# save checkpoints until N steps elapsed (remove older checkpoints if N steps elapsed)
# save_last_n_steps = 5000 

# save states until N steps elapsed (overrides --save_last_n_steps, overrides --save_last_n_steps)
# save_last_n_steps_state = 5000 

# allow resuming training with a different dataset (e.g., for domain adaptation or transfer learning)
# allow_dataset_change = true 

# reset epoch/step calculations when resuming with a different dataset (recommended for proper training behavior)
# reset_training_state = true 

# skip dataset compatibility validation entirely (advanced users only - use with caution)
# ignore_dataset_compatibility = true 

# =============================================================================
# METADATA SETTINGS
# =============================================================================

# embed the original config file content in safetensors metadata (default: True)
embed_config_in_metadata = true

# do not save metadata in output model
# no_metadata = true 

# title for model metadata (default is output_name)
# metadata_title = "" 

# author name for model metadata
# metadata_author = "" 

# description for model metadata
# metadata_description = "" 

# license for model metadata
# metadata_license = "" 

# tags for model metadata, separated by comma
# metadata_tags = "" 

# =============================================================================
# LOGGING SETTINGS
# =============================================================================

# Enhanced progress bar with additional metrics (learning rate, epoch, memory usage, etc.), set to false to use simple progress bar if you encounter any issues
enhanced_progress_bar = true

# Log training configuration
log_config = true

# Enable logging and output TensorBoard log to this directory
logging_dir = "logs/wan22_lora"

# Logging tool to use (only tensorboard is supported)
log_with = "tensorboard"

# Set logging level to DEBUG to see FluxFlow messages
logging_level = "INFO"

# Append small emoji hints to TensorBoard tags (e.g., loss 📉, throughput 📈)
tensorboard_append_direction_hints = true

# Add prefix for each log directory
# log_prefix = "" 

# Name of tracker to use for logging, default is script-specific default name
# log_tracker_name = "" 

# Path to tracker config file to use for logging
# log_tracker_config = "" 

# Automatically launch TensorBoard server on training start
launch_tensorboard_server = true

# Host for TensorBoard server
tensorboard_host = "127.0.0.1"

# Port for TensorBoard server  
tensorboard_port = 6006

# Enable auto-reload of logs (default: true)
tensorboard_auto_reload = true

# =============================================================================
# ADDITIONAL LOGGING SETTINGS
# =============================================================================

# Enable periodic logging of additional training loss diagnostics
log_extra_train_metrics = true

# Interval (in steps) to compute and log extra train metrics
train_metrics_interval = 50

# Log a scatter plot of per-sample loss vs timestep to TensorBoard
log_loss_scatterplot = true

# Interval (in steps) to log the scatter plot
log_loss_scatterplot_interval = 100

# Enable throughput metrics logging (samples per second, steps per second, runtime)
log_throughput_metrics = true

# Window size for throughput calculation (number of recent steps to average)
throughput_window_size = 100

# Enable automatic VRAM estimation validation after first training step
# Compares estimated VRAM vs actual peak usage
# Provides accuracy feedback to help calibrate expectations
log_vram_validation = false

# Progress bar postfix metrics mode:
# true  - alternate each step between showing timing (step_ms) and hardware metrics (peak/util)
# false - show both timing and hardware metrics every step
alternate_perf_postfix = false

# Enable advanced training diagnostics (gradient stability, convergence analysis, loss distributions)
# Provides comprehensive metrics similar to WAN 2.2 training reports
# DEFAULT: false (opt-in only, zero overhead when disabled)
enable_advanced_metrics = false

# Advanced metrics features to enable (optional, defaults to all features when enable_advanced_metrics = true)
# Available features: "gradient_stability", "convergence", "noise_split", "oscillation_bounds"
# "gradient_stability" - Track gradient norms with moving averages and threshold alerts
# "convergence" - Multi-scale R² convergence trend analysis (10/25/50/100 step windows)
# "noise_split" - HIGH/LOW noise loss split tracking (requires DualModelManager for WAN dual LoRA)
# "oscillation_bounds" - Track upper/lower loss bounds to detect training stability patterns
# Example: Enable selective features
advanced_metrics_features = ["gradient_stability", "convergence", "oscillation_bounds"]

# Maximum history length for advanced metrics (prevents unbounded memory growth)
# Older data is discarded when history exceeds this limit. DEFAULT: 10000 steps (~390 KB memory)
advanced_metrics_max_history = 10000

# Gradient stability watch threshold (alert when gradient norm exceeds this value)
# Used to detect gradient explosions or instability during training. DEFAULT: 0.5
gradient_watch_threshold = 0.5

# Gradient stability moving average window size (number of recent steps)
# Larger values provide smoother gradient stability metrics. DEFAULT: 10 steps
gradient_stability_window = 10

# Convergence analysis window sizes for multi-scale R² trend analysis
# Analyzes convergence at different time scales to detect short-term and long-term trends
# DEFAULT: [10, 25, 50, 100] steps
convergence_window_sizes = [10, 25, 50, 100]

# Diagnostic attention metrics, if enabled, compute lightweight cross-attention statistics (entropy, top-k mass, token focus)
# at a configurable interval and for a small number of layers to limit overhead.
# They DO NOT store attention maps; only scalars are logged.
enable_attention_metrics = false

# Interval to collect attention metrics (in steps)
attention_metrics_interval = 100

# Maximum number of layers to collect attention metrics from, per step, collect from at most this many cross-attn layers
attention_metrics_max_layers = 2

# Maximum number of queries to collect attention metrics from (subsample queries per head for bounded memory)
attention_metrics_max_queries = 32

# Top-k mass for attention concentration metric
attention_metrics_topk = 16

# Prefix for logged metric names
attention_metrics_log_prefix = "attn"

# Log a small attention heatmap image to TensorBoard 
# recomputes a tiny [queries x tokens] map from detached Q/K once per window.
attention_metrics_log_heatmap = false

# Max heads to average for the heatmap
attention_metrics_heatmap_max_heads = 2

# Max queries (rows) to include in heatmap (subsampled evenly)
attention_metrics_heatmap_max_queries = 32

# Prefix for heatmap tag in TensorBoard
attention_metrics_heatmap_log_prefix = "attn_hm"

# Attention heatmap colormap name (any Matplotlib colormap string, e.g., "magma", "viridis", "plasma")
attention_metrics_heatmap_cmap = "magma"

# Attention heatmap normalization: "log" for LogNorm, "linear" for no explicit normalization
attention_metrics_heatmap_norm = "log"

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmin_pct = 60.0

# Attention heatmap percentile window for contrast stretching (0-100); ignored if invalid
attention_metrics_heatmap_vmax_pct = 99.5

# Attention heatmap figure width in inches
attention_metrics_heatmap_fig_w = 6.0

# Attention heatmap figure height in inches
attention_metrics_heatmap_fig_h = 4.0

# Performance metrics verbosity level for model prediction and target analysis: minimal, standard, debug
performance_verbosity = "standard"

# Split SNR metrics into essential and other namespaces for clarity
snr_split_namespaces = true

# Split validation metrics into essential and detailed namespaces
val_split_namespaces = true

# EMA smoothing factor (0.9-0.99 typical). Higher = smoother, slower to react.
# ema_loss_beta = 0.98

# Number of steps to defer bias correction for early readability
# ema_loss_bias_warmup_steps = 100

# Enable latent quality analysis to check training data characteristics
# Runs once at startup to analyze cached latents and identify potential issues
latent_quality_analysis = false

# Threshold for acceptable mean deviation from 0.0 (absolute value)
# Images with |mean| > threshold will be flagged as potentially problematic
latent_mean_threshold = 0.16

# Threshold for acceptable standard deviation range around 1.0
# Valid std range: [1.0/threshold, threshold] - values outside trigger warnings
latent_std_threshold = 1.35

# Show visual analysis of worst latent using OpenCV (requires opencv-python)
# Displays red/blue heatmap showing positive/negative latent values
latent_quality_visualizer = false

# Log latent quality statistics to TensorBoard for monitoring
# Creates charts showing mean/std distributions and quality analysis results
latent_quality_tensorboard = true

# Enable video-specific temporal analysis (for video training datasets)
# Analyzes temporal consistency, scene transitions, and motion patterns
# Provides video-specific quality metrics beyond basic mean/std analysis
latent_quality_video_analysis = true

# =============================================================================
# TIMESTEP DISTRIBUTION LOGGING
# =============================================================================

# Timestep distribution logging
# Choose how to visualize timestep distributions in TensorBoard during training.
# "off":     disable logging
# "histogram": log TensorBoard histograms (fast; shows shape clearly)
# "chart":     log a static chart image (uses matplotlib if available)
# "both":      log both histogram and chart
log_timestep_distribution = "off"

# How often to log the live (used) timestep distribution during training (in steps)
log_timestep_distribution_interval = 100

# Number of bins for histogram/chart density
log_timestep_distribution_bins = 100

# Log an initial expected distribution at training start
# Uses precomputed buckets if enabled; otherwise simulates via the configured sampler
log_timestep_distribution_init = true

# Log an initial expected distribution at training start only once
log_timestep_distribution_init_once = true

# Number of samples to draw when simulating the initial distribution (when not using precomputed buckets)
log_timestep_distribution_samples = 20000

# Log a live (used) timestep distribution during training (in steps)
log_timestep_distribution_window = 10000

# Band ratios using configured edges, e.g., "850,900,950"
log_timestep_distribution_bands = "0,100,200,300,400,500,600,700,800,900,1000"

# Log the probability mass function (PMF) of the timestep distribution
log_timestep_distribution_pmf = true

# =============================================================================
# DDP SETTINGS
# =============================================================================

# DDP timeout (min, None for default of accelerate)
# ddp_timeout = 1 

# Enable gradient_as_bucket_view for DDP
# ddp_gradient_as_bucket_view = false 

# Enable static_graph for DDP
# ddp_static_graph = false 

# =============================================================================
# DYNAMO SETTINGS
# =============================================================================

# dynamo backend type (default is None)
dynamo_backend = "NO"

# Dynamo mode (choices: default, reduce-overhead, max-autotune)
dynamo_mode = "default"

# Use fullgraph mode for dynamo
# dynamo_fullgraph = false 

# Use dynamic mode for dynamo
# dynamo_dynamic = false # Use dynamic mode for dynamo

# =============================================================================
# ⚠️ VAE TRAINING SETTINGS
# =============================================================================

# VAE training mode: "full", "decoder_only", "encoder_only"
vae_training_mode = "full"

# Weight for KL divergence loss in VAE training
vae_kl_weight = 1e-6

# Reconstruction loss type: "mse", "l1", "huber"
vae_reconstruction_loss = "mse"

# Loss weights default to classic behaviour (MSE + KL). Increase others to blend terms.
vae_mse_weight = 1.0

# Mean absolute error (L1) reconstruction weight for sharper details.
vae_mae_weight = 0.0

# Learned perceptual metric weight. Requires `pip install lpips` and adds VRAM overhead.
vae_lpips_weight = 0.0

# Sobel edge consistency weight to encourage crisp edges.
vae_edge_weight = 0.0

# Rolling window (in steps) for the optional median loss balancer. 0 disables balancing.
vae_loss_balancer_window = 0

# Percentile used for robust median estimates when balancing (0-100].
vae_loss_balancer_percentile = 95

# Use latent mean instead of sampling in decoder-only mode (disables KL term by default).
vae_decoder_latent_mean = true

# =============================================================================
# ⚠️ ONLINE SELF-CORRECTION SETTINGS
# =============================================================================

# Enable lightweight online self-correction cache updates during training (non-invasive; off by default)
# When enabled, the trainer periodically generates short clips into output_dir/self_correction_cache
# and mixes them into training via a hybrid dataloader wrapper.
self_correction_enabled = false

# Do not start self-correction until after this many optimizer steps
self_correction_warmup_steps = 1000

# Regenerate correction clips every N optimizer steps (0 disables periodic updates)
self_correction_update_frequency = 1000

# Maximum number of correction clips to keep in the cache directory
self_correction_cache_size = 200

# Length (frames) of generated correction clips
self_correction_clip_len = 32

# Probability to draw a batch from the correction cache instead of the main dataset
self_correction_batch_ratio = 0.2

# Number of frames for generation
self_correction_sample_steps = 16

# Output width in pixels
self_correction_width = 256

# Output height in pixels 
self_correction_height = 256

# Classifier-free guidance scale (CFG)
self_correction_guidance_scale = 5.0

# Inline prompts for self-correction 
# Each table can include: text, width, height, frames, step, seed, guidance_scale, enum
# If not provided, captions from datasets are used.

[[self_correction_prompts]]
text = "a person walking on a beach at sunset"
frames = 33
step = 16
guidance_scale = 5.0

[[self_correction_prompts]]
text = "a busy downtown street with cars and pedestrians"
frames = 17
step = 12
guidance_scale = 4.5

# =============================================================================
# SAMPLING SETTINGS
# =============================================================================

# Generate sample images every N steps
sample_every_n_steps = 500

# Generate sample images before training
sample_at_first = true

# Generate sample images every N epochs (overwrites n_steps)
# sample_every_n_epochs = 1 

# =============================================================================
# SAMPLE PROMPTS SETTINGS
# =============================================================================

[[sample_prompts]]

# The main prompt text
text = """
Young woman with short blonde hair and grey eyes, tank top, denim shorts, \
in a cluttered open restaurant on a hill above amazing ocean beach
"""

# Output width in pixels
width = 384

# Output height in pixels
height = 384

# Number of frames for video generation
frames = 45

# Random seed for generation
seed = 20250801

# Step for generation
step = 20

# Classifier-free guidance scale (CFG)
cfg_scale = 6.0

# Optional EqM sampler overrides (only used when enable_eqm_mode = true)
# Step size for EqM sampling
# eqm_step_size = 0.0017

# Momentum for EqM sampling
# eqm_momentum = 0.0

# Sampler for EqM sampling, options: "gd", "ngd", "ode", "sde", or "ode_likelihood"
# eqm_sampler = "gd" 

# Enable adaptive sampler for EqM sampling
# eqm_use_adaptive_sampler = false

# Adaptive step size parameters for EqM sampling
# eqm_adaptive_step_min = 1e-5

# Maximum step size for EqM sampling
# eqm_adaptive_step_max = 0.01

# Growth factor for adaptive step size
# eqm_adaptive_growth = 1.05

# Shrink factor for adaptive step size
# eqm_adaptive_shrink = 0.5

# Restart patience for adaptive step size
# eqm_adaptive_restart_patience = 4

# Alignment threshold for adaptive step size
# eqm_adaptive_alignment_threshold = 0.0

# Weighting schedule for EqM sampling, options: "linear", "cosine", or "sigmoid"
# eqm_weighting_schedule = "linear"  

# Number of steps to apply weighting schedule
# eqm_weighting_steps = 1000

# Save NPZ files for EqM sampling
# eqm_save_npz = false

# Directory to save NPZ files for EqM sampling
# eqm_npz_dir = "output/eqm_npz"

# Maximum number of NPZ files to save
# eqm_npz_limit = 64

# =============================================================================
# LATENT CACHE SETTINGS
# =============================================================================

[datasets.latent_cache]

# cache features in VAE on CPU
vae_cache_cpu = true

# data type for VAE, default is float16
vae_dtype = "float16"

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
# batch_size = 4 

# number of workers for dataset, default is cpu count-1
# num_workers = 4 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# delete all existing latent cache files in cache_directory before caching
# purge_before_run = true

# debug mode, choices: image, console, video
# debug_mode = "image" 

# console width, default is 80
# console_width = 80 

# console background color, default is black
# console_back = "black" 

# debug mode: not interactive, number of images to show for each dataset
# console_num_images = 1 

# =============================================================================
# TEXT ENCODER CACHE SETTINGS
# =============================================================================

[datasets.text_encoder_cache]

# use fp8 for Text Encoder model
fp8_t5 = true

# device to use, default is cuda if available
device = "cuda"

# batch size, override dataset config if dataset batch size > this
batch_size = 16

# number of workers for dataset, default is cpu count-1
# num_workers = 4 

# skip existing cache files
skip_existing = false

# keep cache files not in dataset
keep_cache = true

# delete all existing text encoder cache files in cache_directory before caching
# purge_before_run = true

# =============================================================================
# DATASET SETTINGS
# =============================================================================

[datasets.general]

# File extension for caption files
caption_extension = ".txt"

# Probability to drop entire captions (0.0-1.0). When >0, a random portion of items will have empty captions during
# caching/training. Defaults to 0.0 (disabled).
# caption_dropout_rate = 0.1

# Enable bucket-based training for different resolutions
enable_bucket = true

# Prevent upscaling of images to larger bucket sizes
bucket_no_upscale = true

# Directory containing mask images for masked training
# When using masked training, ensure your dataset includes mask files alongside images/videos
# Masks should be binary: 0 = unmasked regions, 255 = masked regions
# For videos: masks can be per-frame or single mask applied to all frames
# mask_path = "/path/to/mask/images" 

# =============================================================================
# TRAINING DATASETS
# =============================================================================

# Image dataset
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "path/to/images"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/images/cache"

# Target resolution [width, height] (overrides general setting)
resolution = [512, 512]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Path to mask images for this dataset (overrides general setting)
# mask_path = "/path/to/mask/images" 

# Video dataset
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "path/to/videos"

# Path to cache directory
cache_directory = "path/to/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Supported frame extraction modes
# - head: take first window, requires setting target_frames. 
# - middle: take centered window, requires setting target_frames.
# - chunk: non-overlapping contiguous windows of length target_frame across the video, requires setting target_frames.
# - slide: sliding windows, requires setting target_frames and frame_stride (>=1), default is 1. Overlap (frames) = target_frame - frame_stride.
# - slide_end: sliding windows that guarantee the final window ends at the last frame. If the video is shorter than target_frame, produce a single full-length window. Requires target_frames and frame_stride (>=1). Overlap (frames) = target_frame - frame_stride.
# - uniform: fixed count of evenly spaced start positions, requires setting target_frames and frame_sample (>=1). If frame_sample == 1, it is auto-changed to "head" by the loader.
# - multiple_overlapping: minimally cover the video with windows (may overlap), end-aligned, requires setting target_frames.
# - adaptive: intelligently handles variable clip lengths - if shorter than target, takes whole clip; if longer, takes multiple fragments with possible overlap, max_frames caps the effective video length, requires setting target_frames.
# - uniform_adaptive: uniform sampling that accepts short videos - if shorter than target, takes whole clip; if longer, uses uniform sampling, max_frames caps the effective video length, requires setting target_frames and frame_sample.
# - full: use up to max_frames (rounded to N*4+1) starting at 0, requires setting target_frames to be non-empty.
# Uses max_frames to cap length (optional). If multiple target_frames are provided, one identical (0, use_frames) window is produced per entry; recommended target_frames = [1].
frame_extraction = "uniform"

# Extract different frame sequence lengths
target_frames = [1, 17, 33]

# Extract each target_frame sequence once
frame_sample = 1

# Stride between sliding windows; used only when frame_extraction = "slide"
# frame_stride = 1

# Cap total frames used by frame_extraction = "full"; final length is rounded to (4n+1)
# max_frames = 129

# Source FPS
source_fps = 24

# Path to mask images/videos for this dataset
# mask_path = "/path/to/video/masks" 

# Regularization image dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to image directory (overrides general setting)
image_directory = "/path/to/your/regularization_images"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# Marks regularization dataset
is_reg = true

# Regularization video dataset, captions should be generic and NOT include unique trigger word
[[datasets.train]]

# Path to video directory (overrides general setting)
video_directory = "/path/to/your/regularization_videos"

# Number of times to repeat this dataset per epoch (overrides general setting)
num_repeats = 1

# Target resolution [width, height] (overrides general setting)
resolution = [960, 544]

# Batch size for this dataset (overrides general setting)
batch_size = 1

# marks regularization dataset
is_reg = true

# =============================================================================
# VALIDATION DATASETS
# =============================================================================

# Image validation dataset
[[datasets.val]]

# Path to image directory (overrides general setting)
image_directory = "path/to/val/images"

# Path to cache directory
cache_directory = "path/to/val/images/cache"

# Target resolution [width, height]
resolution = [512, 512]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Video validation dataset
[[datasets.val]]

# Path to video directory (overrides general setting)
video_directory = "path/to/val/videos"

# Path to cache directory (overrides general setting)
cache_directory = "path/to/val/videos/cache"

# Target resolution [width, height]
resolution = [256, 256]

# Batch size for this dataset
batch_size = 1

# Number of times to repeat this dataset per epoch
num_repeats = 1

# Use head sampling
frame_extraction = "head"

# Extract different frame sequence lengths
target_frames = [1, 25]

# Extract each target_frame sequence once
frame_sample = 1
