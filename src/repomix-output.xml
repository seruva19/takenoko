This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
caching/cache_latents.py
caching/cache_text_encoder_outputs.py
common/dependencies.py
common/global_seed.py
common/logger.py
common/model_downloader.py
common/performance_logger.py
common/sai_model_spec.py
common/vram_estimator.py
conditioning/control_processor.py
core/checkpoint_manager.py
core/config.py
core/control_signal_processor.py
core/dual_model_manager.py
core/metrics.py
core/model_manager.py
core/optimizer_manager.py
core/repa_helper.py
core/sampling_manager.py
core/training_core.py
core/vae_training_core.py
core/validation_core.py
core/wan_network_trainer.py
criteria/clustered_mse_loss.py
criteria/dispersive_loss.py
criteria/dwt_loss.py
criteria/ew_loss.py
criteria/training_loss.py
dataset/buckets.py
dataset/cache.py
dataset/config_utils.py
dataset/data_sources.py
dataset/datasource_utils.py
dataset/extensions.py
dataset/frame_extraction.py
dataset/hybrid_group.py
dataset/image_video_dataset.py
dataset/item_info.py
generation/sampling.py
masking/mask_utils.py
modules/custom_offloading_utils.py
modules/fp8_optimization_utils.py
modules/scheduling_flow_match_discrete.py
modules/unet_causal_3d_blocks.py
networks/control_lora_wan.py
networks/controlnet_wan.py
networks/lora_wan.py
networks/reward_lora.py
networks/vae_wan.py
optimizers/adafactor.py
optimizers/adamw_8bit_kahan.py
optimizers/automagic.py
optimizers/enhanced_logging.py
optimizers/fira_optimizer.py
optimizers/fourier_loss.py
optimizers/gradient_release.py
optimizers/hina_adaptive.py
optimizers/muon.py
optimizers/optimizer_utils.py
optimizers/prodigy_8bit.py
optimizers/safe_globals_manager.py
optimizers/sana_optimizer.py
optimizers/soap.py
optimizers/sophia.py
reward/clip_model.py
reward/cross_modeling.py
reward/improved_aesthetic_predictor.py
reward/reward_fn.py
reward/reward_training_core.py
reward/siglip_v2_5.py
scheduling/fopp.py
scheduling/fvdm.py
scheduling/timestep_distribution.py
scheduling/timestep_logging.py
scheduling/timestep_utils.py
self_correction/manager.py
self_correction/setup.py
takenoko.py
utils/device_utils.py
utils/ema.py
utils/fluxflow_augmentation.py
utils/lora_utils.py
utils/memory_utils.py
utils/model_utils.py
utils/regularization_utils.py
utils/safetensors_utils.py
utils/tensorboard_utils.py
utils/train_utils.py
utils/tread.py
utils/vae_utils.py
wan/configs/config.py
wan/configs/shared_config.py
wan/configs/wan_t2v_14B.py
wan/configs/wan_t2v_A14B.py
wan/modules/attention.py
wan/modules/model.py
wan/modules/t5.py
wan/modules/tokenizers.py
wan/modules/vae.py
wan/utils/fm_solvers_unipc.py
wan/utils/fm_solvers.py
wan/utils/utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="caching/cache_latents.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/cache_latents.py (Apache)

import argparse
import os
from typing import Any, Optional, Union
import numpy as np
from tqdm import tqdm
from PIL import Image

import torch

from dataset.cache import save_latent_cache_wan
from dataset.image_video_dataset import BaseDataset
from dataset.item_info import ItemInfo
from wan.modules.vae import WanVAE

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def show_image(
    image: Union[list[Union[Image.Image, np.ndarray], Union[Image.Image, np.ndarray]]],  # type: ignore
) -> int:
    """Display image using OpenCV window"""
    import cv2

    imgs = (
        [image]
        if (isinstance(image, np.ndarray) and len(image.shape) == 3)
        or isinstance(image, Image.Image)
        else [image[0], image[-1]]
    )
    if len(imgs) > 1:
        print(f"Number of images: {len(image)}")
    for i, img in enumerate(imgs):
        if len(imgs) > 1:
            print(f"{'First' if i == 0 else 'Last'} image: {img.shape}")  # type: ignore
        else:
            print(f"Image: {img.shape}")  # type: ignore
        cv2_img = np.array(img) if isinstance(img, Image.Image) else img
        cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_RGB2BGR)
        cv2.imshow("image", cv2_img)
        k = cv2.waitKey(0)
        cv2.destroyAllWindows()
        if k == ord("q") or k == ord("d"):
            return k
    return k


def show_console(
    image: Union[list[Union[Image.Image, np.ndarray], Union[Image.Image, np.ndarray]]],  # type: ignore
    width: int,
    back: str,
    interactive: bool = False,
) -> int:
    """Display image using ASCII art in console"""
    from ascii_magic import from_pillow_image, Back  # type: ignore

    # Map provided background name to ascii_magic Back if specified
    if back is not None:
        back = getattr(Back, back.upper())

    k = None
    imgs = (
        [image]
        if (isinstance(image, np.ndarray) and len(image.shape) == 3)
        or isinstance(image, Image.Image)
        else [image[0], image[-1]]
    )
    if len(imgs) > 1:
        print(f"Number of images: {len(image)}")
    for i, img in enumerate(imgs):
        if len(imgs) > 1:
            print(f"{'First' if i == 0 else 'Last'} image: {img.shape}")  # type: ignore
        else:
            print(f"Image: {img.shape}")  # type: ignore
        pil_img = img if isinstance(img, Image.Image) else Image.fromarray(img)
        ascii_img = from_pillow_image(pil_img)
        ascii_img.to_terminal(columns=width, back=back)

        if interactive:
            k = input("Press q to quit, d to next dataset, other key to next: ")
            if k == "q" or k == "d":
                return ord(k)

    if not interactive:
        return ord(" ")
    return ord(k) if k else ord(" ")


def save_video(
    image: Union[list[Union[Image.Image, np.ndarray], Union[Image.Image, np.ndarray]]],  # type: ignore
    cache_path: str,
    fps: int = 24,
):
    """Save video or image to file"""
    import av

    directory = os.path.dirname(cache_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

    if (isinstance(image, np.ndarray) and len(image.shape) == 3) or isinstance(
        image, Image.Image
    ):
        # save image
        image_path = cache_path.replace(".safetensors", ".jpg")
        img = image if isinstance(image, Image.Image) else Image.fromarray(image)
        img.save(image_path)
        print(f"Saved image: {image_path}")
    else:
        imgs = image
        print(f"Number of images: {len(imgs)}")
        # save video
        video_path = cache_path.replace(".safetensors", ".mp4")
        # Determine frame dimensions for stream robustly
        first_frame = imgs[0]
        if isinstance(first_frame, Image.Image):
            first_np = np.array(first_frame)
            height, width = first_np.shape[0:2]
        else:
            height, width = first_frame.shape[0:2]  # numpy array (H, W, C)

        # create output container
        container = av.open(video_path, mode="w")

        # create video stream
        codec = "libx264"
        pixel_format = "yuv420p"
        stream = container.add_stream(codec, rate=fps)
        stream.width = width  # type: ignore
        stream.height = height  # type: ignore
        stream.pix_fmt = pixel_format  # type: ignore
        stream.bit_rate = 1000000  # type: ignore # 1Mbit/s for preview quality

        for frame_img in imgs:
            if isinstance(frame_img, Image.Image):
                frame = av.VideoFrame.from_image(frame_img)
            else:
                frame = av.VideoFrame.from_ndarray(frame_img, format="rgb24")
            packets = stream.encode(frame)  # type: ignore
            for packet in packets:
                container.mux(packet)

        for packet in stream.encode():  # type: ignore
            container.mux(packet)

        container.close()

        print(f"Saved video: {video_path}")


def show_datasets(
    datasets: list[BaseDataset],
    debug_mode: str,
    console_width: int,
    console_back: str,
    console_num_images: Optional[int],
    fps: int = 24,
):
    """Display datasets for debugging purposes"""
    if debug_mode != "video":
        print(f"d: next dataset, q: quit")

    num_workers = max(1, os.cpu_count() - 1)  # type: ignore
    for i, dataset in enumerate(datasets):
        print(f"Dataset [{i}]")
        batch_index = 0
        num_images_to_show = console_num_images
        k = None
        for key, batch in dataset.retrieve_latent_cache_batches(num_workers):
            print(f"bucket resolution: {key}, count: {len(batch)}")
            for j, item_info in enumerate(batch):
                item_info: ItemInfo
                print(f"{batch_index}-{j}: {item_info}")
                if debug_mode == "image":
                    k = show_image(item_info.content)  # type: ignore
                elif debug_mode == "console":
                    k = show_console(
                        item_info.content,  # type: ignore
                        console_width,
                        console_back,
                        console_num_images is None,
                    )
                    if num_images_to_show is not None:
                        num_images_to_show -= 1
                        if num_images_to_show == 0:
                            k = ord("d")  # next dataset
                elif debug_mode == "video":
                    save_video(item_info.content, item_info.latent_cache_path, fps)  # type: ignore
                    k = None  # save next video

                if k == ord("q"):
                    return
                elif k == ord("d"):
                    break
            if k == ord("d"):
                break
            batch_index += 1


def encode_and_save_batch(
    vae: WanVAE,
    batch: list[ItemInfo],
    args: Optional[argparse.Namespace] = None,
):
    """Encode and save a batch of items using WAN VAE"""
    contents = torch.stack([torch.from_numpy(item.content) for item in batch])
    if len(contents.shape) == 4:
        contents = contents.unsqueeze(1)  # B, H, W, C -> B, F, H, W, C

    contents = contents.permute(0, 4, 1, 2, 3).contiguous()  # B, C, F, H, W
    contents = contents.to(vae.device, dtype=vae.dtype)
    contents = contents / 127.5 - 1.0  # normalize to [-1, 1]

    h, w = contents.shape[3], contents.shape[4]
    if h < 8 or w < 8:
        item = batch[0]  # other items should have the same size
        raise ValueError(
            f"Image or video size too small: {item.item_key} and {len(batch) - 1} more, size: {item.original_size}"
        )

    # Encode main content
    with (
        torch.amp.autocast(device_type=vae.device.type, dtype=vae.dtype),  # type: ignore
        torch.no_grad(),
    ):
        latent = vae.encode(contents)  # list of Tensor[C, F, H, W]
    latent = torch.stack(latent, dim=0)  # B, C, F, H, W
    latent = latent.to(vae.dtype)  # convert to bfloat16

    # Handle control signals
    control_latent = None
    if hasattr(batch[0], "control_content") and batch[0].control_content is not None:
        # Process control signals
        control_contents = torch.stack(
            [torch.from_numpy(item.control_content) for item in batch]
        )
        if len(control_contents.shape) == 4:
            control_contents = control_contents.unsqueeze(
                1
            )  # B, H, W, C -> B, F, H, W, C

        control_contents = control_contents.permute(
            0, 4, 1, 2, 3
        ).contiguous()  # B, C, F, H, W
        control_contents = control_contents.to(vae.device, dtype=vae.dtype)
        control_contents = control_contents / 127.5 - 1.0  # normalize to [-1, 1]

        # Apply blurring preprocessing before VAE encoding (matching training behavior)
        control_lora_type = getattr(args, "control_lora_type", "tile")
        control_preprocessing = getattr(args, "control_preprocessing", "blur")
        control_blur_kernel_size = getattr(args, "control_blur_kernel_size", 15)
        control_blur_sigma = getattr(args, "control_blur_sigma", 3.0)

        if control_lora_type == "tile" and control_preprocessing == "blur":
            # Apply blur preprocessing like in training
            from torchvision.transforms import v2

            # Convert to BFCHW format for preprocessing
            control_contents = control_contents.movedim(1, 2)  # BCFHW -> BFCHW

            # Apply blur preprocessing
            height, width = control_contents.shape[-2:]
            blur = v2.Compose(
                [
                    v2.Resize(size=(height // 4, width // 4)),
                    v2.Resize(size=(height, width)),
                    v2.GaussianBlur(
                        kernel_size=control_blur_kernel_size,
                        sigma=control_blur_sigma,
                    ),
                ]
            )

            # Apply blur to each batch item
            blurred_contents = []
            for i in range(control_contents.shape[0]):
                blurred_item = blur(control_contents[i])
                blurred_contents.append(blurred_item)

            control_contents = torch.stack(blurred_contents, dim=0)
            control_contents = torch.clamp(
                torch.nan_to_num(control_contents), min=-1, max=1
            )

            # Convert back to BCFHW format for VAE encoding
            control_contents = control_contents.movedim(1, 2)  # BFCHW -> BCFHW

        with torch.no_grad():
            control_latent = vae.encode(control_contents)
            control_latent = torch.stack(control_latent, dim=0)  # list to tensor

    # Save main latents
    for i, item in enumerate(batch):
        l = latent[i]
        cctx = None
        y_i = None
        save_latent_cache_wan(item, l, cctx, y_i)

    # Save control latents if available
    if control_latent is not None:
        for item, cl in zip(batch, control_latent):
            control_cache_path = item.latent_cache_path.replace(  # type: ignore
                ".safetensors", "_control.safetensors"
            )
            # Create a temporary ItemInfo for control latent
            control_item = ItemInfo(
                item_key=item.item_key,
                caption=item.caption,
                original_size=item.original_size,
                bucket_size=item.bucket_size,
                frame_count=item.frame_count,
                content=item.content,
                latent_cache_path=control_cache_path,
                weight=item.weight,
            )
            save_latent_cache_wan(control_item, cl, None, None)

    # Save mask tensors if provided (per-pixel weights), to be used at training time
    # Expect shape per item: (F, H, W) or (H, W) for images.
    if hasattr(batch[0], "mask_content") and batch[0].mask_content is not None:
        from safetensors.torch import save_file as _save_file

        for item in batch:
            try:
                mask_obj = item.mask_content
                mask_array: Optional[np.ndarray] = None  # type: ignore
                if isinstance(mask_obj, np.ndarray):
                    mask_array = mask_obj
                elif isinstance(mask_obj, (list, tuple)) and len(mask_obj) > 0 and isinstance(mask_obj[0], np.ndarray):  # type: ignore
                    mask_array = np.stack(mask_obj, axis=0)
                else:
                    # Unsupported or empty
                    continue

                # Ensure shape (F, H, W)
                if mask_array.ndim == 2:
                    mask_array = mask_array[None, ...]

                mask_tensor = torch.from_numpy(mask_array).to(torch.float32)
                # Normalize to [0,1] if values appear in [0,255]
                if torch.isfinite(mask_tensor).any() and mask_tensor.max() > 1.0:
                    mask_tensor = mask_tensor / 255.0

                F, H, W = (
                    int(mask_tensor.shape[0]),
                    int(mask_tensor.shape[-2]),
                    int(mask_tensor.shape[-1]),
                )
                key_name = f"latents_{F}x{H}x{W}_float32"

                # Persist alongside latent cache for the same item
                mask_cache_path = item.latent_cache_path.replace(".safetensors", "_mask.safetensors")  # type: ignore
                _save_file({key_name: mask_tensor.cpu()}, mask_cache_path)
            except Exception as e:
                logger.warning(f"Failed to save mask cache for {item.item_key}: {e}")


def encode_datasets(
    datasets: list[BaseDataset], encode: callable, args: argparse.Namespace  # type: ignore
):
    """Encode all datasets using the provided encode function"""
    num_workers = (
        args.num_workers if args.num_workers is not None else max(1, os.cpu_count() - 1)  # type: ignore
    )
    for i, dataset in enumerate(datasets):
        logger.info(f"Encoding dataset [{i}]")
        all_latent_cache_paths = []
        for _, batch in tqdm(dataset.retrieve_latent_cache_batches(num_workers)):
            all_latent_cache_paths.extend([item.latent_cache_path for item in batch])

            if args.skip_existing:
                filtered_batch = [
                    item for item in batch if not os.path.exists(item.latent_cache_path)
                ]
                if len(filtered_batch) == 0:
                    continue
                batch = filtered_batch

            bs = args.batch_size if args.batch_size is not None else len(batch)
            for i in range(0, len(batch), bs):
                encode(batch[i : i + bs])

        # normalize paths
        all_latent_cache_paths = [os.path.normpath(p) for p in all_latent_cache_paths]
        all_latent_cache_paths = set(all_latent_cache_paths)

        # remove old cache files not in the dataset
        all_cache_files = dataset.get_all_latent_cache_files()
        for cache_file in all_cache_files:
            if os.path.normpath(cache_file) not in all_latent_cache_paths:
                if args.keep_cache:
                    logger.info(f"Keep cache file not in the dataset: {cache_file}")
                else:
                    os.remove(cache_file)
                    logger.info(f"Removed old cache file: {cache_file}")
</file>

<file path="caching/cache_text_encoder_outputs.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/cache_text_encoder_outputs.py (Apache)

import argparse
import os
from typing import Optional, Union

import numpy as np
import torch
from tqdm import tqdm

from dataset import config_utils
from dataset.cache import save_text_encoder_output_cache_wan
from dataset.config_utils import BlueprintGenerator, ConfigSanitizer
from dataset.image_video_dataset import BaseDataset
from dataset.item_info import ItemInfo
from wan.configs import wan_t2v_14B
from wan.modules.t5 import T5EncoderModel

import accelerate
import logging
from common.logger import get_logger
from utils.model_utils import str_to_dtype
from common.model_downloader import download_model_if_needed

logger = get_logger(__name__, level=logging.INFO)


def prepare_cache_files_and_paths(datasets: list[BaseDataset]):
    """Prepare cache files and paths for text encoder output caching"""
    all_cache_files_for_dataset = []  # existing cache files
    all_cache_paths_for_dataset = []  # all cache paths in the dataset
    for dataset in datasets:
        all_cache_files = [
            os.path.normpath(file)
            for file in dataset.get_all_text_encoder_output_cache_files()
        ]
        all_cache_files = set(all_cache_files)
        all_cache_files_for_dataset.append(all_cache_files)

        all_cache_paths_for_dataset.append(set())
    return all_cache_files_for_dataset, all_cache_paths_for_dataset


def process_text_encoder_batches(
    num_workers: Optional[int],
    skip_existing: bool,
    batch_size: int,
    datasets: list[BaseDataset],
    all_cache_files_for_dataset: list[set],
    all_cache_paths_for_dataset: list[set],
    encode: callable,  # type: ignore
):
    """Process text encoder batches across all datasets"""
    num_workers = num_workers if num_workers is not None else max(1, os.cpu_count() - 1)  # type: ignore
    for i, dataset in enumerate(datasets):
        logger.info(f"Encoding dataset [{i}]")
        all_cache_files = all_cache_files_for_dataset[i]
        all_cache_paths = all_cache_paths_for_dataset[i]
        for batch in tqdm(
            dataset.retrieve_text_encoder_output_cache_batches(num_workers)
        ):
            # update cache files (it's ok if we update it multiple times)
            all_cache_paths.update(
                [
                    os.path.normpath(item.text_encoder_output_cache_path)
                    for item in batch
                ]
            )

            # skip existing cache files
            if skip_existing:
                filtered_batch = [
                    item
                    for item in batch
                    if not os.path.normpath(item.text_encoder_output_cache_path)
                    in all_cache_files
                ]
                # print(f"Filtered {len(batch) - len(filtered_batch)} existing cache files")
                if len(filtered_batch) == 0:
                    continue
                batch = filtered_batch

            bs = batch_size if batch_size is not None else len(batch)
            for i in range(0, len(batch), bs):
                encode(batch[i : i + bs])


def post_process_cache_files(
    datasets: list[BaseDataset],
    all_cache_files_for_dataset: list[set],
    all_cache_paths_for_dataset: list[set],
    keep_cache: bool,
):
    """Post-process cache files to remove old ones not in dataset"""
    for i, dataset in enumerate(datasets):
        all_cache_files = all_cache_files_for_dataset[i]
        all_cache_paths = all_cache_paths_for_dataset[i]
        for cache_file in all_cache_files:
            if cache_file not in all_cache_paths:
                if keep_cache:
                    logger.info(f"Keep cache file not in the dataset: {cache_file}")
                else:
                    os.remove(cache_file)
                    logger.info(f"Removed old cache file: {cache_file}")


def encode_and_save_text_encoder_output_batch(
    text_encoder: T5EncoderModel,
    batch: list[ItemInfo],
    device: torch.device,
    accelerator: Optional[accelerate.Accelerator],
    args: Optional[argparse.Namespace] = None,  # <-- ADD THIS ARGUMENT
):
    """Encode and save a batch of text prompts using WAN T5 encoder"""
    prompts = [item.caption for item in batch]

    dop_enabled = getattr(args, "diff_output_preservation", False)
    trigger_word = getattr(args, "diff_output_preservation_trigger_word", None)
    preservation_class = getattr(args, "diff_output_preservation_class", None)

    prompts_for_preservation = []
    has_dop_prompts = False

    if dop_enabled and trigger_word and preservation_class:
        for prompt in prompts:
            if trigger_word in prompt:
                prompts_for_preservation.append(
                    prompt.replace(trigger_word, preservation_class)
                )
                has_dop_prompts = True
            else:
                # If a prompt doesn't have the trigger, it doesn't need a preservation version.
                # We add a placeholder to keep the list lengths aligned.
                prompts_for_preservation.append(None)

    # Combine original and preservation prompts for a single encoding pass
    combined_prompts = list(prompts)
    prompts_to_encode = []
    if has_dop_prompts:
        # Only add non-None preservation prompts to the encoding batch
        prompts_to_encode = [p for p in prompts_for_preservation if p is not None]
        combined_prompts.extend(prompts_to_encode)

    with torch.no_grad():
        if accelerator is not None:
            with accelerator.autocast():
                # Encode all prompts in one go
                context = text_encoder(combined_prompts, device)
        else:
            context = text_encoder(combined_prompts, device)

    # Split the results back
    training_context = context[: len(prompts)]
    preservation_context_encoded = context[len(prompts) :]

    # Re-align preservation embeddings with the original batch
    preservation_context_full = []
    encoded_idx = 0
    if has_dop_prompts:
        for prompt in prompts_for_preservation:
            if prompt is not None:
                preservation_context_full.append(
                    preservation_context_encoded[encoded_idx]
                )
                encoded_idx += 1
            else:
                preservation_context_full.append(None)

    # save prompt cache
    for i, item in enumerate(batch):
        ctx = training_context[i]

        # Get the corresponding preservation context
        preservation_ctx = preservation_context_full[i] if has_dop_prompts else None

        # Pass both embeddings to the save function
        save_text_encoder_output_cache_wan(item, ctx, preservation_ctx)
</file>

<file path="common/dependencies.py">
"""
Centralized optional dependency management to prevent duplicate warnings.

This module provides a global warning system that ensures each missing optional
dependency is only warned about once across all modules.
"""

import logging
from typing import Any, Dict, Set


def get_simple_logger(name: str, level=logging.INFO) -> logging.Logger:
    """Simple logger without custom formatting to avoid circular imports."""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(level)
        formatter = logging.Formatter("%(levelname)s: %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.propagate = False
    return logger


logger = get_simple_logger(__name__, level=logging.INFO)

# Global tracking of warnings to prevent duplicates across all modules
_GLOBAL_WARNINGS_SHOWN: Set[str] = set()


def try_import_with_warning(
    module_name: str,
    import_items: list[str] | None = None,
    success_message: str | None = None,
    warning_message: str | None = None,
) -> tuple[bool, Dict[str, Any]]:
    """
    Try to import a module and its items with centralized warning tracking.

    Args:
        module_name: Name of the module to import (e.g., 'flash_attn')
        import_items: List of items to import from the module (e.g., ['flash_attn_func'])
        success_message: Custom success message (defaults to "‚úÖÔ∏è {module_name}: available")
        warning_message: Custom warning message (defaults to "‚ö†Ô∏è {module_name}: not available")

    Returns:
        tuple: (success: bool, imports: Dict[str, Any])
               success is True if import succeeded, False otherwise
               imports contains the imported items (None values if import failed)
    """
    if success_message is None:
        success_message = f"‚úÖÔ∏è {module_name}: available"
    if warning_message is None:
        warning_message = f"‚ö†Ô∏è {module_name}: not available"

    imports = {}

    try:
        from importlib import import_module

        # Import the main module
        module = import_module(module_name)

        # Import specific items if requested
        if import_items:
            for item in import_items:
                if (
                    "." in item
                ):  # Handle nested imports like 'flash_attn.flash_attn_interface.flash_attn_func'
                    full_module_path, attr_name = item.rsplit(".", 1)
                    submodule = import_module(full_module_path)
                    imports[attr_name] = getattr(submodule, attr_name)
                else:
                    imports[item] = getattr(module, item)
        else:
            # For modules without specific import items, use the last part of the module name as key
            key = module_name.split(".")[-1]
            imports[key] = module

        # Only log success message once per module
        if module_name not in _GLOBAL_WARNINGS_SHOWN:
            if success_message != "":
                logger.info(success_message)
            _GLOBAL_WARNINGS_SHOWN.add(module_name)

        return True, imports

    except ImportError:
        # Only show warning once globally using an environment variable
        import os

        env_var = f"{module_name.upper().replace('.', '_')}_WARNING_SHOWN"
        if not os.environ.get(env_var, False):
            logger.warning(warning_message)
            os.environ[env_var] = "1"
        # Set all imports to None
        if import_items:
            for item in import_items:
                if "." in item:
                    imports[item.split(".")[-1]] = None
                else:
                    imports[item] = None
        else:
            key = module_name.split(".")[-1]
            imports[key] = None

        return False, imports


def setup_flash_attention():
    """Setup FlashAttention imports with centralized warning management."""
    # Import the main module first
    success, main_imports = try_import_with_warning(
        "flash_attn",
        success_message="‚úÖÔ∏è FlashAttention: available",
        warning_message="‚ö†Ô∏è FlashAttention: not available",
    )

    if not success:
        return None, None, None, None

    # Import specific functions from submodules
    success, func_imports = try_import_with_warning(
        "flash_attn.flash_attn_interface",
        ["_flash_attn_forward", "flash_attn_varlen_func", "flash_attn_func"],
    )

    return (
        main_imports["flash_attn"],
        func_imports["_flash_attn_forward"],
        func_imports["flash_attn_varlen_func"],
        func_imports["flash_attn_func"],
    )


def setup_sageattention():
    """Setup SageAttention imports with centralized warning management."""
    success, imports = try_import_with_warning(
        "sageattention",
        ["sageattn_varlen", "sageattn"],
        "‚úÖÔ∏è SageAttention: available",
        "‚ö†Ô∏è SageAttention: not available",
    )

    return imports["sageattn_varlen"], imports["sageattn"]


def setup_xformers():
    """Setup Xformers imports with centralized warning management."""
    success, imports = try_import_with_warning(
        "xformers.ops",
        success_message="‚úÖÔ∏è Xformers: available",
        warning_message="‚ö†Ô∏è Xformers: not available",
    )

    return imports.get("ops")


def setup_pillow_extensions():
    """Setup Pillow extension imports with centralized warning management."""
    results = {}

    # Try pillow_avif
    success, imports = try_import_with_warning(
        "pillow_avif",
        success_message="",
        warning_message="üìÑ pillow_avif: not available",
    )
    results["pillow_avif"] = imports.get("pillow_avif")

    # Try jxlpy
    success, imports = try_import_with_warning(
        "jxlpy",
        ["JXLImagePlugin"],
        success_message="",
        warning_message="üìÑ jxlpy: not available",
    )
    results["jxlpy"] = imports.get("JXLImagePlugin")

    # Try pillow_jxl
    success, imports = try_import_with_warning(
        "pillow_jxl", success_message="", warning_message="üìÑ pillow_jxl: not available"
    )
    results["pillow_jxl"] = imports.get("pillow_jxl")

    return results
</file>

<file path="common/global_seed.py">
from __future__ import annotations

import os
import random
from typing import Optional

import numpy as np
import torch


def set_global_seed(seed: int, *, deterministic: bool = True) -> None:
    """Set seeds for Python, NumPy, and PyTorch for reproducibility.

    Parameters
    ----------
    seed : int
        Base random seed to set across libraries. Expected range: [0, 2**31-1].
    deterministic : bool, default True
        If True, configures cuDNN for deterministic behavior. May reduce performance.
    """

    if not isinstance(seed, int):
        raise TypeError("seed must be an integer")
    if seed < 0:
        raise ValueError("seed must be non-negative")

    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

    # cuDNN / backend settings
    try:
        torch.backends.cudnn.deterministic = bool(deterministic)
        # When deterministic is True, benchmark should be False for determinism
        torch.backends.cudnn.benchmark = not bool(deterministic)
    except Exception:
        # Some backends may not be available depending on the build
        pass
</file>

<file path="common/logger.py">
import logging


def _is_main_process() -> bool:
    """Best-effort check for Accelerate rank-0. Falls back to True if unavailable."""
    try:
        from accelerate import PartialState  # type: ignore

        state = PartialState()
        return bool(getattr(state, "is_main_process", True))
    except Exception:
        return True


class PrefixFormatter(logging.Formatter):
    def format(self, record):
        original = super().format(record)
        return f"‚òÑÔ∏è {original}"


def get_logger(name: str = "takenoko", level=logging.DEBUG) -> logging.Logger:
    """
    Get a logger with custom formatting and prevent duplicate handlers.

    This function ensures that each logger only has one handler to prevent
    duplicate log messages. It also sets propagate=False to prevent
    duplicate logging from parent loggers.

    Args:
        name: Logger name (default: "takenoko")
        level: Logging level (default: logging.DEBUG)

    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(name)

    # Demote noisy logs on non-main processes in distributed runs
    if not _is_main_process() and level < logging.WARNING:
        level = logging.WARNING

    logger.setLevel(level)

    # Only add handler if no handlers exist and propagate is True (to avoid duplicate handlers)
    if not logger.handlers and logger.propagate:
        ch = logging.StreamHandler()
        ch.setLevel(level)

        formatter = PrefixFormatter("%(levelname)s: %(message)s")
        ch.setFormatter(formatter)

        logger.addHandler(ch)
        # Set propagate to False to prevent duplicate logging from parent loggers
        logger.propagate = False

    return logger
</file>

<file path="common/model_downloader.py">
"""
Simplified model downloader for direct file downloads.
"""

import os
import hashlib
import logging
import time
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
import requests

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def download_model_if_needed(
    path_or_url: str, cache_dir: Optional[str] = None, **kwargs
) -> str:
    """
    Download a model if it's a URL, otherwise return local path.

    Args:
        path_or_url: Local path or URL to the model
        cache_dir: Directory to cache downloaded models (defaults to 'models')
        **kwargs: Additional arguments (ignored for simplicity)

    Returns:
        Path to the model file
    """
    # If it's a local path and exists, return as-is
    if not path_or_url.startswith(("http://", "https://")) and os.path.exists(
        path_or_url
    ):
        logger.info(f"Using local file: {path_or_url}")
        return path_or_url

    # If it's not a URL, raise error
    if not path_or_url.startswith(("http://", "https://")):
        raise FileNotFoundError(f"Model file not found: {path_or_url}")

    # Set up models directory
    models_dir = Path(cache_dir) if cache_dir else Path("models")
    models_dir.mkdir(parents=True, exist_ok=True)

    # Get filename from URL
    parsed = urlparse(path_or_url)
    filename = os.path.basename(parsed.path)
    if not filename or "." not in filename:
        # Fallback: use hash of URL as filename
        url_hash = hashlib.md5(path_or_url.encode()).hexdigest()[:16]
        filename = f"model_{url_hash}.bin"

    file_path = models_dir / filename

    # Check if already downloaded
    if file_path.exists():
        logger.info(f"Model already cached: {file_path}")
        return str(file_path)

    # Download the file
    logger.info(f"Downloading: {path_or_url}")
    logger.info(f"Saving to: {file_path}")

    try:
        response = requests.get(path_or_url, stream=True)
        response.raise_for_status()

        # Get file size for progress tracking
        total_size = int(response.headers.get("content-length", 0))
        downloaded_size = 0
        start_time = time.time()
        last_progress_time = start_time

        with open(file_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)

                    # Update progress every 0.5 seconds
                    current_time = time.time()
                    if current_time - last_progress_time >= 0.5:
                        elapsed_time = current_time - start_time
                        speed = (
                            downloaded_size / elapsed_time if elapsed_time > 0 else 0
                        )

                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            # Create simple progress bar
                            bar_length = 30
                            filled_length = int(
                                bar_length * downloaded_size // total_size
                            )
                            bar = "‚ñà" * filled_length + "‚ñë" * (
                                bar_length - filled_length
                            )

                            # Format file sizes
                            downloaded_mb = downloaded_size / (1024 * 1024)
                            total_mb = total_size / (1024 * 1024)
                            speed_mb = speed / (1024 * 1024)

                            # Estimate time remaining
                            if speed > 0:
                                remaining_bytes = total_size - downloaded_size
                                eta_seconds = remaining_bytes / speed
                                eta_str = f" ETA: {int(eta_seconds)}s"
                            else:
                                eta_str = ""

                            print(
                                f"\r[{bar}] {progress:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f}MB) {speed_mb:.1f}MB/s{eta_str}",
                                end="",
                                flush=True,
                            )
                        else:
                            # No total size available
                            downloaded_mb = downloaded_size / (1024 * 1024)
                            speed_mb = speed / (1024 * 1024)
                            print(
                                f"\rDownloaded: {downloaded_mb:.1f}MB @ {speed_mb:.1f}MB/s",
                                end="",
                                flush=True,
                            )

                        last_progress_time = current_time

        # Final progress update
        elapsed_time = time.time() - start_time
        avg_speed = downloaded_size / elapsed_time if elapsed_time > 0 else 0
        avg_speed_mb = avg_speed / (1024 * 1024)
        downloaded_mb = downloaded_size / (1024 * 1024)

        print()  # New line after progress bar
        logger.info(f"Successfully downloaded: {file_path}")
        logger.info(
            f"Total: {downloaded_mb:.1f}MB in {elapsed_time:.1f}s (avg: {avg_speed_mb:.1f}MB/s)"
        )
        return str(file_path)

    except Exception as e:
        # Clean up partial download
        if file_path.exists():
            file_path.unlink()
        logger.error(f"Failed to download {path_or_url}: {e}")
        raise
</file>

<file path="common/performance_logger.py">
"""Performance logging functionality for training diagnostics.

This module provides comprehensive logging capabilities for:
- Performance timing analysis (data loading, forward/backward pass, optimizer step)
- Model output and target tensor statistics
- Enhanced hardware utilization metrics
"""

import time
import logging
from typing import Any, Dict, Optional, Tuple
import torch
from accelerate import Accelerator

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class PerformanceLogger:
    """Comprehensive performance logging for training diagnostics.

    This class provides detailed logging capabilities to help diagnose training
    performance bottlenecks, monitor model stability, and track hardware utilization.
    """

    def __init__(self):
        """Initialize the performance logger."""
        self.loop_end_time = None
        self.step_start_time = None
        self.forward_pass_start_time = None
        self.forward_pass_end_time = None
        self.backward_pass_start_time = None
        self.backward_pass_end_time = None
        self.optimizer_step_start_time = None
        self.optimizer_step_end_time = None

    def start_step_timing(self) -> None:
        """Start timing for the current training step."""
        self.step_start_time = time.perf_counter()
        self.data_loading_time = self.step_start_time - (
            self.loop_end_time or self.step_start_time
        )

    def start_forward_pass_timing(self) -> None:
        """Start timing for the forward pass."""
        self.forward_pass_start_time = time.perf_counter()

    def end_forward_pass_timing(self) -> None:
        """End timing for the forward pass."""
        self.forward_pass_end_time = time.perf_counter()

    def start_backward_pass_timing(self) -> None:
        """Start timing for the backward pass."""
        self.backward_pass_start_time = time.perf_counter()

    def end_backward_pass_timing(self) -> None:
        """End timing for the backward pass."""
        self.backward_pass_end_time = time.perf_counter()

    def start_optimizer_step_timing(self) -> None:
        """Start timing for the optimizer step."""
        self.optimizer_step_start_time = time.perf_counter()

    def end_optimizer_step_timing(self) -> None:
        """End timing for the optimizer step."""
        self.optimizer_step_end_time = time.perf_counter()

    def end_step_timing(self) -> None:
        """End timing for the current training step."""
        self.loop_end_time = time.perf_counter()

    def get_timing_metrics(self) -> Dict[str, float]:
        """Get timing metrics for the current step.

        Returns:
            Dict containing timing metrics in milliseconds
        """
        if not all(
            [
                self.step_start_time,
                self.forward_pass_start_time,
                self.forward_pass_end_time,
                self.backward_pass_start_time,
                self.backward_pass_end_time,
                self.optimizer_step_start_time,
                self.optimizer_step_end_time,
                self.loop_end_time,
            ]
        ):
            return {}

        try:
            timings = {
                "timing/data_loading_ms": self.data_loading_time * 1000,
                "timing/forward_pass_ms": (
                    self.forward_pass_end_time - self.forward_pass_start_time
                )  # type: ignore
                * 1000,
                "timing/backward_pass_ms": (
                    self.backward_pass_end_time - self.backward_pass_start_time
                )  # type: ignore
                * 1000,
                "timing/optimizer_step_ms": (
                    self.optimizer_step_end_time - self.optimizer_step_start_time
                )  # type: ignore
                * 1000,
            }

            step_total_time = self.loop_end_time - self.step_start_time  # type: ignore
            timings["timing/total_step_ms"] = step_total_time * 1000

            return timings
        except Exception as e:
            logger.debug(f"Failed to calculate timing metrics: {e}")
            return {}

    def get_model_statistics(
        self,
        model_pred: torch.Tensor,
        target: torch.Tensor,
        is_main_process: bool = True,
    ) -> Dict[str, float]:
        """Get model output and target tensor statistics.

        Args:
            model_pred: Model predictions tensor
            target: Target tensor
            is_main_process: Whether this is the main process (to avoid redundant calculations)

        Returns:
            Dict containing model and target statistics
        """
        if not is_main_process:
            return {}

        try:
            with torch.no_grad():
                model_pred_f = model_pred.float()
                target_f = target.float()

                model_stats = {
                    "prediction/mean": model_pred_f.mean().item(),
                    "prediction/std": model_pred_f.std().item(),
                    "prediction/max": model_pred_f.max().item(),
                    "prediction/min": model_pred_f.min().item(),
                }
                target_stats = {
                    "target_gt/mean": target_f.mean().item(),
                    "target_gt/std": target_f.std().item(),
                    "target_gt/max": target_f.max().item(),
                    "target_gt/min": target_f.min().item(),
                }

                # Combine stats
                stats = {}
                stats.update(model_stats)
                stats.update(target_stats)

                return stats
        except Exception as e:
            logger.debug(f"Failed to calculate model statistics: {e}")
            return {}

    def get_hardware_metrics(self) -> Dict[str, str]:
        """Get enhanced hardware utilization metrics.

        Returns:
            Dict containing hardware metrics for progress bar display
        """
        metrics = {}

        try:
            # Basic CUDA memory metrics
            if torch.cuda.is_available():
                device = torch.device("cuda")
                peak_allocated = torch.cuda.max_memory_allocated(device) / (1024**3)

                if peak_allocated > 0.1:
                    metrics["peak"] = f"{peak_allocated:.2f} GiB"

            # Enhanced GPU metrics with pynvml
            try:
                import pynvml

                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 0
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)

                # Add VRAM usage
                vram_used = float(meminfo.used) / 1024**3
                metrics["peak"] = f"{vram_used:.2f} GiB"

                # Add GPU utilization
                metrics["util"] = f"{utilization.gpu}%"

                # Add memory utilization
                memory_util = (meminfo.used / meminfo.total) * 100  # type: ignore
                metrics["mem_util"] = f"{memory_util:.1f}%"

                pynvml.nvmlShutdown()

            except Exception as e:
                logger.debug(f"Failed to get pynvml metrics: {e}")
                # Fallback to basic CUDA metrics
                pass

        except Exception as e:
            logger.debug(f"Failed to get hardware metrics: {e}")

        return metrics

    def get_comprehensive_metrics(
        self,
        model_pred: Optional[torch.Tensor] = None,
        target: Optional[torch.Tensor] = None,
        is_main_process: bool = True,
    ) -> Dict[str, Any]:
        """Get comprehensive performance metrics.

        Args:
            model_pred: Model predictions tensor (optional)
            target: Target tensor (optional)
            is_main_process: Whether this is the main process

        Returns:
            Dict containing all performance metrics
        """
        metrics = {}

        # Add timing metrics
        timing_metrics = self.get_timing_metrics()
        metrics.update(timing_metrics)

        # Add model statistics if tensors are provided
        if model_pred is not None and target is not None:
            model_stats = self.get_model_statistics(model_pred, target, is_main_process)
            metrics.update(model_stats)

        # Add hardware metrics
        hardware_metrics = self.get_hardware_metrics()
        metrics.update(hardware_metrics)

        return metrics

    def log_performance_summary(
        self, step: int, timing_metrics: Dict[str, float]
    ) -> None:
        """Log a performance summary for the current step.

        Args:
            step: Current training step
            timing_metrics: Timing metrics dictionary
        """
        if not timing_metrics:
            return

        try:
            total_time = timing_metrics.get("timing/total_step_ms", 0)
            forward_time = timing_metrics.get("timing/forward_pass_ms", 0)
            backward_time = timing_metrics.get("timing/backward_pass_ms", 0)
            optimizer_time = timing_metrics.get("timing/optimizer_step_ms", 0)
            data_loading_time = timing_metrics.get("timing/data_loading_ms", 0)

            if total_time > 0:
                forward_pct = (forward_time / total_time) * 100
                backward_pct = (backward_time / total_time) * 100
                optimizer_pct = (optimizer_time / total_time) * 100
                data_loading_pct = (data_loading_time / total_time) * 100

                logger.info(
                    f"Step {step} Performance Summary: "
                    f"Total={total_time:.1f}ms "
                    f"(Forward: {forward_pct:.1f}%, "
                    f"Backward: {backward_pct:.1f}%, "
                    f"Optimizer: {optimizer_pct:.1f}%, "
                    f"Data: {data_loading_pct:.1f}%)"
                )

        except Exception as e:
            logger.debug(f"Failed to log performance summary: {e}")


# Global instance for easy access
performance_logger = PerformanceLogger()


def start_step_timing() -> None:
    """Start timing for the current training step."""
    performance_logger.start_step_timing()


def start_forward_pass_timing() -> None:
    """Start timing for the forward pass."""
    performance_logger.start_forward_pass_timing()


def end_forward_pass_timing() -> None:
    """End timing for the forward pass."""
    performance_logger.end_forward_pass_timing()


def start_backward_pass_timing() -> None:
    """Start timing for the backward pass."""
    performance_logger.start_backward_pass_timing()


def end_backward_pass_timing() -> None:
    """End timing for the backward pass."""
    performance_logger.end_backward_pass_timing()


def start_optimizer_step_timing() -> None:
    """Start timing for the optimizer step."""
    performance_logger.start_optimizer_step_timing()


def end_optimizer_step_timing() -> None:
    """End timing for the optimizer step."""
    performance_logger.end_optimizer_step_timing()


def end_step_timing() -> None:
    """End timing for the current training step."""
    performance_logger.end_step_timing()


def get_timing_metrics() -> Dict[str, float]:
    """Get timing metrics for the current step."""
    return performance_logger.get_timing_metrics()


def get_model_statistics(
    model_pred: torch.Tensor, target: torch.Tensor, is_main_process: bool = True
) -> Dict[str, float]:
    """Get model output and target tensor statistics."""
    return performance_logger.get_model_statistics(model_pred, target, is_main_process)


def get_hardware_metrics() -> Dict[str, str]:
    """Get enhanced hardware utilization metrics."""
    return performance_logger.get_hardware_metrics()


def get_comprehensive_metrics(
    model_pred: Optional[torch.Tensor] = None,
    target: Optional[torch.Tensor] = None,
    is_main_process: bool = True,
) -> Dict[str, Any]:
    """Get comprehensive performance metrics."""
    return performance_logger.get_comprehensive_metrics(
        model_pred, target, is_main_process
    )


def log_performance_summary(step: int, timing_metrics: Dict[str, float]) -> None:
    """Log a performance summary for the current step."""
    performance_logger.log_performance_summary(step, timing_metrics)


# --- GPU memory tracing utilities ---
from typing import Dict as _Dict  # alias to avoid shadowing above imports


def snapshot_gpu_memory(tag: str = "mem") -> _Dict[str, float]:
    """Capture a GPU memory snapshot using both PyTorch and NVML if available.

    Args:
        tag: Label to prefix metrics with for easier correlation

    Returns:
        Dictionary of memory metrics in GiB
    """
    stats: _Dict[str, float] = {}
    try:
        if torch.cuda.is_available():
            device = torch.device("cuda")
            try:
                allocated = torch.cuda.memory_allocated(device) / (1024**3)
                reserved = torch.cuda.memory_reserved(device) / (1024**3)
                free_bytes, total_bytes = torch.cuda.mem_get_info()
                free_gb = free_bytes / (1024**3)
                total_gb = total_bytes / (1024**3)
                stats.update(
                    {
                        f"{tag}/torch_allocated_gb": float(allocated),
                        f"{tag}/torch_reserved_gb": float(reserved),
                        f"{tag}/torch_free_gb": float(free_gb),
                        f"{tag}/torch_total_gb": float(total_gb),
                    }
                )
            except Exception:
                pass

            # NVML snapshot
            try:
                import pynvml  # type: ignore

                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                stats.update(
                    {
                        f"{tag}/nvml_used_gb": float(meminfo.used) / (1024**3),
                        f"{tag}/nvml_free_gb": float(meminfo.free) / (1024**3),
                        f"{tag}/nvml_total_gb": float(meminfo.total) / (1024**3),
                    }
                )
            except Exception:
                # NVML optional
                pass
            finally:
                try:
                    # Safe shutdown if initialized
                    import pynvml  # type: ignore

                    if hasattr(pynvml, "nvmlShutdown"):
                        pynvml.nvmlShutdown()
                except Exception:
                    pass

        # Log rounded values for readability
        logger.info(
            "GPU MEM SNAPSHOT %s: %s",
            tag,
            {k: round(v, 3) for k, v in stats.items()},
        )
    except Exception as e:
        logger.warning("snapshot_gpu_memory failed for %s: %s", tag, e)
    return stats


def force_cuda_cleanup(tag: str = "cleanup") -> None:
    """Aggressively attempt to release CUDA memory from this process.

    This captures a post-cleanup snapshot for verification.

    Args:
        tag: Label used in the post-cleanup snapshot
    """
    try:
        if torch.cuda.is_available():
            try:
                torch.cuda.synchronize()
            except Exception:
                pass
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
            try:
                torch.cuda.ipc_collect()
            except Exception:
                pass
            try:
                torch.cuda.reset_peak_memory_stats()
            except Exception:
                pass
        import gc as _gc

        _gc.collect()
    except Exception as e:
        logger.warning("force_cuda_cleanup encountered an error: %s", e)
    finally:
        snapshot_gpu_memory(f"{tag}/after")
</file>

<file path="common/sai_model_spec.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/sai_model_spec.py (Apache)

# based on https://github.com/Stability-AI/ModelSpec
import datetime
import hashlib
from io import BytesIO
import os
from typing import List, Optional, Tuple, Union
import safetensors
import logging

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


r"""
# Metadata Example
metadata = {
    # === Must ===
    "modelspec.sai_model_spec": "1.0.0", # Required version ID for the spec
    "modelspec.architecture": "stable-diffusion-xl-v1-base", # Architecture, reference the ID of the original model of the arch to match the ID
    "modelspec.implementation": "sgm",
    "modelspec.title": "Example Model Version 1.0", # Clean, human-readable title. May use your own phrasing/language/etc
    # === Should ===
    "modelspec.author": "Example Corp", # Your name or company name
    "modelspec.description": "This is my example model to show you how to do it!", # Describe the model in your own words/language/etc. Focus on what users need to know
    "modelspec.date": "2023-07-20", # ISO-8601 compliant date of when the model was created
    # === Can ===
    "modelspec.license": "ExampleLicense-1.0", # eg CreativeML Open RAIL, etc.
    "modelspec.usage_hint": "Use keyword 'example'" # In your own language, very short hints about how the user should use the model
}
"""

BASE_METADATA = {
    # === Must ===
    "modelspec.sai_model_spec": "1.0.0",  # Required version ID for the spec
    "modelspec.architecture": None,
    "modelspec.implementation": None,
    "modelspec.title": None,
    "modelspec.resolution": None,
    # === Should ===
    "modelspec.description": None,
    "modelspec.author": None,
    "modelspec.date": None,
    # === Can ===
    "modelspec.license": None,
    "modelspec.tags": None,
    "modelspec.merged_from": None,
    "modelspec.prediction_type": None,
    "modelspec.timestep_range": None,
    "modelspec.encoder_layer": None,
}

MODELSPEC_TITLE = "modelspec.title"

# Official Wan2.1 weights does not have sai_model_spec, so we use this as an architecture name
ARCH_WAN = "wan2.1"

ADAPTER_LORA = "lora"

IMPL_WAN = "https://github.com/Wan-Video/Wan2.1"

PRED_TYPE_EPSILON = "epsilon"
# PRED_TYPE_V = "v"


def load_bytes_in_safetensors(tensors):
    bytes = safetensors.torch.save(tensors)  # type: ignore
    b = BytesIO(bytes)

    b.seek(0)
    header = b.read(8)
    n = int.from_bytes(header, "little")

    offset = n + 8
    b.seek(offset)

    return b.read()


def precalculate_safetensors_hashes(state_dict):
    # calculate each tensor one by one to reduce memory usage
    hash_sha256 = hashlib.sha256()
    for tensor in state_dict.values():
        single_tensor_sd = {"tensor": tensor}
        bytes_for_tensor = load_bytes_in_safetensors(single_tensor_sd)
        hash_sha256.update(bytes_for_tensor)

    return f"0x{hash_sha256.hexdigest()}"


def update_hash_sha256(metadata: dict, state_dict: dict):
    raise NotImplementedError


def build_metadata(
    state_dict: Optional[dict],
    timestamp: float,
    title: Optional[str] = None,
    reso: Optional[Union[int, Tuple[int, int]]] = None,
    author: Optional[str] = None,
    description: Optional[str] = None,
    license: Optional[str] = None,
    tags: Optional[str] = None,
    merged_from: Optional[str] = None,
    timesteps: Optional[Tuple[int, int]] = None,
    is_lora: bool = True,
):
    metadata = {}
    metadata.update(BASE_METADATA)

    # TODO implement if we can calculate hash without loading all tensors
    # if state_dict is not None:
    # hash = precalculate_safetensors_hashes(state_dict)
    # metadata["modelspec.hash_sha256"] = hash

    arch = ARCH_WAN
    impl = IMPL_WAN

    if is_lora:
        arch += f"/{ADAPTER_LORA}"
    metadata["modelspec.architecture"] = arch

    metadata["modelspec.implementation"] = impl

    if title is None:
        title = "LoRA" if is_lora else "Hunyuan-Video"
        title += f"@{timestamp}"
    metadata[MODELSPEC_TITLE] = title

    if author is not None:
        metadata["modelspec.author"] = author
    else:
        del metadata["modelspec.author"]

    if description is not None:
        metadata["modelspec.description"] = description
    else:
        del metadata["modelspec.description"]

    if merged_from is not None:
        metadata["modelspec.merged_from"] = merged_from
    else:
        del metadata["modelspec.merged_from"]

    if license is not None:
        metadata["modelspec.license"] = license
    else:
        del metadata["modelspec.license"]

    if tags is not None:
        metadata["modelspec.tags"] = tags
    else:
        del metadata["modelspec.tags"]

    # remove microsecond from time
    int_ts = int(timestamp)

    # time to iso-8601 compliant date
    date = datetime.datetime.fromtimestamp(int_ts).isoformat()
    metadata["modelspec.date"] = date

    if reso is not None:
        # comma separated to tuple
        if isinstance(reso, str):
            reso = tuple(map(int, reso.split(",")))  # type: ignore
        if len(reso) == 1:  # type: ignore
            reso = (reso[0], reso[0])  # type: ignore
    else:
        # resolution is defined in dataset, so use default
        reso = (1280, 720)
    if isinstance(reso, int):
        reso = (reso, reso)

    metadata["modelspec.resolution"] = f"{reso[0]}x{reso[1]}"  # type: ignore

    # metadata["modelspec.prediction_type"] = PRED_TYPE_EPSILON
    del metadata["modelspec.prediction_type"]

    if timesteps is not None:
        if isinstance(timesteps, str) or isinstance(timesteps, int):
            timesteps = (timesteps, timesteps)  # type: ignore
        if len(timesteps) == 1:  # type: ignore
            timesteps = (timesteps[0], timesteps[0])  # type: ignore
        metadata["modelspec.timestep_range"] = f"{timesteps[0]},{timesteps[1]}"  # type: ignore
    else:
        del metadata["modelspec.timestep_range"]

    # if clip_skip is not None:
    #     metadata["modelspec.encoder_layer"] = f"{clip_skip}"
    # else:
    del metadata["modelspec.encoder_layer"]

    # # assert all values are filled
    # assert all([v is not None for v in metadata.values()]), metadata
    if not all([v is not None for v in metadata.values()]):
        logger.error(f"Internal error: some metadata values are None: {metadata}")

    return metadata


def get_title(metadata: dict) -> Optional[str]:
    return metadata.get(MODELSPEC_TITLE, None)


def load_metadata_from_safetensors(model: str) -> dict:
    if not model.endswith(".safetensors"):
        return {}

    with safetensors.safe_open(model, framework="pt") as f:  # type: ignore
        metadata = f.metadata()
    if metadata is None:
        metadata = {}
    return metadata


def build_merged_from(models: List[str]) -> str:
    def get_title(model: str):
        metadata = load_metadata_from_safetensors(model)
        title = metadata.get(MODELSPEC_TITLE, None)
        if title is None:
            title = os.path.splitext(os.path.basename(model))[0]  # use filename
        return title

    titles = [get_title(model) for model in models]
    return ", ".join(titles)
</file>

<file path="common/vram_estimator.py">
from __future__ import annotations

from typing import Dict, Any, Tuple, List


def extract_training_shape_from_config(
    config: Dict[str, Any],
) -> Tuple[int, int, int, int]:
    """Extract a conservative training shape (batch, frames, height, width) from a Takenoko TOML config dict.

    The function scans [[datasets.train]] entries (or legacy [[datasets]] list) and returns the
    maximum batch size, frames, height, and width across entries to avoid underestimation.
    """
    datasets = config.get("datasets", {})
    train_entries: List[Dict[str, Any]] = []
    if isinstance(datasets, dict) and isinstance(datasets.get("train"), list):
        train_entries = [e for e in datasets.get("train", []) if isinstance(e, dict)]
    elif isinstance(datasets, list):
        train_entries = [e for e in datasets if isinstance(e, dict)]

    if not train_entries:
        # Conservative defaults: 1 sample of 960x544, 81 frames
        return (1, 81, 544, 960)

    max_w = 0
    max_h = 0
    max_f = 1
    max_b = 1
    for ds in train_entries:
        # Resolution [W, H]
        res = ds.get("resolution", [512, 512])
        try:
            w = int(res[0])
            h = int(res[1])
        except Exception:
            w, h = 512, 512
        if w > max_w:
            max_w = w
        if h > max_h:
            max_h = h

        # Batch size per dataset
        try:
            bsz = int(ds.get("batch_size", 1))
        except Exception:
            bsz = 1
        if bsz > max_b:
            max_b = bsz

        # Frames for video datasets; image datasets default to 1
        if "video_directory" in ds:
            tf = ds.get("target_frames")
            mf = ds.get("max_frames")
            vf = ds.get("video_length", ds.get("num_frames"))
            candidates: List[int] = []
            if isinstance(tf, list) and len(tf) > 0:
                try:
                    candidates.append(
                        max(int(x) for x in tf if isinstance(x, (int, float)))
                    )
                except Exception:
                    pass
            if isinstance(mf, (int, float)):
                candidates.append(int(mf))
            if isinstance(vf, (int, float)):
                candidates.append(int(vf))
            frames = max(candidates) if candidates else 81
        else:
            frames = 1

        if frames > max_f:
            max_f = int(frames)

    return (max_b, max_f, max_h, max_w)


def estimate_peak_vram_gb_from_config(
    config: Dict[str, Any],
) -> Tuple[float, Dict[str, Any]]:
    """Estimate peak VRAM (in GB) for WAN 14B training from a Takenoko config dict.

    Returns:
        (gb_estimate, breakdown_dict)
    """
    # WAN 14B model dims
    dim = 5120
    ffn_dim = 13824
    num_layers = 40
    text_len = 512
    text_dim = 4096

    # VAE stride and patch size for tokenization
    vae_stride_h, vae_stride_w = 8, 8
    patch_t, patch_h, patch_w = 1, 2, 2

    mixed_precision = str(config.get("mixed_precision", "bf16")).lower()
    bytes_per = 2 if mixed_precision in ("fp16", "bf16") else 4
    gradient_checkpointing = bool(config.get("gradient_checkpointing", True))
    chk_factor = 0.25 if gradient_checkpointing else 0.6
    enable_control_lora = bool(config.get("enable_control_lora", False))
    enable_dual = bool(config.get("enable_dual_model_training", False))
    offload_inactive = bool(config.get("offload_inactive_dit", True))
    dual_factor = (
        2.0 if (enable_dual and not offload_inactive) else (1.3 if enable_dual else 1.0)
    )

    batch_size, frames, height, width = extract_training_shape_from_config(config)

    # Tokens per sample after VAE downsample + patching
    lat_h = max(1, height // vae_stride_h)
    lat_w = max(1, width // vae_stride_w)
    tokens_per_sample = (frames * lat_h * lat_w) // (patch_t * patch_h * patch_w)

    # Activation memory dominates under flash attention (linear in L)
    k_attn = 4.0
    k_ffn = 2.0
    per_token_per_layer = k_attn * dim + k_ffn * ffn_dim
    activations_bytes = (
        batch_size
        * tokens_per_sample
        * per_token_per_layer
        * num_layers
        * bytes_per
        * chk_factor
    )

    # Latents (BCFHW) and a noisy copy
    cin = 16 * (2 if enable_control_lora else 1)
    latents_elems = batch_size * cin * frames * lat_h * lat_w
    latents_bytes = latents_elems * bytes_per * 2

    # Text embeddings
    text_bytes = batch_size * text_len * text_dim * bytes_per

    # LoRA/optimizer/buffers heuristic overhead
    lora_overhead_bytes = int(0.4 * (1024**3))

    # Include base model parameter memory to avoid underestimation.
    # Heuristic default ~14B params for WAN 14B; override with `dit_param_count` if provided.
    param_count_override = config.get("dit_param_count")
    try:
        model_params = (
            int(param_count_override)
            if param_count_override is not None
            else 14_000_000_000
        )
    except Exception:
        model_params = 14_000_000_000

    fp8_scaled = bool(config.get("fp8_scaled", False))
    model_param_bytes_per = 1 if fp8_scaled else 2
    # Base bytes per model (no swapping)
    base_model_bytes = model_params * model_param_bytes_per
    # Account for block swapping by reducing resident parameter fraction
    num_layers_model = int(config.get("dit_num_layers", num_layers))
    try:
        blocks_to_swap = int(config.get("blocks_to_swap", 0) or 0)
    except Exception:
        blocks_to_swap = 0
    blocks_to_swap = max(0, min(num_layers_model, blocks_to_swap))
    resident_blocks = max(0, num_layers_model - blocks_to_swap)
    resident_frac = resident_blocks / max(1, num_layers_model)
    try:
        swap_overhead_fraction = float(config.get("swap_overhead_fraction", 0.15))
    except Exception:
        swap_overhead_fraction = 0.15
    effective_param_resident_frac = min(
        1.0, resident_frac + max(0.0, swap_overhead_fraction)
    )
    model_bytes = int(base_model_bytes * effective_param_resident_frac)
    # Dual model increases total parameter residency (offload mitigates). Reuse dual_factor.
    model_bytes = int(model_bytes * dual_factor)

    total_bytes = (
        dual_factor * activations_bytes
        + latents_bytes
        + text_bytes
        + lora_overhead_bytes
        + model_bytes
    )
    gb = total_bytes / (1024**3)

    details = {
        "batch_size": batch_size,
        "frames": frames,
        "height": height,
        "width": width,
        "lat_h": lat_h,
        "lat_w": lat_w,
        "tokens_per_sample": tokens_per_sample,
        "bytes_per_elem": bytes_per,
        "gradient_checkpointing": gradient_checkpointing,
        "mixed_precision": mixed_precision,
        "enable_control_lora": enable_control_lora,
        "enable_dual_model_training": enable_dual,
        "offload_inactive_dit": offload_inactive,
        "dual_factor": dual_factor,
        "activations_gb": activations_bytes / (1024**3),
        "latents_gb": latents_bytes / (1024**3),
        "text_gb": text_bytes / (1024**3),
        "overhead_gb": lora_overhead_bytes / (1024**3),
        "model_params": model_params,
        "model_param_bytes_per": model_param_bytes_per,
        "blocks_to_swap": blocks_to_swap,
        "dit_num_layers": num_layers_model,
        "model_resident_fraction": effective_param_resident_frac,
        "model_gb": model_bytes / (1024**3),
        "model_base_gb_per_model": base_model_bytes / (1024**3),
        "model_dual_factor": dual_factor,
    }
    return gb, details
</file>

<file path="conditioning/control_processor.py">
## Based on: https://github.com/spacepxl/WanTraining/blob/main/train_wan_lora.py (Apache)

"""
Control signal processor for LoRA training.
Handles different types of control signals like tile, canny, depth, etc.
"""

import torch
import torch.nn.functional as F
from torchvision.transforms import v2
from typing import Optional, Dict, Any, Tuple
import numpy as np
import random

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class ControlSignalProcessor:
    """
    Processor for different types of control signals.
    Supports tile, canny, depth, and other control types.
    Aligned with reference implementation.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.control_type = self.config.get("control_lora_type", "tile")
        self.preprocessing = self.config.get("control_preprocessing", "blur")
        self.blur_kernel_size = self.config.get("control_blur_kernel_size", 15)
        self.blur_sigma = self.config.get("control_blur_sigma", 4.0)
        self.scale_factor = self.config.get("control_scale_factor", 1.0)
        self.concatenation_dim = self.config.get("control_concatenation_dim", -2)

        logger.info(
            f"Control signal processor initialized with type: {self.control_type}"
        )

    def process_control_signal(
        self,
        control_signal: torch.Tensor,
        target_shape: Tuple[int, int, int],
        device: torch.device,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Process control signal according to the specified type and preprocessing.
        Aligned with reference implementation.

        Args:
            control_signal: Input control signal tensor
            target_shape: Target shape (frames, height, width)
            device: Target device
            dtype: Target dtype

        Returns:
            Processed control signal tensor
        """
        if self.control_type == "tile":
            return self._process_tile_control(
                control_signal, target_shape, device, dtype
            )
        elif self.control_type == "canny":
            return self._process_canny_control(
                control_signal, target_shape, device, dtype
            )
        elif self.control_type == "depth":
            return self._process_depth_control(
                control_signal, target_shape, device, dtype
            )
        else:
            logger.warning(
                f"Unknown control type: {self.control_type}, using default processing"
            )
            return self._process_default_control(
                control_signal, target_shape, device, dtype
            )

    def _process_tile_control(
        self,
        control_signal: torch.Tensor,
        target_shape: Tuple[int, int, int],
        device: torch.device,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Process tile control signal with blur preprocessing.
        This is typically used for upscaling tasks.
        Aligned with reference implementation.
        """
        # Ensure control signal is in the right format (CFHW -> BFCHW)
        if control_signal.dim() == 4:  # B, C, H, W
            control_signal = control_signal.movedim(0, 1).unsqueeze(0)  # CFHW -> BFCHW
        elif control_signal.dim() == 5:  # B, C, F, H, W
            control_signal = control_signal.movedim(1, 2)  # BCFHW -> BFCHW

        # Apply preprocessing with random sigma for better generalization
        if self.preprocessing == "blur":
            height, width = control_signal.shape[-2:]

            # Use random sigma like in reference implementation
            sigma = random.uniform(3, 6) if self.blur_sigma == 4.0 else self.blur_sigma

            blur = v2.Compose(
                [
                    v2.Resize(size=(height // 4, width // 4)),
                    v2.Resize(size=(height, width)),
                    v2.GaussianBlur(kernel_size=self.blur_kernel_size, sigma=sigma),
                ]
            )

            control_signal = torch.clamp(
                torch.nan_to_num(blur(control_signal)), min=-1, max=1
            )
            control_signal = control_signal[0].movedim(0, 1)  # BFCHW -> CFHW

        # Resize to target shape if needed
        frames, height, width = target_shape
        if control_signal.shape[-2:] != (height, width):
            control_signal = self._resize_control_signal(
                control_signal, (height, width)
            )

        # Apply scale factor
        control_signal = control_signal * self.scale_factor

        return control_signal.to(device=device, dtype=dtype)

    def _process_canny_control(
        self,
        control_signal: torch.Tensor,
        target_shape: Tuple[int, int, int],
        device: torch.device,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Process canny edge control signal.
        """
        # Convert to grayscale if needed
        if control_signal.shape[1] == 3:  # RGB
            control_signal = self._rgb_to_grayscale(control_signal)

        # Apply canny edge detection
        control_signal = self._apply_canny_edge_detection(control_signal)

        # Resize to target shape
        frames, height, width = target_shape
        control_signal = self._resize_control_signal(control_signal, (height, width))

        # Normalize to [0, 1] range for edge maps
        control_signal = torch.clamp(control_signal, min=0, max=1)

        # Apply scale factor
        control_signal = control_signal * self.scale_factor

        return control_signal.to(device=device, dtype=dtype)

    def _process_depth_control(
        self,
        control_signal: torch.Tensor,
        target_shape: Tuple[int, int, int],
        device: torch.device,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Process depth control signal.
        """
        # Convert to grayscale if needed
        if control_signal.shape[1] == 3:  # RGB
            control_signal = self._rgb_to_grayscale(control_signal)

        # Resize to target shape
        frames, height, width = target_shape
        control_signal = self._resize_control_signal(control_signal, (height, width))

        # Normalize to [0, 1] range for depth maps
        control_signal = torch.clamp(control_signal, min=0, max=1)

        # Apply scale factor
        control_signal = control_signal * self.scale_factor

        return control_signal.to(device=device, dtype=dtype)

    def _process_default_control(
        self,
        control_signal: torch.Tensor,
        target_shape: Tuple[int, int, int],
        device: torch.device,
        dtype: torch.dtype,
    ) -> torch.Tensor:
        """
        Default control signal processing.
        """
        # Resize to target shape
        frames, height, width = target_shape
        control_signal = self._resize_control_signal(control_signal, (height, width))

        # Normalize to [-1, 1] range
        control_signal = control_signal * 2 - 1
        control_signal = torch.clamp(torch.nan_to_num(control_signal), min=-1, max=1)

        # Apply scale factor
        control_signal = control_signal * self.scale_factor

        return control_signal.to(device=device, dtype=dtype)

    def _apply_blur_preprocessing(self, control_signal: torch.Tensor) -> torch.Tensor:
        """Apply Gaussian blur preprocessing."""
        transform = v2.Compose(
            [
                v2.ToDtype(torch.float32, scale=True),
                v2.GaussianBlur(
                    kernel_size=self.blur_kernel_size, sigma=self.blur_sigma
                ),
            ]
        )

        # Apply transform to each frame
        if control_signal.dim() == 5:  # B, C, F, H, W
            B, C, F, H, W = control_signal.shape
            control_signal = control_signal.view(B * F, C, H, W)
            control_signal = transform(control_signal)
            control_signal = control_signal.view(B, F, C, H, W)
        else:
            control_signal = transform(control_signal)

        return control_signal

    def _apply_canny_edge_detection(self, control_signal: torch.Tensor) -> torch.Tensor:
        """Apply Canny edge detection to control signal."""
        # Convert to numpy for OpenCV processing
        if control_signal.dim() == 5:  # B, C, F, H, W
            B, C, F, H, W = control_signal.shape
            control_signal = control_signal.view(B * F, C, H, W)

            # Process each frame
            processed_frames = []
            for i in range(control_signal.shape[0]):
                frame = control_signal[i].cpu().numpy().transpose(1, 2, 0)
                frame = (frame * 255).astype(np.uint8)

                # Apply edge detection
                edge_frame = self._simple_edge_detection(frame)
                processed_frames.append(torch.from_numpy(edge_frame).float() / 255.0)

            control_signal = torch.stack(processed_frames).view(B, F, C, H, W)
        else:
            # Single frame processing
            frame = control_signal.cpu().numpy().transpose(1, 2, 0)
            frame = (frame * 255).astype(np.uint8)
            edge_frame = self._simple_edge_detection(frame)
            control_signal = torch.from_numpy(edge_frame).float() / 255.0

        return control_signal

    def _simple_edge_detection(self, frame: np.ndarray) -> np.ndarray:
        """Simple edge detection using Sobel operators."""
        if len(frame.shape) == 3:
            gray = np.dot(frame[..., :3], [0.299, 0.587, 0.114])
        else:
            gray = frame

        # Sobel operators
        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])

        # Apply convolution
        grad_x = self._convolve2d(gray, sobel_x)
        grad_y = self._convolve2d(gray, sobel_y)

        # Compute magnitude
        magnitude = np.sqrt(grad_x**2 + grad_y**2)
        magnitude = np.clip(magnitude, 0, 255)

        return magnitude

    def _convolve2d(self, image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
        """Simple 2D convolution."""
        kernel_height, kernel_width = kernel.shape
        image_height, image_width = image.shape

        output_height = image_height - kernel_height + 1
        output_width = image_width - kernel_width + 1

        output = np.zeros((output_height, output_width))

        for i in range(output_height):
            for j in range(output_width):
                output[i, j] = np.sum(
                    image[i : i + kernel_height, j : j + kernel_width] * kernel
                )

        return output

    def _rgb_to_grayscale(self, rgb_tensor: torch.Tensor) -> torch.Tensor:
        """Convert RGB tensor to grayscale."""
        # Use standard RGB to grayscale conversion weights
        weights = torch.tensor([0.299, 0.587, 0.114], device=rgb_tensor.device)
        grayscale = torch.sum(
            rgb_tensor * weights.view(1, 3, 1, 1), dim=1, keepdim=True
        )
        return grayscale

    def _resize_control_signal(
        self, control_signal: torch.Tensor, target_size: Tuple[int, int]
    ) -> torch.Tensor:
        """Resize control signal to target size."""
        height, width = target_size

        if control_signal.dim() == 5:  # B, C, F, H, W
            B, C, F, H, W = control_signal.shape
            control_signal = control_signal.view(B * F, C, H, W)
            control_signal = torch.nn.functional.interpolate(
                control_signal,
                size=(height, width),
                mode="bilinear",
                align_corners=False,
            )
            control_signal = control_signal.view(B, F, C, height, width)
        else:
            control_signal = torch.nn.functional.interpolate(
                control_signal,
                size=(height, width),
                mode="bilinear",
                align_corners=False,
            )

        return control_signal

    def concatenate_with_latents(
        self, control_latents: torch.Tensor, noisy_latents: torch.Tensor
    ) -> torch.Tensor:
        """Concatenate control latents with noisy latents.

        Behavior:
        - If tensors are CFHW (4D), default concat on channel dim (0)
        - If tensors are BCFHW (5D), default concat on channel dim (1)
        - If config contains `control_concatenation_dim`, use it when valid.

        Args:
            control_latents: Control tensor (4D CFHW or 5D BCFHW)
            noisy_latents: Noisy input tensor (same shape layout as control_latents)

        Returns:
            Concatenated tensor along the appropriate channel dimension
        """
        # Ensure both tensors are on the same device and dtype
        device = noisy_latents.device
        dtype = noisy_latents.dtype
        control_latents = control_latents.to(device=device, dtype=dtype)

        ndim = noisy_latents.dim()
        configured_dim = getattr(self, "concatenation_dim", None)
        # Determine concat dim with robust defaults
        if configured_dim is None:
            concat_dim = 1 if ndim == 5 else 0
        else:
            # Common mapping: CFHW channel dim(0) roughly corresponds to BCFHW dim(1)
            if ndim == 5 and configured_dim in (0, -2):
                concat_dim = 1
            elif isinstance(configured_dim, int) and -ndim <= configured_dim < ndim:
                # Normalize negative dims
                concat_dim = configured_dim % ndim
            else:
                concat_dim = 1 if ndim == 5 else 0

        return torch.cat([noisy_latents, control_latents], dim=concat_dim)

    def inject_noise(
        self, control_latents: torch.Tensor, noise_strength: float
    ) -> torch.Tensor:
        """
        Inject random noise into control latents.
        Aligned with reference implementation.
        """
        if noise_strength <= 0:
            return control_latents

        # Generate random noise strength for each sample
        inject_strength = torch.rand(1).item() * noise_strength
        noise = torch.randn_like(control_latents) * inject_strength

        return control_latents + noise


def create_control_processor(
    config: Optional[Dict[str, Any]] = None,
) -> ControlSignalProcessor:
    """
    Create a control signal processor with the given configuration.
    """
    return ControlSignalProcessor(config)
</file>

<file path="core/checkpoint_manager.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Checkpoint and resume management for WAN network trainer.

This module handles all checkpoint saving, loading, resuming, and state management.
Extracted from wan_network_trainer.py to improve code organization and maintainability.
"""

import argparse
import json
import os
import pathlib
import re
import time
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple
import torch
from accelerate import Accelerator
from safetensors.torch import save_file, load_file
import safetensors
import ast

import logging
from common.logger import get_logger
from common import sai_model_spec
from utils import train_utils

logger = get_logger(__name__, level=logging.INFO)


class CheckpointManager:
    """Handles checkpoint saving, loading, and resume operations."""

    def __init__(self):
        pass

    def check_control_lora_resume_compatibility(
        self, args: argparse.Namespace, transformer: Any, control_signal_processor: Any
    ) -> bool:
        """Check if control LoRA resume is compatible and apply necessary model modifications."""
        if not args.resume:
            return True

        # Check for control LoRA metadata in the resume state
        control_metadata_path = os.path.join(args.resume, "control_lora_metadata.json")
        control_metadata = None

        if os.path.exists(control_metadata_path):
            try:
                with open(control_metadata_path, "r") as f:
                    control_metadata = json.load(f)
                logger.info(
                    f"üîç Found control LoRA metadata in resume state: {control_metadata_path}"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to load control LoRA metadata: {e}")
                return True  # Continue without metadata

        current_is_control_lora = getattr(args, "enable_control_lora", False)
        saved_is_control_lora = control_metadata is not None and control_metadata.get(
            "enabled", False
        )

        logger.info(f"üîç Control LoRA compatibility check:")
        logger.info(f"   Current config: control_lora={current_is_control_lora}")
        logger.info(f"   Saved state: control_lora={saved_is_control_lora}")

        if current_is_control_lora and saved_is_control_lora:
            logger.info(
                "‚úÖ Both current and saved are control LoRA - checking model compatibility"
            )

            # Ensure model is modified for control LoRA BEFORE resuming
            if not getattr(transformer, "_control_lora_patched", False):
                logger.info("üîß Applying control LoRA model modification for resume...")
                control_signal_processor.modify_model_for_control_lora(
                    transformer, args
                )

            # Validate compatibility
            saved_channels = control_metadata.get("patch_embedding_channels")  # type: ignore
            current_channels = (
                getattr(transformer.patch_embedding, "in_channels", None)
                if hasattr(transformer, "patch_embedding")
                else None
            )

            if (
                saved_channels
                and current_channels
                and saved_channels != current_channels
            ):
                logger.error(
                    f"‚ùå Control LoRA resume incompatibility: "
                    f"saved model has {saved_channels} patch embedding channels "
                    f"but current model has {current_channels} channels"
                )
                return False

            logger.info("‚úÖ Control LoRA resume compatibility verified")

        elif current_is_control_lora and not saved_is_control_lora:
            logger.error(
                "‚ùå Cannot resume regular LoRA state into control LoRA training! "
                "The saved state does not contain control LoRA modifications."
            )
            return False

        elif not current_is_control_lora and saved_is_control_lora:
            logger.error(
                "‚ùå Cannot resume control LoRA state into regular LoRA training! "
                "The saved state contains control LoRA modifications."
            )
            return False
        else:
            logger.info("‚úÖ Both current and saved are regular LoRA")

        return True

    def resume_from_local_if_specified(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        transformer: Optional[Any] = None,
        control_signal_processor: Optional[Any] = None,
    ) -> Optional[int]:
        """
        Loads state and returns the restored step count if available, else None.
        """
        if not args.resume and not args.auto_resume:
            return None

        # If auto_resume is enabled and no specific resume path is provided,
        # automatically find the most recent state folder
        if args.auto_resume and not args.resume:
            latest_state_path = self.find_latest_state_folder(args)
            if latest_state_path:
                args.resume = latest_state_path
                logger.info(
                    f"Auto-resume: Found latest state folder: {latest_state_path}"
                )
            else:
                logger.info(
                    "Auto-resume: No existing state folders found, starting fresh training"
                )
                return None

        if not args.resume:
            return None

        logger.info(f"resume training from local state: {args.resume}")

        # Check control LoRA compatibility and apply model modifications if needed
        if transformer is not None and control_signal_processor is not None:
            if not self.check_control_lora_resume_compatibility(
                args, transformer, control_signal_processor
            ):
                logger.error("‚ùå Control LoRA resume compatibility check failed")
                if args.auto_resume:
                    logger.info(
                        "Auto-resume: Compatibility check failed, starting fresh training"
                    )
                    args.resume = None  # Clear resume path to start fresh
                    return None
                else:
                    raise RuntimeError("Control LoRA resume compatibility check failed")

                # Add error handling for state loading
        try:
            # Add custom optimizer classes to safe globals for PyTorch 2.6+ compatibility
            from optimizers.safe_globals_manager import SafeGlobalsManager

            SafeGlobalsManager.add_custom_optimizer_safe_globals()

            accelerator.load_state(args.resume)
            logger.info(f"Successfully loaded state from: {args.resume}")
            # Try to read step from step.txt
            from utils.train_utils import read_step_from_state_dir

            step = read_step_from_state_dir(args.resume)
            if step is not None:
                logger.info(f"Restored step from step.txt: {step}")
                return step
            # Fallback: parse from directory name
            import re
            import os

            dir_name = os.path.basename(args.resume)
            match = re.search(r"step(\d+)", dir_name)
            if match:
                step = int(match.group(1))
                logger.info(f"Restored step from directory name: {step}")
                return step
            match = re.search(r"-(\d+)-state", dir_name)
            if match:
                step = int(match.group(1))
                logger.info(f"Restored step from directory name: {step}")
                return step
            logger.warning(
                "Could not determine step from state directory; starting from 0"
            )
            return 0
        except Exception as e:
            logger.error(f"Failed to load state from {args.resume}: {e}")
            if args.auto_resume:
                logger.info(
                    "Auto-resume: State loading failed, starting fresh training"
                )
                return None
            else:
                # If manual resume failed, re-raise the exception
                raise

    def find_latest_state_folder(self, args: argparse.Namespace) -> Optional[str]:
        """
        Find the most recent state folder based on the naming convention.
        State folders follow the pattern: {output_name}-{step_number}-state or {output_name}-state
        Returns the path to the most recent state folder, or None if none found.
        """
        output_dir = Path(args.output_dir)
        output_name = args.output_name

        if not output_dir.exists():
            return None

        # Patterns to match numbered state folders
        # 1. Old style: {output_name}-{step_number}-state ‚Üí wan21_lora-000001-state
        # 2. Explicit step: {output_name}-step{step_number}-state ‚Üí wan21_lora-step000001-state
        # 3. Explicit epoch: {output_name}-epoch{epoch_number}-state ‚Üí wan21_lora-epoch000001-state

        numbered_state_pattern = re.compile(  # old style (implicit step)
            rf"^{re.escape(output_name)}-(\d+)-state$"
        )
        step_state_pattern = re.compile(rf"^{re.escape(output_name)}-step(\d+)-state$")
        epoch_state_pattern = re.compile(
            rf"^{re.escape(output_name)}-epoch(\d+)-state$"
        )

        # Pattern to match non-numbered state folders: {output_name}-state
        simple_state_pattern = re.compile(rf"^{re.escape(output_name)}-state$")

        latest_state = None
        latest_step = -1
        simple_state_folder = None

        for item in output_dir.iterdir():
            if item.is_dir():
                # Try all numbered patterns
                numbered_match = numbered_state_pattern.match(item.name)
                step_match = step_state_pattern.match(item.name)
                epoch_match = epoch_state_pattern.match(item.name)

                matched = numbered_match or step_match or epoch_match

                if matched is not None:
                    step_number = int(matched.group(1))
                    if step_number > latest_step:
                        # Validate that the state folder contains the necessary files
                        if self.is_valid_state_folder(item):
                            latest_step = step_number
                            latest_state = str(item)
                        else:
                            logger.warning(
                                f"Skipping invalid numbered state folder: {item.name}"
                            )
                # Check for simple state folders (without numbers)
                elif simple_state_pattern.match(item.name):
                    if self.is_valid_state_folder(item):
                        simple_state_folder = str(item)
                        logger.debug(f"Found simple state folder: {item.name}")
                    else:
                        logger.warning(
                            f"Skipping invalid simple state folder: {item.name}"
                        )

        # Prioritize numbered state folders over simple ones
        if latest_state is not None:
            logger.info(
                f"Found latest numbered state folder: {Path(latest_state).name}"
            )
            return latest_state
        elif simple_state_folder is not None:
            logger.info(f"Found simple state folder: {Path(simple_state_folder).name}")
            return simple_state_folder
        else:
            logger.debug("No valid state folders found")
            return None

    def is_valid_state_folder(self, state_folder: pathlib.Path) -> bool:
        """
        Check if a state folder contains the necessary files for resuming training.
        Returns True if the folder appears to be a valid state folder.
        """
        try:
            # Check if the folder contains the basic accelerate state files
            required_files = [
                "model.safetensors.safetensors",
                "model_1.safetensors",
                "optimizer.bin",
                "random_states_0.pkl",
                "scaler.pt",
                "scheduler.bin",
            ]

            # At least one of the required files should exist
            has_required = any(
                (state_folder / file).exists() for file in required_files
            )

            # Check if it's a directory and not empty
            is_valid = state_folder.is_dir() and has_required

            if not is_valid:
                logger.debug(
                    f"State folder {state_folder.name} appears invalid (missing required files)"
                )

            return is_valid
        except Exception as e:
            logger.debug(f"Error validating state folder {state_folder.name}: {e}")
            return False

    def create_save_model_hook(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        transformer: Any,
        network: Any,
    ) -> Callable:
        """Create the save model hook function for accelerator."""

        def save_model_hook(models, weights, output_dir):
            """Enhanced save hook for control LoRA support.

            For control LoRA, we save both the modified transformer model and the LoRA network,
            plus control LoRA metadata for proper resumption.
            For regular LoRA, we save only the LoRA network.
            """

            if not accelerator.is_main_process:
                return

            is_control_lora = getattr(args, "enable_control_lora", False)

            if is_control_lora:
                logger.info(
                    "üîß Control LoRA save: Saving only LoRA network and modified patch embedding"
                )

                # Save control LoRA metadata for proper resumption
                control_metadata_path = os.path.join(
                    output_dir, "control_lora_metadata.json"
                )
                control_metadata = {
                    "enabled": True,
                    "control_lora_type": getattr(args, "control_lora_type", "tile"),
                    "control_preprocessing": getattr(
                        args, "control_preprocessing", "blur"
                    ),
                    "control_blur_kernel_size": getattr(
                        args, "control_blur_kernel_size", 15
                    ),
                    "control_blur_sigma": getattr(args, "control_blur_sigma", 4.0),
                    "control_scale_factor": getattr(args, "control_scale_factor", 1.0),
                    "input_lr_scale": getattr(args, "input_lr_scale", 1.0),
                    "control_concatenation_dim": getattr(
                        args, "control_concatenation_dim", 0
                    ),
                    "model_modified": getattr(
                        transformer, "_control_lora_patched", False
                    ),
                    "patch_embedding_channels": (
                        getattr(transformer.patch_embedding, "in_channels", None)
                        if hasattr(transformer, "patch_embedding")
                        else None
                    ),
                    "model_in_dim": getattr(transformer, "in_dim", None),
                    "network_module": getattr(
                        args, "network_module", "networks.control_lora_wan"
                    ),
                    "network_dim": getattr(args, "network_dim", 64),
                    "network_alpha": getattr(args, "network_alpha", 64),
                }

                with open(control_metadata_path, "w") as f:
                    json.dump(control_metadata, f, indent=2)

                logger.info(
                    f"üíæ Saved control LoRA metadata to: {control_metadata_path}"
                )
                logger.info(f"   Model modified: {control_metadata['model_modified']}")
                logger.info(
                    f"   Patch embedding channels: {control_metadata['patch_embedding_channels']}"
                )

                # Save only the modified patch embedding layer weights
                if hasattr(transformer, "patch_embedding") and getattr(
                    transformer, "_control_lora_patched", False
                ):
                    patch_embedding_path = os.path.join(
                        output_dir, "control_patch_embedding.safetensors"
                    )

                    # Prepare tensors for safetensors
                    patch_embedding_tensors = {
                        "weight": transformer.patch_embedding.weight.detach().cpu(),
                    }

                    # Add bias if it exists
                    if transformer.patch_embedding.bias is not None:
                        patch_embedding_tensors["bias"] = (
                            transformer.patch_embedding.bias.detach().cpu()
                        )

                    # Prepare metadata (safetensors can handle string metadata)
                    metadata = {
                        "in_channels": str(transformer.patch_embedding.in_channels),
                        "out_channels": str(transformer.patch_embedding.out_channels),
                        "kernel_size": str(
                            list(transformer.patch_embedding.kernel_size)
                        ),
                        "stride": str(list(transformer.patch_embedding.stride)),
                        "padding": str(list(transformer.patch_embedding.padding)),
                        "has_bias": str(transformer.patch_embedding.bias is not None),
                    }

                    save_file(
                        patch_embedding_tensors, patch_embedding_path, metadata=metadata
                    )
                    logger.info(
                        f"üíæ Saved control patch embedding weights to: {patch_embedding_path}"
                    )
                    logger.info(
                        f"   Channels: {metadata['in_channels']} -> {metadata['out_channels']}"
                    )
                    logger.info(f"   Kernel size: {metadata['kernel_size']}")

                # Remove transformer from models to save, keep only LoRA network
                remove_indices = []
                for i, model in enumerate(models):
                    if not isinstance(model, type(accelerator.unwrap_model(network))):
                        remove_indices.append(i)
                for i in reversed(remove_indices):
                    if len(weights) > i:
                        weights.pop(i)

                logger.info(
                    "üîß Removed transformer from save, keeping only LoRA network"
                )
                return

            # Original behaviour (LoRA-only checkpoint) for regular LoRA
            logger.info("üîß Regular LoRA save: Keeping only LoRA network")
            remove_indices = []
            for i, model in enumerate(models):
                if not isinstance(model, type(accelerator.unwrap_model(network))):
                    remove_indices.append(i)
            for i in reversed(remove_indices):
                if len(weights) > i:
                    weights.pop(i)

        return save_model_hook

    def create_load_model_hook(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        transformer: Any,
        network: Any,
    ) -> Callable:
        """Create the load model hook function for accelerator."""

        def load_model_hook(models, input_dir):
            """Enhanced load hook for control LoRA support."""
            logger.info(f"üîÑ Loading state from: {input_dir}")
            logger.info(f"üì¶ Found {len(models)} models in state")

            # Check for control LoRA metadata first
            control_metadata_path = os.path.join(
                input_dir, "control_lora_metadata.json"
            )
            control_metadata = None

            if os.path.exists(control_metadata_path):
                try:
                    with open(control_metadata_path, "r") as f:
                        control_metadata = json.load(f)
                    logger.info(
                        f"üìã Found control LoRA metadata: {control_metadata_path}"
                    )
                    logger.info(
                        f"   Saved model was control LoRA: {control_metadata.get('enabled', False)}"
                    )
                    logger.info(
                        f"   Saved patch embedding channels: {control_metadata.get('patch_embedding_channels', 'unknown')}"
                    )
                    logger.info(
                        f"   Saved model_in_dim: {control_metadata.get('model_in_dim', 'unknown')}"
                    )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Failed to load control LoRA metadata: {e}")
                    control_metadata = None

            # Log what models we found
            for i, model in enumerate(models):
                model_type = type(model).__name__
                logger.info(f"   Model {i}: {model_type}")

            # Handle control LoRA state loading
            current_is_control_lora = getattr(args, "enable_control_lora", False)
            saved_is_control_lora = (
                control_metadata is not None and control_metadata.get("enabled", False)
            )

            logger.info(f"üîç Control LoRA state analysis:")
            logger.info(
                f"   Current training is control LoRA: {current_is_control_lora}"
            )
            logger.info(f"   Saved state is control LoRA: {saved_is_control_lora}")

            if current_is_control_lora and not saved_is_control_lora:
                logger.warning(
                    "‚ö†Ô∏è  Current training is control LoRA but saved state is not! "
                    "This may cause issues during loading."
                )
            elif not current_is_control_lora and saved_is_control_lora:
                logger.warning(
                    "‚ö†Ô∏è  Saved state is control LoRA but current training is not! "
                    "This may cause issues during loading."
                )
            elif current_is_control_lora and saved_is_control_lora:
                logger.info("‚úÖ Both current and saved states are control LoRA")

                # Validate model consistency for control LoRA
                if hasattr(transformer, "patch_embedding"):
                    current_channels = transformer.patch_embedding.in_channels
                    saved_channels = control_metadata.get("patch_embedding_channels")  # type: ignore

                    logger.info(
                        f"   Current patch embedding channels: {current_channels}"
                    )
                    logger.info(f"   Saved patch embedding channels: {saved_channels}")

                    if saved_channels and current_channels != saved_channels:
                        logger.error(
                            f"‚ùå Patch embedding channel mismatch! "
                            f"Current: {current_channels}, Saved: {saved_channels}"
                        )
                        logger.error(
                            "This indicates model modification wasn't applied consistently."
                        )
                    else:
                        logger.info("‚úÖ Patch embedding channels match")

                    # Check model.in_dim consistency
                    current_in_dim = getattr(transformer, "in_dim", None)
                    saved_in_dim = control_metadata.get("model_in_dim")  # type: ignore

                    if saved_in_dim and current_in_dim != saved_in_dim:
                        logger.error(
                            f"‚ùå Model in_dim mismatch! "
                            f"Current: {current_in_dim}, Saved: {saved_in_dim}"
                        )
                    else:
                        logger.info("‚úÖ Model in_dim matches")
            else:
                logger.info("‚úÖ Both current and saved states are regular LoRA")

            # Determine how to handle model loading based on control LoRA state
            if current_is_control_lora:
                # For control LoRA, we only need the LoRA network
                # The patch embedding will be loaded separately
                logger.info(
                    "üîß Control LoRA load: Keeping only LoRA network, loading patch embedding separately"
                )

                # Check if we have the patch embedding weights file
                patch_embedding_path = os.path.join(
                    input_dir, "control_patch_embedding.safetensors"
                )
                if os.path.exists(patch_embedding_path):
                    logger.info(
                        f"üì¶ Found patch embedding weights: {patch_embedding_path}"
                    )

                    # Load the patch embedding weights
                    try:
                        # Load tensors and metadata
                        patch_embedding_tensors = load_file(patch_embedding_path)

                        # Get metadata from the file
                        with safetensors.safe_open(
                            patch_embedding_path, framework="pt"
                        ) as f:  # type: ignore
                            metadata = f.metadata()

                        # Reconstruct the patch embedding layer
                        if hasattr(transformer, "patch_embedding"):
                            # Get current device and dtype from transformer
                            current_device = next(transformer.parameters()).device
                            current_dtype = next(transformer.parameters()).dtype

                            in_cls = transformer.patch_embedding.__class__

                            # Parse metadata
                            in_channels = int(metadata["in_channels"])
                            out_channels = int(metadata["out_channels"])
                            kernel_size = tuple(
                                ast.literal_eval(metadata["kernel_size"])
                            )
                            stride = tuple(ast.literal_eval(metadata["stride"]))
                            padding = tuple(ast.literal_eval(metadata["padding"]))
                            has_bias = metadata["has_bias"] == "True"

                            # NEW: if the existing patch_embedding already has the same geometry, load
                            # the saved weights directly into it so the previously attached LoRA hooks
                            # remain valid.  If the geometry differs we fall back to the original logic
                            # that rebuilds a fresh Conv3d.
                            pe = (
                                transformer.patch_embedding
                                if hasattr(transformer, "patch_embedding")
                                else None
                            )
                            same_geometry = (
                                pe is not None
                                and pe.in_channels == in_channels
                                and pe.out_channels == out_channels
                                and pe.kernel_size == kernel_size
                                and pe.stride == stride
                                and pe.padding == padding
                                and ((pe.bias is not None) == has_bias)
                            )

                            if same_geometry:
                                logger.info(
                                    "üõ†Ô∏è  Loading weights into existing patch_embedding to keep LoRA hooks intact"
                                )
                                # copy weights (and bias if it exists)
                                pe.weight.data.copy_(  # type: ignore
                                    patch_embedding_tensors["weight"].to(
                                        device=current_device, dtype=current_dtype
                                    )
                                )
                                if has_bias and "bias" in patch_embedding_tensors:
                                    pe.bias.data.copy_(  # type: ignore
                                        patch_embedding_tensors["bias"].to(
                                            device=current_device, dtype=current_dtype
                                        )
                                    )
                                new_patch_embedding = (
                                    pe  # so downstream logging works unmodified
                                )
                                transformer._control_lora_patched = True
                            else:
                                logger.warning(
                                    "Patch embedding geometry differs. Rebuilding layer and re-applying LoRA."
                                )

                                # Create new patch embedding with the loaded parameters as before
                                new_patch_embedding = in_cls(
                                    in_channels=in_channels,
                                    out_channels=out_channels,
                                    kernel_size=kernel_size,
                                    stride=stride,
                                    padding=padding,
                                    bias=has_bias,
                                ).to(device=current_device, dtype=current_dtype)

                                # Load the saved weights
                                new_patch_embedding.weight.data = (
                                    patch_embedding_tensors["weight"].to(
                                        device=current_device, dtype=current_dtype
                                    )
                                )

                                if has_bias and "bias" in patch_embedding_tensors:
                                    new_patch_embedding.bias.data = (
                                        patch_embedding_tensors["bias"].to(
                                            device=current_device, dtype=current_dtype
                                        )
                                    )

                                # Replace the patch embedding and update model dimensions
                                transformer.patch_embedding = new_patch_embedding
                                transformer.in_dim = in_channels

                                # CRITICAL FIX: Re-apply the network to the new patch_embedding
                                # This re-attaches the LoRA hooks to the new layer.
                                network.apply_to(
                                    None,
                                    transformer.patch_embedding,
                                    apply_text_encoder=False,
                                    apply_unet=True,
                                )

                                transformer._control_lora_patched = True

                            # Ensure HuggingFace config stays in sync
                            if hasattr(transformer, "register_to_config"):
                                transformer.register_to_config(
                                    in_dim=transformer.in_dim
                                )

                            logger.info(
                                f"‚úÖ Patch embedding restored: {in_channels} channels (bias={has_bias})"
                            )
                            logger.info(
                                f"   Kernel size: {kernel_size}, Stride: {stride}, Padding: {padding}"
                            )
                        else:
                            logger.warning(
                                "‚ö†Ô∏è  Transformer has no patch_embedding attribute"
                            )
                    except Exception as e:
                        logger.error(f"‚ùå Failed to load patch embedding weights: {e}")
                        logger.error(
                            "Will proceed with current patch embedding configuration"
                        )
                else:
                    logger.warning(
                        f"‚ö†Ô∏è  No patch embedding weights found at: {patch_embedding_path}"
                    )
                    logger.warning(
                        "Will proceed with current patch embedding configuration"
                    )

                # Remove everything except the network
                remove_indices = []
                for i, model in enumerate(models):
                    if not isinstance(model, type(accelerator.unwrap_model(network))):
                        remove_indices.append(i)

            else:
                # For regular LoRA, remove everything except the network
                logger.info("üîß Regular LoRA load: Keeping only LoRA network")
                remove_indices = []
                for i, model in enumerate(models):
                    if not isinstance(model, type(accelerator.unwrap_model(network))):
                        remove_indices.append(i)

            logger.info(f"üóëÔ∏è  Removing {len(remove_indices)} models from loading")
            for i in reversed(remove_indices):
                removed_model = (
                    type(models[i]).__name__ if i < len(models) else "unknown"
                )
                logger.info(f"   Removed: {removed_model}")
                models.pop(i)

            logger.info(f"‚úÖ {len(models)} models will be loaded")

            # Log network state information if available
            for i, model in enumerate(models):
                model_type = type(model).__name__
                logger.info(f"üìã Model {i} ({model_type}):")

                if hasattr(model, "state_dict"):
                    try:
                        state_keys = list(model.state_dict().keys())
                        logger.info(f"Contains {len(state_keys)} state keys üëá")
                        if len(state_keys) > 0:
                            logger.info(f"Sample keys: {state_keys[:3]}")

                        # Log control LoRA specific information for network models
                        if hasattr(model, "control_config"):
                            logger.info(f"Control config: {model.control_config}")
                    except Exception as e:
                        logger.warning(f"‚ùå Could not inspect state: {e}")

        return load_model_hook

    def create_save_model_function(
        self,
        args: argparse.Namespace,
        metadata: Dict[str, str],
        minimum_metadata: Dict[str, str],
        dit_dtype: torch.dtype,
    ) -> Callable:
        """Create the save model function."""
        save_dtype = dit_dtype

        def save_model(
            ckpt_name: str,
            unwrapped_nw: Any,
            steps: int,
            epoch_no: int,
            force_sync_upload: bool = False,
        ):
            # Safety check for output_dir
            if not args.output_dir or not args.output_dir.strip():
                logger.error(
                    f"args.output_dir is empty or None: '{args.output_dir}'. Cannot save model."
                )
                return

            os.makedirs(args.output_dir, exist_ok=True)
            ckpt_file = os.path.join(args.output_dir, ckpt_name)

            logger.info(f"saving checkpoint: {ckpt_file}")
            metadata["takenoko_training_finished_at"] = str(time.time())
            metadata["takenoko_steps"] = str(steps)
            metadata["takenoko_epoch"] = str(epoch_no)

            metadata_to_save = minimum_metadata if args.no_metadata else metadata

            title = (
                args.metadata_title
                if args.metadata_title is not None
                else args.output_name
            )
            if args.min_timestep is not None or args.max_timestep is not None:
                min_time_step = (
                    args.min_timestep if args.min_timestep is not None else 0
                )
                max_time_step = (
                    args.max_timestep if args.max_timestep is not None else 1000
                )
                md_timesteps = (min_time_step, max_time_step)
            else:
                md_timesteps = None

            sai_metadata = sai_model_spec.build_metadata(
                None,
                time.time(),
                title,
                None,
                args.metadata_author,
                args.metadata_description,
                args.metadata_license,
                args.metadata_tags,
                timesteps=md_timesteps,
            )

            metadata_to_save.update(sai_metadata)

            unwrapped_nw.save_weights(ckpt_file, save_dtype, metadata_to_save)

        return save_model

    def create_remove_model_function(self, args: argparse.Namespace) -> Callable:
        """Create the remove model function."""

        def remove_model(old_ckpt_name: str):
            # Safety check for output_dir
            if not args.output_dir or not args.output_dir.strip():
                logger.error(
                    f"args.output_dir is empty or None: '{args.output_dir}'. Cannot remove model."
                )
                return

            old_ckpt_file = os.path.join(args.output_dir, old_ckpt_name)
            if os.path.exists(old_ckpt_file):
                logger.info(f"removing old checkpoint: {old_ckpt_file}")
                os.remove(old_ckpt_file)

        return remove_model

    def register_hooks(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        transformer: Any,
        network: Any,
    ) -> None:
        """Register save and load hooks with the accelerator."""
        save_hook = self.create_save_model_hook(accelerator, args, transformer, network)
        load_hook = self.create_load_model_hook(accelerator, args, transformer, network)

        accelerator.register_save_state_pre_hook(save_hook)
        accelerator.register_load_state_pre_hook(load_hook)
</file>

<file path="core/config.py">
"""Minimal configuration helpers for WAN network trainer.

All TOML parsing is handled by `takenoko.py`. This module only provides
helpers for metadata creation and sanitized config extraction.
"""

from typing import Any, Dict, Optional
import os
import argparse


class TrainerConfig:
    """Lightweight helper for training metadata and logging config."""

    def __init__(self) -> None:
        pass

    def create_training_metadata(
        self,
        args: argparse.Namespace,
        session_id: int,
        training_started_at: float,
        train_dataset_group: Any,
        num_train_epochs: int,
        num_batches_per_epoch: int,
        optimizer_name: str,
        optimizer_args: str,
    ) -> Dict[str, str]:
        """Create training metadata dictionary."""
        from utils import train_utils

        metadata = {
            "takenoko_session_id": session_id,
            "takenoko_training_started_at": training_started_at,
            "takenoko_output_name": args.output_name,
            "takenoko_learning_rate": args.learning_rate,
            "takenoko_num_train_items": train_dataset_group.num_train_items,
            "takenoko_num_batches_per_epoch": num_batches_per_epoch,
            "takenoko_num_epochs": num_train_epochs,
            "takenoko_gradient_checkpointing": args.gradient_checkpointing,
            "takenoko_gradient_accumulation_steps": args.gradient_accumulation_steps,
            "takenoko_max_train_steps": args.max_train_steps,
            "takenoko_lr_warmup_steps": args.lr_warmup_steps,
            "takenoko_lr_scheduler": args.lr_scheduler,
            train_utils.TAKENOKO_METADATA_KEY_BASE_MODEL_VERSION: args.target_model,
            train_utils.TAKENOKO_METADATA_KEY_NETWORK_MODULE: args.network_module,
            train_utils.TAKENOKO_METADATA_KEY_NETWORK_DIM: args.network_dim,
            train_utils.TAKENOKO_METADATA_KEY_NETWORK_ALPHA: args.network_alpha,
            "takenoko_network_dropout": args.network_dropout,
            "takenoko_mixed_precision": args.mixed_precision,
            "takenoko_seed": args.seed,
            "takenoko_training_comment": args.training_comment,
            "takenoko_optimizer": optimizer_name
            + (f"({optimizer_args})" if len(optimizer_args) > 0 else ""),
            "takenoko_max_grad_norm": args.max_grad_norm,
        }

        # Add model hashes/names
        if args.dit is not None:
            dit_name = args.dit
            if os.path.exists(dit_name):
                dit_name = os.path.basename(dit_name)
            metadata["takenoko_dit_name"] = dit_name

        if args.vae is not None:
            vae_name = args.vae
            if os.path.exists(vae_name):
                vae_name = os.path.basename(vae_name)
            metadata["takenoko_vae_name"] = vae_name

        # Add serialized config content to metadata
        if hasattr(args, "config_content") and args.config_content is not None:
            # Check if config embedding is enabled (default to True)
            embed_config = getattr(args, "embed_config_in_metadata", True)
            if embed_config:
                metadata["takenoko_config_content"] = args.config_content
                if hasattr(args, "config_file") and args.config_file is not None:
                    metadata["takenoko_config_file"] = os.path.basename(
                        args.config_file
                    )

        # Convert all values to strings
        metadata = {k: str(v) for k, v in metadata.items()}

        return metadata

    def get_sanitized_config_or_none(
        self, args: argparse.Namespace
    ) -> Optional[Dict[str, Any]]:
        """Get sanitized config for logging, or None if logging is disabled."""
        from utils import train_utils

        return train_utils.get_sanitized_config_or_none(args)
</file>

<file path="core/control_signal_processor.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)
## Based on: https://github.com/spacepxl/WanTraining/blob/main/train_wan_lora.py (Apache)

"""Control signal processing for control LoRA training.

This module handles all control LoRA signal processing, preprocessing, and video saving.
Extracted from wan_network_trainer.py to improve code organization and maintainability.
"""

import argparse
import hashlib
import os
import random
import time
from typing import Any, Dict, List, Optional, Tuple
import torch
from accelerate import Accelerator
from torchvision.transforms import v2

import logging
from common.logger import get_logger
from utils.train_utils import clean_memory_on_device
from generation.sampling import save_videos_grid

logger = get_logger(__name__, level=logging.INFO)


class ControlSignalProcessor:
    """Handles control LoRA signal processing and related operations."""

    def __init__(self):
        self.vae = None  # Will be set by the trainer when VAE is loaded
        # Integrate utility processor for reusable ops (preprocess/concat/noise)
        try:
            from conditioning.control_processor import ControlSignalProcessor as _UtilsCSP  # type: ignore

            self._utils_proc = _UtilsCSP()
        except Exception:
            self._utils_proc = None

    def process_control_signal(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        batch: Dict[str, torch.Tensor],
        latents: torch.Tensor,
        network_dtype: torch.dtype,
        vae: Optional[Any] = None,
    ) -> Optional[torch.Tensor]:
        """
        Process control signal for control LoRA training.
        Simplified to match reference implementation exactly.

        Args:
            args: Training arguments
            accelerator: Accelerator instance
            batch: Training batch
            latents: Input latents
            network_dtype: Network dtype
            vae: VAE model for encoding control signals

        Returns:
            Processed control latents or None if not available
        """
        # DISABLED: Cached control signal path - forcing on-the-fly generation for video saving
        # The cached control signal path doesn't support video saving, so we skip it entirely
        # to ensure all control processing goes through the on-the-fly path where video saving happens

        # if "control_signal" in batch:
        #     logger.info("üéØ Found cached control signal in batch")
        #     # ... cached control signal processing (DISABLED)
        #     # return cached_control_latents

        # On-the-fly control generation from raw pixels
        if "pixels" in batch and vae is not None:
            logger.info("üéØ Found pixels in batch for on-the-fly control generation")
            pixels_data = batch["pixels"]

            if isinstance(pixels_data, list) and len(pixels_data) > 0:
                logger.info(
                    f"üéØ Generating control latents on-the-fly from {len(pixels_data)} pixel tensors"
                )

                control_pixels = []
                for pixel_tensor in pixels_data:
                    # Use VAE dtype to avoid dtype mismatch
                    vae_dtype = vae.dtype if vae is not None else torch.float16
                    one_pixels = pixel_tensor.to(device=latents.device, dtype=vae_dtype)
                    if self._utils_proc is not None:
                        # Sync config fields
                        self._utils_proc.control_type = getattr(
                            args, "control_lora_type", "tile"
                        )
                        self._utils_proc.preprocessing = getattr(
                            args, "control_preprocessing", "blur"
                        )
                        self._utils_proc.blur_kernel_size = getattr(
                            args, "control_blur_kernel_size", 15
                        )
                        self._utils_proc.blur_sigma = getattr(
                            args, "control_blur_sigma", 4.0
                        )
                        self._utils_proc.scale_factor = getattr(
                            args, "control_scale_factor", 1.0
                        )
                        self._utils_proc.concatenation_dim = getattr(
                            args, "control_concatenation_dim", -2
                        )

                        # Target shape as (F, H, W); prefer from latents
                        if latents.dim() == 5:
                            target_shape = (
                                latents.shape[2],
                                latents.shape[3],
                                latents.shape[4],
                            )
                        else:
                            target_shape = (
                                latents.shape[1],
                                latents.shape[2],
                                latents.shape[3],
                            )

                        control_pixel = self._utils_proc._process_tile_control(
                            one_pixels, target_shape, latents.device, vae_dtype
                        )
                    else:
                        control_pixel = self.apply_blur_preprocessing_on_the_fly(
                            one_pixels,
                            args,
                        )
                    control_pixels.append(control_pixel)

                vae_device = vae.device
                try:
                    vae.to(latents.device)
                    with torch.no_grad():
                        control_latents = vae.encode(control_pixels)

                        # Optional noise injection
                        if getattr(args, "control_inject_noise", 0.0) > 0:
                            if self._utils_proc is not None:
                                control_latents = [
                                    self._utils_proc.inject_noise(
                                        cl, args.control_inject_noise
                                    )
                                    for cl in control_latents
                                ]
                            else:
                                for i in range(len(control_latents)):
                                    strength = (
                                        torch.rand(1).item() * args.control_inject_noise
                                    )
                                    control_latents[i] += (
                                        torch.randn_like(control_latents[i]) * strength
                                    )

                        if isinstance(control_latents, list):
                            control_latents = torch.stack(control_latents)

                        control_latents = control_latents.to(
                            device=latents.device, dtype=network_dtype
                        )
                        logger.debug(
                            f"On-the-fly control latents shape: {control_latents.shape}"
                        )
                        return control_latents
                finally:
                    vae.to(vae_device)

            else:
                logger.warning(f"Unexpected pixels format: {type(pixels_data)}")

        # No control signal available
        logger.debug("No control signal found in batch")
        return None

    def preprocess_control_reference_style(
        self, pixels: torch.Tensor, args: argparse.Namespace
    ) -> torch.Tensor:
        """
        Apply control preprocessing exactly like in reference implementation.
        Reference: preprocess_control() function in reference_train_wan_lora.py
        """
        control_lora_type = getattr(args, "control_lora_type", "tile")
        control_preprocessing = getattr(args, "control_preprocessing", "blur")

        logger.info(
            f"üéØ preprocess_control_reference_style called with type={control_lora_type}, preprocessing={control_preprocessing}"
        )

        if control_lora_type == "tile" and control_preprocessing == "blur":
            # Reference implementation format conversion: CFHW -> BFCHW
            control = pixels.movedim(0, 1).unsqueeze(0)  # CFHW -> BFCHW
            height, width = control.shape[-2:]

            # Apply blur like in reference implementation
            blur = v2.Compose(
                [
                    v2.Resize(size=(height // 4, width // 4)),
                    v2.Resize(size=(height, width)),
                    v2.GaussianBlur(
                        kernel_size=getattr(args, "control_blur_kernel_size", 15),
                        sigma=getattr(args, "control_blur_sigma", 3.0),
                    ),  # Changed from sigma=4 to sigma=3 to match reference
                ]
            )

            control = torch.clamp(torch.nan_to_num(blur(control)), min=-1, max=1)
            control = control[0].movedim(0, 1)

            # Save control video if enabled using unified path
            if getattr(args, "save_control_videos", False):
                try:
                    self.save_control_video(
                        control,
                        args,
                        f"{control_lora_type}_{control_preprocessing}_reference",
                    )
                except Exception as e:
                    logger.warning(f"Failed to save control video: {e}")

            return control

        else:
            # For other preprocessing types, just return original pixels
            logger.warning(
                f"Control preprocessing '{control_preprocessing}' for type '{control_lora_type}' not implemented, using original pixels"
            )
            return pixels

    def modify_model_for_control_lora(
        self, transformer: Any, args: argparse.Namespace
    ) -> None:
        """
        Modify the model's patch embedding layer to accept additional channels for control LoRA.
        This aligns with the reference implementation.
        """
        # Re-entrancy guard ‚Äì return early if already patched
        if getattr(transformer, "_control_lora_patched", False):
            logger.debug("Control LoRA patch already applied ‚Äì skipping.")
            return

        if hasattr(transformer, "patch_embedding"):
            with torch.no_grad():
                in_cls = transformer.patch_embedding.__class__  # nn.Conv3d
                old_in_dim = transformer.in_dim  # 16
                new_in_dim = old_in_dim * 2  # Double the input channels

                new_in = in_cls(
                    in_channels=new_in_dim,
                    out_channels=transformer.patch_embedding.out_channels,
                    kernel_size=transformer.patch_embedding.kernel_size,
                    stride=transformer.patch_embedding.stride,
                    padding=transformer.patch_embedding.padding,
                ).to(
                    device=transformer.patch_embedding.weight.device,
                    dtype=transformer.patch_embedding.weight.dtype,
                )

                new_in.weight.zero_()
                # Copy original weights to first half of new weights
                new_in.weight[:, :old_in_dim, :, :, :] = (
                    transformer.patch_embedding.weight
                )
                # Copy original bias so the behaviour matches the reference implementation
                if transformer.patch_embedding.bias is not None:
                    new_in.bias.copy_(transformer.patch_embedding.bias)

                # Replace the original patch embedding
                transformer.patch_embedding = new_in
                transformer.in_dim = new_in_dim

                # Update HuggingFace config so that any model save/load cycle retains the new input channel size
                if hasattr(transformer, "register_to_config"):
                    # WanModel may inherit from ConfigMixin in some versions
                    transformer.register_to_config(in_dim=new_in_dim)

                logger.info(
                    f"Modified model for control LoRA: input channels {old_in_dim} -> {new_in_dim}"
                )

                # Ensure gradients are enabled for the new patch_embedding so it can learn
                transformer.patch_embedding.requires_grad_(True)

                # mark patched
                transformer._control_lora_patched = True

    def generate_control_signal_on_the_fly(
        self,
        args: argparse.Namespace,
        pixels: torch.Tensor,
        device: torch.device,
        dtype: torch.dtype,
    ) -> Optional[torch.Tensor]:
        """
        Generate control signal on-the-fly from input pixels.
        Aligned with reference implementation.
        """
        control_type = getattr(args, "control_lora_type", "tile")
        preprocessing = getattr(args, "control_preprocessing", "blur")

        if control_type == "tile" and preprocessing == "blur":
            # Apply blur preprocessing like in reference implementation
            return self.apply_blur_preprocessing_on_the_fly(pixels, args)
        else:
            logger.warning(
                f"On-the-fly control generation not implemented for type: {control_type}, preprocessing: {preprocessing}"
            )
            return None

    def apply_blur_preprocessing_on_the_fly(
        self, pixels: torch.Tensor, args: argparse.Namespace
    ) -> torch.Tensor:
        """
        Apply blur preprocessing on-the-fly like in reference implementation.
        """
        # Convert to CFHW format like in reference implementation
        if pixels.dim() == 4:  # B, C, H, W
            pixels = pixels.movedim(0, 1).unsqueeze(0)  # CFHW -> BFCHW
        elif pixels.dim() == 5:  # B, C, F, H, W
            pixels = pixels.movedim(1, 2)  # BCFHW -> BFCHW

        # Apply blur preprocessing like in reference implementation
        height, width = pixels.shape[-2:]

        # Use configurable sigma
        sigma = getattr(args, "control_blur_sigma", 3.0)
        kernel_size = getattr(args, "control_blur_kernel_size", 15)

        blur = v2.Compose(
            [
                v2.Resize(size=(height // 4, width // 4)),
                v2.Resize(size=(height, width)),
                v2.GaussianBlur(kernel_size=kernel_size, sigma=sigma),
            ]
        )

        # Apply blur to the entire tensor like in reference
        blurred = blur(pixels)

        # Clamp to [-1, 1] like in reference
        result = torch.clamp(torch.nan_to_num(blurred), min=-1, max=1)

        # Convert back to CFHW format like in reference
        result = result[0].movedim(0, 1)  # BFCHW -> CFHW

        # Save control video if enabled
        if hasattr(args, "save_control_videos") and args.save_control_videos:
            logger.info("üéØ Control video saving is enabled")
            save_all = getattr(args, "control_video_save_all", False)
            should_save = False

            if save_all:
                # Save all control videos (every video processed)
                should_save = True
                control_lora_type = getattr(args, "control_lora_type", "tile")
                control_preprocessing = getattr(args, "control_preprocessing", "blur")
                logger.info(
                    f"üé• Control video saved (save_all mode) for on-the-fly preprocessing: {control_lora_type}_{control_preprocessing}"
                )
            else:
                # Save only one control video per unique input video
                if not hasattr(args, "_control_videos_saved_onthefly"):
                    args._control_videos_saved_onthefly = set()

                control_lora_type = getattr(args, "control_lora_type", "tile")
                control_preprocessing = getattr(args, "control_preprocessing", "blur")

                # Create a unique identifier based on input video content
                video_hash = hashlib.md5(pixels.cpu().numpy().tobytes()).hexdigest()[:8]
                video_id = (
                    f"{control_lora_type}_{control_preprocessing}_onthefly_{video_hash}"
                )

                if video_id not in args._control_videos_saved_onthefly:
                    should_save = True
                    args._control_videos_saved_onthefly.add(video_id)
                    logger.info(
                        f"üé• Control video saved (new video {video_hash}) for on-the-fly preprocessing: {control_lora_type}_{control_preprocessing}"
                    )
                else:
                    should_save = False
                    logger.debug(
                        f"Skipping control video save (already saved video {video_hash}) for: {control_lora_type}_{control_preprocessing}"
                    )

            if should_save:
                control_lora_type = getattr(args, "control_lora_type", "tile")
                control_preprocessing = getattr(args, "control_preprocessing", "blur")
                self.save_control_video(
                    result,
                    args,
                    f"{control_lora_type}_{control_preprocessing}_onthefly",
                )

        return result

    def save_control_video(
        self, control_tensor: torch.Tensor, args: argparse.Namespace, suffix: str
    ) -> None:
        """
        Save control video to disk for debugging/inspection purposes.

        Args:
            control_tensor: Control tensor in various formats (CFHW, BFCHW, etc.)
            args: Training arguments containing save configuration
            suffix: Suffix to append to filename (e.g., "tile_blur")
        """
        try:
            # Get save directory from args
            save_dir = getattr(args, "control_video_save_dir", "tmp/control_videos")

            # Create absolute path relative to Takenoko root
            if not os.path.isabs(save_dir):
                # Get the project root directory (parent of src)
                project_root = os.path.dirname(
                    os.path.dirname(os.path.abspath(__file__))
                )
                save_dir = os.path.join(project_root, save_dir)

            # Create directory if it doesn't exist
            os.makedirs(save_dir, exist_ok=True)

            # According to the code analysis, control_tensor should ALWAYS be in CFHW format
            logger.debug(f"Control tensor shape: {control_tensor.shape}")
            logger.debug(f"Control tensor dtype: {control_tensor.dtype}")

            # Validate that we have the expected CFHW format
            if control_tensor.dim() != 4:
                logger.error(
                    f"BUG: Expected 4D tensor in CFHW format, got {control_tensor.dim()}D tensor with shape {control_tensor.shape}"
                )
                logger.error(f"This indicates a bug in the tensor processing pipeline!")
                logger.error(
                    f"Please check the preprocessing methods preprocess_control_reference_style and apply_blur_preprocessing_on_the_fly"
                )

                # Emergency handling for debugging - try to fix common issues
                if control_tensor.dim() == 3:
                    if control_tensor.shape == (
                        288,
                        512,
                        17,
                    ):  # Your reported case: HWF
                        logger.warning(
                            "Emergency fix: Detected HWF format (288, 512, 17), converting to CFHW"
                        )
                        # HWF -> FHW -> CFHW (add channel dimension)
                        control_tensor = (
                            control_tensor.permute(2, 0, 1)
                            .unsqueeze(0)
                            .repeat(3, 1, 1, 1)
                        )
                        logger.warning(f"After emergency fix: {control_tensor.shape}")
                    else:
                        logger.error(
                            f"Cannot handle 3D tensor with shape {control_tensor.shape}"
                        )
                        raise ValueError(
                            f"Unexpected 3D tensor shape: {control_tensor.shape}. Expected 4D CFHW format."
                        )
                else:
                    raise ValueError(
                        f"Cannot handle {control_tensor.dim()}D tensor. Expected 4D CFHW format."
                    )

            # Extract CFHW dimensions
            C, F, H, W = control_tensor.shape
            logger.debug(f"CFHW format: C={C}, F={F}, H={H}, W={W}")

            # Sanity check: channels should be reasonable (1-32), frames should be > 0
            if C > 32:
                logger.warning(
                    f"Unusual number of channels: {C}. This might indicate wrong tensor format!"
                )
            if F == 0:
                logger.error(f"Zero frames detected: F={F}")
                raise ValueError("Invalid frame count: 0")

            # Convert tensor format for saving: CFHW -> BCTHW (save_videos_grid expects BCTHW)
            control_video = control_tensor.unsqueeze(
                0
            )  # CFHW -> BCFHW (Frame=Time, so this is BCTHW)

            # Check current value range and normalize appropriately
            min_val = control_video.min().item()
            max_val = control_video.max().item()
            logger.debug(f"Control video value range: [{min_val:.3f}, {max_val:.3f}]")

            if min_val >= 0 and max_val <= 1:
                # Already in [0, 1] range
                logger.debug("Control video already in [0, 1] range")
            elif min_val >= -1 and max_val <= 1:
                # In [-1, 1] range, normalize to [0, 1]
                logger.debug("Normalizing from [-1, 1] to [0, 1] range")
                control_video = (control_video + 1.0) / 2.0
            else:
                # Unknown range, normalize by min-max scaling
                logger.debug(
                    f"Normalizing from [{min_val:.3f}, {max_val:.3f}] to [0, 1] range"
                )
                control_video = (control_video - min_val) / (max_val - min_val + 1e-8)

            control_video = torch.clamp(control_video, 0, 1)

            # Convert to float32 and move to CPU for saving (video saving requires float32 CPU tensors)
            control_video = control_video.to(dtype=torch.float32).cpu()

            # Create filename with timestamp
            timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
            filename = f"control_{suffix}_{timestamp}.mp4"
            save_path = os.path.join(save_dir, filename)

            logger.debug(f"Saving control video with shape: {control_video.shape}")
            logger.debug(f"Control video dtype: {control_video.dtype}")
            logger.debug(f"Control video device: {control_video.device}")

            # Save video using WAN target FPS (16fps) instead of hardcoded 8fps
            try:
                target_fps = 16  # TARGET_FPS_WAN constant from VideoDataset
                save_videos_grid(
                    control_video, save_path, rescale=False, fps=target_fps
                )
                logger.info(f"Control video saved to: {save_path} (fps={target_fps})")
            except Exception as save_error:
                logger.error(f"Error in save_videos_grid: {save_error}")
                logger.debug(
                    f"Tensor info - shape: {control_video.shape}, dtype: {control_video.dtype}"
                )

                # Try alternative: save as individual frames
                try:
                    import torchvision

                    logger.info("Attempting to save as individual frames instead...")
                    frames_dir = save_path.replace(".mp4", "_frames")
                    os.makedirs(frames_dir, exist_ok=True)

                    # Extract frames: BCTHW -> individual frames
                    B, C, T, H, W = control_video.shape
                    for t in range(T):
                        frame = control_video[0, :, t]  # CHW
                        frame_path = os.path.join(frames_dir, f"frame_{t:04d}.png")
                        torchvision.utils.save_image(frame, frame_path)

                    logger.info(f"Control frames saved to: {frames_dir}")
                except Exception as frame_error:
                    logger.warning(f"Failed to save frames: {frame_error}")
                    raise save_error

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to save control video: {e}")
            import traceback

            logger.debug(f"Full traceback: {traceback.format_exc()}")
</file>

<file path="core/dual_model_manager.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

import argparse
from typing import Any, Optional, Tuple

import torch
from accelerate import Accelerator

from common.logger import get_logger
from scheduling.timestep_utils import get_noisy_model_input_and_timesteps
from utils.device_utils import synchronize_device
from utils.train_utils import clean_memory_on_device


logger = get_logger(__name__)


class DualModelManager:
    """Manage high/low-noise base model contexts under a single LoRA network.

    This swaps only the base model state_dict underneath the active transformer
    instance so the attached LoRA layers remain continuous across swaps.

    MEMORY USAGE: Uses the exact same amount of RAM as single model training:
    - ONE model in GPU memory (active)
    - ONE model state_dict in CPU/GPU memory (inactive)
    - During swap: temporarily holds both state_dicts during transition
    - NO additional GPU memory required beyond single model training
    """

    def __init__(
        self,
        *,
        active_transformer: torch.nn.Module,
        high_noise_state_dict: dict,
        timestep_boundary: float,
        offload_inactive: bool,
        blocks_to_swap: int = 0,
    ) -> None:
        # Normalize boundary to [0,1] (identical to original)
        if timestep_boundary > 1.0:
            timestep_boundary = timestep_boundary / 1000.0
        self.timestep_boundary: float = float(timestep_boundary)
        self.blocks_to_swap: int = int(blocks_to_swap)

        # Mixed mode supported: block swap for active DiT while inactive DiT is offloaded on CPU

        self.offload_inactive: bool = bool(offload_inactive)

        # Active model is the already-prepared/wrapped transformer
        self.active_model: torch.nn.Module = active_transformer

        # MEMORY OPTIMIZATION: Store the inactive state dict exactly like the original implementation
        # The original keeps the high-noise state dict and starts with low-noise active
        # This ensures we only have ONE model in GPU memory at any time, just like single model training
        self.inactive_state_dict: dict = high_noise_state_dict

        # Track which regime is currently loaded (identical to original)
        self.current_model_is_high_noise: bool = False
        self.next_model_is_high_noise: bool = False

    @torch.no_grad()
    def determine_and_prepare_batch(
        self,
        *,
        args: argparse.Namespace,
        noise: torch.Tensor,
        latents: torch.Tensor,
        noise_scheduler: Any,
        device: torch.device,
        dtype: torch.dtype,
        timestep_distribution: Optional[Any] = None,
        presampled_uniform: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """Decide high/low noise regime for this batch and ensure consistency.

        Returns a tuple of (noisy_model_input, timesteps, sigmas).
        All items in the batch are guaranteed to fall on the same side of the
        boundary determined by `self.timestep_boundary`.
        """
        batch_size = latents.shape[0]

        # Probe one sample to decide regime for the entire batch
        probe_noisy, probe_timesteps, _ = get_noisy_model_input_and_timesteps(
            args,
            noise[0:1],
            latents[0:1],
            noise_scheduler,
            device,
            dtype,
            timestep_distribution,
            presampled_uniform=(
                presampled_uniform[0:1]
                if presampled_uniform is not None and len(presampled_uniform) > 0
                else None
            ),
        )
        # Align with reference: normalize by 1000.0 without subtracting 1
        probe_t = (probe_timesteps[0].item()) / 1000.0
        target_is_high = probe_t >= self.timestep_boundary
        self.next_model_is_high_noise = target_is_high

        final_inputs = []
        final_timesteps = []
        # sigmas are only relevant for sigma-based schemes; collect last seen
        last_sigmas = None

        # Strategy controls how per-epoch bucketed uniform samples interact with the boundary
        strategy = str(getattr(args, "dual_timestep_bucket_strategy", "hybrid")).lower()
        max_retries = int(getattr(args, "dual_timestep_bucket_max_retries", 100))
        eps = float(getattr(args, "dual_timestep_bucket_eps", 1e-4))

        def _call_with_uniform(idx: int, u: Optional[torch.Tensor]):
            return get_noisy_model_input_and_timesteps(
                args,
                noise[idx : idx + 1],
                latents[idx : idx + 1],
                noise_scheduler,
                device,
                dtype,
                timestep_distribution,
                presampled_uniform=u,
            )

        for i in range(batch_size):
            matched = False

            # Base presampled uniform from dataset pool if available
            base_u: Optional[torch.Tensor] = (
                presampled_uniform[i : i + 1]
                if presampled_uniform is not None and i < presampled_uniform.shape[0]
                else None
            )

            # Shortcut for "presampled": one attempt only using dataset-provided uniform
            if strategy == "presampled":
                nmi, ts, sig = _call_with_uniform(i, base_u)
                t_norm = (ts[0].item()) / 1000.0
                if (t_norm >= self.timestep_boundary) == target_is_high:
                    final_inputs.append(nmi)
                    final_timesteps.append(ts)
                    last_sigmas = sig
                    matched = True
                # If not matched, accept as-is (intentionally not forcing alignment)
                if not matched:
                    final_inputs.append(nmi)
                    final_timesteps.append(ts)
                    last_sigmas = sig
                continue

            # Strict clamp: if we have a presampled value, adjust it by binary search to cross boundary
            if strategy == "strict_clamp" and base_u is not None:
                lo = 0.0
                hi = 1.0
                if target_is_high:
                    lo = float(base_u[0].item())
                else:
                    hi = float(base_u[0].item())
                chosen_nmi = None
                chosen_ts = None
                chosen_sig = None
                for _ in range(max(4, min(max_retries, 20))):  # small bounded search
                    mid = (lo + hi) / 2.0
                    u_mid = torch.tensor([mid], device=device, dtype=torch.float32)
                    nmi, ts, sig = _call_with_uniform(i, u_mid)
                    t_norm = (ts[0].item()) / 1000.0
                    if target_is_high:
                        if t_norm >= self.timestep_boundary + eps:
                            chosen_nmi, chosen_ts, chosen_sig = nmi, ts, sig
                            hi = mid
                            matched = True
                        else:
                            lo = mid
                    else:
                        if t_norm < self.timestep_boundary - eps:
                            chosen_nmi, chosen_ts, chosen_sig = nmi, ts, sig
                            lo = mid
                            matched = True
                        else:
                            hi = mid
                # If found a clamped candidate, use it; else fall back to one shot with base_u
                if matched and chosen_nmi is not None:
                    final_inputs.append(chosen_nmi)
                    final_timesteps.append(chosen_ts)  # type: ignore[arg-type]
                    last_sigmas = chosen_sig
                    continue
                else:
                    nmi, ts, sig = _call_with_uniform(i, base_u)
                    final_inputs.append(nmi)
                    final_timesteps.append(ts)
                    last_sigmas = sig
                    continue

            # Hybrid/on_demand/general retry logic
            for attempt in range(max_retries):
                if strategy == "on_demand":
                    u_try: Optional[torch.Tensor] = torch.rand(
                        1, device=device, dtype=torch.float32
                    )
                elif strategy == "hybrid" and attempt == 0 and base_u is not None:
                    u_try = base_u
                elif strategy == "hybrid":
                    u_try = torch.rand(1, device=device, dtype=torch.float32)
                else:
                    # Default: behave like original (use provided u if any; else None)
                    u_try = base_u

                nmi, ts, sig = _call_with_uniform(i, u_try)
                t_norm = (ts[0].item()) / 1000.0
                if (t_norm >= self.timestep_boundary) == target_is_high:
                    final_inputs.append(nmi)
                    final_timesteps.append(ts)
                    last_sigmas = sig
                    matched = True
                    break

            if not matched:
                # fallback to the last computed values (extremely rare)
                final_inputs.append(nmi)
                final_timesteps.append(ts)
                last_sigmas = sig

        noisy_model_input = torch.cat(final_inputs, dim=0)
        timesteps = torch.cat(final_timesteps, dim=0)
        return noisy_model_input, timesteps, last_sigmas

    @torch.no_grad()
    def swap_if_needed(self, accelerator: Accelerator) -> None:
        """Swap base weights if the upcoming batch requires the other regime."""
        if self.current_model_is_high_noise == self.next_model_is_high_noise:
            return

        dev = accelerator.device
        src = "High" if self.current_model_is_high_noise else "Low"
        dst = "High" if self.next_model_is_high_noise else "Low"
        logger.info(f"üîÑ Swapping base model weights: {src} ‚Üí {dst} noise regime")

        # Work with the unwrapped module to avoid DDP/Accelerate wrappers issues
        model_to_load = accelerator.unwrap_model(self.active_model)

        # IDENTICAL TO ORIGINAL: Check blocks_to_swap first (lines 559-585 in original)
        if self.blocks_to_swap == 0:
            # If offloading inactive DiT, move the model to CPU first (identical to original)
            if self.offload_inactive:
                model_to_load.to("cpu", non_blocking=True)
                synchronize_device(dev)  # wait for the CPU to finish
                clean_memory_on_device(dev)

            # Get current state dict (CPU or accelerator.device)
            current_sd = model_to_load.state_dict()

            # Load inactive state dict with strict validation (identical to original)
            info = model_to_load.load_state_dict(
                self.inactive_state_dict, strict=True, assign=True
            )
            assert len(info.missing_keys) == 0, f"Missing keys: {info.missing_keys}"
            assert (
                len(info.unexpected_keys) == 0
            ), f"Unexpected keys: {info.unexpected_keys}"

            if self.offload_inactive:
                model_to_load.to(dev, non_blocking=True)
                synchronize_device(dev)

            # Swap the state dict, ensuring the stored inactive copy lives on CPU to avoid
            # keeping a duplicate set of GPU tensors in mixed mode
            try:
                cpu_sd = {}
                for k, v in current_sd.items():
                    try:
                        cpu_sd[k] = v.detach().to("cpu", non_blocking=True)
                    except Exception:
                        cpu_sd[k] = v.detach().cpu()
                self.inactive_state_dict = cpu_sd
            except Exception:
                logger.error(
                    "Failed to convert state dict to CPU, falling back to original behavior"
                )
                # Fallback to original behavior if conversion fails
                self.inactive_state_dict = current_sd
        else:
            # If block swap is enabled, we cannot use offloading inactive DiT,
            # Mixed mode: active model may have swapped blocks; ensure state is valid post-swap
            current_sd = model_to_load.state_dict()

            info = model_to_load.load_state_dict(
                self.inactive_state_dict, strict=True, assign=True
            )
            assert len(info.missing_keys) == 0, f"Missing keys: {info.missing_keys}"
            assert (
                len(info.unexpected_keys) == 0
            ), f"Unexpected keys: {info.unexpected_keys}"

            # Swap the state dict (identical to original)
            self.inactive_state_dict = current_sd
            # If the model supports block swap preparation hooks, refresh them after weight load
            try:
                # Move resident (non-swapped) blocks back to the accelerator device because
                # load_state_dict(assign=True) may attach CPU tensors from the state_dict
                if hasattr(model_to_load, "move_to_device_except_swap_blocks"):
                    model_to_load.move_to_device_except_swap_blocks(accelerator.device)
                if hasattr(model_to_load, "prepare_block_swap_before_forward"):
                    model_to_load.prepare_block_swap_before_forward()
            except Exception as _prep_err:
                logger.debug(f"Block-swap prepare hook skipped: {_prep_err}")

        # Update current regime (identical to original)
        self.current_model_is_high_noise = self.next_model_is_high_noise
</file>

<file path="core/metrics.py">
"""Metrics utilities for training.

This module centralizes metric computations and logging helpers used by the
training loop to keep `training_core.py` lean while preserving functionality.
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

import torch

from common.logger import get_logger

# Enhanced optimizer logging
from optimizers.enhanced_logging import (
    get_enhanced_metrics,
    is_supported,
)


logger = get_logger(__name__)


def generate_parameter_stats(
    model: Any,
    global_step: int,
    log_every_n_steps: int = 100,
    max_params_to_log: int = 20,
) -> Dict[str, float]:
    """Generate parameter statistics for weight drift and gradient monitoring.

    Args:
        model: Model to analyze. Must implement `named_parameters()`.
        global_step: Current training step.
        log_every_n_steps: Log at most every N steps.
        max_params_to_log: Number of largest-norm parameters to log.

    Returns:
        Mapping of metric name to value.
    """
    # Only log periodically to avoid TensorBoard spam
    if getattr(generate_parameter_stats, "_last_log_step", -1) >= 0:
        if (
            global_step - getattr(generate_parameter_stats, "_last_log_step")
            < log_every_n_steps
        ):
            return {}

    setattr(generate_parameter_stats, "_last_log_step", global_step)
    param_stats: Dict[str, float] = {}

    try:
        param_info: List[Dict[str, float]] = []
        for name, param in model.named_parameters():
            if param.requires_grad and param.data is not None:
                param_norm = param.data.norm().item()
                grad_norm = param.grad.norm().item() if param.grad is not None else 0.0
                param_info.append(
                    {
                        "name": name,
                        "param_norm": float(param_norm),
                        "grad_norm": float(grad_norm),
                        "size": float(param.numel()),
                    }
                )

        param_info.sort(key=lambda x: x["param_norm"], reverse=True)
        top_params = param_info[:max_params_to_log]

        for info in top_params:
            name_str = str(info["name"])  # ensure string
            clean_name = name_str.replace(".", "/")
            param_stats[f"param_norm/{clean_name}"] = float(info["param_norm"])
            param_stats[f"grad_norm/{clean_name}"] = float(info["grad_norm"])

        if param_info:
            total_param_norm = float(sum(info["param_norm"] for info in param_info))
            total_grad_norm = float(sum(info["grad_norm"] for info in param_info))
            avg_param_norm = total_param_norm / len(param_info)
            avg_grad_norm = total_grad_norm / len(param_info)

            param_stats.update(
                {
                    "param_stats/total_param_norm": total_param_norm,
                    "param_stats/avg_param_norm": avg_param_norm,
                    "param_stats/total_grad_norm": total_grad_norm,
                    "param_stats/avg_grad_norm": avg_grad_norm,
                    "param_stats/num_params": float(len(param_info)),
                    "param_stats/largest_param_norm": (
                        float(top_params[0]["param_norm"]) if top_params else 0.0
                    ),
                    "param_stats/largest_grad_norm": float(
                        max(info["grad_norm"] for info in param_info)
                    ),
                }
            )

    except Exception as err:
        logger.warning(f"Failed to generate parameter statistics: {err}")

    return param_stats


def compute_per_source_loss(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    batch: Dict[str, Any],
    weighting: Optional[torch.Tensor] = None,
    sample_weights: Optional[torch.Tensor] = None,
) -> Dict[str, float]:
    """Compute loss per data source (video vs image) if source information is available."""
    per_source_losses: Dict[str, float] = {}

    try:
        source_key: Optional[str] = None
        for key in [
            "source",
            "data_source",
            "source_type",
            "video_source",
            "media_type",
        ]:
            if key in batch:
                source_key = key
                break

        if source_key is None:
            # Try to infer from existing batch structure
            sources: Optional[List[str]]
            if "item_info" in batch:
                try:
                    item_infos = batch["item_info"]
                    sources = []
                    for item_info in item_infos:
                        if (
                            hasattr(item_info, "frame_count")
                            and item_info.frame_count
                            and item_info.frame_count > 1
                        ):
                            sources.append("video")
                        else:
                            sources.append("image")
                except Exception:
                    sources = None
            elif "latents" in batch:
                try:
                    latents = batch["latents"]
                    if latents.dim() == 5:
                        frame_counts = latents.shape[2]
                        sources = [
                            "video" if frame_counts > 1 else "image"
                        ] * latents.shape[0]
                    else:
                        sources = ["image"] * latents.shape[0]
                except Exception:
                    sources = None
            elif any("video" in str(k).lower() for k in batch.keys()):
                has_video_keys = any(
                    "video" in str(k).lower() or "frame" in str(k).lower()
                    for k in batch.keys()
                )
                has_image_keys = any(
                    "image" in str(k).lower() or "img" in str(k).lower()
                    for k in batch.keys()
                )
                if has_video_keys and not has_image_keys:
                    sources = ["video"] * model_pred.shape[0]
                elif has_image_keys and not has_video_keys:
                    sources = ["image"] * model_pred.shape[0]
                else:
                    sources = None
            else:
                sources = None

            if sources is None:
                return {}
        else:
            sources = batch[source_key]
            if torch.is_tensor(sources):
                sources = sources.cpu().tolist()

        assert sources is not None
        unique_sources = list(set(sources))
        for source in unique_sources:
            if isinstance(sources, list):
                indices = [i for i, s in enumerate(sources) if s == source]
            else:
                # In practice we always convert to list above
                indices = torch.where(sources == source)[0]

            if len(indices) == 0:
                continue

            source_pred = model_pred[indices]
            source_target = target[indices]

            source_loss = torch.nn.functional.mse_loss(
                source_pred, source_target, reduction="none"
            )

            if sample_weights is not None:
                source_sample_weights = sample_weights[indices]
                while source_sample_weights.dim() < source_loss.dim():
                    source_sample_weights = source_sample_weights.unsqueeze(-1)
                source_loss = source_loss * source_sample_weights

            if weighting is not None:
                source_weighting = weighting[indices]
                source_loss = source_loss * source_weighting

            per_source_losses[f"loss/{source}"] = float(source_loss.mean().item())

    except Exception as err:
        logger.debug(f"Could not compute per-source loss: {err}")

    return per_source_losses


def compute_gradient_norm(
    model: Any,
    max_norm: Optional[float] = None,  # kept for API compatibility
    norm_type: float = 2.0,
) -> float:
    """Compute gradient norm for monitoring gradient flow."""
    try:
        parameters = [p for p in model.parameters() if p.grad is not None]
        if len(parameters) == 0:
            return 0.0

        total_norm = torch.norm(
            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),
            norm_type,
        )
        return float(total_norm.item())
    except Exception as err:
        logger.debug(f"Could not compute gradient norm: {err}")
        return 0.0


def generate_step_logs(
    args: Any,
    current_loss: float,
    avr_loss: float,
    lr_scheduler: Any,
    lr_descriptions: Optional[List[str]],
    optimizer: Optional[torch.optim.Optimizer] = None,
    keys_scaled: Optional[int] = None,
    mean_norm: Optional[float] = None,
    maximum_norm: Optional[float] = None,
    ema_loss: Optional[float] = None,
    model: Optional[Any] = None,
    global_step: Optional[int] = None,
    per_source_losses: Optional[Dict[str, float]] = None,
    gradient_norm: Optional[float] = None,
) -> Dict[str, Any]:
    """Generate scalar logs for a training step."""
    network_train_unet_only = True
    logs: Dict[str, Any] = {"loss/current": current_loss, "loss/average": avr_loss}

    if ema_loss is not None:
        logs["loss/ema"] = ema_loss

    if per_source_losses is not None:
        logs.update(per_source_losses)

    if gradient_norm is not None:
        logs["grad_norm"] = gradient_norm

    # Include max-norm regularization statistics if provided
    if keys_scaled is not None:
        logs["max_norm/keys_scaled"] = keys_scaled
        logs["max_norm/average_key_norm"] = mean_norm
        logs["max_norm/max_key_norm"] = maximum_norm

    if (
        hasattr(args, "log_param_stats")
        and args.log_param_stats
        and model is not None
        and global_step is not None
    ):
        param_stats_interval = getattr(args, "param_stats_every_n_steps", 100)
        max_params = getattr(args, "max_param_stats_logged", 20)
        param_stats = generate_parameter_stats(
            model, global_step, param_stats_interval, max_params
        )
        logs.update(param_stats)

    # Try to resolve the underlying optimizer to read param group fields like 'd'
    actual_optimizer: Optional[torch.optim.Optimizer]
    actual_optimizer = None
    if optimizer is not None:
        actual_optimizer = (
            optimizer.optimizer  # type: ignore[attr-defined]
            if hasattr(optimizer, "optimizer")
            else optimizer
        )
    elif hasattr(lr_scheduler, "optimizer"):
        try:
            actual_optimizer = getattr(lr_scheduler, "optimizer")
        except Exception:
            actual_optimizer = None
    elif hasattr(lr_scheduler, "optimizers"):
        try:
            actual_optimizer = lr_scheduler.optimizers[-1]
        except Exception:
            actual_optimizer = None

    lrs = lr_scheduler.get_last_lr()
    for i, lr in enumerate(lrs):
        if lr_descriptions is not None:
            lr_desc = lr_descriptions[i]
        else:
            idx = i - (0 if network_train_unet_only else -1)
            if idx == -1:
                lr_desc = "textencoder"
            else:
                lr_desc = f"group{idx}" if len(lrs) > 2 else "unet"
        logs[f"lr/{lr_desc}"] = lr

        if (
            args.optimizer_type.lower().startswith("DAdapt".lower())
            or args.optimizer_type.lower() == "Prodigy".lower()
        ):
            try:
                if actual_optimizer is not None:
                    d_val = actual_optimizer.param_groups[i].get("d", None)
                    if d_val is not None:
                        logs[f"lr/d/{lr_desc}"] = d_val
                        logs[f"lr/d*lr/{lr_desc}"] = (
                            d_val * actual_optimizer.param_groups[i]["lr"]
                        )
            except Exception:
                pass
        if (
            args.optimizer_type.lower().endswith("ProdigyPlusScheduleFree".lower())
            and optimizer is not None
        ):
            actual_optimizer = (
                optimizer.optimizer  # type: ignore[attr-defined]
                if hasattr(optimizer, "optimizer")
                else optimizer
            )
            logs["lr/d*lr"] = (
                actual_optimizer.param_groups[0]["d"]
                * actual_optimizer.param_groups[0]["lr"]
            )
    else:
        idx = 0
        if not network_train_unet_only:
            logs["lr/textencoder"] = float(lrs[0])
            idx = 1
        for i in range(idx, len(lrs)):
            logs[f"lr/group{i}"] = float(lrs[i])
            if (
                args.optimizer_type.lower().startswith("DAdapt".lower())
                or args.optimizer_type.lower() == "Prodigy".lower()
            ):
                try:
                    if actual_optimizer is not None:
                        d_val = actual_optimizer.param_groups[i].get("d", None)
                        if d_val is not None:
                            logs[f"lr/d/group{i}"] = d_val
                            logs[f"lr/d*lr/group{i}"] = (
                                d_val * actual_optimizer.param_groups[i]["lr"]
                            )
                except Exception:
                    pass
            if (
                args.optimizer_type.lower().endswith("ProdigyPlusScheduleFree".lower())
                and optimizer is not None
            ):
                actual_optimizer = (
                    optimizer.optimizer  # type: ignore[attr-defined]
                    if hasattr(optimizer, "optimizer")
                    else optimizer
                )
                logs[f"lr/d*lr/group{i}"] = (
                    actual_optimizer.param_groups[i]["d"]
                    * actual_optimizer.param_groups[i]["lr"]
                )

    if optimizer is not None and is_supported(optimizer):
        try:
            enhanced_metrics = get_enhanced_metrics(optimizer)
            logs.update(enhanced_metrics)
        except Exception as err:
            logger.debug(f"Failed to log enhanced optimizer metrics: {err}")

    return logs


def generate_safe_progress_metrics(
    args: Any,
    current_loss: float,
    avr_loss: float,
    lr_scheduler: Any,
    epoch: int,
    global_step: int,
    keys_scaled: Optional[int] = None,
    mean_norm: Optional[float] = None,
    maximum_norm: Optional[float] = None,
    current_step_in_epoch: Optional[int] = None,
    total_steps_in_epoch: Optional[int] = None,
) -> Dict[str, Any]:
    """Generate safe progress bar metrics that won't interfere with training."""
    try:
        metrics: Dict[str, Any] = {
            "loss": f"{current_loss:.4f}",
            "avg": f"{avr_loss:.4f}",
        }

        if current_step_in_epoch is not None and total_steps_in_epoch is not None:
            try:
                steps_remaining = total_steps_in_epoch - current_step_in_epoch
                if steps_remaining >= 0:
                    metrics["left"] = f"{steps_remaining} (ep{epoch + 1})"
            except (TypeError, ValueError):
                pass

        try:
            lrs = lr_scheduler.get_last_lr()
            if lrs and len(lrs) > 0:
                metrics["lr"] = f"{lrs[0]:.1e}"
        except (AttributeError, IndexError, TypeError):
            pass

        try:
            if hasattr(args, "max_train_steps") and args.max_train_steps > 0:
                _ = (global_step / args.max_train_steps) * 100
        except (AttributeError, ZeroDivisionError, TypeError):
            pass

        try:
            if avr_loss > 0:
                _ = current_loss / avr_loss
        except (ZeroDivisionError, TypeError, ValueError):
            pass

        if getattr(args, "scale_weight_norms", False) and keys_scaled is not None:
            try:
                metrics["scaled"] = str(keys_scaled)
                if mean_norm is not None:
                    metrics["norm"] = f"{mean_norm:.3f}"
                if maximum_norm is not None:
                    metrics["max_norm"] = f"{maximum_norm:.3f}"
            except (TypeError, ValueError):
                pass

        try:
            if torch.cuda.is_available():
                device = torch.device("cuda")
                torch.cuda.reset_peak_memory_stats(device)
                peak_allocated = torch.cuda.max_memory_allocated(device) / (1024**3)
                if peak_allocated > 0.1:
                    metrics["peak"] = f"{peak_allocated:.2f} GiB"
                try:
                    import pynvml  # type: ignore

                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    metrics["peak"] = f"{float(meminfo.used)/1024**3:.2f} GiB"
                    metrics["util"] = f"{utilization.gpu}%"
                    memory_util = (float(meminfo.used) / float(meminfo.total)) * 100
                    metrics["mem_util"] = f"{memory_util:.1f}%"
                    pynvml.nvmlShutdown()
                except Exception:
                    pass
        except (RuntimeError, AttributeError):
            pass

        return metrics
    except Exception:
        return {
            "loss": f"{current_loss:.4f}",
            "avg": f"{avr_loss:.4f}",
        }
</file>

<file path="core/model_manager.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Model management for WAN network trainer.

This module handles model loading, configuration, and modification for the training system.
Extracted from wan_network_trainer.py to improve code organization and maintainability.
"""

import argparse
import importlib
import sys
import os
from typing import Any, Dict, Optional, Tuple
import torch
from accelerate import Accelerator
from safetensors.torch import load_file

import utils.fluxflow_augmentation as fluxflow_augmentation
import logging
from common.logger import get_logger
from utils import model_utils
from common.model_downloader import download_model_if_needed
from wan.configs.config import WAN_CONFIGS
from wan.modules.model import WanModel, detect_wan_sd_dtype, load_wan_model
from wan.modules.vae import WanVAE
from core.dual_model_manager import DualModelManager

logger = get_logger(__name__, level=logging.INFO)


class ModelManager:
    """Handles model loading, configuration, and management."""

    def __init__(self):
        self.pos_embed_cache = {}
        self.config = None
        self.dit_dtype = None
        self.default_guidance_scale = 1.0
        self.fluxflow_config = {}
        self._downloaded_dit_path = (
            None  # Store downloaded path to avoid double downloading
        )

    def detect_wan_sd_dtype(self, dit_path: str) -> torch.dtype:
        """Detect the dtype of the WAN model from the checkpoint."""
        return detect_wan_sd_dtype(dit_path)

    def get_attention_mode(self, args: argparse.Namespace) -> str:
        """Get the attention mode based on arguments."""
        if args.sdpa:
            return "torch"
        elif args.flash_attn:
            return "flash"
        elif args.sage_attn:
            return "sageattn"
        elif args.xformers:
            return "xformers"
        elif args.flash3:
            return "flash3"
        else:
            raise ValueError(
                "Either --sdpa, --flash-attn, --flash3, --sage-attn or --xformers must be specified"
            )

    def load_transformer(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        dit_path: str,
        attn_mode: str,
        split_attn: bool,
        loading_device: str,
        dit_weight_dtype: Optional[torch.dtype],
        config: Any,
    ) -> tuple[WanModel, Optional[DualModelManager]]:
        """Load the transformer model and optionally configure dual-model training."""
        # Use already downloaded path if available, otherwise download
        if self._downloaded_dit_path is not None:
            dit_path = self._downloaded_dit_path
            logger.info(f"Using already downloaded DiT model: {dit_path}")
        elif dit_path.startswith(("http://", "https://")):
            logger.info(f"Detected URL for DiT model, downloading: {dit_path}")
            cache_dir = getattr(args, "model_cache_dir", None)
            dit_path = download_model_if_needed(dit_path, cache_dir=cache_dir)
            logger.info(f"Downloaded DiT model to: {dit_path}")

        # Get sparse_algo parameter if nabla sparse attention is enabled
        sparse_algo = None
        if hasattr(args, "nabla_sparse_attention") and args.nabla_sparse_attention:
            sparse_algo = args.nabla_sparse_algo
            if sparse_algo is None:
                default_algo = "nabla-0.7_sta-11-24-24"
                logger.warning(
                    f"--nabla_sparse_attention is set, but --nabla_sparse_algo is not. Using a default: '{default_algo}'"
                )
                sparse_algo = default_algo
            logger.info(
                f"üîß Loading WAN model with sparse attention algorithm: {sparse_algo}"
            )

        # Pass FVDM flag through to the model by inlining the constructor after load
        use_fvdm_flag = hasattr(args, "enable_fvdm") and args.enable_fvdm

        transformer = load_wan_model(
            config,
            accelerator.device,
            dit_path,
            attn_mode,
            split_attn,
            loading_device,
            dit_weight_dtype,
            getattr(args, "fp8_scaled", False),
            sparse_algo=sparse_algo,
            use_fvdm=use_fvdm_flag,
        )
        # WanModel was constructed with use_fvdm above; no runtime mutation needed

        # If dual-model training is disabled, return as-is
        enable_dual = bool(getattr(args, "enable_dual_model_training", False))
        if not enable_dual:
            return transformer, None

        # Validate required args
        high_noise_path = getattr(args, "dit_high_noise", None)
        if not high_noise_path or not str(high_noise_path).strip():
            raise ValueError(
                "enable_dual_model_training=True requires 'dit_high_noise' to be set"
            )

        # Download high-noise model if needed (without building a second module on GPU)
        if str(high_noise_path).startswith(("http://", "https://")):
            logger.info(
                f"Detected URL for high-noise DiT model, downloading: {high_noise_path}"
            )
            cache_dir = getattr(args, "model_cache_dir", None)
            high_noise_path = download_model_if_needed(
                high_noise_path, cache_dir=cache_dir
            )
            logger.info(f"Downloaded high-noise DiT model to: {high_noise_path}")

        timestep_boundary = float(getattr(args, "timestep_boundary", 900))
        blocks_to_swap = int(getattr(args, "blocks_to_swap", 0))

        # Allow mixed mode ONLY if explicitly enabled in config
        allow_mixed = bool(getattr(args, "allow_mixed_block_swap_offload", False))
        if blocks_to_swap > 0 and bool(getattr(args, "offload_inactive_dit", True)):
            if allow_mixed:
                logger.info(
                    "Mixed mode enabled: block swap on active DiT + offloaded inactive DiT on CPU"
                )
            else:
                logger.info(
                    "Block swap specified; offload_inactive_dit disabled (allow_mixed_block_swap_offload=false)"
                )

        # Respect explicit offload flag; if mixed not allowed and blocks_to_swap>0, disable offload
        if not allow_mixed and blocks_to_swap > 0:
            offload_inactive = False
        else:
            offload_inactive = bool(getattr(args, "offload_inactive_dit", True))

        # Load high-noise model to get its state_dict, identical to original implementation
        logger.info("Loading high-noise model to extract state_dict...")
        high_noise_model = load_wan_model(
            config,
            accelerator.device,
            high_noise_path,
            attn_mode,
            split_attn,
            "cpu" if offload_inactive else loading_device,
            dit_weight_dtype,
            getattr(args, "fp8_scaled", False),
            sparse_algo=sparse_algo,
            use_fvdm=use_fvdm_flag,
        )

        # Enable block swap for the temporary high-noise model only when not offloading it
        # When offloading inactive (mixed mode), keep the temp model strictly on CPU to extract state_dict
        if blocks_to_swap > 0 and not offload_inactive:
            logger.info(
                f"Prepare block swap for high noise model, blocks_to_swap={blocks_to_swap}"
            )
            high_noise_model.enable_block_swap(
                blocks_to_swap, accelerator.device, supports_backward=True
            )
            high_noise_model.move_to_device_except_swap_blocks(accelerator.device)
            high_noise_model.prepare_block_swap_before_forward()

        # Extract state_dict from the loaded model (will be on CPU if offload_inactive)
        high_sd = high_noise_model.state_dict()

        # Clean up the temporary model to save memory
        del high_noise_model

        dual_manager = DualModelManager(
            active_transformer=transformer,
            high_noise_state_dict=high_sd,
            timestep_boundary=timestep_boundary,
            offload_inactive=offload_inactive,
            blocks_to_swap=blocks_to_swap,
        )

        logger.info(
            "‚úÖ Dual model manager initialized (single LoRA, base swap strategy)"
        )
        return transformer, dual_manager

    def create_network(
        self,
        args: argparse.Namespace,
        transformer: WanModel,
        vae: Optional[WanVAE],
        control_signal_processor: Any,
    ) -> Any:
        """Create and configure the network for training."""
        # Load network module
        sys.path.append(os.path.dirname(__file__))
        logger.info(f"import network module: {args.network_module}")
        network_module = importlib.import_module(args.network_module)

        if args.base_weights is not None:
            # if base_weights is specified, merge the weights to DiT model
            for i, weight_path in enumerate(args.base_weights):
                if (
                    args.base_weights_multiplier is None
                    or len(args.base_weights_multiplier) <= i
                ):
                    multiplier = 1.0
                else:
                    multiplier = args.base_weights_multiplier[i]

                logger.info(
                    f"merging module: {weight_path} with multiplier {multiplier}"
                )

                weights_sd = load_file(weight_path)
                module = network_module.create_arch_network_from_weights(
                    multiplier, weights_sd, unet=transformer, for_inference=True
                )
                module.merge_to(None, transformer, weights_sd, torch.float32, "cpu")

            logger.info(f"all weights merged: {', '.join(args.base_weights)}")

        # prepare network
        net_kwargs = {}
        if args.network_args is not None:
            for net_arg in args.network_args:
                key, value = net_arg.split("=")
                net_kwargs[key] = value

        sparse_algo_to_pass = None
        if hasattr(args, "nabla_sparse_attention") and args.nabla_sparse_attention:
            # The main switch is on. Now get the algorithm string.
            sparse_algo_to_pass = args.nabla_sparse_algo
            if sparse_algo_to_pass is None:
                # If the user enables sparse attention but forgets the algo string,
                # we can provide a sensible default or raise an error. A default is more user-friendly.
                default_algo = "nabla-0.7_sta-11-24-24"
                logger.warning(
                    f"--nabla_sparse_attention is set, but --nabla_sparse_algo is not. Using a default: '{default_algo}'"
                )
                sparse_algo_to_pass = default_algo

        # Add the result to net_kwargs. It will be None if the flag is off.
        net_kwargs["sparse_algo"] = sparse_algo_to_pass

        # Check if control LoRA is enabled
        control_config = None
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            # Build control config using consistent key names expected by utils processor
            control_config = {
                "control_lora_type": getattr(args, "control_lora_type", "tile"),
                "control_preprocessing": getattr(args, "control_preprocessing", "blur"),
                "control_blur_kernel_size": getattr(
                    args, "control_blur_kernel_size", 15
                ),
                "control_blur_sigma": getattr(args, "control_blur_sigma", 4.0),
                "control_scale_factor": getattr(args, "control_scale_factor", 1.0),
                "control_concatenation_dim": getattr(
                    args, "control_concatenation_dim", -2
                ),
                "control_inject_noise": getattr(args, "control_inject_noise", 0.0),
            }
            logger.info(f"üéØ Control LoRA enabled with config: {control_config}")

        from networks.lora_wan import WAN_TARGET_REPLACE_MODULES

        # Handle VAE training separately
        if args.network_module == "networks.vae_wan":
            logger.info("üé® Creating VAE network for VAE training")
            if vae is None:
                raise ValueError(
                    "VAE model is required for VAE training but was not loaded"
                )

            # Parse VAE-specific arguments
            vae_training_mode = getattr(args, "vae_training_mode", "full")

            # Import VAE network module
            import networks.vae_wan as vae_network_module

            if args.dim_from_weights:
                # Load VAE network from weights
                weights_sd = load_file(args.dim_from_weights)
                network = vae_network_module.create_network_from_weights(
                    1.0,  # multiplier
                    args.network_dim,
                    args.network_alpha,
                    vae,
                    weights_sd=weights_sd,
                    transformer=transformer,
                    training_mode=vae_training_mode,
                    **net_kwargs,
                )
            else:
                # Create new VAE network
                network = vae_network_module.create_network(
                    1.0,  # multiplier
                    args.network_dim,
                    args.network_alpha,
                    vae,
                    transformer=transformer,
                    training_mode=vae_training_mode,
                    **net_kwargs,
                )

            logger.info(
                f"‚úÖ VAE network created with training mode: {vae_training_mode}"
            )
            return network

        if args.dim_from_weights:
            logger.info(f"Loading network from weights: {args.dim_from_weights}")
            weights_sd = load_file(args.dim_from_weights)
            if control_config is not None:
                # Use control LoRA network from weights
                from networks.control_lora_wan import (
                    create_control_network_from_weights,
                )

                network = create_control_network_from_weights(
                    WAN_TARGET_REPLACE_MODULES,
                    1,
                    weights_sd,
                    unet=transformer,
                    control_config=control_config,
                )
            else:
                network, _ = network_module.create_arch_network_from_weights(
                    1, weights_sd, unet=transformer
                )
        else:
            # We use the name create_arch_network for compatibility with LyCORIS
            if hasattr(network_module, "create_arch_network"):
                if control_config is not None:
                    # Use control LoRA network
                    from networks.control_lora_wan import create_control_arch_network

                    network = create_control_arch_network(
                        1.0,
                        args.network_dim,
                        args.network_alpha,
                        vae,  # type: ignore
                        None,
                        transformer,
                        neuron_dropout=args.network_dropout,
                        control_config=control_config,
                        verbose=getattr(args, "verbose_network", False),
                        **net_kwargs,
                    )
                else:
                    network = network_module.create_arch_network(
                        1.0,
                        args.network_dim,
                        args.network_alpha,
                        vae,
                        None,
                        transformer,
                        neuron_dropout=args.network_dropout,
                        verbose=getattr(args, "verbose_network", False),
                        **net_kwargs,
                    )
            else:
                # LyCORIS compatibility
                network = network_module.create_network(
                    1.0,
                    args.network_dim,
                    args.network_alpha,
                    vae,
                    None,
                    transformer,
                    **net_kwargs,
                )

        if network is None:
            return None

        if hasattr(network_module, "prepare_network"):
            network.prepare_network(args)

        # apply network to DiT
        network.apply_to(None, transformer, apply_text_encoder=False, apply_unet=True)

        # Modify model for control LoRA if enabled
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            self.modify_model_for_control_lora(transformer, args)

        # Optionally create ControlNet module (parallel network) if enabled
        self.controlnet = None
        if hasattr(args, "enable_controlnet") and args.enable_controlnet:
            try:
                import networks.controlnet_wan as controlnet_module

                cn_kwargs = {}
                if isinstance(getattr(args, "controlnet", None), dict):
                    cn_kwargs.update(args.controlnet)
                # mypy: pass torch.nn.Module | None for vae arg type
                self.controlnet = controlnet_module.create_network(
                    1.0,
                    args.network_dim,
                    args.network_alpha,
                    vae if isinstance(vae, torch.nn.Module) else None,
                    None,
                    transformer,
                    **cn_kwargs,
                )
                logger.info("‚úÖ ControlNet module created and ready for training")
            except Exception as e:
                logger.warning(f"Failed to set up ControlNet module: {e}")

        if args.network_weights is not None:
            # FIXME consider alpha of weights: this assumes that the alpha is not changed
            info = network.load_weights(args.network_weights)
            logger.info(f"load network weights from {args.network_weights}: {info}")

        if args.gradient_checkpointing:
            transformer.enable_gradient_checkpointing()
            network.enable_gradient_checkpointing()  # may have no effect

        return network

    def handle_model_specific_args(self, args: argparse.Namespace) -> None:
        """Handle model-specific argument processing and validation."""
        self.pos_embed_cache = {}
        self.config = WAN_CONFIGS[args.task]

        # Download model if it's a URL before detecting dtype
        dit_path = args.dit
        if dit_path.startswith(("http://", "https://")):
            logger.info(f"Detected URL for DiT model, downloading: {dit_path}")
            cache_dir = getattr(args, "model_cache_dir", None)
            dit_path = download_model_if_needed(dit_path, cache_dir=cache_dir)
            logger.info(f"Downloaded DiT model to: {dit_path}")
            # Update args.dit to point to the local downloaded path
            args.dit = dit_path
            self._downloaded_dit_path = dit_path  # Store for reuse

        self.dit_dtype = detect_wan_sd_dtype(dit_path)

        if self.dit_dtype == torch.float16:
            assert args.mixed_precision in [
                "fp16",
                "no",
            ], "DiT weights are in fp16, mixed precision must be fp16 or no"
        elif self.dit_dtype == torch.bfloat16:
            assert args.mixed_precision in [
                "bf16",
                "no",
            ], "DiT weights are in bf16, mixed precision must be bf16 or no"

        if args.fp8_scaled and self.dit_dtype.itemsize == 1:
            raise ValueError(
                "DiT weights is already in fp8 format, cannot scale to fp8. Please use fp16/bf16 weights / DiT„ÅÆÈáç„Åø„ÅØ„Åô„Åß„Å´fp8ÂΩ¢Âºè„Åß„Åô„ÄÇfp8„Å´„Çπ„Ç±„Éº„É™„É≥„Ç∞„Åß„Åç„Åæ„Åõ„Çì„ÄÇfp16/bf16„ÅÆÈáç„Åø„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
            )

        # dit_dtype cannot be fp8, so we select the appropriate dtype
        if self.dit_dtype.itemsize == 1:
            self.dit_dtype = (
                torch.float16 if args.mixed_precision == "fp16" else torch.bfloat16
            )

        args.dit_dtype = model_utils.dtype_to_str(self.dit_dtype)

        self.default_guidance_scale = 1.0  # not used
        self.fluxflow_config = fluxflow_augmentation.get_fluxflow_config_from_args(args)

    def load_vae(
        self, args: argparse.Namespace, vae_dtype: torch.dtype, vae_path: str
    ) -> WanVAE:
        """Load the VAE model."""
        # Download model if it's a URL
        if vae_path.startswith(("http://", "https://")):
            logger.info(f"Detected URL for VAE model, downloading: {vae_path}")
            cache_dir = getattr(args, "model_cache_dir", None)
            vae_path = download_model_if_needed(vae_path, cache_dir=cache_dir)
            logger.info(f"Downloaded VAE model to: {vae_path}")
            # Update args.vae to point to the local downloaded path
            args.vae = vae_path

        logger.info(f"Loading VAE model from {vae_path}")
        cache_device = torch.device("cpu") if args.vae_cache_cpu else None

        # Handle device parameter - ensure it's a valid device string
        device = "cuda"  # default
        if hasattr(args, "device") and args.device is not None:
            if isinstance(args.device, str):
                device = args.device
            elif isinstance(args.device, torch.device):
                device = str(args.device)
            else:
                logger.warning(f"Invalid device type {type(args.device)}, using 'cuda'")
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
            logger.info("CUDA not available, using CPU")

        vae = WanVAE(
            vae_path=vae_path,
            device=device,
            dtype=vae_dtype,
            cache_device=cache_device,
        )
        return vae

    @staticmethod
    def scale_shift_latents(latents: torch.Tensor) -> torch.Tensor:
        """Apply scaling and shifting to latents (currently no-op for WAN)."""
        return latents

    def modify_model_for_control_lora(
        self, transformer: WanModel, args: argparse.Namespace
    ) -> None:
        """Modify the model's patch embedding layer to accept additional channels for control LoRA.

        This aligns with the reference implementation.
        """
        # Re-entrancy guard ‚Äì return early if already patched
        if getattr(transformer, "_control_lora_patched", False):
            logger.debug("Control LoRA patch already applied ‚Äì skipping.")
            return

        if hasattr(transformer, "patch_embedding"):
            with torch.no_grad():
                in_cls = transformer.patch_embedding.__class__  # nn.Conv3d
                old_in_dim = transformer.in_dim  # 16
                new_in_dim = old_in_dim * 2  # Double the input channels

                new_in = in_cls(
                    in_channels=new_in_dim,
                    out_channels=transformer.patch_embedding.out_channels,
                    kernel_size=transformer.patch_embedding.kernel_size,  # type: ignore
                    stride=transformer.patch_embedding.stride,  # type: ignore
                    padding=transformer.patch_embedding.padding,  # type: ignore
                ).to(
                    device=transformer.patch_embedding.weight.device,
                    dtype=transformer.patch_embedding.weight.dtype,
                )

                new_in.weight.zero_()
                # Copy original weights to first half of new weights
                new_in.weight[:, :old_in_dim, :, :, :] = (
                    transformer.patch_embedding.weight
                )
                # Copy original bias so the behaviour matches the reference implementation
                if transformer.patch_embedding.bias is not None:
                    new_in.bias.copy_(transformer.patch_embedding.bias)  # type: ignore

                # Replace the original patch embedding
                transformer.patch_embedding = new_in
                transformer.in_dim = new_in_dim

                # Update HuggingFace config so that any model save/load cycle retains the new input channel size
                if hasattr(transformer, "register_to_config"):
                    # WanModel may inherit from ConfigMixin in some versions
                    transformer.register_to_config(in_dim=new_in_dim)  # type: ignore

                logger.info(
                    f"Modified model for control LoRA: input channels {old_in_dim} -> {new_in_dim}"
                )

                # Ensure gradients are enabled for the new patch_embedding so it can learn
                transformer.patch_embedding.requires_grad_(True)

                # mark patched
                transformer._control_lora_patched = True  # type: ignore

    def get_model_config(self) -> Dict[str, Any]:
        """Get the current model configuration."""
        if self.config is None:
            raise ValueError(
                "Model config not initialized. Call handle_model_specific_args first."
            )
        return self.config

    def get_dit_dtype(self) -> torch.dtype:
        """Get the DiT data type."""
        if self.dit_dtype is None:
            raise ValueError(
                "DiT dtype not initialized. Call handle_model_specific_args first."
            )
        return self.dit_dtype

    def get_fluxflow_config(self) -> Dict[str, Any]:
        """Get the FluxFlow configuration."""
        return self.fluxflow_config

    def get_default_guidance_scale(self) -> float:
        """Get the default guidance scale."""
        return self.default_guidance_scale

    def validate_attention_mode(self, args: argparse.Namespace) -> str:
        """Validate and return the attention mode based on arguments."""
        if args.sdpa:
            return "torch"
        elif args.flash_attn:
            return "flash"
        elif args.sage_attn:
            return "sageattn"
        elif args.xformers:
            return "xformers"
        elif args.flash3:
            return "flash3"
        else:
            raise ValueError(
                "Either --sdpa, --flash-attn, --flash3, --sage-attn or --xformers must be specified"
            )

    def prepare_model_for_training(
        self,
        transformer: WanModel,
        args: argparse.Namespace,
        dit_weight_dtype: Optional[torch.dtype] = None,
    ) -> None:
        """Prepare the model for training (dtype casting, etc.)."""
        if dit_weight_dtype != self.dit_dtype and dit_weight_dtype is not None:
            logger.info(f"casting model to {dit_weight_dtype}")
            transformer.to(dit_weight_dtype)

        # Apply control LoRA modifications if needed (centralized entry point)
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            logger.info("Applying control LoRA model modifications...")
            self.modify_model_for_control_lora(transformer, args)

        # Set appropriate training/eval mode
        if args.gradient_checkpointing:
            transformer.train()
        else:
            transformer.eval()

    def setup_block_swapping(
        self, transformer: WanModel, accelerator: Accelerator, blocks_to_swap: int
    ) -> WanModel:
        """Setup block swapping for memory optimization."""
        if blocks_to_swap > 0:
            logger.info(
                f"enable swap {blocks_to_swap} blocks to CPU from device: {accelerator.device}"
            )
            transformer.enable_block_swap(
                blocks_to_swap, accelerator.device, supports_backward=True
            )
            transformer.move_to_device_except_swap_blocks(accelerator.device)

            # Prepare with device placement
            transformer = accelerator.prepare(
                transformer, device_placement=[not blocks_to_swap > 0]
            )
            accelerator.unwrap_model(transformer).move_to_device_except_swap_blocks(
                accelerator.device
            )  # reduce peak memory usage
            accelerator.unwrap_model(transformer).prepare_block_swap_before_forward()
        else:
            transformer = accelerator.prepare(transformer)

        return transformer
</file>

<file path="core/optimizer_manager.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Optimizer and scheduler management for WAN network trainer.

This module handles all optimizer and learning rate scheduler creation, configuration,
and logging functionality. Extracted from wan_network_trainer.py to improve code
organization and maintainability.
"""

import ast
import importlib
import argparse
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
import torch
import transformers
from diffusers.optimization import (
    SchedulerType as DiffusersSchedulerType,
    TYPE_TO_SCHEDULER_FUNCTION as DIFFUSERS_TYPE_TO_SCHEDULER_FUNCTION,
)
from transformers.optimization import SchedulerType, TYPE_TO_SCHEDULER_FUNCTION

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class OptimizerManager:
    """Handles optimizer and learning rate scheduler creation and management."""

    def __init__(self):
        pass

    @staticmethod
    def generate_step_logs(
        args: argparse.Namespace,
        current_loss: float,
        avr_loss: float,
        lr_scheduler: Any,
        lr_descriptions: Optional[List[str]],
        optimizer: Optional[torch.optim.Optimizer] = None,
        keys_scaled: Optional[int] = None,
        mean_norm: Optional[float] = None,
        maximum_norm: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Generate step logs for training metrics.

        Delegates to the shared implementation in core.metrics.generate_step_logs
        to keep behavior consistent across the codebase.
        """
        from core.metrics import generate_step_logs as _gsl

        return _gsl(
            args,
            current_loss,
            avr_loss,
            lr_scheduler,
            lr_descriptions,
            optimizer,
            keys_scaled,
            mean_norm,
            maximum_norm,
            None,  # ema_loss not used here
            None,  # model not available here
            None,  # global_step not available here
            None,  # per_source_losses not available here
            None,  # gradient_norm not available here
        )

    @staticmethod
    def get_optimizer(
        args: argparse.Namespace,
        transformer: torch.nn.Module,
        trainable_params: List[torch.nn.Parameter],
    ) -> Tuple[str, str, torch.optim.Optimizer, Callable, Callable]:
        """Create and configure the optimizer based on arguments.

        Returns:
            Tuple of (optimizer_name, optimizer_args_str, optimizer, train_fn, eval_fn)
        """
        # adamw, adamw8bit, adafactor
        optimizer_type = args.optimizer_type.lower()

        # split optimizer_type and optimizer_args
        optimizer_kwargs = {}
        if args.optimizer_args is not None and len(args.optimizer_args) > 0:
            logger.info(f"Processing optimizer args: {args.optimizer_args}")
            for arg in args.optimizer_args:
                key, value = arg.split("=", 1)  # Split only on first '='
                try:
                    # Try to parse as literal first
                    parsed_value = ast.literal_eval(value)
                except (ValueError, SyntaxError):
                    # If that fails, treat as string (remove quotes if present)
                    parsed_value = value.strip("'\"")
                optimizer_kwargs[key] = parsed_value
                logger.info(
                    f"  Parsed: {key} = {parsed_value} (type: {type(parsed_value)})"
                )

        lr = args.learning_rate
        optimizer = None
        optimizer_class = None

        if optimizer_type.endswith("8bit".lower()):
            try:
                import bitsandbytes as bnb
            except ImportError:
                raise ImportError(
                    "bitsandbytes is not installed. Please install bitsandbytes to use 8-bit optimizers."
                )

            if optimizer_type == "AdamW8bit".lower():
                logger.info(f"using AdamW8bit optimizer | {optimizer_kwargs}")
                optimizer_class = bnb.optim.AdamW8bit
                optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "Adafactor".lower():
            # Adafactor: check relative_step and warmup_init
            if "relative_step" not in optimizer_kwargs:
                optimizer_kwargs["relative_step"] = True  # default
            if not optimizer_kwargs["relative_step"] and optimizer_kwargs.get(
                "warmup_init", False
            ):
                logger.info("set relative_step to True because warmup_init is True")
                optimizer_kwargs["relative_step"] = True
            logger.info(f"using Adafactor optimizer | {optimizer_kwargs}")

            if optimizer_kwargs["relative_step"]:
                logger.info(f"relative_step is true")
                if lr != 0.0:
                    logger.warning(
                        "The specified learning rate will be used as initial_lr for Adafactor with relative_step=True."
                    )
                args.learning_rate = None

                if args.lr_scheduler != "adafactor":
                    logger.info(f"using adafactor_scheduler")
                args.lr_scheduler = f"adafactor:{lr}"

                lr = None
            else:
                if args.max_grad_norm != 0.0:
                    logger.warning(
                        "max_grad_norm is set, so gradient clipping is enabled. Consider setting it to 0 to disable clipping."
                    )
                if args.lr_scheduler != "constant_with_warmup":
                    logger.warning(
                        "It is recommended to use the 'constant_with_warmup' scheduler with Adafactor when relative_step is False."
                    )
                if optimizer_kwargs.get("clip_threshold", 1.0) != 1.0:
                    logger.warning(
                        "It is recommended to set clip_threshold=1.0 for Adafactor."
                    )

            optimizer_class = transformers.optimization.Adafactor
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "AdamW".lower():
            logger.info(f"using AdamW optimizer | {optimizer_kwargs}")
            optimizer_class = torch.optim.AdamW
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "CAME8Bit".lower():
            # https://github.com/NVlabs/Sana/commit/dd38c12744ac652b01e9b653412fc76c355798bd
            try:
                from optimizers.sana_optimizer import CAME8BitWrapper

                optimizer_class = CAME8BitWrapper
                logger.info(
                    "using CamE8Bit optimizer (SANA implementation) | %s",
                    optimizer_kwargs,
                )
                optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)
            except Exception as err:
                logger.warning(
                    "‚ö†Ô∏è Failed to import CamE8Bit implementation (%s).",
                    err,
                )
                raise ImportError("CamE8Bit implementation could not be used") from err

        elif optimizer_type == "Automagic".lower():
            logger.info(f"using Automagic optimizer | {optimizer_kwargs}")

            from optimizers.automagic import Automagic

            optimizer_class = Automagic
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "AdamW8bitKahan".lower():
            logger.info(f"using AdamW8bitKahan optimizer | {optimizer_kwargs}")

            from optimizers.adamw_8bit_kahan import AdamW8bitKahan

            optimizer_class = AdamW8bitKahan
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "AdamWOptimi".lower():
            logger.info(f"using optimi.AdamW optimizer | {optimizer_kwargs}")

            from optimi import AdamW

            optimizer_class = AdamW
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "LionOptimi".lower():
            logger.info(f"using optimi.Lion optimizer | {optimizer_kwargs}")

            from optimi import Lion

            optimizer_class = Lion
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "Fira".lower():
            logger.info(f"using Fira optimizer | {optimizer_kwargs}")

            from optimizers.fira_optimizer import FiraOptimizerManager
            from fira import FiraAdamW

            optimizer_class = FiraAdamW
            optimizer, functions = FiraOptimizerManager.create_fira_optimizer(
                args, transformer, trainable_params, lr, optimizer_kwargs
            )
            train_fn = functions["train_fn"]
            eval_fn = functions["eval_fn"]

        elif optimizer_type == "SophiaG".lower():
            logger.info(f"using SophiaG optimizer | {optimizer_kwargs}")

            from optimizers.sophia import SophiaG

            optimizer_class = SophiaG
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "Soap".lower():
            logger.info(f"using Soap optimizer | {optimizer_kwargs}")

            from optimizers.soap import SOAP

            optimizer_class = SOAP
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        elif optimizer_type == "Muon".lower():
            logger.info(f"using Muon optimizer | {optimizer_kwargs}")

            # Use SingleDeviceMuonWithAuxAdam for single-GPU training (avoids distributed training requirements)
            from optimizers.muon import SingleDeviceMuonWithAuxAdam

            # Separate trainable parameters by dimensionality
            # Muon should be applied to hidden weights (‚â•2D parameters) - Linear layers
            # AdamW should be applied to biases/gains (<2D) and other parameters

            # Handle both parameter lists and parameter group dictionaries
            def extract_params(params_list):
                """Extract individual parameters from parameter groups or parameter lists."""
                extracted_params = []
                for i, item in enumerate(params_list):
                    if isinstance(item, dict) and "params" in item:
                        # This is a parameter group dictionary
                        logger.debug(
                            f"Parameter group {i}: {len(item['params'])} parameters"
                        )
                        extracted_params.extend(
                            list(item["params"])
                        )  # Convert dict_values to list
                    elif isinstance(item, torch.nn.Parameter):
                        # This is already a parameter
                        logger.debug(
                            f"Parameter {i}: shape {item.shape}, ndim {item.ndim}"
                        )
                        extracted_params.append(item)
                    else:
                        # Skip non-parameter items
                        logger.debug(f"Skipping item {i}: {type(item)}")
                        continue
                return extracted_params

            # Extract all individual parameters
            all_params = extract_params(trainable_params)

            # Separate by dimensionality
            hidden_weights = [p for p in all_params if p.ndim >= 2]
            hidden_gains_biases = [p for p in all_params if p.ndim < 2]

            # Log parameter distribution for debugging
            logger.info(f"Total trainable parameters: {len(trainable_params)}")

            # Detailed logging of the trainable parameter structure
            for i, param_item in enumerate(trainable_params):
                if isinstance(param_item, dict):
                    logger.info(f"Parameter group {i}:")
                    logger.info(f"  - Type: Parameter group (dictionary)")
                    logger.info(f"  - Keys: {list(param_item.keys())}")
                    if "params" in param_item:
                        params_list = list(
                            param_item["params"]  # type: ignore
                        )  # Convert dict_values to list
                        logger.info(f"  - Number of parameters: {len(params_list)}")
                        if len(params_list) > 0:
                            logger.info(
                                f"  - Parameter types: {[type(p).__name__ for p in params_list[:5]]}..."
                            )
                            logger.info(
                                f"  - Parameter shapes: {[p.shape for p in params_list[:5]]}..."
                            )
                    if "lr" in param_item:
                        logger.info(f"  - Learning rate: {param_item['lr']}")  # type: ignore
                    if "weight_decay" in param_item:
                        logger.info(f"  - Weight decay: {param_item['weight_decay']}")  # type: ignore
                elif isinstance(param_item, torch.nn.Parameter):
                    logger.info(f"Parameter {i}:")
                    logger.info(f"  - Type: Individual parameter")
                    logger.info(f"  - Shape: {param_item.shape}")
                    logger.info(f"  - Dtype: {param_item.dtype}")
                else:
                    logger.info(f"Item {i}:")
                    logger.info(f"  - Type: {type(param_item).__name__}")
                    logger.info(f"  - Content: {param_item}")

            logger.info(f"Extracted individual parameters: {len(all_params)}")
            logger.info(f"Muon: {len(hidden_weights)} hidden weight parameters (‚â•2D)")
            logger.info(f"AdamW: {len(hidden_gains_biases)} bias/gain parameters (<2D)")

            # Validate that we have parameters to optimize
            if len(hidden_weights) == 0 and len(hidden_gains_biases) == 0:
                raise ValueError("No trainable parameters found for Muon optimizer!")

            if len(hidden_weights) == 0:
                logger.warning(
                    "No hidden weight parameters (‚â•2D) found for Muon. Consider using a different optimizer."
                )

            if len(hidden_gains_biases) == 0:
                logger.info(
                    "No bias/gain parameters (<2D) found. This is normal for WAN LoRA networks."
                )

            # Use learning rate from args, with Muon group using higher LR as recommended
            muon_lr = optimizer_kwargs.get(
                "muon_lr", 0.001
            )  # Conservative Muon LR for LoRA
            adam_lr = optimizer_kwargs.get("adam_lr", lr)  # Use specified LR for AdamW
            weight_decay = optimizer_kwargs.get(
                "weight_decay", 0.001
            )  # Lower weight decay for LoRA
            betas = optimizer_kwargs.get("betas", (0.9, 0.95))

            # Muon-specific parameters based on theory
            momentum = optimizer_kwargs.get(
                "momentum", 0.9
            )  # Lower momentum for stability
            ns_steps = optimizer_kwargs.get("ns_steps", 3)  # Fewer Newton-Schulz steps

            # Only include parameter groups that have parameters
            param_groups = []

            if len(hidden_weights) > 0:
                param_groups.append(
                    dict(
                        params=hidden_weights,
                        use_muon=True,
                        lr=muon_lr,
                        weight_decay=weight_decay,
                        momentum=momentum,  # Add momentum for Muon group
                    )
                )

            if len(hidden_gains_biases) > 0:
                param_groups.append(
                    dict(
                        params=hidden_gains_biases,
                        use_muon=False,
                        lr=adam_lr,
                        betas=betas,
                        weight_decay=weight_decay,
                    )
                )

            # Ensure we have at least one parameter group
            if len(param_groups) == 0:
                raise ValueError("No parameter groups created for Muon optimizer!")

            optimizer_class = SingleDeviceMuonWithAuxAdam
            optimizer = optimizer_class(param_groups)

            # Log configuration for transparency
            logger.info(f"Muon configuration:")
            logger.info(f"  - Muon LR: {muon_lr}")
            logger.info(f"  - AdamW LR: {adam_lr}")
            logger.info(f"  - Weight decay: {weight_decay}")
            logger.info(f"  - Momentum: {momentum}")
            logger.info(f"  - Newton-Schulz steps: {ns_steps}")

        elif optimizer_type == "Prodigy".lower():
            # Prodigy optimizer from prodigyopt
            try:
                from prodigyopt import Prodigy  # type: ignore
            except Exception as err:  # pragma: no cover - import-time failure
                raise ImportError(
                    "Prodigy not available. Please install with `pip install prodigyopt`."
                ) from err

            # Map commonly used kwargs with sensible defaults
            # - d_coef: multiplicative factor D (logged as `d` in param_groups)
            # - decouple: decoupled weight decay
            # - betas: 2 or 3 beta values are accepted by prodigyopt
            # - use_bias_correction / safeguard_warmup: stability toggles
            d_coef = optimizer_kwargs.get("d_coef", 1.5)
            decouple = optimizer_kwargs.get("decouple", True)
            weight_decay = optimizer_kwargs.get("weight_decay", 0.1)
            betas = optimizer_kwargs.get("betas", (0.9, 0.999))
            use_bias_correction = optimizer_kwargs.get("use_bias_correction", False)
            safeguard_warmup = optimizer_kwargs.get("safeguard_warmup", False)

            # Ensure tuple for betas
            if isinstance(betas, list):
                betas = tuple(betas)

            logger.info(
                "using Prodigy optimizer | d_coef=%s, decouple=%s, weight_decay=%s, betas=%s, use_bias_correction=%s, safeguard_warmup=%s",
                d_coef,
                decouple,
                weight_decay,
                betas,
                use_bias_correction,
                safeguard_warmup,
            )

            optimizer_class = Prodigy
            optimizer = optimizer_class(
                trainable_params,
                lr=lr,
                d_coef=d_coef,
                decouple=decouple,
                weight_decay=weight_decay,
                betas=betas,  # type: ignore[arg-type]
                use_bias_correction=use_bias_correction,
                safeguard_warmup=safeguard_warmup,
            )

        if optimizer is None:
            case_sensitive_optimizer_type = args.optimizer_type  # not lower
            logger.info(f"using {case_sensitive_optimizer_type} | {optimizer_kwargs}")

            if "." not in case_sensitive_optimizer_type:  # from torch.optim
                optimizer_module = torch.optim
            else:  # from other library
                values = case_sensitive_optimizer_type.split(".")
                optimizer_module = importlib.import_module(".".join(values[:-1]))
                case_sensitive_optimizer_type = values[-1]

            optimizer_class = getattr(optimizer_module, case_sensitive_optimizer_type)
            optimizer = optimizer_class(trainable_params, lr=lr, **optimizer_kwargs)

        # for logging - fix potential None issue
        if optimizer_class is not None:
            optimizer_name = optimizer_class.__module__ + "." + optimizer_class.__name__
        else:
            optimizer_name = "unknown"
        optimizer_args = ",".join([f"{k}={v}" for k, v in optimizer_kwargs.items()])

        # get train and eval functions
        if hasattr(optimizer, "train") and callable(optimizer.train):  # type: ignore
            train_fn = optimizer.train  # type: ignore
            eval_fn = optimizer.eval  # type: ignore
        else:
            train_fn = lambda: None
            eval_fn = lambda: None

        return optimizer_name, optimizer_args, optimizer, train_fn, eval_fn

    @staticmethod
    def is_schedulefree_optimizer(
        optimizer: torch.optim.Optimizer, args: argparse.Namespace
    ) -> bool:
        """Check if the optimizer is a schedulefree optimizer."""
        return args.optimizer_type.lower().endswith(
            "schedulefree".lower()
        )  # or args.optimizer_schedulefree_wrapper

    @staticmethod
    def get_dummy_scheduler(optimizer: torch.optim.Optimizer) -> Any:
        """Get a dummy scheduler for schedulefree optimizer.

        This scheduler supports only empty step(), get_last_lr() and optimizers.
        This scheduler is used for logging only.
        This isn't wrapped by accelerator because this class is not a subclass of torch.optim.lr_scheduler._LRScheduler
        """

        class DummyScheduler:
            def __init__(self, optimizer: torch.optim.Optimizer):
                self.optimizer = optimizer

            def step(self):
                pass

            def get_last_lr(self):
                return [group["lr"] for group in self.optimizer.param_groups]

        return DummyScheduler(optimizer)

    @staticmethod
    def get_lr_scheduler(
        args: argparse.Namespace, optimizer: torch.optim.Optimizer, num_processes: int
    ) -> Any:
        """Unified API to get any scheduler from its name."""
        # if schedulefree optimizer, return dummy scheduler
        if OptimizerManager.is_schedulefree_optimizer(optimizer, args):
            return OptimizerManager.get_dummy_scheduler(optimizer)

        name = args.lr_scheduler
        num_training_steps = (
            args.max_train_steps * num_processes
        )  # * args.gradient_accumulation_steps
        num_warmup_steps: Optional[int] = (
            int(args.lr_warmup_steps * num_training_steps)
            if isinstance(args.lr_warmup_steps, float)
            else args.lr_warmup_steps
        )
        num_decay_steps: Optional[int] = (
            int(args.lr_decay_steps * num_training_steps)
            if isinstance(args.lr_decay_steps, float)
            else args.lr_decay_steps
        )

        # Fix potential None issues
        if num_warmup_steps is None:
            num_warmup_steps = 0
        if num_decay_steps is None:
            num_decay_steps = 0

        num_stable_steps = num_training_steps - num_warmup_steps - num_decay_steps
        num_cycles = args.lr_scheduler_num_cycles
        power = args.lr_scheduler_power
        timescale = args.lr_scheduler_timescale
        min_lr_ratio = args.lr_scheduler_min_lr_ratio

        lr_scheduler_kwargs = {}  # get custom lr_scheduler kwargs
        if args.lr_scheduler_args is not None and len(args.lr_scheduler_args) > 0:
            for arg in args.lr_scheduler_args:
                key, value = arg.split("=")
                value = ast.literal_eval(value)
                lr_scheduler_kwargs[key] = value

        def wrap_check_needless_num_warmup_steps(return_vals):
            if num_warmup_steps is not None and num_warmup_steps != 0:
                raise ValueError(
                    f"{name} does not require `num_warmup_steps`. Set None or 0."
                )
            return return_vals

        # using any lr_scheduler from other library
        if args.lr_scheduler_type:
            lr_scheduler_type = args.lr_scheduler_type
            logger.info(
                f"using {lr_scheduler_type} | {lr_scheduler_kwargs} as lr_scheduler"
            )
            if "." not in lr_scheduler_type:  # default to use torch.optim
                lr_scheduler_module = torch.optim.lr_scheduler
            else:
                values = lr_scheduler_type.split(".")
                lr_scheduler_module = importlib.import_module(".".join(values[:-1]))
                lr_scheduler_type = values[-1]
            lr_scheduler_class = getattr(lr_scheduler_module, lr_scheduler_type)
            lr_scheduler = lr_scheduler_class(optimizer, **lr_scheduler_kwargs)
            return lr_scheduler

        if name.startswith("adafactor"):
            assert (
                type(optimizer) == transformers.optimization.Adafactor
            ), f"adafactor scheduler must be used with Adafactor optimizer"
            initial_lr = float(name.split(":")[1])
            # logger.info(f"adafactor scheduler init lr {initial_lr}")
            return wrap_check_needless_num_warmup_steps(
                transformers.optimization.AdafactorSchedule(optimizer, initial_lr)
            )

        if name == DiffusersSchedulerType.PIECEWISE_CONSTANT.value:
            name = DiffusersSchedulerType(name)
            schedule_func = DIFFUSERS_TYPE_TO_SCHEDULER_FUNCTION[name]
            return schedule_func(
                optimizer, **lr_scheduler_kwargs
            )  # step_rules and last_epoch are given as kwargs

        name = SchedulerType(name)
        schedule_func = TYPE_TO_SCHEDULER_FUNCTION[name]

        if name == SchedulerType.CONSTANT:
            return wrap_check_needless_num_warmup_steps(
                schedule_func(optimizer, **lr_scheduler_kwargs)
            )

        # All other schedulers require `num_warmup_steps`
        if num_warmup_steps is None:
            raise ValueError(
                f"{name} requires `num_warmup_steps`, please provide that argument."
            )

        if name == SchedulerType.CONSTANT_WITH_WARMUP:
            return schedule_func(
                optimizer, num_warmup_steps=num_warmup_steps, **lr_scheduler_kwargs
            )

        if name == SchedulerType.INVERSE_SQRT:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                timescale=timescale,
                **lr_scheduler_kwargs,
            )

        # All other schedulers require `num_training_steps`
        if num_training_steps is None:
            raise ValueError(
                f"{name} requires `num_training_steps`, please provide that argument."
            )

        if name == SchedulerType.COSINE_WITH_RESTARTS:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps,
                num_cycles=num_cycles,
                **lr_scheduler_kwargs,
            )

        if name == SchedulerType.POLYNOMIAL:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps,
                power=power,
                **lr_scheduler_kwargs,
            )

        if name == SchedulerType.COSINE_WITH_MIN_LR:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps,
                num_cycles=num_cycles / 2,
                min_lr_rate=min_lr_ratio,
                **lr_scheduler_kwargs,
            )

        # these schedulers do not require `num_decay_steps`
        if name == SchedulerType.LINEAR or name == SchedulerType.COSINE:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps,
                **lr_scheduler_kwargs,
            )

        # All other schedulers require `num_decay_steps`
        if num_decay_steps is None:
            raise ValueError(
                f"{name} requires `num_decay_steps`, please provide that argument."
            )
        if name == SchedulerType.WARMUP_STABLE_DECAY:
            return schedule_func(
                optimizer,
                num_warmup_steps=num_warmup_steps,
                num_stable_steps=num_stable_steps,
                num_decay_steps=num_decay_steps,
                num_cycles=num_cycles / 2,
                min_lr_ratio=min_lr_ratio if min_lr_ratio is not None else 0.0,
                **lr_scheduler_kwargs,
            )

        return schedule_func(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps,
            num_decay_steps=num_decay_steps,
            **lr_scheduler_kwargs,
        )
</file>

<file path="core/repa_helper.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from torchvision import transforms
from typing import Optional, Any, Union
import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class RepaHelper(nn.Module):
    """
    A helper module to encapsulate all logic for REPA (Representation Alignment).

    This module handles:
    1. Loading a frozen pretrained visual encoder (e.g., DINOv2)
    2. Creating an MLP projection head to align diffusion model features
    3. Setting up forward hooks to capture hidden states
    4. Computing the REPA loss for representation alignment
    """

    def __init__(self, diffusion_model: Any, args: Any):
        super().__init__()
        self.args = args
        self.diffusion_model = diffusion_model

        # 1. Load the frozen, pretrained visual encoder (e.g., DINOv2)
        logger.info(
            f"REPA: Loading pretrained visual encoder: {args.repa_encoder_name}"
        )
        try:
            self.visual_encoder: nn.Module = timm.create_model(
                args.repa_encoder_name,
                pretrained=True,
                cache_dir="models",
                num_classes=0,  # Remove the classification head
            ).eval()
            self.visual_encoder.requires_grad_(False)

            # Get the feature dimension from the encoder
            if hasattr(self.visual_encoder, "embed_dim"):
                embed_dim = getattr(self.visual_encoder, "embed_dim")
                self.encoder_feature_dim = (
                    int(embed_dim) if embed_dim is not None else 768
                )
            elif hasattr(self.visual_encoder, "num_features"):
                num_features = getattr(self.visual_encoder, "num_features")
                self.encoder_feature_dim = (
                    int(num_features) if num_features is not None else 768
                )
            else:
                # Fallback: try to infer from the model
                with torch.no_grad():
                    dummy_input = torch.randn(1, 3, 224, 224)
                    features = self.visual_encoder.forward_features(dummy_input)  # type: ignore
                    if isinstance(features, dict) and "x_norm_patchtokens" in features:
                        self.encoder_feature_dim = features["x_norm_patchtokens"].shape[
                            -1
                        ]
                    else:
                        # Try to get the last dimension
                        if isinstance(features, torch.Tensor):
                            self.encoder_feature_dim = features.shape[-1]
                        else:
                            raise ValueError(
                                f"Could not determine feature dimension for {args.repa_encoder_name}"
                            )

            logger.info(
                f"REPA: Visual encoder loaded successfully. Feature dim: {self.encoder_feature_dim}"
            )

        except Exception as e:
            logger.error(
                f"REPA ERROR: Failed to load visual encoder {args.repa_encoder_name}: {e}"
            )
            raise

        # 2. Create the MLP projection head
        # Get the diffusion model's hidden dimension
        if hasattr(diffusion_model, "dim"):
            self.diffusion_hidden_dim = int(diffusion_model.dim)
        elif hasattr(diffusion_model, "hidden_size"):
            self.diffusion_hidden_dim = int(diffusion_model.hidden_size)
        else:
            # Try to infer from the model structure
            for module in diffusion_model.modules():
                if hasattr(module, "in_features"):
                    self.diffusion_hidden_dim = int(module.in_features)
                    break
            else:
                # Fallback to a reasonable default
                self.diffusion_hidden_dim = 1024
                logger.warning(
                    f"REPA: Could not determine diffusion model hidden dim, using default: {self.diffusion_hidden_dim}"
                )

        self.projection_head = self._create_projection_head()

        # 3. Image normalization transform for the visual encoder
        # DINOv2 and most timm models expect this standard ImageNet normalization
        self.image_transform = transforms.Normalize(
            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
        )

        # 4. Placeholders for hooks and captured features
        self.hook_handle: Optional[Any] = None
        self.captured_features: Optional[torch.Tensor] = None

        logger.info(
            f"REPA: Initialized with diffusion hidden dim: {self.diffusion_hidden_dim}, encoder feature dim: {self.encoder_feature_dim}"
        )

    def _create_projection_head(self) -> nn.Module:
        """Creates a 3-layer MLP as described in the REPA paper."""
        return nn.Sequential(
            nn.Linear(self.diffusion_hidden_dim, self.diffusion_hidden_dim * 4),
            nn.SiLU(),
            nn.Linear(self.diffusion_hidden_dim * 4, self.encoder_feature_dim),
        )

    def _get_hook(self):
        """Closure to capture the hidden state from the target layer."""

        def hook(model, input, output):
            # The output of a transformer block is typically a tuple (hidden_state, ...).
            # We are interested in the first element.
            self.captured_features = output[0] if isinstance(output, tuple) else output

        return hook

    def setup_hooks(self) -> None:
        """Finds the target layer and attaches the forward hook."""
        depth = self.args.repa_alignment_depth
        try:
            # The target module is inside the `blocks` list of your `WanModel`
            if hasattr(self.diffusion_model, "blocks"):
                target_module = self.diffusion_model.blocks[depth]
            elif hasattr(self.diffusion_model, "layers"):
                target_module = self.diffusion_model.layers[depth]
            elif hasattr(self.diffusion_model, "transformer_blocks"):
                target_module = self.diffusion_model.transformer_blocks[depth]
            else:
                # Try to find blocks in a different way
                blocks = []
                for name, module in self.diffusion_model.named_modules():
                    if "block" in name.lower() or "layer" in name.lower():
                        blocks.append(module)
                if len(blocks) > depth:
                    target_module = blocks[depth]
                else:
                    raise ValueError(
                        f"Could not find blocks in diffusion model. Available modules: {list(self.diffusion_model.named_modules())}"
                    )

            self.hook_handle = target_module.register_forward_hook(self._get_hook())
            logger.info(f"REPA: Hook attached to layer {depth} of the diffusion model.")

        except Exception as e:
            logger.error(
                f"REPA ERROR: Could not attach hook to layer {depth}. Please check `repa_alignment_depth`. Error: {e}"
            )
            raise

    def remove_hooks(self) -> None:
        """Removes the forward hook."""
        if self.hook_handle:
            self.hook_handle.remove()
            self.hook_handle = None
            logger.info("REPA: Hook removed successfully.")

    def get_repa_loss(
        self, clean_pixels: torch.Tensor, vae: Optional[Any] = None
    ) -> torch.Tensor:
        """
        Calculates the REPA loss for a given batch.

        Args:
            clean_pixels: Clean pixel values in range [-1, 1], shape (B, C, H, W) or (B, C, F, H, W) for video
            vae: VAE model (not used in current implementation but kept for compatibility)

        Returns:
            REPA loss tensor
        """
        if self.captured_features is None:
            return torch.tensor(0.0, device=clean_pixels.device)

        # If clean_pixels is a video batch (B, C, F, H, W), take the first frame
        if clean_pixels.dim() == 5:
            clean_pixels = clean_pixels[:, :, 0, :, :]  # Take the first frame

        with torch.no_grad():
            # The visual encoder expects clean images, not latents.
            # `clean_pixels` are already normalized to [-1, 1]. We need to convert them to [0, 1].
            images = (clean_pixels + 1) / 2.0
            images = self.image_transform(images)  # Apply ImageNet normalization

            # Get target representations from the visual encoder
            # The output is patch features, not a global feature.
            target_features = self.visual_encoder.forward_features(images)  # type: ignore

            # Handle different output formats from timm models
            if isinstance(target_features, dict):
                if "x_norm_patchtokens" in target_features:
                    target_features = target_features["x_norm_patchtokens"]
                elif "x_norm_clstoken" in target_features:
                    target_features = target_features["x_norm_clstoken"]
                else:
                    # Try to find any patch-like features
                    for key, value in target_features.items():
                        if isinstance(value, torch.Tensor) and value.dim() >= 2:
                            target_features = value
                            break
                    else:
                        raise ValueError(
                            f"Could not find suitable features in encoder output: {target_features.keys()}"
                        )

            # Ensure target_features is a tensor at this point
            if not isinstance(target_features, torch.Tensor):
                raise ValueError(
                    f"Expected tensor output from visual encoder, got {type(target_features)}"
                )

            # Ensure we have the right shape: (B, N, D) where N is number of patches/tokens, D is feature dim
            if target_features.dim() == 2:
                # If we got (B, D), add a dimension to make it (B, 1, D)
                target_features = target_features.unsqueeze(1)
            elif target_features.dim() > 3:
                # If we got (B, C, H, W), flatten spatial dimensions
                B, C, H, W = target_features.shape
                target_features = target_features.view(B, C, H * W).transpose(
                    1, 2
                )  # (B, H*W, C)

        # Project the captured hidden state from the diffusion model
        projected_features = self.projection_head(self.captured_features)

        # Ensure both tensors have the same shape for comparison
        if projected_features.shape != target_features.shape:
            # Try to match the shapes by taking the mean over spatial dimensions if needed
            if projected_features.dim() == 3 and target_features.dim() == 3:
                # Both are (B, N, D), but N might be different
                if projected_features.shape[1] != target_features.shape[1]:
                    # Take mean over the spatial dimension to get (B, D)
                    projected_features = projected_features.mean(dim=1)
                    target_features = target_features.mean(dim=1)
            elif projected_features.dim() == 2 and target_features.dim() == 3:
                # projected_features is (B, D), target_features is (B, N, D)
                target_features = target_features.mean(dim=1)  # (B, D)
            elif projected_features.dim() == 3 and target_features.dim() == 2:
                # projected_features is (B, N, D), target_features is (B, D)
                projected_features = projected_features.mean(dim=1)  # (B, D)

        # Calculate similarity loss (negative cosine similarity is common)
        if self.args.repa_similarity_fn == "cosine":
            # Ensure both tensors are 2D for cosine similarity
            if projected_features.dim() > 2:
                projected_features = projected_features.mean(dim=1)
            if target_features.dim() > 2:
                target_features = target_features.mean(dim=1)

            # Normalize both tensors for cosine similarity
            projected_features = F.normalize(projected_features, dim=-1)
            target_features = F.normalize(target_features, dim=-1)

            # Cosine similarity: dot product of normalized vectors
            similarity = (projected_features * target_features).sum(dim=-1)
            loss = -similarity.mean()  # Negative because we want to maximize similarity

        else:
            raise NotImplementedError(
                f"REPA similarity function '{self.args.repa_similarity_fn}' not implemented."
            )

        # Clear captured features for the next step
        self.captured_features = None

        return loss * self.args.repa_loss_lambda
</file>

<file path="core/sampling_manager.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Sampling and inference management for WAN network trainer.

This module handles all image/video sampling, inference, and related functionality.
Extracted from wan_network_trainer.py to improve code organization and maintainability.
"""

import argparse
import os
import time
from typing import Any, Dict, List, Optional, Union, Tuple
import torch
from tqdm import tqdm
from accelerate import Accelerator, PartialState

import logging
from dataset.image_video_dataset import TARGET_FPS_WAN
from common.logger import get_logger
from utils.train_utils import clean_memory_on_device, should_sample_images, load_prompts
from common.model_downloader import download_model_if_needed
from wan.modules.t5 import T5EncoderModel
from wan.modules.vae import WanVAE
from wan.modules.model import WanModel
from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler
from generation.sampling import save_images_grid, save_videos_grid
from torch.utils.tensorboard.writer import SummaryWriter

logger = get_logger(__name__, level=logging.INFO)


class SamplingManager:
    """Handles image/video sampling and inference operations."""

    def __init__(self, config: Dict[str, Any], default_guidance_scale: float = 1.0):
        self.config = config
        self.default_guidance_scale = default_guidance_scale
        self._vae_config = None  # Store VAE config for lazy loading

    def set_vae_config(self, vae_config: Dict[str, Any]) -> None:
        """Set VAE configuration for lazy loading."""
        self._vae_config = vae_config

    def _load_vae_lazy(self) -> Optional[WanVAE]:
        """Load VAE on-demand for sampling."""
        if self._vae_config is None:
            return None

        logger.info("Loading VAE on-demand for sampling...")
        from core.model_manager import ModelManager

        model_manager = ModelManager()

        vae = model_manager.load_vae(
            self._vae_config["args"],
            vae_dtype=self._vae_config["vae_dtype"],
            vae_path=self._vae_config["vae_path"],
        )
        vae.requires_grad_(False)
        vae.eval()

        return vae

    def _unload_vae(self, vae: Optional[WanVAE]) -> None:
        """Unload VAE from memory after use."""
        if vae is not None:
            logger.info("Unloading VAE from memory after sampling...")
            vae.to("cpu")
            del vae
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            import gc

            gc.collect()

    def sample_images(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        epoch: Optional[int],
        steps: int,
        vae: WanVAE,
        transformer: Any,
        sample_parameters: Optional[List[Dict[str, Any]]],
        dit_dtype: torch.dtype,
        dual_model_manager: Optional[Any] = None,
    ) -> None:
        """Architecture independent sample images generation."""
        if not should_sample_images(args, steps, epoch):
            return

        logger.info(f"üñºÔ∏è Generating sample images at step: {steps}")
        if sample_parameters is None:
            logger.warning("No sample parameters available, skipping sample generation")
            return

        # Handle lazy VAE loading if vae is None
        should_unload_vae = False
        if vae is None and self._vae_config is not None:
            vae = self._load_vae_lazy()
            should_unload_vae = True
            if vae is None:
                logger.warning("Failed to load VAE for sampling, skipping...")
                return

        distributed_state = (
            PartialState()
        )  # for multi gpu distributed inference. this is a singleton, so it's safe to use it here

        # Use the unwrapped model
        transformer = accelerator.unwrap_model(transformer)
        transformer.switch_block_swap_for_inference()

        # Safety check for output_dir
        if not args.output_dir or not args.output_dir.strip():
            logger.error(
                f"args.output_dir is empty or None: '{args.output_dir}'. Cannot save samples."
            )
            return

        save_dir = os.path.join(args.output_dir, "sample")
        logger.info(f"save_dir={save_dir}")
        os.makedirs(save_dir, exist_ok=True)

        # save random state to restore later
        rng_state = torch.get_rng_state()
        cuda_rng_state = None
        try:
            cuda_rng_state = (
                torch.cuda.get_rng_state() if torch.cuda.is_available() else None
            )
        except Exception:
            pass

        if distributed_state.num_processes <= 1:
            # If only one device is available, just use the original prompt list. We don't need to care about the distribution of prompts.
            with torch.no_grad(), accelerator.autocast():  # type: ignore[misc]
                # Create a single SummaryWriter instance for all videos to avoid file locking issues
                writer = None
                if hasattr(args, "logging_dir"):
                    try:
                        from tensorboardX import SummaryWriter

                        use_tensorboardx = True
                    except ImportError:
                        from torch.utils.tensorboard.writer import SummaryWriter

                        use_tensorboardx = False
                    writer = SummaryWriter(log_dir=args.logging_dir)

                for sample_parameter in sample_parameters:
                    self.sample_image_inference(
                        accelerator,
                        args,
                        transformer,
                        dit_dtype,
                        vae,
                        save_dir,
                        sample_parameter,
                        epoch,
                        steps,
                        writer=writer,
                        use_tensorboardx=use_tensorboardx if writer else False,
                        dual_model_manager=dual_model_manager,
                    )
                    clean_memory_on_device(accelerator.device)

                # Close the writer after all videos are logged
                if writer:
                    writer.close()
                    import gc

                    gc.collect()
        else:
            # Creating list with N elements, where each element is a list of prompt_dicts, and N is the number of processes available (number of devices available)
            # prompt_dicts are assigned to lists based on order of processes, to attempt to time the image creation time to match enum order. Probably only works when steps and sampler are identical.
            per_process_params = []  # list of lists
            for i in range(distributed_state.num_processes):
                per_process_params.append(
                    sample_parameters[i :: distributed_state.num_processes]
                )

            with torch.no_grad():
                # Create a single SummaryWriter instance for all videos to avoid file locking issues
                writer = None
                if hasattr(args, "logging_dir"):
                    try:
                        from tensorboardX import SummaryWriter

                        use_tensorboardx = True
                    except ImportError:
                        from torch.utils.tensorboard.writer import SummaryWriter

                        use_tensorboardx = False
                    writer = SummaryWriter(log_dir=args.logging_dir)

                with distributed_state.split_between_processes(
                    per_process_params
                ) as sample_parameter_lists:
                    for sample_parameter in sample_parameter_lists[0]:
                        self.sample_image_inference(
                            accelerator,
                            args,
                            transformer,
                            dit_dtype,
                            vae,
                            save_dir,
                            sample_parameter,  # type: ignore
                            epoch,
                            steps,
                            writer=writer,
                            use_tensorboardx=use_tensorboardx if writer else False,
                            dual_model_manager=dual_model_manager,
                        )
                        clean_memory_on_device(accelerator.device)

                # Close the writer after all videos are logged
                if writer:
                    writer.close()
                    import gc

                    gc.collect()

        torch.set_rng_state(rng_state)
        if cuda_rng_state is not None:
            torch.cuda.set_rng_state(cuda_rng_state)

        transformer.switch_block_swap_for_training()
        clean_memory_on_device(accelerator.device)

        # Unload VAE if it was loaded lazily
        if should_unload_vae:
            self._unload_vae(vae)

    def sample_image_inference(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        transformer: Any,
        dit_dtype: torch.dtype,
        vae: WanVAE,
        save_dir: str,
        sample_parameter: Dict[str, Any],
        epoch: Optional[int],
        steps: int,
        writer: Optional[Any] = None,
        use_tensorboardx: bool = False,
        dual_model_manager: Optional[Any] = None,
    ) -> None:
        """Architecture independent sample images inference. Logs generated video to TensorBoard if possible."""
        sample_steps = sample_parameter.get("sample_steps", 20)
        width = sample_parameter.get(
            "width", 256
        )  # make smaller for faster and memory saving inference
        height = sample_parameter.get("height", 256)
        frame_count = sample_parameter.get("frame_count", 1)
        guidance_scale = sample_parameter.get(
            "guidance_scale", self.default_guidance_scale
        )
        discrete_flow_shift = sample_parameter.get("discrete_flow_shift", 7)
        seed = sample_parameter.get("seed")
        prompt: str = sample_parameter.get("prompt", "")
        cfg_scale = sample_parameter.get(
            "cfg_scale", None
        )  # None for architecture default
        # If cfg_scale not explicitly provided, fall back to guidance_scale from prompt,
        # then to global args.guidance_scale if present
        if cfg_scale is None:
            if guidance_scale is not None:
                cfg_scale = guidance_scale
            elif hasattr(args, "guidance_scale"):
                cfg_scale = getattr(args, "guidance_scale", None)
        negative_prompt = sample_parameter.get("negative_prompt", None)

        # round width and height to multiples of 8
        width = (width // 8) * 8
        height = (height // 8) * 8

        frame_count = (frame_count - 1) // 4 * 4 + 1  # 1, 5, 9, 13, ...

        image_path = None
        control_video_path = None

        device = accelerator.device
        if seed is not None:
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            generator = torch.Generator(device=device).manual_seed(seed)
        else:
            # True random sample image generation
            torch.seed()
            torch.cuda.seed()
            generator = torch.Generator(device=device).manual_seed(torch.initial_seed())

        logger.info(f"üí¨ prompt: {prompt}")
        logger.info(f"üñºÔ∏è height: {height}")
        logger.info(f"üñºÔ∏è width: {width}")
        logger.info(f"üé• frame count: {frame_count}")
        logger.info(f"‚ö° sample steps: {sample_steps}")
        logger.info(f"üéØ guidance scale: {guidance_scale}")
        logger.info(f"üîÑ discrete flow shift: {discrete_flow_shift}")
        if seed is not None:
            logger.info(f"üé≤ seed: {seed}")

        do_classifier_free_guidance = False
        if negative_prompt is not None:
            do_classifier_free_guidance = True
            logger.info(f"üö´ negative prompt: {negative_prompt}")
            logger.info(f"‚öôÔ∏è cfg scale: {cfg_scale}")

        video = self.do_inference(
            accelerator,
            args,
            sample_parameter,
            vae,
            dit_dtype,
            transformer,
            discrete_flow_shift,
            sample_steps,
            width,
            height,
            frame_count,
            generator,
            do_classifier_free_guidance,
            guidance_scale,
            cfg_scale,
            image_path=image_path,
            control_video_path=control_video_path,
            dual_model_manager=dual_model_manager,
        )

        # Save video
        if video is None:
            logger.error("No video was generated for the given sample parameters.")
            return

        # --- TensorBoardX video logging ---

        try:
            if writer is not None:
                # Create a copy for TensorBoard logging to avoid modifying the original video tensor
                # TensorBoard expects (N, T, C, H, W) format
                video_to_log = (
                    video.permute(0, 2, 1, 3, 4).detach().cpu().float().clamp(0, 1)
                )

                # Make tag unique by including prompt index to avoid overwriting multiple videos at same step
                prompt_idx = sample_parameter.get("enum", 0)
                tag = f"generated/sample_video_step_{steps}_prompt_{prompt_idx:02d}"

                if use_tensorboardx:
                    # TensorBoardX supports better video logging
                    # Convert to uint8 format for better compatibility
                    video_to_log_uint8 = (
                        (video_to_log * 255).clamp(0, 255).to(torch.uint8)
                    )
                    writer.add_video(
                        tag, video_to_log_uint8, global_step=steps, fps=TARGET_FPS_WAN
                    )
                    logger.info(f"üé¨ Logged video to TensorBoardX: {tag}")
                else:
                    # Standard TensorBoard video logging
                    writer.add_video(
                        tag, video_to_log, global_step=steps, fps=TARGET_FPS_WAN
                    )
                    logger.info(f"üé¨ Logged video to TensorBoard: {tag}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to log video to TensorBoard: {e}")

        ts_str = time.strftime("%Y%m%d%H%M%S", time.localtime())
        num_suffix = f"e{epoch:06d}" if epoch is not None else f"{steps:06d}"
        seed_suffix = "" if seed is None else f"_{seed}"
        prompt_idx = sample_parameter.get("enum", 0)

        # Safety check for output_name
        output_name_prefix = ""
        if args.output_name is not None and args.output_name.strip():
            output_name_prefix = args.output_name + "_"
        else:
            logger.warning(f"args.output_name is None or empty: '{args.output_name}'")

        save_path = (
            f"{output_name_prefix}{num_suffix}_{prompt_idx:02d}_{ts_str}{seed_suffix}"
        )

        logger.info(f"save_path={save_path}")
        logger.info(f"video.shape={video.shape}")
        logger.info(f"save_dir={save_dir}")

        if video.shape[2] == 1:
            logger.info(f"Saving as images grid")
            save_images_grid(video, save_dir, save_path, create_subdir=False)
        else:
            logger.info(
                f"Saving as video grid to {os.path.join(save_dir, save_path) + '.mp4'}"
            )
            save_videos_grid(video, os.path.join(save_dir, save_path) + ".mp4")

        # Move models back to initial state
        vae.to("cpu")
        clean_memory_on_device(device)

    def process_sample_prompts(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        sample_prompts: Union[str, List[Dict[str, Any]]],
    ) -> Optional[List[Dict[str, Any]]]:
        """Process sample prompts and prepare embeddings."""
        config = self.config
        device = accelerator.device
        t5_path, clip_path, fp8_t5 = args.t5, args.clip, args.fp8_t5

        logger.info(f"cache Text Encoder outputs for sample prompt: {sample_prompts}")
        logger.info(f"sample_prompts type = {type(sample_prompts)}")
        logger.info(f"sample_prompts value = {sample_prompts}")

        # Handle both file paths and direct lists of prompt dictionaries
        if isinstance(sample_prompts, str):
            # If it's a file path, load prompts from file
            if not sample_prompts.strip():
                logger.warning(
                    "Empty sample_prompts string provided, skipping sample generation"
                )
                return None
            logger.info(f"Loading prompts from file: {sample_prompts}")
            prompts = load_prompts(sample_prompts)
        elif isinstance(sample_prompts, list):
            # If it's already a list of prompt dictionaries, use it directly
            logger.info(f"Using direct list of prompts, count: {len(sample_prompts)}")
            prompts = []
            for i, prompt_dict in enumerate(sample_prompts):
                # Convert the new structure to the expected format
                converted_dict = {}
                if "text" in prompt_dict:
                    converted_dict["prompt"] = prompt_dict["text"]
                if "width" in prompt_dict:
                    converted_dict["width"] = prompt_dict["width"]
                if "height" in prompt_dict:
                    converted_dict["height"] = prompt_dict["height"]
                if "frames" in prompt_dict:
                    converted_dict["frame_count"] = prompt_dict["frames"]
                if "seed" in prompt_dict:
                    converted_dict["seed"] = prompt_dict["seed"]
                if "step" in prompt_dict:
                    converted_dict["sample_steps"] = prompt_dict["step"]
                if "control_path" in prompt_dict:
                    converted_dict["control_path"] = prompt_dict["control_path"]
                if "control_video_path" in prompt_dict:
                    converted_dict["control_path"] = prompt_dict["control_video_path"]
                # Guidance / CFG controls
                if "guidance_scale" in prompt_dict:
                    converted_dict["guidance_scale"] = prompt_dict["guidance_scale"]
                if "cfg_scale" in prompt_dict:
                    converted_dict["cfg_scale"] = prompt_dict["cfg_scale"]
                # Add other fields as needed
                converted_dict["enum"] = i
                prompts.append(converted_dict)
        else:
            raise ValueError(
                f"sample_prompts must be a string (file path) or list, got {type(sample_prompts)}"
            )

        if not prompts:
            logger.warning("No prompts found, skipping sample generation")
            return None

        logger.info(f"Processed {len(prompts)} prompts")

        def encode_for_text_encoder(text_encoder):
            sample_prompts_te_outputs = {}  # (prompt) -> (embeds, mask)
            # with accelerator.autocast(), torch.no_grad(): # this causes NaN if dit_dtype is fp16
            t5_dtype = config.t5_dtype  # type: ignore
            with (
                torch.autocast(device_type=device.type, dtype=t5_dtype),
                torch.no_grad(),
            ):
                for prompt_dict in prompts:
                    if "negative_prompt" not in prompt_dict:
                        prompt_dict["negative_prompt"] = self.config[
                            "sample_neg_prompt"
                        ]
                    for p in [
                        prompt_dict.get("prompt", ""),
                        prompt_dict.get("negative_prompt", None),
                    ]:
                        if p is None:
                            continue
                        if p not in sample_prompts_te_outputs:
                            logger.info(f"cache Text Encoder outputs for prompt: {p}")

                            prompt_outputs = text_encoder([p], device)
                            sample_prompts_te_outputs[p] = prompt_outputs

            return sample_prompts_te_outputs

        # Download T5 model if it's a URL
        if t5_path.startswith(("http://", "https://")):
            logger.info(f"Detected URL for T5 model, downloading: {t5_path}")
            cache_dir = getattr(args, "model_cache_dir", None)
            t5_path = download_model_if_needed(t5_path, cache_dir=cache_dir)
            logger.info(f"Downloaded T5 model to: {t5_path}")

        # Load Text Encoder 1 and encode
        logger.info(f"loading T5: {t5_path}")
        t5 = T5EncoderModel(
            text_len=config.text_len,  # type: ignore
            dtype=config.t5_dtype,  # type: ignore
            device=device,
            weight_path=t5_path,
            fp8=fp8_t5,
        )

        logger.info("encoding with Text Encoder 1")
        te_outputs_1 = encode_for_text_encoder(t5)
        del t5

        # prepare sample parameters
        sample_parameters = []
        for prompt_dict in prompts:
            prompt_dict_copy = prompt_dict.copy()

            p = prompt_dict.get("prompt", "")
            prompt_dict_copy["t5_embeds"] = te_outputs_1[p][0]

            p = prompt_dict.get("negative_prompt", None)
            if p is not None:
                prompt_dict_copy["negative_t5_embeds"] = te_outputs_1[p][0]

            sample_parameters.append(prompt_dict_copy)

        clean_memory_on_device(accelerator.device)

        return sample_parameters

    def do_inference(
        self,
        accelerator: Accelerator,
        args: argparse.Namespace,
        sample_parameter: Dict[str, Any],
        vae: WanVAE,
        dit_dtype: torch.dtype,
        transformer: WanModel,
        discrete_flow_shift: float,
        sample_steps: int,
        width: int,
        height: int,
        frame_count: int,
        generator: torch.Generator,
        do_classifier_free_guidance: bool,
        guidance_scale: float,
        cfg_scale: Optional[float],
        image_path: Optional[str] = None,
        control_video_path: Optional[str] = None,
        dual_model_manager: Optional[Any] = None,
    ) -> Optional[torch.Tensor]:
        model: WanModel = transformer
        device = accelerator.device
        if cfg_scale is None:
            cfg_scale = 5.0
        do_classifier_free_guidance = do_classifier_free_guidance and cfg_scale != 1.0

        # Ensure model is properly modified for control LoRA during inference
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            # Prefer centralized patching via ControlSignalProcessor if available
            try:
                from trainer.control_signal_processor import ControlSignalProcessor as _CSP  # type: ignore

                _CSP().modify_model_for_control_lora(model, args)
            except Exception:
                # Fallback to local implementation
                if not getattr(model, "_control_lora_patched", False):
                    logger.info("Modifying model for control LoRA during inference...")
                    self._modify_model_for_control_lora(model, args)

            # Validate patch embedding channel count
            if hasattr(model, "patch_embedding") and hasattr(
                model.patch_embedding, "in_channels"
            ):
                expected_channels = getattr(
                    model, "in_dim", model.patch_embedding.in_channels
                )
                actual_channels = model.patch_embedding.in_channels
                assert actual_channels == expected_channels, (
                    f"Patch embedding in_channels={actual_channels} but model.in_dim={expected_channels}. "
                    "Control LoRA patching may have been skipped or applied multiple times."
                )

        # Calculate latent video length based on VAE version
        latent_video_length = (frame_count - 1) // self.config["vae_stride"][0] + 1

        # Get embeddings
        context = sample_parameter["t5_embeds"].to(device=device)
        if do_classifier_free_guidance:
            context_null = sample_parameter["negative_t5_embeds"].to(device=device)
        else:
            context_null = None

        num_channels_latents = 16  # model.in_dim
        vae_scale_factor = self.config["vae_stride"][1]

        # Initialize latents
        lat_h = height // vae_scale_factor
        lat_w = width // vae_scale_factor
        shape_or_frame = (1, num_channels_latents, 1, lat_h, lat_w)
        latents = []
        for _ in range(latent_video_length):
            latents.append(
                torch.randn(
                    shape_or_frame,
                    generator=generator,
                    device=device,
                    dtype=torch.float32,
                )
            )
        latents = torch.cat(latents, dim=2)

        image_latents = None

        # Check if control LoRA is enabled and needs control signal
        control_latents = None
        extracted_properties = None
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            control_latents, extracted_properties = (
                self._generate_control_latents_for_inference(
                    args, vae, device, width, height, frame_count, sample_parameter
                )
            )

            # Use extracted properties to override parameters if available
            if (
                extracted_properties
                and extracted_properties["source"] != "fallback_zero_signal"
            ):
                # Only override frame_count if not explicitly specified in sample_parameter
                if "frame_count" not in sample_parameter or sample_parameter.get(
                    "auto_extract_from_control", True
                ):
                    logger.info(
                        f"üéØ Using frame count from control signal: {extracted_properties['frame_count']} (was {frame_count})"
                    )
                    frame_count = extracted_properties["frame_count"]
                    # Recalculate latent dimensions with new frame count
                    latent_video_length = (frame_count - 1) // self.config[
                        "vae_stride"
                    ][0] + 1

                # Override resolution if not explicitly specified in sample_parameter
                if "width" not in sample_parameter or sample_parameter.get(
                    "auto_extract_from_control", True
                ):
                    logger.info(
                        f"üéØ Using width from control signal: {extracted_properties['width']} (was {width})"
                    )
                    width = extracted_properties["width"]

                if "height" not in sample_parameter or sample_parameter.get(
                    "auto_extract_from_control", True
                ):
                    logger.info(
                        f"üéØ Using height from control signal: {extracted_properties['height']} (was {height})"
                    )
                    height = extracted_properties["height"]

                # Recalculate latent dimensions with new resolution
                vae_scale_factor = self.config["vae_stride"][1]
                lat_h = height // vae_scale_factor
                lat_w = width // vae_scale_factor

        # use the default value for num_train_timesteps (1000)
        scheduler = FlowUniPCMultistepScheduler(shift=1, use_dynamic_shifting=False)
        scheduler.set_timesteps(sample_steps, device=device, shift=discrete_flow_shift)
        timesteps = scheduler.timesteps

        # Generate noise for the required number of frames only (keep on device)
        noise = torch.randn(
            16,
            latent_video_length,
            lat_h,
            lat_w,
            dtype=torch.float32,
            generator=generator,
            device=device,
        )

        # prepare the model input
        # Align seq_len computation with training (divide by full 3D patch volume)
        max_seq_len = (
            latent_video_length
            * lat_h
            * lat_w
            // (
                self.config["patch_size"][0]
                * self.config["patch_size"][1]
                * self.config["patch_size"][2]
            )  # type: ignore
        )
        arg_c = {"context": [context], "seq_len": max_seq_len}
        arg_null = {"context": [context_null], "seq_len": max_seq_len}

        # Wrap the inner loop with tqdm to track progress over timesteps
        prompt_idx = sample_parameter.get("enum", 0)
        latent = noise
        # Prepare timestep boundary (normalize int 0..1000 to float 0..1 if needed)
        boundary_cfg = getattr(args, "timestep_boundary", 875)
        boundary = float(boundary_cfg)
        if boundary > 1.0:
            boundary = boundary / 1000.0

        with torch.no_grad():
            for i, t in enumerate(
                tqdm(timesteps, desc=f"üé• Sampling timesteps for prompt {prompt_idx+1}")
            ):
                # Dual-mode inference: swap base weights if crossing boundary
                if dual_model_manager is not None:
                    # Align with reference normalization: t/1000.0
                    t_norm = float((t.item()) / 1000.0)
                    try:
                        dual_model_manager.next_model_is_high_noise = t_norm >= boundary
                        dual_model_manager.swap_if_needed(accelerator)
                        model = (
                            dual_model_manager.active_model
                        )  # keep local reference fresh
                    except Exception as _inf_swap_err:
                        logger.debug(
                            f"Dual swap during inference skipped: {_inf_swap_err}"
                        )
                # Prepare model input - concatenate control latents if available
                if hasattr(args, "enable_control_lora") and args.enable_control_lora:
                    # Always concatenate along the channel dimension to match training
                    channel_dim = 0  # CFHW format at sampling time
                    if control_latents is not None:
                        # Debug logging
                        logger.debug(f"Latent shape: {latent.shape}")
                        logger.debug(f"Control latents shape: {control_latents.shape}")
                        # Ensure device/dtype alignment
                        cat_latent = torch.cat(
                            [
                                latent.to(device=device),
                                control_latents.to(device=device, dtype=latent.dtype),
                            ],
                            dim=channel_dim,
                        )
                        latent_model_input = [cat_latent]
                        logger.debug(
                            f"Concatenated input shape: {latent_model_input[0].shape}"
                        )
                    else:
                        # Fallback: use current latent as control signal (matches training fallback)
                        logger.warning(
                            "Control LoRA enabled but no control signal available, using latent clone as fallback"
                        )
                        fallback_control = latent.detach().clone()
                        cat_latent = torch.cat(
                            [
                                latent.to(device=device),
                                fallback_control.to(device=device),
                            ],
                            dim=channel_dim,
                        )
                        latent_model_input = [cat_latent]
                        logger.debug(
                            f"Concatenated input shape (fallback clone): {latent_model_input[0].shape}"
                        )
                else:
                    # Control LoRA not enabled - use original latent
                    latent_model_input = [latent.to(device=device)]
                timestep = t.unsqueeze(0)

                with accelerator.autocast():
                    noise_pred_cond = model(latent_model_input, t=timestep, **arg_c)[0]
                    if do_classifier_free_guidance:
                        noise_pred_uncond = model(
                            latent_model_input, t=timestep, **arg_null
                        )[0]
                    else:
                        noise_pred_uncond = None

                if do_classifier_free_guidance:
                    noise_pred = noise_pred_uncond + cfg_scale * (
                        noise_pred_cond - noise_pred_uncond
                    )
                else:
                    noise_pred = noise_pred_cond

                temp_x0 = scheduler.step(
                    noise_pred.unsqueeze(0),
                    t,
                    latent.unsqueeze(0),
                    return_dict=False,
                    generator=generator,
                )[0]
                latent = temp_x0.squeeze(0)

        # Move VAE to the appropriate device for sampling
        vae.to(device)
        vae.eval()

        # Decode latents to video
        logger.info(f"Decoding video from latents: {latent.shape}")
        latent = latent.unsqueeze(0)  # add batch dim
        latent = latent.to(device=device)

        with torch.autocast(device_type=device.type, dtype=vae.dtype), torch.no_grad():
            video = vae.decode(latent)[0]  # vae returns list
        video = video.unsqueeze(0)  # add batch dim
        del latent

        logger.info(f"Decoding complete")
        video = video.to(torch.float32).cpu()
        video = (video / 2 + 0.5).clamp(0, 1)  # -1 to 1 -> 0 to 1

        vae.to("cpu")
        clean_memory_on_device(device)

        return video

    def _generate_control_latents_for_inference(
        self,
        args: argparse.Namespace,
        vae: WanVAE,
        device: torch.device,
        width: int,
        height: int,
        frame_count: int,
        sample_parameter: Dict[str, Any],
    ) -> Tuple[Optional[torch.Tensor], Dict[str, Any]]:
        """
        Generate control latents for inference, similar to the reference implementation.
        Returns both control latents and extracted video properties.
        """
        import os
        from torchvision.transforms import v2

        # Check if control video/image path is specified in sample parameter
        control_path = sample_parameter.get("control_path", None)
        logger.info(f"Sample parameter keys: {list(sample_parameter.keys())}")
        logger.info(f"Control path from sample parameter: {control_path}")

        if control_path is None:
            # Try to get from args
            control_path = getattr(args, "control_video_path", None)
            logger.info(f"Control path from args: {control_path}")

        if control_path is None:
            logger.warning(
                "No control path specified for control LoRA inference; will fall back to latent clone during sampling"
            )
            # Return None to trigger per-step latent-clone fallback, keep properties for logging
            extracted_properties = {
                "frame_count": frame_count,
                "width": width,
                "height": height,
                "source": "fallback_zero_signal",
            }
            return None, extracted_properties

        # Load control video/image
        if control_path.lower().endswith((".mp4", ".avi", ".mov", ".mkv")):
            # Load video
            try:
                import decord

                decord.bridge.set_bridge("torch")

                # Check if file exists first
                logger.info(f"Checking if control video file exists: {control_path}")
                if not os.path.exists(control_path):
                    logger.error(f"Control video file not found: {control_path}")
                    logger.error(
                        "Please check that the path is correct and the file exists"
                    )
                    raise FileNotFoundError(
                        f"Control video file not found: {control_path}"
                    )
                else:
                    logger.info(f"Control video file found: {control_path}")

                logger.info(f"Loading control video from: {control_path}")
                vr = decord.VideoReader(control_path)
                logger.info(
                    f"Video loaded successfully. Total frames: {len(vr)}, requesting: {frame_count}"
                )

                control_pixels = vr[:frame_count]
                control_pixels = control_pixels.movedim(3, 1).unsqueeze(
                    0
                )  # FHWC -> FCHW -> BFCHW
                logger.info(
                    f"Control pixels shape after loading: {control_pixels.shape}"
                )

            except ImportError:
                logger.error(
                    "decord not available for video loading, using PIL for first frame"
                )
                from PIL import Image
                import torchvision.transforms.functional as TF

                # Fallback to using first frame repeated
                img = Image.open(control_path).convert("RGB")
                control_pixels = (
                    TF.to_tensor(img).unsqueeze(0).unsqueeze(0)
                )  # CHW -> BFCHW
                control_pixels = control_pixels.repeat(
                    1, frame_count, 1, 1, 1
                )  # Repeat frame
            except FileNotFoundError as e:
                logger.error(f"Control video file not found: {e}")
                logger.error("Falling back to zero control signal")
                # Return zero control signal as fallback
                return self._create_zero_control_signal_fallback(
                    width, height, frame_count, device
                )
            except Exception as e:
                logger.error(f"Error loading control video '{control_path}': {e}")
                logger.error(f"Error type: {type(e).__name__}")
                logger.error("Falling back to zero control signal")
                # Return zero control signal as fallback
                return self._create_zero_control_signal_fallback(
                    width, height, frame_count, device
                )
        else:
            # Load image and repeat for video
            try:
                from PIL import Image
                import torchvision.transforms.functional as TF

                # Check if file exists first
                if not os.path.exists(control_path):
                    logger.error(f"Control image file not found: {control_path}")
                    logger.error(
                        "Please check that the path is correct and the file exists"
                    )
                    raise FileNotFoundError(
                        f"Control image file not found: {control_path}"
                    )

                logger.info(f"Loading control image from: {control_path}")
                img = Image.open(control_path).convert("RGB")
                control_pixels = (
                    TF.to_tensor(img).unsqueeze(0).unsqueeze(0)
                )  # CHW -> BFCHW
                control_pixels = control_pixels.repeat(
                    1, frame_count, 1, 1, 1
                )  # Repeat frame
                logger.info(
                    f"Control pixels shape after loading: {control_pixels.shape}"
                )

            except FileNotFoundError as e:
                logger.error(f"Control image file not found: {e}")
                logger.error("Falling back to zero control signal")
                # Return zero control signal as fallback
                return self._create_zero_control_signal_fallback(
                    width, height, frame_count, device
                )
            except Exception as e:
                logger.error(f"Error loading control image '{control_path}': {e}")
                logger.error(f"Error type: {type(e).__name__}")
                logger.error("Falling back to zero control signal")
                # Return zero control signal as fallback
                return self._create_zero_control_signal_fallback(
                    width, height, frame_count, device
                )

        # Apply preprocessing similar to reference implementation
        control_lora_type = getattr(args, "control_lora_type", "tile")
        control_preprocessing = getattr(args, "control_preprocessing", "blur")

        if control_lora_type == "tile" and control_preprocessing == "blur":
            transform = v2.Compose(
                [
                    v2.ToDtype(torch.float32, scale=True),
                    v2.Resize(size=(height // 4, width // 4)),
                    v2.Resize(size=(height, width)),
                    v2.GaussianBlur(
                        kernel_size=getattr(args, "control_blur_kernel_size", 15),
                        sigma=getattr(args, "control_blur_sigma", 3.0),
                    ),  # Changed from sigma=4 to sigma=3 to match reference
                ]
            )

            control_pixels = transform(control_pixels) * 2 - 1  # Scale to [-1, 1]
            control_pixels = torch.clamp(
                torch.nan_to_num(control_pixels), min=-1, max=1
            )
        else:
            # Default preprocessing: just scale to [-1, 1]
            control_pixels = control_pixels * 2 - 1
            control_pixels = torch.clamp(control_pixels, min=-1, max=1)

        # Convert to CFHW format like in reference implementation
        control_pixels = control_pixels[0].movedim(0, 1)  # BFCHW -> CFHW

        # Encode with VAE
        vae.to(device)
        try:
            with torch.no_grad():
                with torch.autocast(device_type=device.type, dtype=vae.dtype):
                    control_latents = vae.encode(
                        [control_pixels.to(dtype=vae.dtype, device=device)]
                    )[0]
                    control_latents = control_latents.to(device)
        finally:
            vae.to("cpu")
            clean_memory_on_device(device)

        # Extract properties from control video/image
        actual_frame_count = (
            control_pixels.shape[1] if control_pixels.dim() == 4 else frame_count
        )
        actual_height, actual_width = control_pixels.shape[-2:]

        extracted_properties = {
            "frame_count": actual_frame_count,
            "width": actual_width,
            "height": actual_height,
            "source": (
                "control_video"
                if control_path.lower().endswith((".mp4", ".avi", ".mov", ".mkv"))
                else "control_image"
            ),
        }

        logger.info(f"Generated control latents with shape: {control_latents.shape}")
        logger.info(f"Extracted properties from control signal: {extracted_properties}")
        return control_latents, extracted_properties

    def _create_zero_control_signal_fallback(
        self, width: int, height: int, frame_count: int, device: torch.device
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """Create zero control signal as fallback when control signal loading fails."""
        latent_height = height // self.config["vae_stride"][1]
        latent_width = width // self.config["vae_stride"][1]
        latent_frames = (frame_count - 1) // self.config["vae_stride"][0] + 1

        extracted_properties = {
            "frame_count": frame_count,
            "width": width,
            "height": height,
            "source": "fallback_zero_signal",
        }

        # For control LoRA, we need to return a tensor that matches the expected input channels
        # The model expects 32 channels (16 original + 16 control), so we need to provide 16 control channels
        control_channels = 16  # This matches the original latent channels

        return (
            torch.zeros(
                control_channels,
                latent_frames,
                latent_height,
                latent_width,
                dtype=torch.float32,
                device=device,
            ),
            extracted_properties,
        )

    def _modify_model_for_control_lora(
        self, transformer: Any, args: argparse.Namespace
    ) -> None:
        """
        Modify the model's patch embedding layer to accept additional channels for control LoRA.
        This aligns with the reference implementation.
        """
        # Re-entrancy guard ‚Äì return early if already patched
        if getattr(transformer, "_control_lora_patched", False):
            logger.debug("Control LoRA patch already applied ‚Äì skipping.")
            return

        if hasattr(transformer, "patch_embedding"):
            with torch.no_grad():
                in_cls = transformer.patch_embedding.__class__  # nn.Conv3d
                old_in_dim = transformer.in_dim  # 16
                new_in_dim = old_in_dim * 2  # Double the input channels

                new_in = in_cls(
                    in_channels=new_in_dim,
                    out_channels=transformer.patch_embedding.out_channels,
                    kernel_size=transformer.patch_embedding.kernel_size,
                    stride=transformer.patch_embedding.stride,
                    padding=transformer.patch_embedding.padding,
                ).to(
                    device=transformer.patch_embedding.weight.device,
                    dtype=transformer.patch_embedding.weight.dtype,
                )

                new_in.weight.zero_()
                # Copy original weights to first half of new weights
                new_in.weight[:, :old_in_dim, :, :, :] = (
                    transformer.patch_embedding.weight
                )
                # Copy original bias so the behaviour matches the reference implementation
                if transformer.patch_embedding.bias is not None:
                    new_in.bias.copy_(transformer.patch_embedding.bias)

                # Replace the original patch embedding
                transformer.patch_embedding = new_in
                transformer.in_dim = new_in_dim

                # Update HuggingFace config so that any model save/load cycle retains the new input channel size
                if hasattr(transformer, "register_to_config"):
                    # WanModel may inherit from ConfigMixin in some versions
                    transformer.register_to_config(in_dim=new_in_dim)

                logger.info(
                    f"‚úÖ Modified model for control LoRA: input channels {old_in_dim} -> {new_in_dim}"
                )

                # Ensure gradients are enabled for the new patch_embedding so it can learn
                transformer.patch_embedding.requires_grad_(True)

                # mark patched
                transformer._control_lora_patched = True
</file>

<file path="core/training_core.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Core training logic for WAN network trainer.

This module handles the main training loop, model calling, loss computation, and validation.
Extracted from wan_network_trainer.py to improve code organization and maintainability.
"""

import argparse
import math
import numpy as np
import time
from multiprocessing import Value
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import torch
from tqdm import tqdm
import accelerate
from accelerate import Accelerator, PartialState
import torch.nn.functional as F

import utils.fluxflow_augmentation as fluxflow_augmentation
import scheduling.fvdm as fvdm

from modules.scheduling_flow_match_discrete import FlowMatchDiscreteScheduler
from utils.train_utils import (
    compute_loss_weighting_for_sd3,
    get_sigmas,
    clean_memory_on_device,
    should_sample_images,
    LossRecorder,
)
from scheduling.timestep_distribution import (
    TimestepDistribution,
    should_use_precomputed_timesteps,
)
from scheduling.timestep_logging import (
    log_initial_timestep_distribution,
    log_live_timestep_distribution,
    log_loss_scatterplot,
    log_show_timesteps_figure_unconditional,
)
from scheduling.timestep_utils import (
    get_noisy_model_input_and_timesteps,
    initialize_timestep_distribution,
    time_shift,
    get_lin_function,
)
from core.validation_core import ValidationCore
from criteria.dispersive_loss import dispersive_loss_info_nce
from criteria.training_loss import TrainingLossComputer, LossComponents


from scheduling.fopp import (
    FoPPScheduler,
    get_alpha_bar_schedule,
    apply_asynchronous_noise,
)

# Enhanced optimizer logging
from optimizers.enhanced_logging import (
    get_enhanced_metrics,
    get_histogram_data,
    is_supported,
)

# Performance logging
from common.performance_logger import (
    start_step_timing,
    start_forward_pass_timing,
    end_forward_pass_timing,
    start_backward_pass_timing,
    end_backward_pass_timing,
    start_optimizer_step_timing,
    end_optimizer_step_timing,
    end_step_timing,
    get_timing_metrics,
    get_model_statistics,
    get_hardware_metrics,
    log_performance_summary,
)

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class TrainingCore:
    """Handles core training logic, model calling, and validation."""

    def __init__(self, config: Any, fluxflow_config: Dict[str, Any]):
        self.config = config
        self.fluxflow_config = fluxflow_config

        # Initialize validation core
        self.validation_core = ValidationCore(config, fluxflow_config)

        # EMA loss tracking for smoother TensorBoard visualization
        self.ema_loss: Optional[float] = None
        self.ema_beta: float = 0.98  # Smoothing factor (0.9-0.99 are good choices)
        self.ema_step_count: int = 0

        # Parameter statistics tracking
        self.last_param_log_step: int = -1

        # Per-source loss tracking
        self.per_source_losses: Dict[str, List[float]] = {}

        # Pre-computed timestep distribution (initialized when needed)
        self.timestep_distribution = TimestepDistribution()
        self._timestep_logging_initialized: bool = False

        # Gradient norm tracking
        self.gradient_norms: List[float] = []

        # TensorBoardX enhanced logging
        self.tensorboardx_writer = None
        self.use_tensorboardx = False

        # Centralized loss computation
        self.loss_computer = TrainingLossComputer(self.config)

    def set_ema_beta(self, beta: float) -> None:
        """Set the EMA smoothing factor. Higher values (closer to 1.0) = more smoothing."""
        if not 0.0 < beta < 1.0:
            raise ValueError("EMA beta must be between 0.0 and 1.0")
        self.ema_beta = beta
        logger.info(f"EMA beta set to {beta}")

    def configure_advanced_logging(self, args: argparse.Namespace) -> None:
        """Configure advanced logging settings including parameter stats, per-source losses, and gradient norms.

        Sets default values for various logging options if not already set in args.
        """
        # Parameter statistics logging
        if not hasattr(args, "log_param_stats"):
            args.log_param_stats = False  # Disabled by default
        if not hasattr(args, "param_stats_every_n_steps"):
            args.param_stats_every_n_steps = 100  # Log every 100 steps
        if not hasattr(args, "max_param_stats_logged"):
            args.max_param_stats_logged = 20  # Log top 20 parameters by norm

        # Per-source loss logging
        if not hasattr(args, "log_per_source_loss"):
            args.log_per_source_loss = False  # Disabled by default

        # Gradient norm logging
        if not hasattr(args, "log_gradient_norm"):
            args.log_gradient_norm = False  # Disabled by default

        # Extra train metrics (periodic)
        if not hasattr(args, "log_extra_train_metrics"):
            args.log_extra_train_metrics = True  # Enabled by default
        if not hasattr(args, "train_metrics_interval"):
            args.train_metrics_interval = 50  # Log every 50 steps by default

        # Report enabled features
        enabled_features = []

        if args.log_param_stats:
            enabled_features.append("Parameter Statistics")
            logger.info(f"Parameter statistics logging enabled:")
            logger.info(f"  - Logging every {args.param_stats_every_n_steps} steps")
            logger.info(
                f"  - Tracking top {args.max_param_stats_logged} parameters by norm"
            )
            logger.info(
                f"  - Will create TensorBoard metrics: param_norm/*, grad_norm/*, param_stats/*"
            )

        if args.log_per_source_loss:
            enabled_features.append("Per-Source Loss")
            logger.info("Per-source loss logging enabled:")
            logger.info("  - Will attempt to detect video vs image sources")
            logger.info("  - Will create TensorBoard metrics: loss/video, loss/image")

        if args.log_gradient_norm:
            enabled_features.append("Gradient Norm")
            logger.info("Gradient norm logging enabled:")
            logger.info("  - Will create TensorBoard metric: grad_norm")

        if enabled_features:
            logger.info(
                f"Advanced logging features enabled: {', '.join(enabled_features)}"
            )
        else:
            logger.info(
                "No advanced logging features enabled (use configure_advanced_logging to enable)"
            )

    def update_ema_loss(self, current_loss: float) -> float:
        """Update EMA loss and return bias-corrected value."""
        self.ema_step_count += 1

        if self.ema_loss is None:
            self.ema_loss = current_loss
        else:
            self.ema_loss = (
                self.ema_beta * self.ema_loss + (1 - self.ema_beta) * current_loss
            )

        # Bias correction for early steps
        corrected_ema = self.ema_loss / (1 - self.ema_beta**self.ema_step_count)
        return corrected_ema

    # Metric helpers moved to trainer.metrics
    # kept methods as thin wrappers for backward compatibility
    def generate_parameter_stats(
        self,
        model: Any,
        global_step: int,
        log_every_n_steps: int = 100,
        max_params_to_log: int = 20,
    ) -> Dict[str, float]:
        from core.metrics import generate_parameter_stats as _gps

        # gate by local last_param_log_step to keep same behavior
        if global_step - self.last_param_log_step < log_every_n_steps:
            return {}
        self.last_param_log_step = global_step
        return _gps(model, global_step, log_every_n_steps, max_params_to_log)

    def compute_per_source_loss(
        self,
        model_pred: torch.Tensor,
        target: torch.Tensor,
        batch: Dict[str, Any],
        weighting: Optional[torch.Tensor] = None,
        sample_weights: Optional[torch.Tensor] = None,
    ) -> Dict[str, float]:
        from core.metrics import compute_per_source_loss as _cpsl

        return _cpsl(model_pred, target, batch, weighting, sample_weights)

    def compute_gradient_norm(
        self, model: Any, max_norm: Optional[float] = None, norm_type: float = 2.0
    ) -> float:
        from core.metrics import compute_gradient_norm as _cgn

        return _cgn(model, max_norm, norm_type)

    def generate_step_logs(
        self,
        args: argparse.Namespace,
        current_loss: float,
        avr_loss: float,
        lr_scheduler: Any,
        lr_descriptions: List[str],
        optimizer: Optional[torch.optim.Optimizer] = None,
        keys_scaled: Optional[int] = None,
        mean_norm: Optional[float] = None,
        maximum_norm: Optional[float] = None,
        ema_loss: Optional[float] = None,
        model: Optional[Any] = None,
        global_step: Optional[int] = None,
        per_source_losses: Optional[Dict[str, float]] = None,
        gradient_norm: Optional[float] = None,
    ) -> Dict[str, Any]:
        from core.metrics import generate_step_logs as _gsl

        # Include max-norm details if available
        logs = _gsl(
            args,
            current_loss,
            avr_loss,
            lr_scheduler,
            lr_descriptions,
            optimizer,
            keys_scaled,
            mean_norm,
            maximum_norm,
            ema_loss,
            model,
            global_step,
            per_source_losses,
            gradient_norm,
        )
        return logs

    def generate_safe_progress_metrics(
        self,
        args: argparse.Namespace,
        current_loss: float,
        avr_loss: float,
        lr_scheduler: Any,
        epoch: int,
        global_step: int,
        keys_scaled: Optional[int] = None,
        mean_norm: Optional[float] = None,
        maximum_norm: Optional[float] = None,
        current_step_in_epoch: Optional[int] = None,
        total_steps_in_epoch: Optional[int] = None,
    ) -> Dict[str, Any]:
        from core.metrics import generate_safe_progress_metrics as _gspm

        return _gspm(
            args,
            current_loss,
            avr_loss,
            lr_scheduler,
            epoch,
            global_step,
            keys_scaled,
            mean_norm,
            maximum_norm,
            current_step_in_epoch,
            total_steps_in_epoch,
        )

    def scale_shift_latents(self, latents: torch.Tensor) -> torch.Tensor:
        """Scale and shift latents if needed."""
        return latents

    def call_dit(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        transformer: Any,
        latents: torch.Tensor,
        batch: Dict[str, torch.Tensor],
        noise: torch.Tensor,
        noisy_model_input: torch.Tensor,
        timesteps: torch.Tensor,
        network_dtype: torch.dtype,
        control_signal_processor: Optional[Any] = None,
        controlnet: Optional[Any] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """Call the DiT model and compute target for loss calculation."""
        model = transformer

        current_logging_level = getattr(args, "logging_level", "INFO").upper()
        perturbed_latents_for_target = latents.clone()  # Start with a clone
        if self.fluxflow_config.get("enable_fluxflow", False):
            frame_dim = self.fluxflow_config.get("frame_dim_in_batch", 2)
            if latents.ndim > frame_dim and latents.shape[frame_dim] > 1:
                # Always log FluxFlow application at INFO level so user can see it
                if accelerator.is_main_process:
                    logger.info(
                        f"FLUXFLOW: Applying temporal augmentation - mode={self.fluxflow_config.get('mode','frame')}, "
                        f"latents shape={latents.shape}, frame_dim={frame_dim}, "
                        f"num_frames={latents.shape[frame_dim]}"
                    )

                perturbed_latents_for_target = (
                    fluxflow_augmentation.apply_fluxflow_to_batch(
                        perturbed_latents_for_target, self.fluxflow_config
                    )
                )

                # Log if perturbation was successful
                if accelerator.is_main_process:
                    perturbation_applied = not torch.equal(
                        latents, perturbed_latents_for_target
                    )
                    logger.info(
                        f"FLUXFLOW: Temporal perturbation applied: {perturbation_applied}"
                    )

            else:
                # Always log why FluxFlow is being skipped
                if accelerator.is_main_process:
                    logger.info(
                        f"FLUXFLOW: Skipping temporal augmentation - latents shape={latents.shape}, "
                        f"frame_dim={frame_dim}, num_frames={latents.shape[frame_dim] if latents.ndim > frame_dim else 'N/A'}, "
                        f"reason={'not enough dimensions' if latents.ndim <= frame_dim else 'only 1 frame'}"
                    )

        # I2V training and Control training
        image_latents = None
        clip_fea = None

        context = [
            t.to(device=accelerator.device, dtype=network_dtype) for t in batch["t5"]
        ]

        # ensure the hidden state will require grad
        if args.gradient_checkpointing:
            noisy_model_input.requires_grad_(True)
            for t in context:
                t.requires_grad_(True)

        # Control LoRA processing (aligned with reference implementation)
        control_latents = None
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            # Log that control LoRA is engaged
            logger.info("üéØ Control LoRA processing engaged in training loop")

            # Pass VAE to control signal processing (must be provided by training loop)
            vae = (
                getattr(control_signal_processor, "vae", None)
                if control_signal_processor
                else None
            )
            if vae is None:
                logger.error(
                    "VAE not available for control LoRA training - this will cause training to fail"
                )

            control_latents = (
                control_signal_processor.process_control_signal(
                    args, accelerator, batch, latents, network_dtype, vae
                )
                if control_signal_processor
                else None
            )

            # If control signal could not be generated, fall back to using the image
            # latents themselves (same behaviour as V1 implementation).
            if control_latents is None:
                logger.warning(
                    "No control signal or pixels found; using image latents as control signal fallback"
                )
                control_latents = latents.detach().clone()

        # Optional: run ControlNet to get per-layer control states
        control_states = None
        try:
            if (
                hasattr(args, "enable_controlnet")
                and args.enable_controlnet
                and controlnet is not None
            ):
                # Control hint: prefer explicit control_signal (CFHW) -> BCFHW, else None
                control_pixels = None
                if (
                    "pixels" in batch
                    and isinstance(batch["pixels"], list)
                    and len(batch["pixels"]) > 0
                ):
                    # Pixels list entries are CFHW in our dataset; stack to BCFHW then move C back
                    cfhw_list = [
                        p.to(device=accelerator.device, dtype=network_dtype)
                        for p in batch["pixels"]
                    ]
                    control_pixels = torch.stack(cfhw_list, dim=0)  # B, C, F, H, W
                elif "control_signal" in batch:
                    # Already batched (B, C, F, H, W)
                    control_pixels = batch["control_signal"].to(
                        device=accelerator.device, dtype=network_dtype
                    )

                if control_pixels is not None:
                    control_states_tuple = controlnet(
                        hidden_states=noisy_model_input.to(
                            device=accelerator.device, dtype=network_dtype
                        ),
                        timestep=timesteps.to(device=accelerator.device),
                        encoder_hidden_states=(
                            torch.stack(context, dim=0)
                            if isinstance(context, list)
                            else context
                        ),
                        controlnet_states=control_pixels,
                        return_dict=False,
                    )
                    # controlnet returns ((layer_states,...),)
                    if (
                        isinstance(control_states_tuple, tuple)
                        and len(control_states_tuple) > 0
                    ):
                        control_states = control_states_tuple[0]
        except Exception as e:
            logger.warning(
                f"ControlNet forward failed; continuing without control. Error: {e}"
            )

        # call DiT
        lat_f, lat_h, lat_w = latents.shape[2:5]
        seq_len = (
            lat_f
            * lat_h
            * lat_w
            // (
                self.config.patch_size[0]
                * self.config.patch_size[1]
                * self.config.patch_size[2]
            )
        )
        latents = latents.to(device=accelerator.device, dtype=network_dtype)
        noisy_model_input = noisy_model_input.to(
            device=accelerator.device, dtype=network_dtype
        )

        # Prepare model input with control signal if control LoRA is enabled
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            if control_latents is not None:
                control_latents = control_latents.to(
                    device=noisy_model_input.device, dtype=noisy_model_input.dtype
                )

                # Honor control_concatenation_dim when valid; default to channel dim
                concat_dim = getattr(args, "control_concatenation_dim", None)
                if concat_dim is None:
                    concat_dim = 1 if noisy_model_input.dim() == 5 else 0
                else:
                    if noisy_model_input.dim() == 5 and concat_dim in (0, -2):
                        concat_dim = 1
                    else:
                        # Normalize negative dims and clamp
                        ndim = noisy_model_input.dim()
                        if not isinstance(concat_dim, int) or not (
                            -ndim <= concat_dim < ndim
                        ):
                            concat_dim = 1 if ndim == 5 else 0
                        else:
                            concat_dim = concat_dim % ndim

                model_input = torch.cat(
                    [noisy_model_input, control_latents], dim=concat_dim
                )
            else:
                logger.error(
                    "Control LoRA is enabled but control_latents is None. This will likely fail."
                )
                zero_control = torch.zeros_like(noisy_model_input)
                concat_dim = 1 if noisy_model_input.dim() == 5 else 0
                model_input = torch.cat(
                    [noisy_model_input, zero_control], dim=concat_dim
                )
        else:
            model_input = noisy_model_input

        with accelerator.autocast():
            # Build force_keep_mask for TREAD masked training (preserve masked tokens)
            force_keep_mask = None
            try:
                if (
                    getattr(args, "enable_tread", False)
                    and getattr(args, "tread_config", None) is not None
                    and "mask_signal" in batch
                    and batch["mask_signal"] is not None
                ):
                    # mask_signal shape could be (B, T, H, W) or (B, 1, T, H, W) or similar
                    ms = batch["mask_signal"]
                    if ms.dim() == 4:
                        # (B, T, H, W) -> (B, 1, T, H, W)
                        ms = ms.unsqueeze(1)
                    # Normalize to [0,1] if in [-1,1]
                    if ms.min() < 0 or ms.max() > 1:
                        ms = (ms + 1) / 2

                    # Compute token grid sizes from latent shape and patch size
                    lat_f, lat_h, lat_w = latents.shape[2:5]
                    pt, ph, pw = self.config.patch_size
                    t_tokens = max(1, lat_f // pt)
                    h_tokens = max(1, lat_h // ph)
                    w_tokens = max(1, lat_w // pw)

                    # Downsample to token grid
                    ms_tok = F.interpolate(
                        ms.float(),
                        size=(t_tokens, h_tokens, w_tokens),
                        mode="trilinear",
                        align_corners=False,
                    )
                    # Flatten with same T->H->W order as patch embedding
                    force_keep_mask = ms_tok.squeeze(1).flatten(1) > 0.5
            except Exception:
                force_keep_mask = None

            model_pred = model(
                model_input,
                t=timesteps,
                context=context,
                clip_fea=clip_fea,
                seq_len=seq_len,
                y=image_latents,
                force_keep_mask=force_keep_mask,
                controlnet_states=control_states,
                controlnet_weight=getattr(args, "controlnet_weight", 1.0),
                controlnet_stride=int(getattr(args, "controlnet_stride", 1)),
                dispersive_loss_target_block=getattr(
                    args, "dispersive_loss_target_block", None
                ),
                return_intermediate=bool(
                    getattr(args, "enable_dispersive_loss", False)
                ),
            )
        # Unpack optional intermediate
        intermediate_z: Optional[torch.Tensor] = None
        if isinstance(model_pred, tuple) and len(model_pred) == 2:
            model_pred, intermediate_z = model_pred  # type: ignore
        model_pred = torch.stack(model_pred, dim=0)  # list to tensor

        if model_pred.grad_fn is None:
            print(
                "model_pred is detached from the graph before returning from call_dit"
            )

        # flow matching loss - compute target using perturbed latents if fluxflow is enabled
        target = noise - perturbed_latents_for_target.to(
            device=accelerator.device, dtype=network_dtype
        )

        return model_pred, target, intermediate_z

    def run_training_loop(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        transformer: Any,
        network: Any,
        training_model: Any,
        optimizer: torch.optim.Optimizer,
        lr_scheduler: Any,
        lr_descriptions: List[str],
        train_dataloader: Any,
        val_dataloader: Optional[Any],
        noise_scheduler: FlowMatchDiscreteScheduler,
        network_dtype: torch.dtype,
        dit_dtype: torch.dtype,
        num_train_epochs: int,
        global_step: int,
        progress_bar: tqdm,
        metadata: Dict[str, str],
        loss_recorder: LossRecorder,
        sampling_manager: Optional[Any] = None,
        checkpoint_manager: Optional[Any] = None,
        control_signal_processor: Optional[Any] = None,
        current_epoch: Optional[Any] = None,
        current_step: Optional[Any] = None,
        optimizer_train_fn: Optional[Callable] = None,
        optimizer_eval_fn: Optional[Callable] = None,
        vae: Optional[Any] = None,
        sample_parameters: Optional[Any] = None,
        save_model: Optional[Callable] = None,
        remove_model: Optional[Callable] = None,
        is_main_process: bool = False,
        val_epoch_step_sync: Optional[Tuple[Any, Any]] = None,
        repa_helper: Optional[Any] = None,
        controlnet: Optional[Any] = None,
        dual_model_manager: Optional[Any] = None,
    ) -> Tuple[int, Any]:
        """Run the main training loop."""

        # Calculate starting epoch when resuming from checkpoint
        if global_step > 0:
            # We're resuming from a checkpoint - calculate which epoch we should be in
            steps_per_epoch = len(train_dataloader)
            epoch_to_start = global_step // steps_per_epoch

            # If we're exactly at the end of an epoch, we should start the next epoch
            if global_step % steps_per_epoch == 0 and global_step > 0:
                # We completed the previous epoch, start the next one
                pass  # epoch_to_start is already correct

            logger.info(
                f"Resuming training from step {global_step}, starting at epoch {epoch_to_start + 1}"
            )
        else:
            epoch_to_start = 0

        # Track the last step where sampling occurred to prevent duplicates
        # When resuming from a checkpoint, initialize based on whether sampling would have occurred at the current step
        if global_step > 0:
            # We're resuming from a checkpoint
            # Check if sampling would occur at the current global_step
            if should_sample_images(args, global_step, epoch=None):
                # Sampling would have occurred at this step before checkpoint was saved
                last_sampled_step = global_step
            else:
                last_sampled_step = -1

            # Check if validation would occur at the current global_step
            if (
                args.validate_every_n_steps is not None
                and global_step % args.validate_every_n_steps == 0
            ):
                last_validated_step = global_step
            else:
                last_validated_step = -1
        else:
            # Fresh training start
            last_sampled_step = -1
            last_validated_step = -1

        for epoch in range(epoch_to_start, num_train_epochs):
            accelerator.print(f"\nepoch {epoch+1}/{num_train_epochs}")
            if current_epoch is not None:
                current_epoch.value = epoch + 1

            metadata["takenoko_epoch"] = str(epoch + 1)

            accelerator.unwrap_model(network).on_epoch_start(transformer)

            # Calculate step offset when resuming in the middle of an epoch
            step_offset = 0
            if global_step > 0 and epoch == epoch_to_start:
                steps_per_epoch = len(train_dataloader)
                step_offset = global_step % steps_per_epoch
                if step_offset > 0:
                    logger.info(
                        f"Skipping first {step_offset} batches in epoch {epoch + 1} (resuming from middle of epoch)"
                    )

            for step, batch in enumerate(train_dataloader):
                # Start performance timing for this step
                start_step_timing()

                # Skip batches when resuming in the middle of an epoch
                if epoch == epoch_to_start and step < step_offset:
                    continue
                latents = batch["latents"]
                bsz = latents.shape[0]
                if current_step is not None:
                    current_step.value = global_step

                # Initialize metrics for this step
                per_source_losses = {}
                gradient_norm = None

                with accelerator.accumulate(training_model):
                    accelerator.unwrap_model(network).on_step_start()

                    latents = self.scale_shift_latents(latents)

                    # Sample noise that we'll add to the latents
                    noise = torch.randn_like(latents)

                    if hasattr(args, "enable_fvdm") and args.enable_fvdm:
                        # This inner 'if' is ONLY for the one-time log message
                        if accelerator.is_main_process and step == 0:
                            logger.info(
                                "FVDM training enabled. Using FVDM timestep sampling."
                            )

                        # This function call happens on EVERY step when FVDM is enabled
                        noisy_model_input, timesteps, sigmas = (
                            fvdm.get_noisy_model_input_and_timesteps_fvdm(
                                args,
                                noise,
                                latents,
                                noise_scheduler,
                                accelerator.device,
                                dit_dtype,
                            )
                        )
                    else:
                        # Initialize timestep distribution if needed
                        initialize_timestep_distribution(
                            args, self.timestep_distribution
                        )

                        # Optionally log the initial expected timestep distribution
                        if (
                            accelerator.is_main_process
                            and not self._timestep_logging_initialized
                            and getattr(args, "log_timestep_distribution_init", True)
                            and args.log_with in ["tensorboard", "all"]
                        ):
                            try:
                                log_initial_timestep_distribution(
                                    accelerator, args, self.timestep_distribution
                                )
                            finally:
                                self._timestep_logging_initialized = True

                        try:
                            if accelerator.is_main_process:
                                log_show_timesteps_figure_unconditional(
                                    accelerator,
                                    args,
                                    self.timestep_distribution,
                                    noise_scheduler,
                                )
                        except Exception:
                            pass

                        # calculate model input and timesteps
                        if dual_model_manager is not None:
                            noisy_model_input, timesteps, sigmas = (
                                dual_model_manager.determine_and_prepare_batch(
                                    args=args,
                                    noise=noise,
                                    latents=latents,
                                    noise_scheduler=noise_scheduler,
                                    device=accelerator.device,
                                    dtype=dit_dtype,
                                    timestep_distribution=self.timestep_distribution,
                                    presampled_uniform=(
                                        None
                                        if (
                                            hasattr(args, "use_precomputed_timesteps")
                                            and getattr(
                                                args, "use_precomputed_timesteps", False
                                            )
                                        )
                                        else batch.get("timesteps", None)
                                    ),
                                )
                            )
                            # swap base weights if regime changed
                            try:
                                dual_model_manager.swap_if_needed(accelerator)
                            except Exception as _swap_err:
                                logger.warning(
                                    f"DualModelManager swap failed: {_swap_err}"
                                )
                                # proceed without swap to avoid breaking training
                        else:
                            # If dataset provided per-batch pre-sampled uniform t values and
                            # precomputed timesteps are NOT enabled, map them through the
                            # selected sampling strategy. Otherwise, ignore and use current path.
                            batch_timesteps_uniform = None
                            try:
                                if hasattr(
                                    args, "use_precomputed_timesteps"
                                ) and getattr(args, "use_precomputed_timesteps", False):
                                    batch_timesteps_uniform = None
                                else:
                                    bt = batch.get("timesteps", None)
                                    if bt is not None:
                                        # bt may be list[float] length B
                                        batch_timesteps_uniform = torch.tensor(
                                            bt,
                                            device=accelerator.device,
                                            dtype=torch.float32,
                                        )
                            except Exception:
                                batch_timesteps_uniform = None

                            # Centralized utility with optional presampled uniform
                            noisy_model_input, timesteps, sigmas = (
                                get_noisy_model_input_and_timesteps(
                                    args,
                                    noise,
                                    latents,
                                    noise_scheduler,
                                    accelerator.device,
                                    dit_dtype,
                                    self.timestep_distribution,
                                    batch_timesteps_uniform,
                                )
                            )

                    weighting = compute_loss_weighting_for_sd3(
                        args.weighting_scheme,
                        noise_scheduler,
                        timesteps,
                        accelerator.device,
                        dit_dtype,
                    )

                    # Start forward pass timing
                    start_forward_pass_timing()

                    # choose active transformer when dual mode is enabled
                    active_transformer = transformer
                    if dual_model_manager is not None:
                        active_transformer = dual_model_manager.active_model

                    model_result = self.call_dit(
                        args,
                        accelerator,
                        active_transformer,
                        latents,
                        batch,
                        noise,
                        noisy_model_input,
                        timesteps,
                        network_dtype,
                        control_signal_processor,
                        controlnet,
                    )

                    # End forward pass timing
                    end_forward_pass_timing()

                    # Handle case where control LoRA failed to process
                    if model_result is None or model_result[0] is None:
                        logger.warning(
                            "Skipping batch due to control LoRA processing failure"
                        )
                        continue

                    model_pred, target, intermediate_z = model_result

                    # Loss will be computed later by centralized loss computer

                    # Compute per-source losses if enabled (before backprop), separate from loss
                    if (
                        hasattr(args, "log_per_source_loss")
                        and args.log_per_source_loss
                    ):
                        try:
                            sample_weights_local = batch.get("weight", None)
                            if sample_weights_local is not None:
                                sample_weights_local = sample_weights_local.to(
                                    device=accelerator.device, dtype=network_dtype
                                )
                            per_source_losses = self.compute_per_source_loss(
                                model_pred.to(network_dtype),
                                target,
                                batch,
                                weighting,
                                sample_weights_local,
                            )
                        except Exception as e:
                            logger.debug(f"‚ö†Ô∏è  Failed to compute per-source losses: {e}")
                            per_source_losses = {}

                    # Centralized training loss computation (includes DOP/REPA/Dispersive/OpticalFlow)
                    loss_components = self.loss_computer.compute_training_loss(
                        args=args,
                        accelerator=accelerator,
                        latents=latents,
                        noise=noise,
                        noisy_model_input=noisy_model_input,
                        timesteps=timesteps,
                        network_dtype=network_dtype,
                        model_pred=model_pred,
                        target=target,
                        weighting=weighting,
                        batch=batch,
                        intermediate_z=intermediate_z,
                        vae=vae,
                        transformer=active_transformer,
                        network=network,
                        control_signal_processor=control_signal_processor,
                        repa_helper=repa_helper,
                        raft=getattr(self, "raft", None),
                        warp_fn=getattr(self, "warp", None),
                    )

                    # Start backward pass timing
                    start_backward_pass_timing()
                    accelerator.backward(loss_components.total_loss)

                    # End backward pass timing
                    end_backward_pass_timing()

                    if accelerator.is_main_process:
                        # Check if ANY trainable parameter has a gradient
                        has_grad = any(
                            p.grad is not None
                            for p in network.parameters()
                            if p.requires_grad
                        )
                        if not has_grad:
                            print(
                                "WARNING: No gradients were computed for any parameter. The computation graph is broken."
                            )
                            # raise RuntimeError("Computation graph broken: no gradients found.")

                    # Compute gradient norm if enabled (before clipping)
                    gradient_norm = None
                    if (
                        accelerator.sync_gradients
                        and hasattr(args, "log_gradient_norm")
                        and args.log_gradient_norm
                    ):
                        try:
                            gradient_norm = self.compute_gradient_norm(network)
                        except Exception as e:
                            logger.debug(f"‚ö†Ô∏è Failed to compute gradient norm: {e}")

                if accelerator.sync_gradients:
                    # sync DDP grad manually
                    state = accelerate.PartialState()
                    if state.distributed_type != accelerate.DistributedType.NO:
                        for param in network.parameters():
                            if param.grad is not None:
                                param.grad = accelerator.reduce(
                                    param.grad, reduction="mean"
                                )

                    # Update GGPO gradient norms once gradients are synchronized
                    try:
                        if hasattr(network, "update_grad_norms"):
                            accelerator.unwrap_model(network).update_grad_norms()
                    except Exception:
                        pass

                    # Optional separate clipping for ControlNet
                    try:
                        if (
                            hasattr(args, "controlnet_max_grad_norm")
                            and args.controlnet_max_grad_norm is not None
                            and float(args.controlnet_max_grad_norm) > 0.0
                            and controlnet is not None
                        ):
                            accelerator.clip_grad_norm_(
                                controlnet.parameters(),
                                float(args.controlnet_max_grad_norm),
                            )
                    except Exception:
                        # Non-fatal: fall back to global clipping only
                        pass

                    if args.max_grad_norm != 0.0:
                        params_to_clip = accelerator.unwrap_model(
                            network
                        ).get_trainable_params()
                        accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)

                    # Start optimizer step timing
                    start_optimizer_step_timing()
                    try:
                        optimizer.step()
                    except RuntimeError as e:
                        logger.error(
                            "üö® Make sure you're not adding empty parameter groups to the optimizer"
                        )
                        raise e
                    # End optimizer step timing
                    end_optimizer_step_timing()

                    # Update GGPO weight norms after parameter update
                    try:
                        if hasattr(network, "update_norms"):
                            accelerator.unwrap_model(network).update_norms()
                    except Exception:
                        pass

                    lr_scheduler.step()
                    optimizer.zero_grad(set_to_none=True)

                if args.scale_weight_norms:
                    keys_scaled, mean_norm, maximum_norm = accelerator.unwrap_model(
                        network
                    ).apply_max_norm_regularization(
                        args.scale_weight_norms, accelerator.device
                    )
                    max_mean_logs = {
                        "Keys Scaled": keys_scaled,
                        "Average key norm": mean_norm,
                    }
                else:
                    keys_scaled, mean_norm, maximum_norm = None, None, None

                # Checks if the accelerator has performed an optimization step behind the scenes
                if accelerator.sync_gradients:
                    progress_bar.update(1)
                    global_step += 1

                    # Lightweight periodic self-correction cache refresh (fully gated)
                    try:
                        if (
                            bool(getattr(args, "self_correction_enabled", False))
                            and global_step
                            > int(getattr(args, "self_correction_warmup_steps", 1000))
                            and int(
                                getattr(args, "self_correction_update_frequency", 1000)
                            )
                            > 0
                            and global_step
                            % int(
                                getattr(args, "self_correction_update_frequency", 1000)
                            )
                            == 0
                        ):
                            # Access manager via trainer if present
                            mgr = getattr(
                                accelerator.state, "_self_correction_manager", None
                            )
                            # Fallback: some callers may attach it to the transformer
                            if mgr is None:
                                try:
                                    mgr = getattr(
                                        transformer, "_self_correction_manager", None
                                    )
                                except Exception:
                                    mgr = None
                            if mgr is not None:
                                # Switch to eval for generation
                                try:
                                    training_was = training_model.training
                                    training_model.eval()
                                except Exception:
                                    training_was = None
                                accelerator.wait_for_everyone()
                                if accelerator.is_main_process:
                                    mgr.update_cache(accelerator.unwrap_model(transformer))  # type: ignore
                                accelerator.wait_for_everyone()
                                # Restore mode
                                try:
                                    if training_was is True:
                                        training_model.train()
                                except Exception:
                                    pass
                    except Exception as _sc_err:
                        # Non-fatal path: continue training
                        if accelerator.is_main_process:
                            logger.debug(f"Self-correction update skipped: {_sc_err}")

                    # to avoid calling optimizer_eval_fn() too frequently, we call it only when we need to sample images, validate, or save the model
                    should_sampling = should_sample_images(
                        args, global_step, epoch=epoch + 1
                    )
                    should_saving = (
                        args.save_every_n_steps is not None
                        and global_step % args.save_every_n_steps == 0
                    )
                    should_validating = self.validation_core.should_validate(
                        args, global_step, val_dataloader, last_validated_step
                    )

                    if should_sampling or should_saving or should_validating:
                        if optimizer_eval_fn:
                            optimizer_eval_fn()
                        if should_sampling and sampling_manager:
                            # Use epoch-based naming only if sampling was triggered by epoch, not steps
                            # This prevents filename conflicts when resuming training in the same epoch
                            epoch_for_naming = None
                            if (
                                args.sample_every_n_epochs is not None
                                and args.sample_every_n_epochs > 0
                                and (epoch + 1) % args.sample_every_n_epochs == 0
                            ):
                                # This sampling was triggered by epoch boundary
                                epoch_for_naming = epoch + 1
                            # Otherwise, leave epoch_for_naming as None to use step-based naming

                            sampling_manager.sample_images(
                                accelerator,
                                args,
                                epoch_for_naming,  # Use None for step-based sampling
                                global_step,
                                vae,
                                transformer,
                                sample_parameters,
                                dit_dtype,
                            )
                            # Track that sampling occurred at this step
                            last_sampled_step = global_step

                        if should_validating:
                            # Sync validation datasets before validation runs
                            self.validation_core.sync_validation_epoch(
                                val_dataloader,
                                val_epoch_step_sync,
                                current_epoch.value if current_epoch else epoch + 1,
                                global_step,
                            )

                            val_loss = self.validation_core.validate(
                                accelerator,
                                transformer,
                                val_dataloader,
                                noise_scheduler,
                                args,
                                control_signal_processor,
                                vae,
                                global_step,
                            )
                            self.validation_core.log_validation_results(
                                accelerator, val_loss, global_step
                            )
                            # Track that validation occurred at this step
                            last_validated_step = global_step

                        if should_saving:
                            accelerator.wait_for_everyone()
                            if accelerator.is_main_process and save_model:
                                from utils import train_utils

                                ckpt_name = train_utils.get_step_ckpt_name(
                                    args.output_name, global_step
                                )
                                save_model(
                                    ckpt_name,
                                    accelerator.unwrap_model(network),
                                    global_step,
                                    epoch + 1,
                                )

                                if args.save_state:
                                    train_utils.save_and_remove_state_stepwise(
                                        args, accelerator, global_step
                                    )

                                remove_step_no = train_utils.get_remove_step_no(
                                    args, global_step
                                )
                                if remove_step_no is not None and remove_model:
                                    remove_ckpt_name = train_utils.get_step_ckpt_name(
                                        args.output_name, remove_step_no
                                    )
                                    remove_model(remove_ckpt_name)
                        if optimizer_train_fn:
                            optimizer_train_fn()

                current_loss = float(loss_components.total_loss.detach().item())
                loss_recorder.add(epoch=epoch + 1, step=step, loss=current_loss)
                avr_loss: float = loss_recorder.moving_average

                # Update EMA loss for TensorBoard logging
                ema_loss_value = self.update_ema_loss(current_loss)

                # Generate enhanced progress bar metrics safely if enabled
                if getattr(args, "enhanced_progress_bar", True):
                    try:
                        # Calculate epoch progress information
                        current_step_in_epoch = step + 1  # step is 0-indexed
                        total_steps_in_epoch = len(train_dataloader)

                        enhanced_logs = self.generate_safe_progress_metrics(
                            args,
                            current_loss,
                            avr_loss,
                            lr_scheduler,
                            epoch,
                            global_step,
                            keys_scaled,
                            mean_norm,
                            maximum_norm,
                            current_step_in_epoch,
                            total_steps_in_epoch,
                        )

                        # Add hardware metrics to progress bar
                        hardware_metrics = get_hardware_metrics()
                        enhanced_logs.update(hardware_metrics)
                        progress_bar.set_postfix(enhanced_logs)
                    except Exception:
                        # Fallback to original simple display if enhanced metrics fail
                        logs = {"avr_loss": avr_loss}
                        progress_bar.set_postfix(logs)
                        if args.scale_weight_norms:
                            progress_bar.set_postfix({**max_mean_logs, **logs})
                else:
                    # Use original simple progress bar
                    logs = {"avr_loss": avr_loss}
                    progress_bar.set_postfix(logs)
                    if args.scale_weight_norms:
                        progress_bar.set_postfix({**max_mean_logs, **logs})

                # Only the main process should handle logging and saving
                if accelerator.is_main_process and len(accelerator.trackers) > 0:
                    logs = self.generate_step_logs(
                        args,
                        current_loss,
                        avr_loss,
                        lr_scheduler,
                        lr_descriptions,
                        optimizer,
                        keys_scaled,
                        mean_norm,
                        maximum_norm,
                        ema_loss_value,
                        network,  # Pass the model for parameter stats
                        global_step,  # Pass global_step for parameter stats
                        per_source_losses,  # Pass per-source losses
                        gradient_norm,  # Pass gradient norm
                    )

                    # Add performance metrics
                    if accelerator.sync_gradients:
                        # Get timing metrics
                        timing_metrics = get_timing_metrics()
                        logs.update(timing_metrics)

                        # Get model statistics
                        model_stats = get_model_statistics(
                            model_pred.to(network_dtype),
                            target,
                            accelerator.is_main_process,
                        )
                        logs.update(model_stats)

                        # Log performance summary
                        log_performance_summary(global_step, timing_metrics)

                    # Attach GGPO metrics if available
                    try:
                        if hasattr(network, "grad_norms") and hasattr(
                            network, "combined_weight_norms"
                        ):
                            gn = accelerator.unwrap_model(network).grad_norms()
                            wn = accelerator.unwrap_model(
                                network
                            ).combined_weight_norms()
                            if gn is not None:
                                logs["norm/avg_grad_norm"] = float(gn.item())
                            if wn is not None:
                                logs["norm/avg_combined_norm"] = float(wn.item())
                    except Exception:
                        pass

                    # Attach component losses if available
                    base_loss_val = getattr(loss_components, "base_loss", None)
                    if base_loss_val is not None:
                        logs["loss/mse"] = float(base_loss_val.item())
                    if loss_components.dispersive_loss is not None:
                        logs["loss/dispersive"] = float(
                            loss_components.dispersive_loss.item()
                        )
                    if loss_components.dop_loss is not None:
                        logs["loss/dop"] = float(loss_components.dop_loss.item())
                    if loss_components.optical_flow_loss is not None:
                        logs["loss/optical_flow"] = float(
                            loss_components.optical_flow_loss.item()
                        )
                    if loss_components.repa_loss is not None:
                        logs["loss/repa"] = float(loss_components.repa_loss.item())

                    # Optionally compute extra training metrics periodically
                    try:
                        if (
                            getattr(args, "log_extra_train_metrics", True)
                            and (args.train_metrics_interval or 0) > 0
                            and (global_step % int(args.train_metrics_interval) == 0)
                        ):
                            extra_metrics = (
                                self.loss_computer.compute_extra_train_metrics(
                                    model_pred=model_pred,
                                    target=target,
                                    noise=noise,
                                    timesteps=timesteps,
                                    noise_scheduler=noise_scheduler,
                                    accelerator=accelerator,
                                )
                            )
                            if extra_metrics:
                                logs.update(extra_metrics)
                    except Exception:
                        pass

                    # Log scalar metrics
                    accelerator.log(logs, step=global_step)

                    # Enhanced optimizer-specific histogram logging
                    if optimizer is not None and is_supported(optimizer):
                        try:
                            histogram_data = get_histogram_data(optimizer)
                            if histogram_data:
                                metric_name, tensor_data = histogram_data
                                # Make sure tensor_data is not empty
                                if tensor_data.numel() > 0:
                                    # Log histogram directly to TensorBoard writer
                                    # TensorBoard expects histograms to be logged via add_histogram
                                    for tracker in accelerator.trackers:
                                        if tracker.name == "tensorboard":
                                            tracker.writer.add_histogram(
                                                metric_name, tensor_data, global_step
                                            )
                                            break
                        except Exception as e:
                            logger.debug(
                                f"Failed to log enhanced optimizer histogram: {e}"
                            )

                    # Periodic live histogram of used timesteps (1..1000)
                    try:
                        log_live_timestep_distribution(
                            accelerator, args, timesteps, global_step
                        )
                    except Exception:
                        pass

                    # Periodic loss-vs-timestep scatter figure
                    try:
                        log_loss_scatterplot(
                            accelerator,
                            args,
                            timesteps,
                            model_pred,
                            target,
                            global_step,
                        )
                    except Exception:
                        pass

                # End step timing
                end_step_timing()

                if global_step >= args.max_train_steps:
                    break

            if global_step >= args.max_train_steps:
                break

            if accelerator.is_main_process and len(accelerator.trackers) > 0:
                logs = {"loss/epoch": loss_recorder.moving_average}
                accelerator.log(logs, step=epoch + 1)

            accelerator.wait_for_everyone()

            # save model at the end of epoch if needed
            if optimizer_eval_fn:
                optimizer_eval_fn()
            if args.save_every_n_epochs is not None:
                saving = (epoch + 1) % args.save_every_n_epochs == 0 and (
                    epoch + 1
                ) < num_train_epochs
                if is_main_process and saving and save_model:
                    from utils import train_utils

                    ckpt_name = train_utils.get_epoch_ckpt_name(
                        args.output_name, epoch + 1
                    )
                    save_model(
                        ckpt_name,
                        accelerator.unwrap_model(network),
                        global_step,
                        epoch + 1,
                    )

                    remove_epoch_no = train_utils.get_remove_epoch_no(args, epoch + 1)
                    if remove_epoch_no is not None and remove_model:
                        remove_ckpt_name = train_utils.get_epoch_ckpt_name(
                            args.output_name, remove_epoch_no
                        )
                        remove_model(remove_ckpt_name)

                    if args.save_state:
                        from utils import train_utils

                        train_utils.save_and_remove_state_on_epoch_end(
                            args, accelerator, epoch + 1
                        )

            # Only sample at end of epoch if epoch-based sampling is enabled AND it's not already sampled during the last step
            # This prevents double sampling when the last step of an epoch also triggers sampling
            should_sample_at_epoch_end = (
                args.sample_every_n_epochs is not None
                and args.sample_every_n_epochs > 0
                and (epoch + 1) % args.sample_every_n_epochs == 0
            )
            # Only sample if epoch-based sampling is enabled AND we haven't already sampled at this step
            if (
                should_sample_at_epoch_end
                and last_sampled_step != global_step
                and sampling_manager
            ):
                sampling_manager.sample_images(
                    accelerator,
                    args,
                    epoch + 1,
                    global_step,
                    vae,
                    transformer,
                    sample_parameters,
                    dit_dtype,
                )
            if optimizer_train_fn:
                optimizer_train_fn()

            # Do validation only if validation dataloader exists, validation hasn't already run at this step, and epoch-end validation is enabled
            should_validate_on_epoch_end = getattr(args, "validate_on_epoch_end", False)
            if (
                val_dataloader is not None
                and last_validated_step != global_step
                and should_validate_on_epoch_end
            ):
                # Sync validation datasets before validation runs
                self.validation_core.sync_validation_epoch(
                    val_dataloader,
                    val_epoch_step_sync,
                    current_epoch.value if current_epoch else epoch + 1,
                    global_step,
                )

                val_loss = self.validation_core.validate(
                    accelerator,
                    transformer,
                    val_dataloader,
                    noise_scheduler,
                    args,
                    control_signal_processor,
                    vae,
                    global_step,
                )
                self.validation_core.log_validation_results(
                    accelerator, val_loss, global_step, epoch + 1
                )
            elif val_dataloader is None:
                accelerator.print(
                    f"\n[Epoch {epoch+1}] No validation dataset configured"
                )
            elif not should_validate_on_epoch_end:
                accelerator.print(f"\n[Epoch {epoch+1}] Epoch-end validation disabled")
            else:
                accelerator.print(
                    f"\n[Epoch {epoch+1}] Validation already performed at step {global_step}"
                )

            # end of epoch

        return global_step, network
</file>

<file path="core/vae_training_core.py">
"""VAE-specific training core for WAN network trainer.

This module handles VAE training logic, including reconstruction loss and KL divergence.
Separate from the main training_core.py to handle the different training paradigm.
"""

import argparse
import math
import numpy as np
import time
from multiprocessing import Value
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import torch
from tqdm import tqdm
import accelerate
from accelerate import Accelerator, PartialState
import torch.nn.functional as F

from utils.train_utils import (
    clean_memory_on_device,
    should_sample_images,
    LossRecorder,
)

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class VaeTrainingCore:
    """Handles VAE-specific training logic."""

    def __init__(self, config: Any):
        self.config = config

    def compute_vae_loss(
        self,
        reconstructed: torch.Tensor,
        original: torch.Tensor,
        mu: Optional[torch.Tensor] = None,
        logvar: Optional[torch.Tensor] = None,
        kl_weight: float = 1e-6,
        reconstruction_loss_type: str = "mse",
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """Compute VAE loss including reconstruction loss and KL divergence."""

        # Reconstruction loss
        if reconstruction_loss_type == "mse":
            reconstruction_loss = F.mse_loss(reconstructed, original, reduction="mean")
        elif reconstruction_loss_type == "l1":
            reconstruction_loss = F.l1_loss(reconstructed, original, reduction="mean")
        elif reconstruction_loss_type == "huber":
            reconstruction_loss = F.smooth_l1_loss(
                reconstructed, original, reduction="mean"
            )
        else:
            raise ValueError(
                f"Unknown reconstruction loss type: {reconstruction_loss_type}"
            )

        # KL divergence loss (if VAE outputs mu and logvar)
        kl_loss = torch.tensor(0.0, device=reconstructed.device)
        if mu is not None and logvar is not None:
            # KL divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            kl_loss = kl_loss / original.numel()  # Normalize by number of elements

        # Total loss
        total_loss = reconstruction_loss + kl_weight * kl_loss

        loss_dict = {
            "reconstruction_loss": reconstruction_loss,
            "kl_loss": kl_loss,
            "total_loss": total_loss,
        }

        return total_loss, loss_dict

    def call_vae(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        vae: Any,
        images: torch.Tensor,
        network_dtype: torch.dtype,
    ) -> Tuple[
        torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]
    ]:
        """Call the VAE model and compute reconstruction."""

        # Ensure images are in the right format and dtype
        images = images.to(device=accelerator.device, dtype=network_dtype)

        with accelerator.autocast():
            # For VAE training, we need the full forward pass
            if hasattr(vae, "encode") and hasattr(vae, "decode"):
                # Standard VAE with separate encode/decode
                encoded = vae.encode(images)

                # Handle different VAE output formats
                if isinstance(encoded, tuple):
                    # (mu, logvar) format
                    mu, logvar = encoded
                    # Reparameterization trick
                    std = torch.exp(0.5 * logvar)
                    eps = torch.randn_like(std)
                    z = mu + eps * std
                else:
                    # Direct latent format
                    z = encoded
                    mu, logvar = None, None

                # Decode
                reconstructed = vae.decode(z)
            else:
                # Direct forward pass
                output = vae(images)
                if isinstance(output, tuple):
                    reconstructed, mu, logvar = output
                else:
                    reconstructed = output
                    mu, logvar = None, None

        return reconstructed, images, mu, logvar

    def validate_vae(
        self,
        accelerator: Accelerator,
        vae: Any,
        val_dataloader: Any,
        args: argparse.Namespace,
    ) -> float:
        """Run VAE validation and return average validation loss."""
        logger.info("Running VAE validation...")

        vae.eval()
        losses = []

        with torch.no_grad():
            for step, batch in enumerate(val_dataloader):
                # For VAE validation, we need original images, not latents
                if "images" in batch:
                    images = batch["images"]
                elif "latents" in batch:
                    # If we only have latents, we can't validate properly
                    logger.warning(
                        "VAE validation requires original images, but only latents found. Skipping validation."
                    )
                    return 0.0
                else:
                    logger.warning("No images found in validation batch. Skipping.")
                    continue

                images = images.to(accelerator.device, dtype=vae.dtype)

                # Forward pass
                reconstructed, original, mu, logvar = self.call_vae(
                    args, accelerator, vae, images, vae.dtype
                )

                # Compute loss
                val_loss, _ = self.compute_vae_loss(
                    reconstructed,
                    original,
                    mu,
                    logvar,
                    kl_weight=getattr(args, "vae_kl_weight", 1e-6),
                    reconstruction_loss_type=getattr(
                        args, "vae_reconstruction_loss", "mse"
                    ),
                )

                losses.append(val_loss)

        if losses:
            losses_tensor = torch.stack(losses)
            gathered_losses = accelerator.gather_for_metrics(losses_tensor)
            final_avg_loss = gathered_losses.mean().item()  # type: ignore
        else:
            final_avg_loss = 0.0

        vae.train()
        logger.info(f"VAE validation finished. Average loss: {final_avg_loss:.5f}")
        return final_avg_loss

    def run_vae_training_loop(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        vae: Any,
        network: Any,
        training_model: Any,
        optimizer: torch.optim.Optimizer,
        lr_scheduler: Any,
        lr_descriptions: List[str],
        train_dataloader: Any,
        val_dataloader: Optional[Any],
        network_dtype: torch.dtype,
        num_train_epochs: int,
        global_step: int,
        progress_bar: tqdm,
        metadata: Dict[str, str],
        loss_recorder: LossRecorder,
        current_epoch: Optional[Value] = None,  # type: ignore
        current_step: Optional[Value] = None,  # type: ignore
        optimizer_train_fn: Optional[callable] = None,  # type: ignore
        optimizer_eval_fn: Optional[callable] = None,  # type: ignore
        save_model: Optional[callable] = None,  # type: ignore
        remove_model: Optional[callable] = None,  # type: ignore
        is_main_process: bool = False,
    ) -> Tuple[int, Any]:
        """Run the VAE training loop."""

        # VAE-specific training parameters
        kl_weight = getattr(args, "vae_kl_weight", 1e-6)
        reconstruction_loss_type = getattr(args, "vae_reconstruction_loss", "mse")

        logger.info(
            f"Starting VAE training with KL weight: {kl_weight}, reconstruction loss: {reconstruction_loss_type}"
        )

        for epoch in range(num_train_epochs):
            accelerator.print(f"\nVAE epoch {epoch+1}/{num_train_epochs}")
            if current_epoch is not None:
                current_epoch.value = epoch + 1

            metadata["takenoko_epoch"] = str(epoch + 1)

            for step, batch in enumerate(train_dataloader):
                if current_step is not None:
                    current_step.value = global_step

                with accelerator.accumulate(training_model):
                    # For VAE training, we need original images
                    if "images" in batch:
                        images = batch["images"]
                    else:
                        logger.error(
                            "VAE training requires original images in batch. Please ensure dataset provides 'images' key."
                        )
                        continue

                    # Apply sample weights if present
                    sample_weights = batch.get("weight", None)

                    # Forward pass through VAE
                    reconstructed, original, mu, logvar = self.call_vae(
                        args, accelerator, vae, images, network_dtype
                    )

                    # Compute VAE loss
                    loss, loss_dict = self.compute_vae_loss(
                        reconstructed,
                        original,
                        mu,
                        logvar,
                        kl_weight=kl_weight,
                        reconstruction_loss_type=reconstruction_loss_type,
                    )

                    # Apply sample weights if present
                    if sample_weights is not None:
                        sample_weights = sample_weights.to(
                            device=accelerator.device, dtype=network_dtype
                        )
                        loss = (
                            loss * sample_weights.mean()
                        )  # Apply weight to total loss

                    accelerator.backward(loss)

                    if accelerator.sync_gradients:
                        if args.max_grad_norm != 0.0:
                            params_to_clip = network.get_trainable_params()
                            accelerator.clip_grad_norm_(
                                params_to_clip, args.max_grad_norm
                            )

                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad(set_to_none=True)

                # Checks if the accelerator has performed an optimization step behind the scenes
                if accelerator.sync_gradients:
                    progress_bar.update(1)
                    global_step += 1

                    # Validation
                    should_validating = (
                        args.validate_every_n_steps is not None
                        and global_step % args.validate_every_n_steps == 0
                        and val_dataloader is not None
                    )

                    # Model saving
                    should_saving = (
                        args.save_every_n_steps is not None
                        and global_step % args.save_every_n_steps == 0
                    )

                    if should_validating or should_saving:
                        if optimizer_eval_fn:
                            optimizer_eval_fn()

                        if should_validating:
                            val_loss = self.validate_vae(
                                accelerator, vae, val_dataloader, args
                            )
                            accelerator.print(
                                f"[Step {global_step}] VAE val_loss={val_loss:0.5f}"
                            )
                            accelerator.log({"val_loss": val_loss}, step=global_step)

                        if should_saving:
                            accelerator.wait_for_everyone()
                            if accelerator.is_main_process and save_model:
                                from utils import train_utils

                                ckpt_name = train_utils.get_step_ckpt_name(
                                    args.output_name, global_step
                                )
                                save_model(ckpt_name, network, global_step, epoch + 1)

                        if optimizer_train_fn:
                            optimizer_train_fn()

                current_loss = loss.detach().item()
                loss_recorder.add(epoch=epoch + 1, step=step, loss=current_loss)
                avr_loss: float = loss_recorder.moving_average

                # Log detailed VAE losses
                logs = {
                    "loss/total": current_loss,
                    "loss/average": avr_loss,
                    "loss/reconstruction": loss_dict["reconstruction_loss"].item(),
                    "loss/kl": loss_dict["kl_loss"].item(),
                }

                # Add learning rates
                lrs = lr_scheduler.get_last_lr()
                for i, lr in enumerate(lrs):
                    if i < len(lr_descriptions):
                        logs[f"lr/{lr_descriptions[i]}"] = lr
                    else:
                        logs[f"lr/group{i}"] = lr

                progress_bar.set_postfix(logs)

                if len(accelerator.trackers) > 0:
                    accelerator.log(logs, step=global_step)

                if global_step >= args.max_train_steps:
                    break

            if global_step >= args.max_train_steps:
                break

            # End of epoch validation
            should_validate_on_epoch_end = getattr(args, "validate_on_epoch_end", False)
            if val_dataloader is not None and should_validate_on_epoch_end:
                val_loss = self.validate_vae(accelerator, vae, val_dataloader, args)
                accelerator.print(f"[Epoch {epoch+1}] VAE val_loss={val_loss:0.5f}")
                accelerator.log({"val_loss": val_loss}, step=global_step)
            elif val_dataloader is not None and not should_validate_on_epoch_end:
                accelerator.print(
                    f"\n[Epoch {epoch+1}] VAE epoch-end validation disabled"
                )

            # Save model at end of epoch if needed
            if args.save_every_n_epochs is not None:
                saving = (epoch + 1) % args.save_every_n_epochs == 0 and (
                    epoch + 1
                ) < num_train_epochs
                if is_main_process and saving and save_model:
                    from utils import train_utils

                    ckpt_name = train_utils.get_epoch_ckpt_name(
                        args.output_name, epoch + 1
                    )
                    save_model(ckpt_name, network, global_step, epoch + 1)

        return global_step, network
</file>

<file path="core/validation_core.py">
"""Core validation logic for WAN network trainer.

This module handles validation, model evaluation, and validation metrics computation.
Extracted from training_core.py to improve code organization and maintainability.
"""

import argparse
import logging
from typing import Any, Dict, Optional
import torch
import numpy as np
from accelerate import Accelerator

from modules.scheduling_flow_match_discrete import FlowMatchDiscreteScheduler
from utils.train_utils import get_sigmas
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def compute_snr(
    noise_scheduler: FlowMatchDiscreteScheduler, timesteps: torch.Tensor
) -> torch.Tensor:
    """Compute Signal-to-Noise Ratio (SNR) for given timesteps.

    Args:
        noise_scheduler: The noise scheduler instance
        timesteps: Tensor of timesteps

    Returns:
        SNR values for each timestep
    """
    # Get sigma values for the timesteps
    sigmas = get_sigmas(
        noise_scheduler,
        timesteps,
        timesteps.device,
        n_dim=4,  # BCFHW format
        dtype=timesteps.dtype,
        source="validation/snr",
    )

    # SNR = 1 / sigma^2 (signal power / noise power)
    snr = 1.0 / (sigmas**2)
    return snr


class ValidationCore:
    """Handles validation logic, model evaluation, and validation metrics."""

    def __init__(self, config: Any, fluxflow_config: Dict[str, Any]):
        self.config = config
        self.fluxflow_config = fluxflow_config
        self._setup_validation_timesteps()

    def _setup_validation_timesteps(self) -> None:
        """Parse and setup validation timesteps from configuration only.

        Accepts either a single integer (as string or int) or a comma-separated
        list of integers. Any invalid value defaults to [600, 700, 800, 900].
        """
        raw = getattr(self.config, "validation_timesteps", "600,700,800,900")
        try:
            # Single integer path
            if isinstance(raw, int):
                self.validation_timesteps = [raw]
                return
            raw_str = str(raw).strip()
            if "," in raw_str:
                self.validation_timesteps = [
                    int(t.strip()) for t in raw_str.split(",") if t.strip()
                ]
            else:
                self.validation_timesteps = [int(raw_str)]
        except Exception:
            logger.warning(
                f"Invalid validation_timesteps format: {raw}. Using default [600, 700, 800, 900]"
            )
            self.validation_timesteps = [600, 700, 800, 900]

        logger.info(f"Validation timesteps: {self.validation_timesteps}")

    def _compute_and_gather_metrics(
        self,
        accelerator: Accelerator,
        loss_tensors_list: list[torch.Tensor],
        metric_name: str,
    ) -> Dict[str, float]:
        """Concatenates, gathers, and computes statistics for a list of loss tensors.

        Args:
            accelerator: The accelerator instance for distributed training
            loss_tensors_list: List of loss tensors to process
            metric_name: Base name for the metrics (e.g., "velocity_loss")

        Returns:
            Dictionary containing computed statistics (mean, std, min, max)
        """
        if not loss_tensors_list:
            return {}

        # 1. Concatenate ONCE
        local_tensor = torch.cat(loss_tensors_list)

        # 2. Gather ONCE
        gathered_tensor = accelerator.gather_for_metrics(local_tensor)

        # 3. Compute all stats from the final gathered tensor
        if accelerator.is_main_process:
            if isinstance(gathered_tensor, torch.Tensor):
                stats: Dict[str, float] = {
                    f"{metric_name}_mean": gathered_tensor.mean().item(),
                    f"{metric_name}_std": gathered_tensor.std().item(),
                    f"{metric_name}_min": gathered_tensor.min().item(),
                    f"{metric_name}_max": gathered_tensor.max().item(),
                }
                # Quantile statistics (defensive in case torch.quantile not available)
                try:
                    median_val = torch.quantile(gathered_tensor, 0.5).item()
                    p90_val = torch.quantile(gathered_tensor, 0.9).item()
                    stats[f"{metric_name}_p50"] = median_val
                    stats[f"{metric_name}_p90"] = p90_val
                except Exception:
                    try:
                        stats[f"{metric_name}_p50"] = torch.median(
                            gathered_tensor
                        ).item()
                    except Exception:
                        pass
                return stats
            else:
                # Handle case where gathered_tensor is not a tensor
                gathered_tensor = torch.tensor(gathered_tensor)
                stats: Dict[str, float] = {
                    f"{metric_name}_mean": gathered_tensor.mean().item(),
                    f"{metric_name}_std": gathered_tensor.std().item(),
                    f"{metric_name}_min": gathered_tensor.min().item(),
                    f"{metric_name}_max": gathered_tensor.max().item(),
                }
                try:
                    median_val = torch.quantile(gathered_tensor, 0.5).item()
                    p90_val = torch.quantile(gathered_tensor, 0.9).item()
                    stats[f"{metric_name}_p50"] = median_val
                    stats[f"{metric_name}_p90"] = p90_val
                except Exception:
                    try:
                        stats[f"{metric_name}_p50"] = torch.median(
                            gathered_tensor
                        ).item()
                    except Exception:
                        pass
                return stats
        return {}  # Return empty dict on non-main processes

    def validate(
        self,
        accelerator: Accelerator,
        transformer: Any,
        val_dataloader: Any,
        noise_scheduler: FlowMatchDiscreteScheduler,
        args: argparse.Namespace,
        control_signal_processor: Optional[Any] = None,
        vae: Optional[Any] = None,
        global_step: Optional[int] = None,
        show_progress: bool = True,
    ) -> tuple[float, float]:
        # Prefer validation timesteps provided via args (from TOML) if available
        try:
            # TODO: check if duplicate code
            raw = getattr(args, "validation_timesteps", None)
            if raw is not None:
                raw_str = str(raw).strip()
                if "," in raw_str:
                    self.validation_timesteps = [
                        int(t.strip()) for t in raw_str.split(",") if t.strip()
                    ]
                else:
                    self.validation_timesteps = [int(raw_str)]
        except Exception:
            # Fallback to whatever was set up previously
            pass
        """Run validation and return average validation loss for both velocity and direct noise prediction.

        Args:
            global_step: Current training step for logging per-timestep metrics
            show_progress: Whether to show progress bars

        Note:
            Noise generation behavior is controlled by args.use_unique_noise_per_batch:
            - True (recommended): Each batch gets unique but deterministic noise (seed = 42 + batch_idx)
            - False (legacy): All batches use the same noise pattern (seed = 42)

            Using unique noise per batch provides more reliable validation metrics.

        Returns:
            tuple[float, float]: (velocity_loss, direct_noise_loss)

        Example:
            velocity_loss, direct_noise_loss = validation_core.validate(...)
            print(f"Velocity loss: {velocity_loss:.5f}")
            print(f"Direct noise loss: {direct_noise_loss:.5f}")
        """
        logger.info("Running validation...")
        unwrapped_model = accelerator.unwrap_model(transformer)
        unwrapped_model.switch_block_swap_for_inference()
        unwrapped_model.eval()

        fixed_seed = 42
        total_velocity_loss = 0.0
        total_direct_noise_loss = 0.0
        num_timesteps = len(self.validation_timesteps)

        # Collect timestep losses and SNRs for statistics (across all timesteps)
        all_timestep_avg_velocity_losses = []
        all_timestep_avg_direct_noise_losses = []
        all_timestep_snrs = []

        if num_timesteps == 0:
            logger.warning(
                "Validation timesteps list is empty. Skipping validation and returning 0 loss."
            )
            # Switch back to train mode before returning
            unwrapped_model.train()
            unwrapped_model.switch_block_swap_for_training()
            return 0.0, 0.0

        logger.info(
            f"Validating across {num_timesteps} timesteps: {self.validation_timesteps}"
        )

        # Setup nested progress bars for better UX
        if show_progress and accelerator.is_main_process:
            from tqdm import tqdm

            timestep_pbar = tqdm(
                self.validation_timesteps,
                desc="Validating Timesteps",
                disable=not accelerator.is_main_process,
                leave=True,
            )
        else:
            timestep_pbar = self.validation_timesteps

        with torch.no_grad():
            # Validate across all timesteps
            for timestep_idx, current_timestep in enumerate(timestep_pbar):
                if show_progress and accelerator.is_main_process:
                    dataloader_pbar = tqdm(
                        val_dataloader,
                        desc=f"Timestep {current_timestep}",
                        leave=False,
                        disable=not accelerator.is_main_process,
                    )
                else:
                    dataloader_pbar = val_dataloader

                # These lists collect per-batch losses for the CURRENT timestep
                batch_velocity_losses = []
                batch_direct_noise_losses = []

                # Calculate SNR only once for the current timestep (optimization)
                fixed_timesteps_tensor = torch.full(
                    (1,),
                    current_timestep,
                    device=accelerator.device,
                    dtype=torch.float32,
                )
                snr_for_timestep = (
                    compute_snr(noise_scheduler, fixed_timesteps_tensor).mean().item()
                )

                # Track sample-weighted (per-example) sums across all batches/timesteps
                if timestep_idx == 0:
                    # Initialize accumulators once (on first timestep)
                    local_velocity_loss_sum = torch.tensor(
                        0.0, device=accelerator.device, dtype=torch.float32
                    )
                    local_velocity_loss_count = torch.tensor(
                        0.0, device=accelerator.device, dtype=torch.float32
                    )
                    local_direct_noise_loss_sum = torch.tensor(
                        0.0, device=accelerator.device, dtype=torch.float32
                    )
                    local_direct_noise_loss_count = torch.tensor(
                        0.0, device=accelerator.device, dtype=torch.float32
                    )

                for step, batch in enumerate(dataloader_pbar):
                    latents = batch["latents"]

                    # Text embedding preparation
                    if "t5" in batch:
                        llm_embeds = [
                            t.to(device=accelerator.device, dtype=unwrapped_model.dtype)
                            for t in batch["t5"]
                        ]
                    else:
                        t5_keys = [
                            k for k in batch.keys() if k.startswith("varlen_t5_")
                        ]
                        if t5_keys:
                            llm_embeds = batch[t5_keys[0]]
                        else:
                            raise ValueError(
                                "No text encoder outputs found in validation batch."
                            )

                    latents = latents.to(accelerator.device, dtype=torch.float32)
                    if not isinstance(llm_embeds, list):
                        llm_embeds = llm_embeds.to(
                            accelerator.device, dtype=unwrapped_model.dtype
                        )

                    # Deterministic noise generation with unique seed per batch for proper validation
                    with torch.random.fork_rng(devices=[accelerator.device]):
                        use_unique_noise = True
                        if hasattr(args, "use_unique_noise_per_batch"):
                            use_unique_noise = bool(args.use_unique_noise_per_batch)
                        if not use_unique_noise and accelerator.is_main_process:
                            logger.warning(
                                "use_unique_noise_per_batch=False is deprecated for validation and may lead to less reliable metrics; defaulting to True in future releases."
                            )

                        if use_unique_noise:
                            # Use unique seed per batch for proper validation (recommended)
                            batch_seed = fixed_seed + step
                            torch.manual_seed(batch_seed)
                            if accelerator.device.type == "cuda":
                                torch.cuda.manual_seed(batch_seed)
                        else:
                            # Legacy behavior: same seed for all batches (not recommended)
                            torch.manual_seed(fixed_seed)
                            if accelerator.device.type == "cuda":
                                torch.cuda.manual_seed(fixed_seed)
                        noise = torch.randn_like(latents)

                    timesteps = torch.full(
                        (latents.size(0),),
                        current_timestep,
                        device=accelerator.device,
                        dtype=torch.float32,
                    )

                    sigma = get_sigmas(
                        noise_scheduler,
                        timesteps,
                        accelerator.device,
                        latents.dim(),
                        latents.dtype,
                        source="validation",
                    )
                    noisy_model_input = sigma * noise + (1.0 - sigma) * latents

                    # Apply FluxFlow temporal augmentation if enabled (same as training)
                    perturbed_latents_for_target = latents.clone()
                    if self.fluxflow_config.get("enable_fluxflow", False):
                        frame_dim = self.fluxflow_config.get("frame_dim_in_batch", 2)
                        if latents.ndim > frame_dim and latents.shape[frame_dim] > 1:
                            import utils.fluxflow_augmentation as fluxflow_augmentation

                            perturbed_latents_for_target = (
                                fluxflow_augmentation.apply_fluxflow_to_batch(
                                    perturbed_latents_for_target, self.fluxflow_config
                                )
                            )

                    # Control LoRA processing (same as training)
                    control_latents = None
                    if (
                        hasattr(args, "enable_control_lora")
                        and args.enable_control_lora
                    ):
                        if control_signal_processor is not None:
                            # Match training network dtype for consistency
                            network_dtype = noisy_model_input.dtype
                            control_latents = (
                                control_signal_processor.process_control_signal(
                                    args,
                                    accelerator,
                                    batch,
                                    latents,
                                    network_dtype,
                                    vae,
                                )
                            )

                        # Fallback to using image latents as control signal
                        if control_latents is None:
                            control_latents = latents.detach().clone()

                    # Calculate sequence length
                    lat_f, lat_h, lat_w = latents.shape[2:5]
                    seq_len = (
                        lat_f
                        * lat_h
                        * lat_w
                        // (
                            self.config.patch_size[0]
                            * self.config.patch_size[1]
                            * self.config.patch_size[2]
                        )
                    )

                    # Prepare model input with control signal if control LoRA is enabled
                    if (
                        hasattr(args, "enable_control_lora")
                        and args.enable_control_lora
                    ):
                        if control_latents is not None:
                            control_latents = control_latents.to(
                                device=noisy_model_input.device,
                                dtype=noisy_model_input.dtype,
                            )
                            # Concatenate along the channel dimension (dim=1 for BCFHW)
                            model_input = torch.cat(
                                [noisy_model_input, control_latents], dim=1
                            )
                        else:
                            # Fallback: create zero tensor for control part
                            zero_control = torch.zeros_like(noisy_model_input)
                            model_input = torch.cat(
                                [noisy_model_input, zero_control], dim=1
                            )
                    else:
                        model_input = noisy_model_input

                    # Forward pass with consistent model calling
                    pred = unwrapped_model(
                        model_input,
                        t=timesteps,
                        context=llm_embeds,
                        seq_len=seq_len,
                        clip_fea=None,
                    )

                    # Model returns a list of output tensors; stack them for consistent processing
                    pred = torch.stack(pred, dim=0)

                    # Calculate both velocity and direct noise prediction losses

                    # 1. Velocity prediction (existing approach)
                    velocity_target = noise - perturbed_latents_for_target.to(
                        device=accelerator.device, dtype=torch.float32
                    )
                    velocity_loss = torch.nn.functional.mse_loss(
                        pred, velocity_target, reduction="none"
                    )
                    velocity_loss = velocity_loss.mean(
                        dim=[1, 2, 3, 4]
                    )  # Per-batch item mean
                    batch_velocity_losses.append(velocity_loss)
                    # Update sample-weighted accumulators
                    local_velocity_loss_sum = (
                        local_velocity_loss_sum + velocity_loss.sum()
                    )
                    local_velocity_loss_count = (
                        local_velocity_loss_count
                        + torch.tensor(
                            float(velocity_loss.numel()),
                            device=accelerator.device,
                            dtype=torch.float32,
                        )
                    )

                    # 2. Direct noise prediction (new approach - matches your example)
                    direct_noise_target = noise.to(
                        device=accelerator.device, dtype=torch.float32
                    )
                    direct_noise_loss = torch.nn.functional.mse_loss(
                        pred, direct_noise_target, reduction="none"
                    )
                    direct_noise_loss = direct_noise_loss.mean(
                        dim=[1, 2, 3, 4]
                    )  # Per-batch item mean
                    batch_direct_noise_losses.append(direct_noise_loss)
                    local_direct_noise_loss_sum = (
                        local_direct_noise_loss_sum + direct_noise_loss.sum()
                    )
                    local_direct_noise_loss_count = (
                        local_direct_noise_loss_count
                        + torch.tensor(
                            float(direct_noise_loss.numel()),
                            device=accelerator.device,
                            dtype=torch.float32,
                        )
                    )

                # Gather and compute metrics efficiently (refactored approach)
                velocity_metrics = self._compute_and_gather_metrics(
                    accelerator, batch_velocity_losses, "velocity_loss"
                )
                noise_metrics = self._compute_and_gather_metrics(
                    accelerator, batch_direct_noise_losses, "direct_noise_loss"
                )

                # The main process now holds all the computed stats
                if accelerator.is_main_process:
                    timestep_velocity_avg = velocity_metrics.get(
                        "velocity_loss_mean", 0.0
                    )
                    timestep_direct_noise_avg = noise_metrics.get(
                        "direct_noise_loss_mean", 0.0
                    )

                    total_velocity_loss += timestep_velocity_avg
                    total_direct_noise_loss += timestep_direct_noise_avg

                    # Store timestep average losses and SNR for statistics (across all timesteps)
                    all_timestep_avg_velocity_losses.append(timestep_velocity_avg)
                    all_timestep_avg_direct_noise_losses.append(
                        timestep_direct_noise_avg
                    )
                    all_timestep_snrs.append(snr_for_timestep)

                    # Log per-timestep metrics
                    if global_step is not None:
                        # Group detailed timestep metrics under val_timesteps/ for better TensorBoard organization
                        log_dict = {
                            f"val_timesteps/snr_t{current_timestep}": snr_for_timestep
                        }

                        # Dynamically add all computed metrics to the log under timesteps category
                        for key, value in velocity_metrics.items():
                            log_dict[f"val_timesteps/{key}_t{current_timestep}"] = value
                        for key, value in noise_metrics.items():
                            log_dict[f"val_timesteps/{key}_t{current_timestep}"] = value

                        accelerator.log(log_dict, step=global_step)

                    logger.info(
                        f"Timestep {current_timestep} - Velocity loss: {timestep_velocity_avg:.5f}, "
                        f"Direct noise loss: {timestep_direct_noise_avg:.5f}, "
                        f"SNR: {snr_for_timestep:.3f}"
                    )

        # Calculate final average losses across all timesteps (timestep-weighted)
        velocity_final_avg_loss = total_velocity_loss / num_timesteps
        direct_noise_final_avg_loss = total_direct_noise_loss / num_timesteps

        # Calculate sample-weighted (per-example) averages across all batches and timesteps
        velocity_weighted_avg_loss = 0.0
        direct_noise_weighted_avg_loss = 0.0
        try:
            gathered_vel_sum = accelerator.gather_for_metrics(local_velocity_loss_sum)
            gathered_vel_cnt = accelerator.gather_for_metrics(local_velocity_loss_count)
            gathered_dir_sum = accelerator.gather_for_metrics(
                local_direct_noise_loss_sum
            )
            gathered_dir_cnt = accelerator.gather_for_metrics(
                local_direct_noise_loss_count
            )

            if accelerator.is_main_process:
                # Handle potential non-tensor returns defensively
                if not isinstance(gathered_vel_sum, torch.Tensor):
                    gathered_vel_sum = torch.tensor(gathered_vel_sum)
                if not isinstance(gathered_vel_cnt, torch.Tensor):
                    gathered_vel_cnt = torch.tensor(gathered_vel_cnt)
                if not isinstance(gathered_dir_sum, torch.Tensor):
                    gathered_dir_sum = torch.tensor(gathered_dir_sum)
                if not isinstance(gathered_dir_cnt, torch.Tensor):
                    gathered_dir_cnt = torch.tensor(gathered_dir_cnt)

                total_vel_sum = gathered_vel_sum.sum().item()
                total_vel_cnt = max(gathered_vel_cnt.sum().item(), 1.0)
                total_dir_sum = gathered_dir_sum.sum().item()
                total_dir_cnt = max(gathered_dir_cnt.sum().item(), 1.0)

                velocity_weighted_avg_loss = total_vel_sum / total_vel_cnt
                direct_noise_weighted_avg_loss = total_dir_sum / total_dir_cnt
        except Exception as ex:
            if accelerator.is_main_process:
                logger.warning(
                    f"Failed to compute sample-weighted validation metrics: {ex}"
                )

        # Log average losses and correlation statistics to tensorboard under val category
        if global_step is not None and accelerator.is_main_process:
            # Calculate statistics from collected timestep losses
            velocity_loss_std = 0.0
            direct_noise_loss_std = 0.0

            # Only calculate std if we have at least 2 values
            if len(all_timestep_avg_velocity_losses) > 1:
                velocity_loss_std = torch.std(
                    torch.tensor(all_timestep_avg_velocity_losses)
                ).item()
            if len(all_timestep_avg_direct_noise_losses) > 1:
                direct_noise_loss_std = torch.std(
                    torch.tensor(all_timestep_avg_direct_noise_losses)
                ).item()

            # Calculate loss consistency metrics
            velocity_loss_range = 0.0
            direct_noise_loss_range = 0.0

            # Only calculate range if we have at least 2 values
            if len(all_timestep_avg_velocity_losses) > 1:
                velocity_loss_range = max(all_timestep_avg_velocity_losses) - min(
                    all_timestep_avg_velocity_losses
                )
            if len(all_timestep_avg_direct_noise_losses) > 1:
                direct_noise_loss_range = max(
                    all_timestep_avg_direct_noise_losses
                ) - min(all_timestep_avg_direct_noise_losses)

            # Calculate relative performance ratio
            loss_ratio = (
                velocity_final_avg_loss / direct_noise_final_avg_loss
                if direct_noise_final_avg_loss > 0
                else 0.0
            )

            # Calculate Loss/SNR Correlation
            velocity_loss_snr_corr = 0.0
            noise_loss_snr_corr = 0.0
            # Correlation requires at least 2 data points
            if num_timesteps > 1:
                # --- For velocity loss ---
                corr_matrix_vel = np.corrcoef(
                    all_timestep_snrs, all_timestep_avg_velocity_losses
                )
                # Handle NaN case (if one input has zero variance)
                if not np.isnan(corr_matrix_vel[0, 1]):
                    velocity_loss_snr_corr = corr_matrix_vel[0, 1]

                # --- For direct noise loss ---
                corr_matrix_noise = np.corrcoef(
                    all_timestep_snrs, all_timestep_avg_direct_noise_losses
                )
                # Handle NaN case
                if not np.isnan(corr_matrix_noise[0, 1]):
                    noise_loss_snr_corr = corr_matrix_noise[0, 1]

            # Additional across-timestep diagnostics
            velocity_cv_across_timesteps = 0.0
            noise_cv_across_timesteps = 0.0
            if velocity_final_avg_loss > 0:
                velocity_cv_across_timesteps = velocity_loss_std / max(
                    velocity_final_avg_loss, 1e-12
                )
            if direct_noise_final_avg_loss > 0:
                noise_cv_across_timesteps = direct_noise_loss_std / max(
                    direct_noise_final_avg_loss, 1e-12
                )

            # Percentiles of per-timestep average losses (p25/p50/p75)
            vel_p25 = vel_p50 = vel_p75 = 0.0
            noi_p25 = noi_p50 = noi_p75 = 0.0
            if len(all_timestep_avg_velocity_losses) > 0:
                try:
                    vel_tensor = torch.tensor(all_timestep_avg_velocity_losses)
                    vel_p25 = torch.quantile(vel_tensor, 0.25).item()
                    vel_p50 = torch.quantile(vel_tensor, 0.50).item()
                    vel_p75 = torch.quantile(vel_tensor, 0.75).item()
                except Exception:
                    vel_p50 = float(np.median(all_timestep_avg_velocity_losses))
            if len(all_timestep_avg_direct_noise_losses) > 0:
                try:
                    noi_tensor = torch.tensor(all_timestep_avg_direct_noise_losses)
                    noi_p25 = torch.quantile(noi_tensor, 0.25).item()
                    noi_p50 = torch.quantile(noi_tensor, 0.50).item()
                    noi_p75 = torch.quantile(noi_tensor, 0.75).item()
                except Exception:
                    noi_p50 = float(np.median(all_timestep_avg_direct_noise_losses))

            # Best/Worst timesteps by loss
            best_timestep_velocity = worst_timestep_velocity = 0
            best_timestep_direct = worst_timestep_direct = 0
            best_velocity_loss = worst_velocity_loss = 0.0
            best_direct_loss = worst_direct_loss = 0.0
            if len(all_timestep_avg_velocity_losses) > 0:
                vel_losses_np = np.array(all_timestep_avg_velocity_losses)
                best_idx_vel = int(np.argmin(vel_losses_np))
                worst_idx_vel = int(np.argmax(vel_losses_np))
                best_timestep_velocity = int(self.validation_timesteps[best_idx_vel])
                worst_timestep_velocity = int(self.validation_timesteps[worst_idx_vel])
                best_velocity_loss = float(vel_losses_np[best_idx_vel])
                worst_velocity_loss = float(vel_losses_np[worst_idx_vel])
            if len(all_timestep_avg_direct_noise_losses) > 0:
                dir_losses_np = np.array(all_timestep_avg_direct_noise_losses)
                best_idx_dir = int(np.argmin(dir_losses_np))
                worst_idx_dir = int(np.argmax(dir_losses_np))
                best_timestep_direct = int(self.validation_timesteps[best_idx_dir])
                worst_timestep_direct = int(self.validation_timesteps[worst_idx_dir])
                best_direct_loss = float(dir_losses_np[best_idx_dir])
                worst_direct_loss = float(dir_losses_np[worst_idx_dir])

            # Correlation with raw timestep values
            velocity_loss_t_corr = 0.0
            noise_loss_t_corr = 0.0
            if num_timesteps > 1:
                ts_array = np.array(self.validation_timesteps, dtype=np.float32)
                try:
                    corr_vel_t = np.corrcoef(
                        ts_array, np.array(all_timestep_avg_velocity_losses)
                    )
                    if not np.isnan(corr_vel_t[0, 1]):
                        velocity_loss_t_corr = float(corr_vel_t[0, 1])
                except Exception:
                    pass
                try:
                    corr_noise_t = np.corrcoef(
                        ts_array, np.array(all_timestep_avg_direct_noise_losses)
                    )
                    if not np.isnan(corr_noise_t[0, 1]):
                        noise_loss_t_corr = float(corr_noise_t[0, 1])
                except Exception:
                    pass

            accelerator.log(
                {
                    "val/velocity_loss_avg": velocity_final_avg_loss,
                    "val/direct_noise_loss_avg": direct_noise_final_avg_loss,
                    "val/velocity_loss_avg_weighted": velocity_weighted_avg_loss,
                    "val/direct_noise_loss_avg_weighted": direct_noise_weighted_avg_loss,
                    "val/velocity_loss_std": velocity_loss_std,
                    "val/direct_noise_loss_std": direct_noise_loss_std,
                    "val/velocity_loss_range": velocity_loss_range,
                    "val/direct_noise_loss_range": direct_noise_loss_range,
                    "val/loss_ratio": loss_ratio,
                    "val/velocity_loss_snr_correlation": velocity_loss_snr_corr,
                    "val/noise_loss_snr_correlation": noise_loss_snr_corr,
                    # Additional diagnostics
                    "val/velocity_loss_cv_across_timesteps": velocity_cv_across_timesteps,
                    "val/direct_noise_loss_cv_across_timesteps": noise_cv_across_timesteps,
                    "val/velocity_loss_avg_p25": vel_p25,
                    "val/velocity_loss_avg_p50": vel_p50,
                    "val/velocity_loss_avg_p75": vel_p75,
                    "val/direct_noise_loss_avg_p25": noi_p25,
                    "val/direct_noise_loss_avg_p50": noi_p50,
                    "val/direct_noise_loss_avg_p75": noi_p75,
                    "val/best_timestep_velocity": best_timestep_velocity,
                    "val/worst_timestep_velocity": worst_timestep_velocity,
                    "val/best_velocity_loss": best_velocity_loss,
                    "val/worst_velocity_loss": worst_velocity_loss,
                    "val/best_timestep_direct": best_timestep_direct,
                    "val/worst_timestep_direct": worst_timestep_direct,
                    "val/best_direct_loss": best_direct_loss,
                    "val/worst_direct_loss": worst_direct_loss,
                    "val/velocity_loss_timestep_correlation": velocity_loss_t_corr,
                    "val/noise_loss_timestep_correlation": noise_loss_t_corr,
                },
                step=global_step,
            )

        # Switch back to train mode
        unwrapped_model.train()
        unwrapped_model.switch_block_swap_for_training()

        logger.info(
            f"Validation finished. Average velocity loss: {velocity_final_avg_loss:.5f}, Average direct noise loss: {direct_noise_final_avg_loss:.5f}"
        )
        return velocity_final_avg_loss, direct_noise_final_avg_loss

    def sync_validation_epoch(
        self,
        val_dataloader: Any,
        val_epoch_step_sync: Optional[tuple[Any, Any]],
        current_epoch_value: int,
        current_step_value: int,
    ) -> None:
        """Synchronize validation dataset epochs before validation runs."""
        if val_epoch_step_sync is None:
            return

        val_current_epoch, val_current_step = val_epoch_step_sync

        # Update validation epoch/step tracking to match training
        val_current_epoch.value = current_epoch_value
        val_current_step.value = current_step_value

        # Force update validation datasets to prevent unwanted shuffling
        if hasattr(val_dataloader, "dataset"):
            dataset = val_dataloader.dataset
            if hasattr(dataset, "set_current_epoch"):
                dataset.set_current_epoch(
                    current_epoch_value,
                    force_shuffle=False,  # Validation should never shuffle
                    reason="validation_sync",
                )

    def should_validate(
        self,
        args: argparse.Namespace,
        global_step: int,
        val_dataloader: Optional[Any],
        last_validated_step: int,
    ) -> bool:
        """Check if validation should be performed at the current step."""
        if val_dataloader is None:
            return False

        # Check if validation already occurred at this step
        if last_validated_step == global_step:
            return False

        # Check if step-based validation is enabled
        if (
            args.validate_every_n_steps is not None
            and global_step % args.validate_every_n_steps == 0
        ):
            return True

        return False

    def log_validation_results(
        self,
        accelerator: Accelerator,
        val_loss: tuple[float, float],
        global_step: int,
        epoch: Optional[int] = None,
    ) -> None:
        """Log validation results to console only.

        Note: Tensorboard logging is now handled within the validate() method
        to avoid redundancy and provide more detailed per-timestep metrics.
        """
        velocity_loss, direct_noise_loss = val_loss

        if epoch is not None:
            accelerator.print(
                f"[Epoch {epoch}] velocity_loss={velocity_loss:0.5f}, direct_noise_loss={direct_noise_loss:0.5f}"
            )
        else:
            accelerator.print(
                f"[Step {global_step}] velocity_loss={velocity_loss:0.5f}, direct_noise_loss={direct_noise_loss:0.5f}"
            )

    def validate_with_progress(
        self,
        accelerator: Accelerator,
        transformer: Any,
        val_dataloader: Any,
        noise_scheduler: FlowMatchDiscreteScheduler,
        args: argparse.Namespace,
        control_signal_processor: Optional[Any] = None,
        vae: Optional[Any] = None,
        global_step: Optional[int] = None,
        show_progress: bool = True,
    ) -> tuple[float, float]:
        """Run validation with optional progress bar.

        Args:
            global_step: Current training step for logging per-timestep metrics
            show_progress: Whether to show progress bars
        """
        return self.validate(
            accelerator,
            transformer,
            val_dataloader,
            noise_scheduler,
            args,
            control_signal_processor,
            vae,
            global_step,
            show_progress,
        )
</file>

<file path="core/wan_network_trainer.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

"""Refactored WAN network trainer orchestrator.

This is the main orchestrator class that coordinates all the refactored training components.
Much cleaner and more maintainable than the original monolithic implementation.
"""

import argparse
import json
import math
import os
import random
import time
import gc
from multiprocessing import Value
from typing import Any, Dict, List, Optional
import torch
from tqdm import tqdm
from accelerate.utils import set_seed

import utils.fluxflow_augmentation as fluxflow_augmentation
from dataset import config_utils
from dataset.config_utils import BlueprintGenerator, ConfigSanitizer
from modules.scheduling_flow_match_discrete import FlowMatchDiscreteScheduler
from utils.train_utils import (
    collator_class,
    prepare_accelerator,
    clean_memory_on_device,
    should_sample_images,
    LossRecorder,
)
from utils import model_utils
from wan.configs.config import WAN_CONFIGS
from wan.modules.vae import WanVAE
from utils.tread import TREADRouter

# Import all our refactored components
from core.config import TrainerConfig
from core.optimizer_manager import OptimizerManager
from core.model_manager import ModelManager
from core.sampling_manager import SamplingManager
from core.control_signal_processor import ControlSignalProcessor
from core.checkpoint_manager import CheckpointManager
from core.training_core import TrainingCore
from core.vae_training_core import VaeTrainingCore
from reward.reward_training_core import RewardTrainingCore
from core.repa_helper import RepaHelper
from scheduling.timestep_utils import (
    initialize_timestep_distribution,
    get_noisy_model_input_and_timesteps,
)
import logging
from common.logger import get_logger
from common.performance_logger import snapshot_gpu_memory

logger = get_logger(__name__, level=logging.INFO)


class WanNetworkTrainer:
    """Main orchestrator class for WAN network training using refactored components."""

    def __init__(self):
        self.blocks_to_swap = None
        self.fluxflow_config = {}
        self.config = None
        # Store original config file content for saving with training states
        self.original_config_content = None
        self.original_config_path = None

        # Initialize all component managers
        self.trainer_config = TrainerConfig()
        self.optimizer_manager = OptimizerManager()
        self.model_manager = ModelManager()
        self.sampling_manager = None  # Will be initialized with config
        self.control_signal_processor = ControlSignalProcessor()
        self.checkpoint_manager = CheckpointManager()
        self.training_core = None  # Will be initialized with config
        self.vae_training_core = None  # Will be initialized for VAE training

    def handle_model_specific_args(self, args: argparse.Namespace) -> None:
        """Handle model-specific arguments and configuration."""
        self.pos_embed_cache = {}
        self.config = WAN_CONFIGS[args.task]

        # Call model manager's handle_model_specific_args to handle downloads
        self.model_manager.handle_model_specific_args(args)

        # Get the dit_dtype from the model manager after downloads
        dit_dtype = self.model_manager.get_dit_dtype()

        if dit_dtype == torch.float16:
            assert args.mixed_precision in [
                "fp16",
                "no",
            ], "DiT weights are in fp16, mixed precision must be fp16 or no"
        elif dit_dtype == torch.bfloat16:
            assert args.mixed_precision in [
                "bf16",
                "no",
            ], "DiT weights are in bf16, mixed precision must be bf16 or no"

        if args.fp8_scaled and dit_dtype.itemsize == 1:
            raise ValueError(
                "DiT weights is already in fp8 format, cannot scale to fp8. Please use fp16/bf16 weights"
            )

        # dit_dtype cannot be fp8, so we select the appropriate dtype
        if dit_dtype.itemsize == 1:
            dit_dtype = (
                torch.float16 if args.mixed_precision == "fp16" else torch.bfloat16
            )

        args.dit_dtype = model_utils.dtype_to_str(dit_dtype)

        self.default_guidance_scale = 1.0  # not used
        self.fluxflow_config = fluxflow_augmentation.get_fluxflow_config_from_args(args)

        # Initialize training cores with config
        self.training_core = TrainingCore(self.config, self.fluxflow_config)
        self.vae_training_core = VaeTrainingCore(self.config)
        self.reward_training_core = RewardTrainingCore(self.config)

        # Initialize sampling manager now that we have config
        self.sampling_manager = SamplingManager(
            self.config, self.default_guidance_scale
        )

    def show_timesteps(self, args: argparse.Namespace) -> None:
        """Show timesteps distribution for debugging purposes."""
        N_TRY = 100000
        BATCH_SIZE = 1000
        CONSOLE_WIDTH = 64
        N_TIMESTEPS_PER_LINE = 25

        noise_scheduler = FlowMatchDiscreteScheduler(
            shift=args.discrete_flow_shift, reverse=True, solver="euler"
        )

        latents = torch.zeros(BATCH_SIZE, 1, 1, 1, 1, dtype=torch.float16)
        noise = torch.ones_like(latents)

        # sample timesteps
        sampled_timesteps = [0] * 1000  # Use fixed size instead of config access
        for i in tqdm(range(N_TRY // BATCH_SIZE)):
            # we use noise=1, so returned noisy_model_input is same as timestep
            # Initialize timestep distribution if needed

            # Ensure training_core is initialized
            if self.training_core is None:
                raise RuntimeError(
                    "Training core not initialized. Call handle_model_specific_args first."
                )

            initialize_timestep_distribution(
                args, self.training_core.timestep_distribution
            )

            actual_timesteps, _, _ = get_noisy_model_input_and_timesteps(
                args,
                noise,
                latents,
                noise_scheduler,
                torch.device("cpu"),
                torch.float16,
                self.training_core.timestep_distribution,
            )
            actual_timesteps = actual_timesteps[:, 0, 0, 0, 0] * 1000
            for t in actual_timesteps:
                t = int(t.item())
                if 0 <= t < len(sampled_timesteps):
                    sampled_timesteps[t] += 1

        # sample weighting
        sampled_weighting = [0] * 1000  # Use fixed size
        for i in tqdm(range(len(sampled_weighting))):
            timesteps = torch.tensor([i + 1], device="cpu")
            from utils.train_utils import compute_loss_weighting_for_sd3

            weighting = compute_loss_weighting_for_sd3(
                args.weighting_scheme, noise_scheduler, timesteps, "cpu", torch.float16
            )
            if weighting is None:
                weighting = torch.tensor(1.0, device="cpu")
            elif torch.isinf(weighting).any():
                weighting = torch.tensor(1.0, device="cpu")
            sampled_weighting[i] = weighting.item()  # type: ignore

        # show results
        if args.show_timesteps == "image":
            # Recompute using shared helper for consistency
            try:
                from scheduling.timestep_utils import (
                    compute_sampled_timesteps_and_weighting,
                )

                # Re-assert training_core is available for static typing
                training_core = self.training_core
                if training_core is None:
                    raise RuntimeError(
                        "Training core not initialized. Call handle_model_specific_args first."
                    )
                sampled_timesteps, sampled_weighting = (
                    compute_sampled_timesteps_and_weighting(
                        args,
                        training_core.timestep_distribution,
                        noise_scheduler,
                        num_samples=100000,
                        batch_size=1000,
                    )
                )
            except Exception:
                pass

            # show timesteps with matplotlib (non-interactive backend)
            import matplotlib

            matplotlib.use("Agg")
            import matplotlib.pyplot as plt

            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.bar(range(len(sampled_timesteps)), sampled_timesteps, width=1.0)
            plt.title("Sampled timesteps")
            plt.xlabel("Timestep")
            plt.ylabel("Count")

            plt.subplot(1, 2, 2)
            plt.bar(range(len(sampled_weighting)), sampled_weighting, width=1.0)
            plt.title("Sampled loss weighting")
            plt.xlabel("Timestep")
            plt.ylabel("Weighting")

            plt.tight_layout()
            # Also log this figure to TensorBoard if configured
            try:
                log_dir = getattr(args, "logging_dir", None)
                log_with = str(getattr(args, "log_with", "tensorboard")).lower()
                if log_dir and log_with in ("tensorboard", "all"):
                    os.makedirs(log_dir, exist_ok=True)
                    try:
                        from tensorboardX import SummaryWriter as _SummaryWriter  # type: ignore
                    except ImportError:
                        from torch.utils.tensorboard.writer import SummaryWriter as _SummaryWriter  # type: ignore
                    fig = plt.gcf()
                    writer = _SummaryWriter(log_dir=log_dir)
                    try:
                        writer.add_figure(
                            "timestep/show_timesteps_chart", fig, global_step=0
                        )
                        logger.info(
                            "Logged timestep distribution figure to TensorBoard"
                        )
                    finally:
                        writer.close()
            except Exception as _tb_err:
                logger.debug(
                    f"Failed to log show_timesteps figure to TensorBoard: {_tb_err}"
                )
            plt.show()

        else:
            import numpy as np

            sampled_timesteps = np.array(sampled_timesteps)
            sampled_weighting = np.array(sampled_weighting)

            # average per line
            sampled_timesteps = sampled_timesteps.reshape(
                -1, N_TIMESTEPS_PER_LINE
            ).mean(axis=1)
            sampled_weighting = sampled_weighting.reshape(
                -1, N_TIMESTEPS_PER_LINE
            ).mean(axis=1)

            max_count = max(sampled_timesteps)
            print(f"Sampled timesteps: max count={max_count}")
            for i, t in enumerate(sampled_timesteps):
                line = (
                    f"{(i)*N_TIMESTEPS_PER_LINE:4d}-{(i+1)*N_TIMESTEPS_PER_LINE-1:4d}: "
                )
                line += "#" * int(t / max_count * CONSOLE_WIDTH)
                print(line)

            max_weighting = max(sampled_weighting)
            print(f"Sampled loss weighting: max weighting={max_weighting}")
            for i, w in enumerate(sampled_weighting):
                line = f"{i*N_TIMESTEPS_PER_LINE:4d}-{(i+1)*N_TIMESTEPS_PER_LINE-1:4d}: {w:8.2f} "
                line += "#" * int(w / max_weighting * CONSOLE_WIDTH)
                print(line)

    def train(self, args: argparse.Namespace) -> None:
        """Main training orchestration method"""

        trace_memory: bool = bool(getattr(args, "trace_memory", False))
        if trace_memory:
            snapshot_gpu_memory("train/start")

        # ========== Validation and Setup ==========
        # Check required arguments
        if args.dataset_config is None:
            raise ValueError("dataset_config is required")
        if args.dit is None:
            raise ValueError("path to DiT model is required")
        if args.output_dir is None or not args.output_dir.strip():
            raise ValueError("output_dir is required and cannot be empty")
        if args.output_name is None or not args.output_name.strip():
            raise ValueError("output_name is required and cannot be empty")
        if args.log_tracker_config is not None and not args.log_tracker_config.strip():
            logger.warning(f"log_tracker_config is empty, setting to None")
            args.log_tracker_config = None
        assert not args.fp8_scaled or args.fp8_base, "fp8_scaled requires fp8_base"

        if args.sage_attn:
            raise ValueError(
                "SageAttention doesn't support training currently. Please use `--sdpa` or `--xformers` etc. instead."
            )

        if args.fp16_accumulation:
            logger.info("Enabling FP16 accumulation")
            if hasattr(torch.backends.cuda.matmul, "allow_fp16_accumulation"):
                logger.warning(
                    "üí° Note: fp16 accumulation may degrade training quality"
                )
                torch.backends.cuda.matmul.allow_fp16_accumulation = True
            else:
                logger.warning(
                    "üö® FP16 accumulation not available, requires at least PyTorch 2.7.0"
                )

        # Handle model-specific arguments
        self.handle_model_specific_args(args)

        # Show timesteps for debugging if requested
        if args.show_timesteps:
            self.show_timesteps(args)
            return

        # ========== Session Setup ==========
        session_id = random.randint(0, 2**32)
        training_started_at = time.time()

        if args.seed is None:
            args.seed = random.randint(0, 2**32)
        set_seed(args.seed)

        # ========== Dataset Setup ==========
        blueprint_generator = BlueprintGenerator(ConfigSanitizer())
        logger.info(f"Load dataset config from {args.dataset_config}")
        user_config = config_utils.load_user_config(args.dataset_config)
        blueprint = blueprint_generator.generate(user_config, args)

        # Conflict handling: prefer precomputed timesteps when both are requested
        try:
            if (
                getattr(args, "use_precomputed_timesteps", False)
                and getattr(args, "num_timestep_buckets", None) is not None
            ):
                logger.warning(
                    "üí° Both use_precomputed_timesteps and num_timestep_buckets are set; "
                    "preferring precomputed distribution and ignoring per-epoch timestep buckets."
                )
        except Exception:
            pass

        train_dataset_group = config_utils.generate_dataset_group_by_blueprint(
            blueprint.train_dataset_group,
            training=True,
            enable_control_lora=getattr(args, "enable_control_lora", False),
            prior_loss_weight=getattr(args, "prior_loss_weight", 1.0),
            num_timestep_buckets=(
                None
                if getattr(args, "use_precomputed_timesteps", False)
                else getattr(args, "num_timestep_buckets", None)
            ),
        )

        # Log regularization information
        from utils.regularization_utils import (
            log_regularization_info,
            validate_regularization_config,
        )

        log_regularization_info(train_dataset_group)
        validate_regularization_config(args)

        # Only create validation dataset group if there are validation datasets
        val_dataset_group = None
        if len(blueprint.val_dataset_group.datasets) > 0:
            val_dataset_group = config_utils.generate_dataset_group_by_blueprint(
                blueprint.val_dataset_group,
                training=True,
                enable_control_lora=getattr(args, "enable_control_lora", False),
                prior_loss_weight=getattr(args, "prior_loss_weight", 1.0),
                num_timestep_buckets=(
                    None
                    if getattr(args, "use_precomputed_timesteps", False)
                    else getattr(args, "num_timestep_buckets", None)
                ),
            )

        # Optionally wrap with self-correction hybrid group (delegated to helper)
        try:
            from self_correction.setup import maybe_wrap_with_self_correction

            train_dataset_group = maybe_wrap_with_self_correction(
                args,
                blueprint_generator,
                user_config,
                self.config,
                train_dataset_group,
            )
        except Exception as _sc_wrap_err:
            logger.warning(f"Self-correction hybrid setup skipped: {_sc_wrap_err}")

        current_epoch = Value("i", 0)
        current_step = Value("i", 0)
        ds_for_collator = (
            train_dataset_group if args.max_data_loader_n_workers == 0 else None
        )
        collator = collator_class(current_epoch, current_step, ds_for_collator)

        # ========== Accelerator Setup ==========
        # Reset Accelerator state to avoid errors if an Accelerator was created earlier in the same Python process (e.g. during latent/text caching)
        try:
            from accelerate.state import AcceleratorState  # type: ignore

            # Private API ‚Äì safe to use here because we are in a controlled environment
            AcceleratorState._reset_state()  # pylint: disable=protected-access
            logger.debug("Accelerator state reset successfully")
        except Exception as reset_err:  # pragma: no cover
            # If reset fails, we continue; an error will surface when creating Accelerator if truly incompatible
            logger.debug(f"Unable to reset Accelerator state: {reset_err}")

        logger.info("preparing accelerator")
        accelerator = prepare_accelerator(args)
        is_main_process = accelerator.is_main_process
        if trace_memory:
            snapshot_gpu_memory("train/after_accelerator")

        # ========== Precision Setup ==========
        weight_dtype = torch.float32
        if args.mixed_precision == "fp16":
            weight_dtype = torch.float16
        elif args.mixed_precision == "bf16":
            weight_dtype = torch.bfloat16

        dit_dtype = (
            torch.bfloat16
            if args.dit_dtype is None
            else model_utils.str_to_dtype(args.dit_dtype)
        )
        dit_weight_dtype = (
            (None if args.fp8_scaled else torch.float8_e4m3fn)
            if args.fp8_base
            else dit_dtype
        )
        logger.info(f"DiT precision: {dit_dtype}, weight precision: {dit_weight_dtype}")

        vae_dtype = (
            torch.float16
            if args.vae_dtype is None
            else model_utils.str_to_dtype(args.vae_dtype)
        )

        # ========== Model Loading ==========
        sample_parameters = None
        vae = None

        # Load VAE when it will be needed ------------------------------------------------
        need_vae_for_control = (
            hasattr(args, "enable_control_lora") and args.enable_control_lora
        )
        need_vae_for_sampling = args.sample_prompts is not None

        if need_vae_for_sampling and self.sampling_manager is not None:
            sample_parameters = self.sampling_manager.process_sample_prompts(
                args, accelerator, args.sample_prompts
            )

        # Decide VAE loading strategy
        if need_vae_for_control:
            # Hard fail early if Control LoRA is enabled without a VAE path
            if not getattr(args, "vae", None) or not str(args.vae).strip():
                raise ValueError(
                    "Control LoRA requires a VAE checkpoint. Set 'vae' in the config when enable_control_lora is True."
                )

            # Control-LoRA requires an actual VAE during training ‚Äì load it now.
            vae = self.model_manager.load_vae(
                args, vae_dtype=vae_dtype, vae_path=args.vae
            )
            if vae is None:
                raise RuntimeError(
                    "Failed to load VAE for Control LoRA. Please verify the 'vae' path in the config."
                )
            vae.requires_grad_(False)
            vae.eval()
            # Expose to control-signal processor so it can encode control latents
            # Store on processor for later use without breaking static typing
            setattr(self.control_signal_processor, "vae", vae)
        else:
            # We may still need a VAE later for sampling; configure lazy load
            vae = None

        # Provide VAE config to SamplingManager for lazy loading when necessary
        self._vae_config = {
            "args": args,
            "vae_dtype": vae_dtype,
            "vae_path": args.vae,
        }
        if self.sampling_manager is not None:
            self.sampling_manager.set_vae_config(self._vae_config)

        # Load DiT model
        blocks_to_swap = args.blocks_to_swap if args.blocks_to_swap else 0
        self.blocks_to_swap = blocks_to_swap
        loading_device = "cpu" if blocks_to_swap > 0 else accelerator.device

        logger.info(f"Loading DiT model from {args.dit}")
        attn_mode = self.model_manager.get_attention_mode(args)
        transformer, dual_model_manager = self.model_manager.load_transformer(
            accelerator,
            args,
            args.dit,
            attn_mode,
            args.split_attn,
            loading_device,
            dit_weight_dtype,
            self.config,
        )
        if trace_memory:
            snapshot_gpu_memory("train/after_transformer_load")

        transformer.eval()
        transformer.requires_grad_(False)

        # Configure TREAD routing if enabled and routes provided
        if getattr(args, "enable_tread", False):
            tread_cfg = getattr(args, "tread_config", None)
            routes = tread_cfg.get("routes") if isinstance(tread_cfg, dict) else None
            if routes and len(routes) > 0:
                try:
                    router = TREADRouter(
                        seed=getattr(args, "seed", 42) or 42,
                        device=accelerator.device,
                    )
                    # set on the raw module (not wrapped yet)
                    transformer.set_router(router, routes)  # type: ignore
                    # Store tread mode on the model for runtime gating
                    try:
                        setattr(
                            transformer,
                            "_tread_mode",
                            getattr(args, "tread_mode", "full"),
                        )
                    except Exception:
                        pass
                    logger.info("TREAD routing enabled with %d route(s)", len(routes))
                except Exception as e:
                    logger.warning(f"Failed to enable TREAD routing: {e}")
            else:
                logger.info(
                    "enable_tread is True but no routes configured; TREAD disabled"
                )

        if blocks_to_swap > 0:
            logger.info(
                f"enable swap {blocks_to_swap} blocks to CPU from device: {accelerator.device}"
            )
            transformer.enable_block_swap(
                blocks_to_swap, accelerator.device, supports_backward=True
            )
            transformer.move_to_device_except_swap_blocks(accelerator.device)

        # ========== Network Setup ==========
        network = self.model_manager.create_network(
            args, transformer, vae, self.control_signal_processor
        )
        controlnet = getattr(self.model_manager, "controlnet", None)
        if network is None:
            return

        # ========== Verbose Network Information ==========
        if getattr(args, "verbose_network", False):
            self._log_detailed_network_info(network, transformer, args)

        # ========== Optimizer and Scheduler Setup ==========
        # Check if we're using Lycoris network (which doesn't accept input_lr_scale)
        # Lycoris networks have a different prepare_optimizer_params signature that doesn't
        # include input_lr_scale parameter, unlike our custom network implementations
        is_lycoris_network = args.network_module == "lycoris.kohya"

        if is_lycoris_network:
            trainable_params, lr_descriptions = network.prepare_optimizer_params(
                unet_lr=args.learning_rate,
            )
        else:
            trainable_params, lr_descriptions = network.prepare_optimizer_params(
                unet_lr=args.learning_rate,
                input_lr_scale=getattr(args, "input_lr_scale", 1.0),
            )

        # If ControlNet is enabled, append its optimizer params
        if (
            hasattr(args, "enable_controlnet")
            and args.enable_controlnet
            and controlnet is not None
        ):
            cn_params, cn_desc = controlnet.prepare_optimizer_params(
                unet_lr=args.learning_rate
            )
            if cn_params:
                trainable_params.extend(cn_params)
                lr_descriptions.extend(cn_desc)

        # Add patch embedding parameters for control LoRA, ONLY if it's enabled.
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            logger.info("Adding patch embedding parameters for control LoRA")
            if hasattr(transformer, "patch_embedding"):
                patch_params = list(transformer.patch_embedding.parameters())
                if patch_params:
                    # For Control LoRA, the patch_embedding layer is replaced and should be trained
                    trainable_params.append(
                        {
                            "params": patch_params,
                            "lr": args.learning_rate
                            * getattr(args, "input_lr_scale", 1.0),
                        }
                    )
                    lr_descriptions.append("patch_embedding")

        (
            optimizer_name,
            optimizer_args,
            optimizer,
            optimizer_train_fn,
            optimizer_eval_fn,
        ) = self.optimizer_manager.get_optimizer(args, transformer, trainable_params)

        # ========== DataLoader Setup ==========
        n_workers = min(args.max_data_loader_n_workers, os.cpu_count() or 1)

        train_dataloader = torch.utils.data.DataLoader(
            train_dataset_group,
            batch_size=1,
            shuffle=True,
            collate_fn=collator,
            num_workers=n_workers,
            persistent_workers=args.persistent_data_loader_workers,
        )

        val_dataloader = None
        if val_dataset_group is not None:
            # Use separate epoch/step tracking for validation to prevent cross-contamination
            val_current_epoch = Value("i", 0)
            val_current_step = Value("i", 0)
            val_collator = collator_class(
                val_current_epoch, val_current_step, ds_for_collator
            )
            val_dataloader = torch.utils.data.DataLoader(
                val_dataset_group,
                batch_size=1,
                shuffle=False,
                collate_fn=val_collator,
                num_workers=args.max_data_loader_n_workers,
            )

        # ========== Training Parameters ==========
        if args.max_train_epochs is not None:
            args.max_train_steps = args.max_train_epochs * math.ceil(
                len(train_dataloader)
                / accelerator.num_processes
                / args.gradient_accumulation_steps
            )
            accelerator.print(
                f"override steps. steps for {args.max_train_epochs} epochs is: {args.max_train_steps}"
            )

        train_dataset_group.set_max_train_steps(args.max_train_steps)

        lr_scheduler = self.optimizer_manager.get_lr_scheduler(
            args, optimizer, accelerator.num_processes
        )

        # ========== Model Preparation ==========
        network_dtype = torch.float32
        args.full_fp16 = args.full_bf16 = False

        if dit_weight_dtype != dit_dtype and dit_weight_dtype is not None:
            logger.info(f"casting model to {dit_weight_dtype}")
            transformer.to(dit_weight_dtype)

        if blocks_to_swap > 0:
            transformer = accelerator.prepare(
                transformer, device_placement=[not blocks_to_swap > 0]
            )
            accelerator.unwrap_model(transformer).move_to_device_except_swap_blocks(
                accelerator.device
            )
            accelerator.unwrap_model(transformer).prepare_block_swap_before_forward()
        else:
            transformer = accelerator.prepare(transformer)
        if trace_memory:
            snapshot_gpu_memory("train/after_prepare_transformer")

        if controlnet is not None:
            network, controlnet, optimizer, train_dataloader, lr_scheduler = (
                accelerator.prepare(
                    network, controlnet, optimizer, train_dataloader, lr_scheduler
                )
            )
            # update prepared instance back to model_manager
            self.model_manager.controlnet = controlnet
        else:
            network, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
                network, optimizer, train_dataloader, lr_scheduler
            )
        training_model = network

        if args.gradient_checkpointing:
            transformer.train()
        else:
            transformer.eval()

        accelerator.unwrap_model(network).prepare_grad_etc(transformer)

        # ========== Checkpoint Hooks ==========
        self.checkpoint_manager.register_hooks(accelerator, args, transformer, network)

        # Resume from checkpoint if specified
        restored_step = self.checkpoint_manager.resume_from_local_if_specified(
            accelerator, args, transformer, self.control_signal_processor
        )

        # ========== Training Setup ==========
        num_update_steps_per_epoch = math.ceil(
            len(train_dataloader) / args.gradient_accumulation_steps
        )
        num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

        accelerator.print(f"üöÄ Starting WAN Network Training")
        accelerator.print(f"üìä Training Configuration:")
        accelerator.print(
            f"   ‚Ä¢ Total training items: {train_dataset_group.num_train_items:,}"
        )
        accelerator.print(f"   ‚Ä¢ Batches per epoch: {len(train_dataloader):,}")
        accelerator.print(f"   ‚Ä¢ Number of epochs: {num_train_epochs:,}")
        accelerator.print(f"   ‚Ä¢ Total optimization steps: {args.max_train_steps:,}")
        accelerator.print(
            f"   ‚Ä¢ Gradient accumulation steps: {args.gradient_accumulation_steps}"
        )
        accelerator.print(
            f"   ‚Ä¢ Effective batch size: {args.gradient_accumulation_steps * sum(d.batch_size for d in train_dataset_group.datasets):,}"
        )
        accelerator.print(
            f"   ‚Ä¢ Batch sizes per device: {', '.join([str(d.batch_size) for d in train_dataset_group.datasets])}"
        )

        accelerator.print(f"‚öôÔ∏è  Optimizer & Learning Rate:")
        accelerator.print(f"   ‚Ä¢ Optimizer: {optimizer_name}")
        if optimizer_args:
            accelerator.print(f"   ‚Ä¢ Optimizer args: {optimizer_args}")
        accelerator.print(f"   ‚Ä¢ Base learning rate: {args.learning_rate:.2e}")
        if hasattr(args, "lr_scheduler") and args.lr_scheduler:
            accelerator.print(f"   ‚Ä¢ LR scheduler: {args.lr_scheduler}")

        accelerator.print(f"üîß Model Configuration:")
        accelerator.print(f"   ‚Ä¢ Model dtype: {dit_dtype}")
        accelerator.print(
            f"   ‚Ä¢ Gradient checkpointing: {'enabled' if args.gradient_checkpointing else 'disabled'}"
        )
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            accelerator.print(f"   ‚Ä¢ Control LoRA: enabled")
        if blocks_to_swap > 0:
            accelerator.print(f"   ‚Ä¢ Block swapping: {blocks_to_swap} blocks")

        accelerator.print(f"üíæ Checkpoint & Logging:")
        accelerator.print(f"   ‚Ä¢ Save every {args.save_every_n_steps:,} steps")
        if hasattr(args, "sample_every_n_steps") and args.sample_every_n_steps:
            accelerator.print(f"   ‚Ä¢ Sample every {args.sample_every_n_steps:,} steps")
        if hasattr(args, "log_every_n_steps") and args.log_every_n_steps:
            accelerator.print(f"   ‚Ä¢ Log every {args.log_every_n_steps:,} steps")

        # ========== Metadata Setup ==========
        metadata = self.trainer_config.create_training_metadata(
            args,
            session_id,
            training_started_at,
            train_dataset_group,
            num_train_epochs,
            len(train_dataloader),
            optimizer_name,
            optimizer_args,
        )

        minimum_metadata = {}
        from utils.train_utils import TAKENOKO_METADATA_MINIMUM_KEYS

        for key in TAKENOKO_METADATA_MINIMUM_KEYS:
            if key in metadata:
                minimum_metadata[key] = metadata[key]

        # ========== Tracker Setup ==========
        if accelerator.is_main_process:
            init_kwargs = {}
            if args.log_tracker_config is not None and args.log_tracker_config.strip():
                import toml

                try:
                    init_kwargs = toml.load(args.log_tracker_config)
                except Exception:
                    init_kwargs = {}
            try:
                accelerator.init_trackers(
                    (
                        "network_train"
                        if args.log_tracker_name is None
                        else args.log_tracker_name
                    ),
                    config=self.trainer_config.get_sanitized_config_or_none(args),
                    init_kwargs=init_kwargs,
                )
            except Exception as e:
                # Continue without trackers
                logger.warning(
                    f"‚ö†Ô∏è  Tracker initialization failed, continuing without logging: {e}"
                )
                pass

            # Non-intrusive registration of TensorBoard metric descriptions
            try:
                from utils.tensorboard_utils import (
                    get_default_metric_descriptions,
                    register_metric_descriptions_non_intrusive,
                )

                tag_to_desc = get_default_metric_descriptions()
                register_metric_descriptions_non_intrusive(
                    accelerator, args, tag_to_desc
                )
            except Exception:
                logger.warning(
                    "‚ö†Ô∏è  TensorBoard metric descriptions registration failed, continuing without metric descriptions."
                )
                pass

        # ========== Training Loop Setup ==========
        global_step = restored_step if restored_step is not None else 0
        progress_bar = tqdm(
            range(args.max_train_steps),
            initial=global_step,  # Ensure progress bar resumes at correct step
            smoothing=0,
            disable=not accelerator.is_local_main_process,
            desc="steps",
        )

        noise_scheduler = FlowMatchDiscreteScheduler(
            shift=args.discrete_flow_shift, reverse=True, solver="euler"
        )

        loss_recorder = LossRecorder()
        del train_dataset_group

        # Create save/remove model functions
        save_model = self.checkpoint_manager.create_save_model_function(
            args, metadata, minimum_metadata, dit_dtype
        )
        remove_model = self.checkpoint_manager.create_remove_model_function(args)

        # Prepare validation epoch/step sync objects for training core
        val_epoch_step_sync = None
        if val_dataloader is not None:
            val_epoch_step_sync = (val_current_epoch, val_current_step)

        # ========== Initial Sampling ==========
        # Only run initial sampling if:
        # 1. We should sample at this step, AND
        # 2. We're NOT resuming from a step where sampling would have already occurred
        should_do_initial_sampling = should_sample_images(args, global_step, epoch=0)
        if restored_step is not None and should_do_initial_sampling:
            # We're resuming and sampling would occur - check if sampling happened before save
            # If save_every_n_steps matches sample_every_n_steps and we're at that step,
            # sampling would have occurred before the checkpoint was saved
            if (
                args.save_every_n_steps is not None
                and args.sample_every_n_steps is not None
                and global_step % args.save_every_n_steps == 0
                and global_step % args.sample_every_n_steps == 0
            ):
                logger.info(
                    f"Skipping initial sampling at step {global_step} - sampling already occurred before checkpoint was saved"
                )
                should_do_initial_sampling = False

        if should_do_initial_sampling:
            try:
                if optimizer_eval_fn:
                    optimizer_eval_fn()
                # Prefer the already-loaded VAE (if any); otherwise load lazily
                sampling_vae = (
                    vae
                    if vae is not None
                    else (
                        self.sampling_manager._load_vae_lazy()
                        if self.sampling_manager
                        else None
                    )
                )
                if sampling_vae is None:
                    logger.warning("No VAE available for sampling, skipping...")
                    return
                self.sampling_manager.sample_images(  # type: ignore
                    accelerator,
                    args,
                    0,
                    global_step,
                    sampling_vae,
                    transformer,
                    sample_parameters,
                    dit_dtype,
                    dual_model_manager=dual_model_manager,
                )
                # Unload only if we loaded lazily here
                if sampling_vae is not vae and self.sampling_manager is not None:
                    self.sampling_manager._unload_vae(sampling_vae)
                if optimizer_train_fn:
                    optimizer_train_fn()
            except Exception as e:
                logger.error(f"üí• Initial sampling failed: {e}")
                raise

        if len(accelerator.trackers) > 0:
            try:
                accelerator.log({}, step=0)
            except Exception as e:
                logger.error(f"üí• Accelerator logging failed: {e}")
                raise

        # ========== Main Training Loop ==========
        logger.info(f"DiT dtype: {transformer.dtype}, device: {transformer.device}")
        clean_memory_on_device(accelerator.device)

        # Determine if this is VAE training
        is_vae_training = args.network_module == "networks.vae_wan"

        enable_reward_training = bool(getattr(args, "enable_reward_lora", False))

        if is_vae_training:
            logger.info("üé® Starting VAE training mode")
            # Run VAE training loop
            global_step, network = self.vae_training_core.run_vae_training_loop(  # type: ignore
                args=args,
                accelerator=accelerator,
                vae=vae,  # Pass the VAE as the main model to train
                network=network,
                training_model=training_model,
                optimizer=optimizer,
                lr_scheduler=lr_scheduler,
                lr_descriptions=lr_descriptions,
                train_dataloader=train_dataloader,
                val_dataloader=val_dataloader,
                network_dtype=network_dtype,
                num_train_epochs=num_train_epochs,
                global_step=global_step,
                progress_bar=progress_bar,
                metadata=metadata,
                loss_recorder=loss_recorder,
                current_epoch=current_epoch,
                current_step=current_step,
                optimizer_train_fn=optimizer_train_fn,
                optimizer_eval_fn=optimizer_eval_fn,
                save_model=save_model,
                remove_model=remove_model,
                is_main_process=is_main_process,
            )
        elif enable_reward_training:
            logger.info("üèÜ Starting Reward LoRA training mode")
            # Ensure VAE is available for decoding
            if vae is None:
                # Attempt lazy load (reusing SamplingManager path)
                if self.sampling_manager is not None:
                    vae = self.sampling_manager._load_vae_lazy()
                if vae is None:
                    raise ValueError(
                        "Reward training requires a VAE checkpoint (set 'vae' in config)"
                    )

            # Run reward training loop
            global_step, network = self.reward_training_core.run_reward_training_loop(
                args=args,
                accelerator=accelerator,
                transformer=transformer,
                network=network,
                optimizer=optimizer,
                lr_scheduler=lr_scheduler,
                trainable_params=trainable_params,
                save_model=save_model,
                remove_model=remove_model,
                vae=vae,
                is_main_process=is_main_process,
                global_step=global_step,
            )
        else:
            logger.info("ü§ñ Starting DiT training mode")

            # ========== REPA Helper Setup ==========
            repa_helper = None
            if hasattr(args, "enable_repa") and args.enable_repa:
                logger.info("REPA is enabled. Setting up the helper module.")
                repa_helper = RepaHelper(transformer, args)
                repa_helper.setup_hooks()

                # Prepare repa_helper with accelerator
                repa_helper = accelerator.prepare(repa_helper)

            # Run the main training loop using TrainingCore
            # Attach a self-correction manager instance if enabled so the core can call it
            try:
                from self_correction.setup import (
                    maybe_attach_self_correction_manager,
                )

                maybe_attach_self_correction_manager(
                    args,
                    accelerator,
                    self.sampling_manager,
                    blueprint,
                    vae_dtype,
                    transformer,
                )
            except Exception as _sc_init_err:
                logger.warning(f"Self-correction manager init failed: {_sc_init_err}")

            global_step, network = self.training_core.run_training_loop(  # type: ignore
                args=args,
                accelerator=accelerator,
                transformer=transformer,
                network=network,
                controlnet=controlnet,
                training_model=training_model,
                optimizer=optimizer,
                lr_scheduler=lr_scheduler,
                lr_descriptions=lr_descriptions,
                train_dataloader=train_dataloader,
                val_dataloader=val_dataloader,
                noise_scheduler=noise_scheduler,
                network_dtype=network_dtype,
                dit_dtype=dit_dtype,
                num_train_epochs=num_train_epochs,
                global_step=global_step,
                progress_bar=progress_bar,
                metadata=metadata,
                loss_recorder=loss_recorder,
                sampling_manager=self.sampling_manager,
                checkpoint_manager=self.checkpoint_manager,
                control_signal_processor=self.control_signal_processor,
                # ControlNet is managed as part of network_module option; pass through if created later
                current_epoch=current_epoch,
                current_step=current_step,
                optimizer_train_fn=optimizer_train_fn,
                optimizer_eval_fn=optimizer_eval_fn,
                # Pass the (possibly eager-loaded) VAE so SampleManager can reuse it
                vae=vae,
                sample_parameters=sample_parameters,
                save_model=save_model,
                remove_model=remove_model,
                is_main_process=is_main_process,
                val_epoch_step_sync=val_epoch_step_sync,
                repa_helper=repa_helper,  # NEW ARGUMENT
                dual_model_manager=dual_model_manager,
            )

        # ========== Training Completion ==========
        # Clean up REPA hooks
        if "repa_helper" in locals() and repa_helper is not None:
            repa_helper.remove_hooks()

        metadata["takenoko_training_finished_at"] = str(time.time())

        if is_main_process:
            network = accelerator.unwrap_model(network)

        accelerator.end_training()
        if optimizer_eval_fn:
            optimizer_eval_fn()

        if is_main_process and (args.save_state or args.save_state_on_train_end):
            from utils import train_utils

            train_utils.save_state_on_train_end(args, accelerator)

        if is_main_process:
            from utils import train_utils

            ckpt_name = train_utils.get_last_ckpt_name(args.output_name)
            save_model(
                ckpt_name,
                network,
                global_step,
                num_train_epochs,
                force_sync_upload=True,
            )

    def _log_detailed_network_info(
        self, network: Any, transformer: Any, args: argparse.Namespace
    ) -> None:
        """Log detailed information about the LoRA network and trainable parameters."""
        logger.info("üîç Detailed LoRA Network Information:")
        logger.info("=" * 80)

        # Count different types of parameters
        total_params = 0
        trainable_params = 0
        lora_params = 0
        patch_embedding_params = 0

        # Log network modules if available
        if hasattr(network, "unet_loras"):
            logger.info(f"üìå LoRA Modules ({len(network.unet_loras)} modules):")
            for i, lora in enumerate(network.unet_loras):
                if hasattr(lora, "lora_name") and hasattr(lora, "lora_dim"):
                    logger.info(
                        f"  {i+1:3d}: {lora.lora_name:<50} dim={lora.lora_dim}, alpha={getattr(lora, 'alpha', 'N/A')}"
                    )
                else:
                    logger.info(f"  {i+1:3d}: {type(lora).__name__}")

        # Log detailed parameter information
        logger.info("\nüìä Trainable Parameters:")
        for name, param in network.named_parameters():
            if param.requires_grad:
                param_count = param.numel()
                trainable_params += param_count

                # Categorize parameters
                if "lora" in name.lower():
                    lora_params += param_count
                    param_type = "LoRA"
                elif "patch_embedding" in name.lower():
                    patch_embedding_params += param_count
                    param_type = "Patch"
                else:
                    param_type = "Other"

                logger.info(
                    f"  üìç {name:<60} {str(param.shape):<20} {param_count:>10,} [{param_type}]"
                )
            total_params += param.numel()

        # Log patch embedding parameters from transformer if control LoRA is enabled
        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            if hasattr(transformer, "patch_embedding"):
                logger.info("\nüéØ Control LoRA Patch Embedding Parameters:")
                for name, param in transformer.patch_embedding.named_parameters():
                    if param.requires_grad:
                        param_count = param.numel()
                        patch_embedding_params += param_count
                        trainable_params += param_count
                        logger.info(
                            f"  üìç patch_embedding.{name:<45} {str(param.shape):<20} {param_count:>10,} [Patch]"
                        )

        # Summary
        logger.info("\nüìà Parameter Summary:")
        logger.info(f"  Total parameters:         {total_params:>12,}")
        logger.info(f"  Trainable parameters:     {trainable_params:>12,}")
        logger.info(f"    ‚îú‚îÄ LoRA parameters:     {lora_params:>12,}")
        logger.info(f"    ‚îî‚îÄ Patch embedding:     {patch_embedding_params:>12,}")
        logger.info(
            f"  Trainable ratio:          {trainable_params/total_params*100:>11.2f}%"
        )

        # Network configuration
        logger.info("\n‚öôÔ∏è  Network Configuration:")
        logger.info(
            f"  Network module:           {getattr(args, 'network_module', 'N/A')}"
        )
        logger.info(
            f"  Network dimension (rank): {getattr(args, 'network_dim', 'N/A')}"
        )
        logger.info(
            f"  Network alpha:            {getattr(args, 'network_alpha', 'N/A')}"
        )
        logger.info(
            f"  Network dropout:          {getattr(args, 'network_dropout', 'N/A')}"
        )

        if hasattr(args, "enable_control_lora") and args.enable_control_lora:
            logger.info(f"  Control LoRA enabled:     ‚úÖ")
            logger.info(
                f"  Control type:             {getattr(args, 'control_lora_type', 'N/A')}"
            )
            logger.info(
                f"  Input LR scale:           {getattr(args, 'input_lr_scale', 'N/A')}"
            )
        else:
            logger.info(f"  Control LoRA enabled:     ‚ùå")

        logger.info("=" * 80)
</file>

<file path="criteria/clustered_mse_loss.py">
# Based on: https://github.com/Anzhc/Timestep-Attention-and-other-shenanigans/blob/main/Code/Clustered_MSE_loss.py (AGPL-3.0)

import torch
from sklearn.cluster import KMeans


def adaptive_clustered_mse_loss(
    input,
    target,
    timesteps,
    loss_map,
    reduction="none",
    min_clusters=4,
    max_clusters=100,
):

    device = input.device
    target = target.to(device)

    batch_size = input.size(0)
    adjusted_loss = torch.zeros_like(input, dtype=torch.float32)

    for i in range(batch_size):
        initial_loss = (input[i] - target[i]) ** 2

        timestep_loss = loss_map.get(timesteps[i].item(), 1.0)
        n_clusters = min_clusters + (timestep_loss - min(loss_map.values())) / (
            max(loss_map.values()) - min(loss_map.values())
        ) * (max_clusters - min_clusters)
        n_clusters = max(min(int(n_clusters), max_clusters), min_clusters)

        loss_flat = initial_loss.view(-1).detach().cpu().numpy().reshape(-1, 1)

        kmeans = KMeans(n_clusters=n_clusters)
        clusters = kmeans.fit_predict(loss_flat)

        clusters = clusters.reshape(initial_loss.shape)

        clusters = torch.tensor(clusters, device=device, dtype=torch.long)

        unique_clusters = torch.unique(clusters)
        adjusted_loss_i = torch.zeros_like(initial_loss)
        for cluster in unique_clusters:
            cluster_mask = (clusters == cluster).float()
            cluster_loss = initial_loss * cluster_mask
            cluster_mean_loss = cluster_loss.sum() / cluster_mask.sum()
            adjusted_loss_i += cluster_mask * cluster_mean_loss

        adjusted_loss[i] = adjusted_loss_i

    # Apply the reduction
    if reduction == "mean":
        return adjusted_loss.mean()
    elif reduction == "sum":
        return adjusted_loss.sum()
    elif reduction == "none":
        return adjusted_loss
    else:
        raise ValueError(f"Invalid reduction type: {reduction}")
</file>

<file path="criteria/dispersive_loss.py">
## Based on https://arxiv.org/abs/2506.09027v1

import torch


def _pairwise_l2_sq(z_flat: torch.Tensor) -> torch.Tensor:
    return torch.pdist(z_flat, p=2).pow(2)


def _pairwise_neg_cosine(z_flat: torch.Tensor) -> torch.Tensor:
    z_norm = torch.nn.functional.normalize(z_flat, p=2, dim=1)
    sim = torch.mm(z_norm, z_norm.t())
    idx = torch.triu_indices(sim.size(0), sim.size(1), offset=1)
    return -sim[idx[0], idx[1]]


def dispersive_loss_info_nce(
    z: torch.Tensor, tau: float = 0.5, metric: str = "l2_sq"
) -> torch.Tensor:
    """Compute InfoNCE-style dispersive loss with selectable metric.

    Args:
        z: Tensor (B, ...)
        tau: Temperature (> 0)
        metric: "l2_sq" or "cosine"
    Returns:
        Scalar tensor loss. Zero if batch size <= 1.
    """
    if z is None:
        return torch.tensor(0.0, device="cpu")
    if z.shape[0] <= 1:
        return torch.zeros((), device=z.device, dtype=z.dtype)

    z_flat = z.view(z.shape[0], -1)

    if metric == "l2_sq":
        d = _pairwise_l2_sq(z_flat)
    elif metric == "cosine":
        d = _pairwise_neg_cosine(z_flat)
    else:
        raise ValueError(f"Unknown dispersive loss metric: {metric}")

    scaled = -d / max(tau, 1e-6)
    loss = torch.log(torch.mean(torch.exp(scaled)) + 1e-12)
    return loss
</file>

<file path="criteria/dwt_loss.py">
## Based on: https://github.com/spacepxl/WanTraining/blob/main/utils/dwt_loss.py (Apache)

import torch
from pytorch_wavelets import DWTForward  # type: ignore


def dwt_transform(latent):
    """
    Decompose input latent using discrete wavelet transform,
    and return the wavelets stacked along the channel dimension

    Args: latent (torch.Tensor): latent image with shape (B, C, H, W)

    Returns: torch.Tensor: wavelet decomposed latent with shape (B, C*4, H, W)
    """
    assert latent.dim() == 4
    dwt = DWTForward(J=1, mode="zero", wave="haar").to(
        device=latent.device, dtype=latent.dtype
    )

    latent_xll, latent_xh = dwt(latent)
    latent_xlh, latent_xhl, latent_xhh = torch.unbind(
        latent_xh[0], dim=2
    )  # split along the extra dim

    return torch.cat(
        [latent_xll, latent_xlh, latent_xhl, latent_xhh], dim=1
    )  # concat along channel dim


def dwt_loss(target, pred, reduction="mean"):
    """
    Calculate MSE loss in wavelet domain

    Args:
        target (torch.Tensor): clean latent image with shape (B, C, H, W)
        pred (torch.Tensor): clean latent image with shape (B, C, H, W)
        reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.

    Returns: torch.Tensor: MSE loss in wavelet domain
    """
    assert target.shape == pred.shape

    dwt_target = dwt_transform(target)
    dwt_pred = dwt_transform(pred)

    return torch.nn.functional.mse_loss(dwt_target, dwt_pred, reduction=reduction)
</file>

<file path="criteria/ew_loss.py">
# Based on: https://github.com/Anzhc/Timestep-Attention-and-other-shenanigans/blob/main/Code/EW_loss.py (AGPL-3.0)

import torch
import torch.nn.functional as F


def exponential_weighted_loss(
    noise_pred,
    target,
    alphas_cumprod,
    timesteps,
    loss_map,
    reduction="none",
    boundary_shift=0.0,
):
    mae_loss = F.l1_loss(noise_pred, target, reduction="none")
    mean_mae_loss = mae_loss.mean()
    mean_mae_loss += boundary_shift
    weight_map = torch.exp(mae_loss - mean_mae_loss)
    weighted_loss = mae_loss * weight_map
    mean_weighted_loss = weighted_loss.mean(dim=(2, 3), keepdim=True)
    std_weighted_loss = weighted_loss.std(dim=(2, 3), keepdim=True)
    ac = alphas_cumprod[timesteps].view(-1, 1, 1, 1)
    final_weighted_loss = mean_weighted_loss * ac + std_weighted_loss * (1 - ac)
    mse_loss = F.mse_loss(noise_pred, target, reduction="none")
    mean_final_weighted_loss = final_weighted_loss.mean()
    below_mean_mask = final_weighted_loss < mean_final_weighted_loss
    above_mean_mask = ~below_mean_mask
    loss = torch.zeros_like(final_weighted_loss)
    mse_loss_mean = mse_loss.mean(dim=(2, 3), keepdim=True)
    loss[below_mean_mask] = torch.min(
        final_weighted_loss[below_mean_mask],
        mse_loss_mean.expand_as(final_weighted_loss)[below_mean_mask],
    )
    loss[above_mean_mask] = (
        0.85 * final_weighted_loss[above_mean_mask]
        + 0.15 * mse_loss_mean.expand_as(final_weighted_loss)[above_mean_mask]
    )
    all_loss_values = torch.tensor(
        list(loss_map.values()), dtype=torch.float32, device=loss.device
    )
    min_loss_value = all_loss_values.min()
    max_loss_value = all_loss_values.max()
    normalized_factors = torch.tensor(
        [loss_map.get(t.item(), 1.0) for t in timesteps],
        dtype=torch.float32,
        device=loss.device,
    )
    interpolation_factors = (max_loss_value - normalized_factors) / (
        max_loss_value - min_loss_value
    )
    median_interpolation_factors = interpolation_factors.median()
    scaled_interpolation_factors = (
        0.5 * interpolation_factors + 0.5 * median_interpolation_factors
    )
    loss = (
        1 - scaled_interpolation_factors.view(-1, 1, 1, 1)
    ) * mse_loss + scaled_interpolation_factors.view(-1, 1, 1, 1) * loss

    # Apply the reduction
    if reduction == "mean":
        return loss.mean()
    elif reduction == "sum":
        return loss.sum()
    elif reduction == "none":
        return loss
</file>

<file path="criteria/training_loss.py">
"""Training loss computation utilities.

This module encapsulates all logic related to computing the training loss, so the
main training loop can remain focused on orchestration. It preserves behavior from
the previous inlined implementation in `core/training_core.py`.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Callable, List

import torch
import torch.nn.functional as F

from common.logger import get_logger
from criteria.dispersive_loss import dispersive_loss_info_nce
from utils.train_utils import get_sigmas


logger = get_logger(__name__)


@dataclass
class LossComponents:
    """Container for individual loss terms and the final total loss.

    Attributes
    ----------
    total_loss: torch.Tensor
        The final scalar loss to backpropagate.
    base_loss: Optional[torch.Tensor]
        The base flow-matching loss (after reductions and any weighting).
    dop_loss: Optional[torch.Tensor]
        The Diff Output Preservation loss component, if enabled.
    dispersive_loss: Optional[torch.Tensor]
        The dispersive (InfoNCE-style) loss component, if enabled.
    optical_flow_loss: Optional[torch.Tensor]
        The optical flow consistency loss, if enabled.
    repa_loss: Optional[torch.Tensor]
        The REPA alignment loss, if enabled.
    """

    total_loss: torch.Tensor
    base_loss: Optional[torch.Tensor] = None
    dop_loss: Optional[torch.Tensor] = None
    dispersive_loss: Optional[torch.Tensor] = None
    optical_flow_loss: Optional[torch.Tensor] = None
    repa_loss: Optional[torch.Tensor] = None


class TrainingLossComputer:
    """Compute training losses with feature flags preserved via `args`.

    Parameters
    ----------
    config: Any
        Configuration object providing model/patch settings. Must expose
        `patch_size` as a tuple (pt, ph, pw).
    """

    def __init__(self, config: Any) -> None:
        self.config = config

    # ---- Internal helpers ----
    def _compute_seq_len(self, latents: torch.Tensor) -> int:
        lat_f, lat_h, lat_w = latents.shape[2:5]
        pt, ph, pw = self.config.patch_size
        return (lat_f * lat_h * lat_w) // (pt * ph * pw)

    def _maybe_get_control_latents(
        self,
        args: Any,
        accelerator: Any,
        batch: Dict[str, Any],
        latents: torch.Tensor,
        network_dtype: torch.dtype,
        vae: Optional[Any],
        control_signal_processor: Optional[Any],
    ) -> Optional[torch.Tensor]:
        if (
            hasattr(args, "enable_control_lora")
            and args.enable_control_lora
            and control_signal_processor is not None
            and hasattr(control_signal_processor, "process_control_signal")
        ):
            try:
                return control_signal_processor.process_control_signal(
                    args, accelerator, batch, latents, network_dtype, vae
                )
            except Exception as e:
                logger.warning(f"Control signal processing failed for DOP path: {e}")
                return None
        return None

    def _concat_control_if_available(
        self,
        noisy_model_input: torch.Tensor,
        control_latents: Optional[torch.Tensor],
    ) -> torch.Tensor:
        if control_latents is None:
            return noisy_model_input
        concat_dim = 1 if noisy_model_input.dim() == 5 else 0
        return torch.cat(
            [noisy_model_input, control_latents.to(noisy_model_input)], dim=concat_dim
        )

    # ---- Public API ----
    @torch.no_grad()
    def _compute_prior_pred_for_dop(
        self,
        args: Any,
        accelerator: Any,
        transformer: Any,
        network: Any,
        latents: torch.Tensor,
        noisy_model_input: torch.Tensor,
        timesteps: torch.Tensor,
        dop_context: List[torch.Tensor],
        model_input_control: Optional[torch.Tensor],
    ) -> torch.Tensor:
        """Compute the base model prediction with LoRA disabled for DOP.

        Returns a detached tensor suitable for MSE target.
        """
        # Temporarily disable LoRA
        original_multiplier = accelerator.unwrap_model(network).multiplier
        accelerator.unwrap_model(network).multiplier = 0.0
        try:
            seq_len = self._compute_seq_len(latents)
            model_input_prior = (
                model_input_control
                if model_input_control is not None
                else noisy_model_input
            )
            prior_pred_list = transformer(
                model_input_prior,
                t=timesteps,
                context=dop_context,
                seq_len=seq_len,
                y=None,
            )
            prior_pred = torch.stack(prior_pred_list, dim=0).detach()
            return prior_pred
        finally:
            # Restore
            accelerator.unwrap_model(network).multiplier = original_multiplier

    def compute_training_loss(
        self,
        *,
        args: Any,
        accelerator: Any,
        latents: torch.Tensor,
        noise: torch.Tensor,
        noisy_model_input: torch.Tensor,
        timesteps: torch.Tensor,
        network_dtype: torch.dtype,
        model_pred: torch.Tensor,
        target: torch.Tensor,
        weighting: Optional[torch.Tensor],
        batch: Dict[str, Any],
        intermediate_z: Optional[torch.Tensor],
        vae: Optional[Any] = None,
        transformer: Optional[Any] = None,
        network: Optional[Any] = None,
        control_signal_processor: Optional[Any] = None,
        repa_helper: Optional[Any] = None,
        raft: Optional[Any] = None,
        warp_fn: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
    ) -> LossComponents:
        """Compute the full training loss and its components.

        The returned `total_loss` is the scalar to backpropagate.
        """

        # ---- Base or Contrastive Flow Matching ----
        if (
            hasattr(args, "enable_contrastive_flow_matching")
            and args.enable_contrastive_flow_matching
        ):
            batch_size = latents.size(0)
            shuffled_indices = torch.randperm(batch_size, device=accelerator.device)
            negative_latents = latents[shuffled_indices]
            negative_noise = noise[shuffled_indices]
            negative_target = negative_noise - negative_latents.to(
                device=accelerator.device, dtype=network_dtype
            )

            loss_fm = F.mse_loss(model_pred.to(network_dtype), target, reduction="none")
            loss_contrastive = F.mse_loss(
                model_pred.to(network_dtype), negative_target, reduction="none"
            )
            lambda_val = float(getattr(args, "contrastive_flow_lambda", 0.05))
            loss = loss_fm - lambda_val * loss_contrastive
        else:
            loss = F.mse_loss(model_pred.to(network_dtype), target, reduction="none")

        # ---- Dataset sample weights ----
        sample_weights = batch.get("weight", None)
        if sample_weights is not None:
            sample_weights = sample_weights.to(
                device=accelerator.device, dtype=network_dtype
            )
            while sample_weights.dim() < loss.dim():
                sample_weights = sample_weights.unsqueeze(-1)
            loss = loss * sample_weights

        # ---- Masked training ----
        mask = batch.get("mask_signal", None)
        if mask is not None:
            mask = mask.to(device=accelerator.device, dtype=network_dtype)
            while mask.dim() < loss.dim():
                mask = mask.unsqueeze(-1)
            loss = loss * mask

        if weighting is not None:
            loss = loss * weighting

        loss = loss.mean()
        base_loss = loss

        # ---- Optional Dispersive Loss ----
        dispersive_loss_value: Optional[torch.Tensor] = None
        if (
            hasattr(args, "enable_dispersive_loss")
            and args.enable_dispersive_loss
            and intermediate_z is not None
            and float(getattr(args, "dispersive_loss_lambda", 0.0)) != 0.0
        ):
            try:
                pooled_z = intermediate_z
                pooling_mode = str(
                    getattr(args, "dispersive_loss_pooling", "none")
                ).lower()
                if pooling_mode != "none":
                    try:
                        lat_f, lat_h, lat_w = latents.shape[2:5]
                        pt, ph, pw = self.config.patch_size
                        t_tokens = max(1, lat_f // pt)
                        h_tokens = max(1, lat_h // ph)
                        w_tokens = max(1, lat_w // pw)
                        tokens_per_frame = h_tokens * w_tokens
                        bsz, seq_len, hidden = pooled_z.shape
                        if seq_len == t_tokens * tokens_per_frame:
                            pooled_z = pooled_z.view(
                                bsz, t_tokens, tokens_per_frame, hidden
                            )
                            if pooling_mode == "frame_mean":
                                pooled_z = pooled_z.mean(dim=2)  # (B, T, C)
                                pooled_z = pooled_z.reshape(bsz, -1)
                    except Exception:
                        pooled_z = intermediate_z

                dispersive_val = dispersive_loss_info_nce(
                    pooled_z,
                    tau=float(getattr(args, "dispersive_loss_tau", 0.5)),
                    metric=str(getattr(args, "dispersive_loss_metric", "l2_sq")),
                )
                loss = (
                    loss
                    + float(getattr(args, "dispersive_loss_lambda", 0.0))
                    * dispersive_val
                )
                dispersive_loss_value = dispersive_val.detach()
            except Exception as e:
                logger.warning(f"Dispersive loss computation failed: {e}")

        # ---- Optional REPA Loss ----
        repa_loss_value: Optional[torch.Tensor] = None
        if repa_helper is not None:
            try:
                if "pixels" in batch:
                    clean_pixels = torch.stack(
                        batch["pixels"], dim=0
                    )  # (B, C, F, H, W)
                    first_frame_pixels = clean_pixels[:, :, 0, :, :]
                    repa_val = repa_helper.get_repa_loss(first_frame_pixels, vae)
                    loss = loss + repa_val
                    repa_loss_value = repa_val.detach()
                else:
                    logger.warning(
                        "REPA enabled, but no 'pixels' found in batch. Skipping REPA loss."
                    )
            except Exception as e:
                logger.warning(f"REPA loss computation failed: {e}")

        # ---- Optional Optical Flow Loss (RAFT) ----
        optical_flow_loss_value: Optional[torch.Tensor] = None
        if (
            hasattr(args, "enable_optical_flow_loss")
            and args.enable_optical_flow_loss
            and float(getattr(args, "lambda_optical_flow", 0.0)) > 0.0
        ):
            try:
                assert vae is not None, "VAE must be provided for optical flow loss"
                with torch.no_grad():
                    pred_latents = model_pred.to(network_dtype)
                    decoded = vae.decode(
                        pred_latents / getattr(vae, "scaling_factor", 1.0)
                    )
                    pred_frames = (
                        torch.stack(decoded, dim=0)
                        if isinstance(decoded, list)
                        else decoded
                    )
                assert (
                    pred_frames.dim() == 5
                ), f"Expected pred_frames shape (B, T, C, H, W), got {pred_frames.shape}"
                bsz, t_frames, c, h, w = pred_frames.shape
                if t_frames < 2:
                    raise ValueError("Need at least 2 frames for optical flow loss")
                assert (
                    raft is not None
                ), "RAFT model must be available for optical flow loss"
                assert (
                    warp_fn is not None
                ), "Warp function must be provided for optical flow loss"
                with torch.no_grad():
                    frame0 = pred_frames[:, :-1].reshape(-1, c, h, w)
                    frame1 = pred_frames[:, 1:].reshape(-1, c, h, w)
                    flow = raft(frame0, frame1)
                warped = warp_fn(frame0, flow)
                flow_loss = F.l1_loss(warped, frame1)
                loss = loss + float(args.lambda_optical_flow) * flow_loss
                optical_flow_loss_value = flow_loss.detach()
            except Exception as e:
                logger.warning(f"Optical flow loss computation failed: {e}")

        # ---- Diff Output Preservation (DOP) ----
        dop_loss_value: Optional[torch.Tensor] = None
        if (
            getattr(args, "diff_output_preservation", False)
            and "t5_preservation" in batch
            and transformer is not None
            and network is not None
        ):
            try:
                dop_embeds = [
                    t.to(device=accelerator.device, dtype=network_dtype)
                    for t in batch["t5_preservation"]
                ]
                # Control-aware inputs, mirroring training path
                control_latents_dop = self._maybe_get_control_latents(
                    args,
                    accelerator,
                    batch,
                    latents,
                    network_dtype,
                    vae,
                    control_signal_processor,
                )
                model_input_control = None
                if control_latents_dop is not None:
                    model_input_control = self._concat_control_if_available(
                        noisy_model_input, control_latents_dop
                    )

                # Base model (LoRA disabled) prior prediction
                prior_pred = self._compute_prior_pred_for_dop(
                    args=args,
                    accelerator=accelerator,
                    transformer=transformer,
                    network=network,
                    latents=latents,
                    noisy_model_input=noisy_model_input,
                    timesteps=timesteps,
                    dop_context=dop_embeds,
                    model_input_control=model_input_control,
                )

                # LoRA-enabled prediction on preservation prompt
                seq_len = self._compute_seq_len(latents)
                model_input_dop = (
                    model_input_control
                    if model_input_control is not None
                    else noisy_model_input
                )
                # Match original behavior: compute DOP prediction under autocast
                with accelerator.autocast():
                    dop_pred_list = transformer(
                        model_input_dop,
                        t=timesteps,
                        context=dop_embeds,
                        seq_len=seq_len,
                        y=None,
                    )
                dop_pred = torch.stack(dop_pred_list, dim=0)
                dop_loss_val = F.mse_loss(dop_pred, prior_pred) * float(
                    getattr(args, "diff_output_preservation_multiplier", 1.0)
                )
                loss = loss + dop_loss_val
                dop_loss_value = dop_loss_val.detach()
            except Exception as e:
                logger.warning(f"DOP loss computation failed: {e}")

        return LossComponents(
            total_loss=loss,
            base_loss=base_loss.detach(),
            dop_loss=dop_loss_value,
            dispersive_loss=dispersive_loss_value,
            optical_flow_loss=optical_flow_loss_value,
            repa_loss=repa_loss_value,
        )

    @torch.no_grad()
    def compute_extra_train_metrics(
        self,
        *,
        model_pred: torch.Tensor,
        target: torch.Tensor,
        noise: torch.Tensor,
        timesteps: torch.Tensor,
        noise_scheduler: Any,
        accelerator: Any,
    ) -> Dict[str, float]:
        """Compute periodic extra training loss metrics using existing tensors.

        Returns
        -------
        Dict[str, float]
            A mapping with keys:
            - "train/loss_p50"
            - "train/loss_p90"
            - "train/loss_cv_in_batch"
            - "train/direct_noise_loss_mean"
            - "train/loss_snr_correlation_batch"
        """
        metrics: Dict[str, float] = {}
        try:
            per_sample_vel = torch.nn.functional.mse_loss(
                model_pred.to(torch.float32), target.to(torch.float32), reduction="none"
            ).mean(dim=[1, 2, 3, 4])

            # Percentiles
            try:
                p50 = torch.quantile(per_sample_vel, 0.5).item()
            except Exception:
                p50 = torch.median(per_sample_vel).item()
            try:
                p90 = torch.quantile(per_sample_vel, 0.9).item()
            except Exception:
                k = max(1, int(0.1 * per_sample_vel.numel()))
                p90 = per_sample_vel.topk(k).values.min().item()

            # CV (robust to batch size 1; use population std to avoid ddof warnings)
            mean_loss = per_sample_vel.mean()
            batch_items = int(per_sample_vel.numel())
            if batch_items > 1:
                std_loss = per_sample_vel.to(torch.float32).std(correction=0)
                cv_in_batch = (
                    (std_loss / (mean_loss + 1e-12)).item()
                    if torch.isfinite(mean_loss)
                    else 0.0
                )
            else:
                std_loss = torch.tensor(
                    0.0, device=per_sample_vel.device, dtype=torch.float32
                )
                cv_in_batch = 0.0

            # Direct noise loss mean
            direct_noise_loss_mean = (
                torch.nn.functional.mse_loss(
                    model_pred.to(torch.float32),
                    noise.to(torch.float32),
                    reduction="none",
                )
                .mean(dim=[1, 2, 3, 4])
                .mean()
                .item()
            )

            # SNR correlation with loss
            try:
                sigmas = get_sigmas(
                    noise_scheduler,
                    timesteps,
                    accelerator.device,
                    n_dim=4,
                    dtype=timesteps.dtype,
                    source="training/metrics",
                )
                if sigmas.dim() > 1:
                    sigmas_reduced = sigmas.view(sigmas.shape[0], -1).mean(dim=1)
                else:
                    sigmas_reduced = sigmas
                snr_vals = (1.0 / (sigmas_reduced.to(torch.float32) ** 2)).flatten()
                if (
                    per_sample_vel.numel() > 1
                    and snr_vals.numel() == per_sample_vel.numel()
                ):
                    x = per_sample_vel.to(torch.float32)
                    y = snr_vals
                    x = x - x.mean()
                    y = y - y.mean()
                    # Use population std (correction=0) to avoid degrees-of-freedom issues
                    denom = x.std(correction=0) * y.std(correction=0)
                    corr = (
                        (x * y).mean() / (denom + 1e-12)
                        if torch.isfinite(denom) and float(denom.item()) > 0.0
                        else torch.tensor(0.0, device=x.device)
                    )
                    loss_snr_corr = float(corr.item())
                else:
                    loss_snr_corr = 0.0
            except Exception:
                loss_snr_corr = 0.0

            metrics.update(
                {
                    "train/loss_p50": float(p50),
                    "train/loss_p90": float(p90),
                    "train/loss_cv_in_batch": float(cv_in_batch),
                    "train/direct_noise_loss_mean": float(direct_noise_loss_mean),
                    "train/loss_snr_correlation_batch": float(loss_snr_corr),
                }
            )
        except Exception:
            return {}

        return metrics
</file>

<file path="dataset/buckets.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

import math
import random
from typing import Any, Tuple
import logging
import os
import numpy as np
import torch
from safetensors.torch import load_file


from dataset.item_info import ItemInfo
from common.logger import get_logger


logger = get_logger(__name__, level=logging.INFO)


def divisible_by(num: int, divisor: int) -> int:
    return num - num % divisor


class BucketSelector:
    # TODO: REFACTOR - Hardcoded constant should be configurable per architecture
    RESOLUTION_STEPS_WAN_2 = 16

    def __init__(
        self,
        resolution: Tuple[int, int],
        enable_bucket: bool = True,
        no_upscale: bool = False,
    ):
        # TODO: REFACTOR - Complex bucket calculation logic should be extracted to separate methods
        self.resolution = resolution
        self.bucket_area = resolution[0] * resolution[1]

        self.reso_steps = BucketSelector.RESOLUTION_STEPS_WAN_2

        if not enable_bucket:
            # only define one bucket
            self.bucket_resolutions = [resolution]
            self.no_upscale = False
        else:
            # prepare bucket resolution
            self.no_upscale = no_upscale
            sqrt_size = int(math.sqrt(self.bucket_area))
            min_size = divisible_by(sqrt_size // 2, self.reso_steps)
            self.bucket_resolutions = []
            for w in range(min_size, sqrt_size + self.reso_steps, self.reso_steps):
                h = divisible_by(self.bucket_area // w, self.reso_steps)
                self.bucket_resolutions.append((w, h))
                self.bucket_resolutions.append((h, w))

            self.bucket_resolutions = list(set(self.bucket_resolutions))
            self.bucket_resolutions.sort()

        # calculate aspect ratio to find the nearest resolution
        self.aspect_ratios = np.array([w / h for w, h in self.bucket_resolutions])

    def get_bucket_resolution(self, image_size: tuple[int, int]) -> tuple[int, int]:
        """
        return the bucket resolution for the given image size, (width, height)
        """
        area = image_size[0] * image_size[1]
        if self.no_upscale and area <= self.bucket_area:
            w, h = image_size
            w = divisible_by(w, self.reso_steps)
            h = divisible_by(h, self.reso_steps)
            return w, h

        aspect_ratio = image_size[0] / image_size[1]
        ar_errors = self.aspect_ratios - aspect_ratio
        bucket_id = np.abs(ar_errors).argmin()
        return self.bucket_resolutions[bucket_id]


class BucketBatchManager:
    # TODO: REFACTOR - Type hints are too generic (tuple[Any])
    # Should use proper type hints like tuple[int, int] or tuple[int, int, int]

    def __init__(
        self,
        bucketed_item_info: dict[Any, list[ItemInfo]],
        batch_size: int,
        prior_loss_weight: float = 1.0,
    ):
        self.batch_size = batch_size
        self.buckets = bucketed_item_info
        self.prior_loss_weight = prior_loss_weight  # <-- Store this
        self.bucket_resos = list(self.buckets.keys())
        self.bucket_resos.sort()
        # Optional per-epoch timestep bucketing
        self.num_timestep_buckets: int | None = None
        self._timestep_pool_batches: list[list[float]] | None = None

        # indices for enumerating batches. each batch is reso + batch_idx. reso is (width, height) or (width, height, frames)
        self.bucket_batch_indices: list[tuple[tuple[Any], int]] = []
        for bucket_reso in self.bucket_resos:
            bucket = self.buckets[bucket_reso]
            num_batches = math.ceil(len(bucket) / self.batch_size)
            for i in range(num_batches):
                self.bucket_batch_indices.append((bucket_reso, i))

        # do no shuffle here to avoid multiple datasets have different order
        # self.shuffle()

    def show_bucket_info(self):
        for bucket_reso in self.bucket_resos:
            bucket = self.buckets[bucket_reso]
            logger.info(f"bucket: {bucket_reso}, count: {len(bucket)}")

        logger.info(f"total batches: {len(self)}")

    def shuffle(self):
        # shuffle each bucket
        for bucket in self.buckets.values():
            random.shuffle(bucket)

        # shuffle the order of batches
        random.shuffle(self.bucket_batch_indices)

        # Prepare per-epoch timestep pool if enabled
        self._prepare_timestep_pool_internal()

    def set_num_timestep_buckets(self, num_timestep_buckets: int | None) -> None:
        self.num_timestep_buckets = (
            int(num_timestep_buckets) if num_timestep_buckets is not None else None
        )

    def prepare_timestep_pool(self) -> None:
        """Create/refresh the per-epoch timestep pool without shuffling batches."""
        self._prepare_timestep_pool_internal()

    def _prepare_timestep_pool_internal(self) -> None:
        self._timestep_pool_batches = None
        if self.num_timestep_buckets is not None and self.num_timestep_buckets > 1:
            try:
                num_batches = len(self.bucket_batch_indices)
                total_timesteps_needed = num_batches * self.batch_size
                samples_per_bucket = math.ceil(
                    total_timesteps_needed / int(self.num_timestep_buckets)
                )
                all_timesteps: list[float] = []
                for i in range(int(self.num_timestep_buckets)):
                    min_t = i / float(self.num_timestep_buckets)
                    max_t = (i + 1) / float(self.num_timestep_buckets)
                    for _ in range(samples_per_bucket):
                        all_timesteps.append(random.uniform(min_t, max_t))
                random.shuffle(all_timesteps)
                # trim
                all_timesteps = all_timesteps[:total_timesteps_needed]
                # chunk into batches
                self._timestep_pool_batches = []
                for i in range(num_batches):
                    s = i * self.batch_size
                    e = s + self.batch_size
                    self._timestep_pool_batches.append(all_timesteps[s:e])
            except Exception:
                # Non-fatal: fallback to no preset timesteps for this epoch
                self._timestep_pool_batches = None

    def __len__(self):
        return len(self.bucket_batch_indices)

    def __getitem__(self, idx):
        # TODO: REFACTOR - This method is too complex and handles multiple responsibilities
        # Consider extracting tensor processing logic into separate methods
        bucket_reso, batch_idx = self.bucket_batch_indices[idx]
        bucket = self.buckets[bucket_reso]
        start = batch_idx * self.batch_size
        end = min(start + self.batch_size, len(bucket))

        batch_tensor_data = {}
        varlen_keys = set()
        weights = []  # Collect weights for the batch
        control_signals = []  # Collect control signals for the batch
        mask_signals = []  # Collect mask signals for the batch
        pixels_list = []  # Collect original (or resized) pixel tensors

        for item_info in bucket[start:end]:
            if item_info.latent_cache_path is not None:
                sd_latent = load_file(item_info.latent_cache_path)
            else:
                sd_latent = {}

            if item_info.text_encoder_output_cache_path is not None:
                sd_te = load_file(item_info.text_encoder_output_cache_path)
            else:
                sd_te = {}

            sd = {**sd_latent, **sd_te}

            # Add weight to the batch
            # This is the key change: determine the weight for this specific item
            loss_weight = self.prior_loss_weight if item_info.is_reg else 1.0
            weights.append(loss_weight)

            # Load control signal if available
            if (
                hasattr(item_info, "control_content")
                and item_info.control_content is not None
            ):
                # Convert control content to tensor
                control_tensor = torch.from_numpy(item_info.control_content).float()
                control_signals.append(control_tensor)
            else:
                # Check if control cache file exists
                if item_info.latent_cache_path is not None:
                    control_cache_path = item_info.latent_cache_path.replace(
                        ".safetensors", "_control.safetensors"
                    )
                    if os.path.exists(control_cache_path):
                        try:
                            control_sd = load_file(control_cache_path)
                            # Find the control latent key
                            control_key = None
                            for key in control_sd.keys():
                                if key.startswith("latents_"):
                                    control_key = key
                                    break
                            if control_key:
                                control_signals.append(control_sd[control_key])
                            else:
                                control_signals.append(None)
                        except Exception as e:
                            logger.warning(
                                f"Failed to load control cache {control_cache_path}: {e}"
                            )
                            control_signals.append(None)
                    else:
                        control_signals.append(None)
                else:
                    control_signals.append(None)

            # Load mask signal if available
            if (
                hasattr(item_info, "mask_content")
                and item_info.mask_content is not None
            ):
                # Convert mask content to tensor
                mask_tensor = torch.from_numpy(item_info.mask_content).float()
                mask_signals.append(mask_tensor)
            else:
                # Check if mask cache file exists
                if item_info.latent_cache_path is not None:
                    mask_cache_path = item_info.latent_cache_path.replace(
                        ".safetensors", "_mask.safetensors"
                    )
                    if os.path.exists(mask_cache_path):
                        try:
                            mask_sd = load_file(mask_cache_path)
                            # Find the mask latent key
                            mask_key = None
                            for key in mask_sd.keys():
                                if key.startswith("latents_"):
                                    mask_key = key
                                    break
                            if mask_key:
                                mask_signals.append(mask_sd[mask_key])
                            else:
                                mask_signals.append(None)
                        except Exception as e:
                            logger.warning(
                                f"Failed to load mask cache {mask_cache_path}: {e}"
                            )
                            mask_signals.append(None)
                    else:
                        mask_signals.append(None)
                else:
                    mask_signals.append(None)

            # TODO: REFACTOR - Complex key processing logic should be extracted to a separate method
            # This logic is hard to understand and maintain
            for key in sd.keys():
                is_varlen_key = key.startswith("varlen_")  # varlen keys are not stacked
                content_key = key

                if is_varlen_key:
                    content_key = content_key.replace("varlen_", "")

                # We need a more robust way to parse keys now.
                # Example keys: "t5_bf16", "t5_preservation_bf16"
                if "preservation" in content_key:
                    # This is a preservation embedding
                    parts = content_key.split("_")
                    # Should be [text_encoder_type, "preservation", dtype]
                    # We will name it "t5_preservation" in the batch
                    if len(parts) == 3:
                        content_key = f"{parts[0]}_preservation"

                elif content_key.endswith("_mask"):
                    pass
                else:
                    content_key = content_key.rsplit("_", 1)[0]  # remove dtype
                    if content_key.startswith("latents_"):
                        content_key = content_key.rsplit("_", 1)[0]  # remove FxHxW

                if content_key not in batch_tensor_data:
                    batch_tensor_data[content_key] = []
                batch_tensor_data[content_key].append(sd[key])

                if is_varlen_key:
                    varlen_keys.add(content_key)

            # Collect pixels for on-the-fly control processing if available
            if item_info.content is not None:
                # item_info.content is an np.ndarray resized to bucket resolution
                arr = item_info.content  # (H,W,C) for images, (F,H,W,C) for videos
                if arr.ndim == 3:  # single image -> create dummy frame dim F=1
                    tensor = (
                        torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(1)
                    )  # C,F,H,W
                elif arr.ndim == 4:  # video
                    tensor = torch.from_numpy(arr).permute(3, 0, 1, 2)  # C,F,H,W
                else:
                    raise ValueError(f"Unsupported content shape: {arr.shape}")

                # Normalize to [-1,1] like reference implementation
                tensor = tensor.float().div(127.5).sub(1.0)
                pixels_list.append(tensor)

        for key in batch_tensor_data.keys():
            if key not in varlen_keys:
                batch_tensor_data[key] = torch.stack(batch_tensor_data[key])

        # Add weights to the batch tensor data
        batch_tensor_data["weight"] = torch.tensor(weights, dtype=torch.float32)

        # Add control signals to batch if any are available
        if any(cs is not None for cs in control_signals):
            # Determine reference tensor to preserve shape/dtype
            ref_cs = next((cs for cs in control_signals if cs is not None), None)
            if ref_cs is not None:
                stacked_control_signals: list[torch.Tensor] = []
                for cs in control_signals:
                    if cs is None:
                        stacked_control_signals.append(torch.zeros_like(ref_cs))
                    else:
                        stacked_control_signals.append(cs)
                batch_tensor_data["control_signal"] = torch.stack(
                    stacked_control_signals
                )

        # Add mask signals to batch if any are available
        if any(ms is not None for ms in mask_signals):
            ref_ms = next((ms for ms in mask_signals if ms is not None), None)
            if ref_ms is not None:
                stacked_mask_signals: list[torch.Tensor] = []
                for ms in mask_signals:
                    if ms is None:
                        stacked_mask_signals.append(torch.zeros_like(ref_ms))
                    else:
                        stacked_mask_signals.append(ms)
                batch_tensor_data["mask_signal"] = torch.stack(stacked_mask_signals)

        # Add pixels tensor if collected - format to match reference implementation
        if pixels_list:
            # Reference expects individual CFHW tensors, not stacked tensor
            # Our pixels_list contains CFHW tensors, which is correct
            # Store as individual tensors to match reference collate_batch format
            batch_tensor_data["pixels"] = pixels_list

        # Add item_info to batch for control video caching
        batch_tensor_data["item_info"] = bucket[start:end]

        # Attach per-batch timesteps if prepared
        if self._timestep_pool_batches is not None:
            try:
                # Respect possible last partial batch size
                timesteps = self._timestep_pool_batches[idx][: end - start]
                batch_tensor_data["timesteps"] = timesteps
            except Exception:
                batch_tensor_data["timesteps"] = None
        else:
            batch_tensor_data["timesteps"] = None

        return batch_tensor_data
</file>

<file path="dataset/cache.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

import os
from typing import Optional
import logging
import torch
from safetensors.torch import save_file

from dataset.item_info import ItemInfo
from utils import safetensors_utils
from utils.model_utils import dtype_to_str

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

# the keys of the dict are `<content_type>_FxHxW_<dtype>` for latents
# and `<content_type>_<dtype|mask>` for other tensors


def save_latent_cache_wan(
    item_info: ItemInfo,
    latent: torch.Tensor,
    clip_embed: Optional[torch.Tensor],
    image_latent: Optional[torch.Tensor],
):
    """Wan architecture only"""
    assert (
        latent.dim() == 4
    ), "latent should be 4D tensor (frame, channel, height, width)"

    _, F, H, W = latent.shape
    dtype_str = dtype_to_str(latent.dtype)
    sd = {f"latents_{F}x{H}x{W}_{dtype_str}": latent.detach().cpu()}

    if clip_embed is not None:
        sd[f"clip_{dtype_str}"] = clip_embed.detach().cpu()

    if image_latent is not None:
        sd[f"latents_image_{F}x{H}x{W}_{dtype_str}"] = image_latent.detach().cpu()

    save_latent_cache_common(item_info, sd)


def save_latent_cache_common(item_info: ItemInfo, sd: dict[str, torch.Tensor]):
    metadata = {
        "architecture": "wan21",
        "width": f"{item_info.original_size[0]}",
        "height": f"{item_info.original_size[1]}",
        "format_version": "1.0.1",
    }
    if item_info.frame_count is not None:
        metadata["frame_count"] = f"{item_info.frame_count}"

    for key, value in sd.items():
        # NaN check and show warning, replace NaN with 0
        if torch.isnan(value).any():
            logger.warning(
                f"{key} tensor has NaN: {item_info.item_key}, replace NaN with 0"
            )
            value[torch.isnan(value)] = 0

    latent_dir = os.path.dirname(item_info.latent_cache_path)  # type: ignore
    os.makedirs(latent_dir, exist_ok=True)

    save_file(sd, item_info.latent_cache_path, metadata=metadata)  # type: ignore


def save_text_encoder_output_cache_wan(
    item_info: ItemInfo,
    embed: torch.Tensor,
    preservation_embed: Optional[torch.Tensor] = None,
):
    sd = {}
    dtype_str = dtype_to_str(embed.dtype)
    text_encoder_type = "t5"
    sd[f"varlen_{text_encoder_type}_{dtype_str}"] = embed.detach().cpu()

    if preservation_embed is not None:
        # Save the preservation embedding with a distinct key
        # The key format is chosen to be easy to parse later
        sd[f"varlen_{text_encoder_type}_preservation_{dtype_str}"] = (
            preservation_embed.detach().cpu()
        )

    save_text_encoder_output_cache_common(item_info, sd)


def save_text_encoder_output_cache_common(
    item_info: ItemInfo, sd: dict[str, torch.Tensor]
):
    for key, value in sd.items():
        # NaN check and show warning, replace NaN with 0
        if torch.isnan(value).any():
            logger.warning(
                f"{key} tensor has NaN: {item_info.item_key}, replace NaN with 0"
            )
            value[torch.isnan(value)] = 0

    metadata = {
        "architecture": "wan21",
        "caption1": item_info.caption,
        "format_version": "1.0.1",
    }

    if os.path.exists(item_info.text_encoder_output_cache_path):  # type: ignore
        # load existing cache and update metadata
        with safetensors_utils.MemoryEfficientSafeOpen(
            item_info.text_encoder_output_cache_path
        ) as f:
            existing_metadata = f.metadata()
            for key in f.keys():
                if (
                    key not in sd
                ):  # avoid overwriting by existing cache, we keep the new one
                    sd[key] = f.get_tensor(key)

        assert (
            existing_metadata["architecture"] == metadata["architecture"]
        ), "architecture mismatch"
        if existing_metadata["caption1"] != metadata["caption1"]:
            logger.warning(
                f"caption mismatch: existing={existing_metadata['caption1']}, new={metadata['caption1']}, overwrite"
            )
        # TODO verify format_version

        existing_metadata.pop("caption1", None)
        existing_metadata.pop("format_version", None)
        metadata.update(
            existing_metadata
        )  # copy existing metadata except caption and format_version
    else:
        text_encoder_output_dir = os.path.dirname(
            item_info.text_encoder_output_cache_path  # type: ignore
        )
        os.makedirs(text_encoder_output_dir, exist_ok=True)

    safetensors_utils.mem_eff_save_file(
        sd, item_info.text_encoder_output_cache_path, metadata=metadata  # type: ignore
    )
</file>

<file path="dataset/config_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/config_utils.py (Apache)

import argparse
from dataclasses import (
    asdict,
    dataclass,
)
import functools
import logging
import random
from textwrap import dedent
from pathlib import Path

# from toolz import curry
from typing import List, Optional, Sequence, Tuple, Union

import toml
import voluptuous
from voluptuous import (
    Any,
    ExactSequence,
    MultipleInvalid,
    Object,
    Schema,
    Optional as VOptional,
)

from dataset.image_video_dataset import DatasetGroup, ImageDataset, VideoDataset

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


@dataclass
class BaseDatasetParams:
    resolution: Tuple[int, int] = (960, 544)
    enable_bucket: bool = False
    bucket_no_upscale: bool = False
    caption_extension: Optional[str] = None
    caption_dropout_rate: float = 0.0
    batch_size: int = 1
    num_repeats: int = 1
    cache_directory: Optional[str] = None
    debug_dataset: bool = False  # Fixed: was tuple, now proper bool
    is_val: bool = False  # Fixed: was tuple, now proper bool
    target_model: Optional[str] = None  # Model type for cache postfix determination
    is_reg: bool = False  # for regularization datasets
    mask_path: Optional[str] = None  # Path to mask images/videos for masked training


@dataclass
class ImageDatasetParams(BaseDatasetParams):
    image_directory: Optional[str] = None


@dataclass
class VideoDatasetParams(BaseDatasetParams):
    video_directory: Optional[str] = None
    target_frames: Sequence[int] = (1,)
    frame_extraction: Optional[str] = "head"
    frame_stride: Optional[int] = 1
    frame_sample: Optional[int] = 1
    max_frames: Optional[int] = 129
    source_fps: Optional[float] = None


@dataclass
class DatasetBlueprint:
    is_image_dataset: bool
    params: Union[ImageDatasetParams, VideoDatasetParams]


@dataclass
class DatasetGroupBlueprint:
    datasets: Sequence[DatasetBlueprint]


@dataclass
class Blueprint:
    # dataset_group: DatasetGroupBlueprint
    train_dataset_group: DatasetGroupBlueprint
    val_dataset_group: DatasetGroupBlueprint


class ConfigSanitizer:
    # @curry  # TODO: Commented out decorator - consider if curry is needed or remove comment
    @staticmethod
    def __validate_and_convert_twodim(klass, value: Sequence) -> Tuple:
        Schema(ExactSequence([klass, klass]))(value)
        return tuple(value)

    # @curry  # TODO: Commented out decorator - consider if curry is needed or remove comment
    @staticmethod
    def __validate_and_convert_scalar_or_twodim(
        klass, value: Union[float, Sequence]
    ) -> Tuple:
        Schema(Any(klass, ExactSequence([klass, klass])))(value)
        try:
            # First check if it's a scalar value
            Schema(klass)(value)
            return (value, value)
        except (MultipleInvalid, ValueError):
            # If scalar validation fails, it must be a sequence, so validate as twodim
            if isinstance(value, (list, tuple)):
                return ConfigSanitizer.__validate_and_convert_twodim(klass, value)
            else:
                # If it's not a scalar that matches klass and not a sequence, re-raise
                raise

    # datasets schema
    DATASET_ASCENDABLE_SCHEMA = {
        "caption_extension": str,
        "caption_dropout_rate": float,
        "batch_size": int,
        "num_repeats": int,
        "resolution": functools.partial(
            __validate_and_convert_scalar_or_twodim.__func__, int
        ),
        "enable_bucket": bool,
        "bucket_no_upscale": bool,
        "mask_path": str,
    }
    IMAGE_DATASET_DISTINCT_SCHEMA = {
        "image_directory": str,
        "cache_directory": str,
    }
    VIDEO_DATASET_DISTINCT_SCHEMA = {
        "video_directory": str,
        "target_frames": [int],
        "frame_extraction": str,
        "frame_stride": int,
        "frame_sample": int,
        "max_frames": int,
        "cache_directory": str,
        "source_fps": float,
    }

    # options handled by argparse but not handled by user config
    ARGPARSE_SPECIFIC_SCHEMA = {
        "debug_dataset": bool,
        "target_model": str,  # Model type for cache postfix determination
    }

    def __init__(self) -> None:
        self.image_dataset_schema = self.__merge_dict(
            self.DATASET_ASCENDABLE_SCHEMA,
            self.IMAGE_DATASET_DISTINCT_SCHEMA,
        )
        self.video_dataset_schema = self.__merge_dict(
            self.DATASET_ASCENDABLE_SCHEMA,
            self.VIDEO_DATASET_DISTINCT_SCHEMA,
        )

        def validate_flex_dataset(dataset_config: dict):
            if "video_directory" in dataset_config:
                return Schema(self.video_dataset_schema)(dataset_config)
            else:
                return Schema(self.image_dataset_schema)(dataset_config)

        # REFACTOR: Consider extracting this to a separate method for better testability and readability

        self.dataset_schema = validate_flex_dataset

        self.general_schema = self.__merge_dict(
            self.DATASET_ASCENDABLE_SCHEMA,
        )

        nested_or_flat_datasets_schema = Any(
            {
                "general": self.general_schema,
                "train": VOptional([self.dataset_schema]),
                "val": VOptional([self.dataset_schema]),
            },
            [self.dataset_schema],  # legacy flat list of datasets
        )

        # Allow all keys to pass through; rely on downstream processing for detailed checks
        self.user_config_validator = Schema({}, extra=voluptuous.ALLOW_EXTRA)

        # TODO: Large block of commented out code - consider removing if no longer needed
        # self.user_config_validator = Schema(
        #     {
        #         "general": self.general_schema,
        #         "datasets": [self.dataset_schema],
        #     }
        # )

        self.argparse_schema = self.__merge_dict(
            self.ARGPARSE_SPECIFIC_SCHEMA,
        )
        self.argparse_config_validator = Schema(
            Object(self.argparse_schema), extra=voluptuous.ALLOW_EXTRA
        )

    def sanitize_user_config(self, user_config: dict) -> dict:
        try:
            sanitized = self.user_config_validator(user_config)
            logger.info(f"Sanitized config keys: {list(sanitized.keys())}")
            if "datasets" in sanitized:
                datasets_section = sanitized["datasets"]
                if isinstance(datasets_section, list):
                    # Flat list style: datasets is a list of dataset configs
                    logger.info(
                        f"Datasets structure: flat list with {len(datasets_section)} items"
                    )
                    logger.info(f"Train datasets count: {len(datasets_section)}")
                else:
                    # Nested dictionary style: datasets contains train/val/general sections
                    logger.info(f"Datasets keys: {list(datasets_section.keys())}")
                    logger.info(
                        f"Train datasets count: {len(datasets_section.get('train', []))}"
                    )
            return sanitized
        except MultipleInvalid:
            # TODO: clarify the error message - should provide more specific error details
            logger.error("Invalid user config")
            raise

    # NOTE: In nature, argument parser result is not needed to be sanitize
    #   However this will help us to detect program bug
    # TODO: Consider if this sanitization is still necessary or if it adds unnecessary overhead
    def sanitize_argparse_namespace(
        self, argparse_namespace: argparse.Namespace
    ) -> argparse.Namespace:
        try:
            return self.argparse_config_validator(argparse_namespace)
        except MultipleInvalid:
            # XXX: this should be a bug
            logger.error(
                "Invalid cmdline parsed arguments. This should be a bug."
            )  # TODO: XXX comment suggests this is a critical issue - investigate
            raise

    # NOTE: value would be overwritten by latter dict if there is already the same key
    @staticmethod
    def __merge_dict(*dict_list: dict) -> dict:
        merged = {}
        for schema in dict_list:
            # merged |= schema  # TODO: Commented out modern dict union operator - consider using it
            for k, v in schema.items():
                merged[k] = v
        return merged


class BlueprintGenerator:
    BLUEPRINT_PARAM_NAME_TO_CONFIG_OPTNAME = (
        {}
    )  # TODO: Empty dict - consider if this should be populated or removed - REFACTOR: Either populate this mapping or remove if unused

    def __init__(self, sanitizer: ConfigSanitizer):
        self.sanitizer = sanitizer

    # runtime_params is for parameters which is only configurable on runtime, such as tokenizer
    def generate(
        self,
        user_config: dict,
        argparse_namespace: argparse.Namespace,
        **runtime_params,
    ) -> Blueprint:
        sanitized_user_config = self.sanitizer.sanitize_user_config(user_config)
        sanitized_argparse_namespace = self.sanitizer.sanitize_argparse_namespace(
            argparse_namespace
        )

        argparse_config = {
            k: v for k, v in vars(sanitized_argparse_namespace).items() if v is not None
        }
        datasets_section = sanitized_user_config.get("datasets", {})
        # Determine general, train, val depending on structure
        if isinstance(datasets_section, list):
            # Flat list style: everything in datasets is training list
            general_config = sanitized_user_config.get("general", {})
            train_dataset_configs = datasets_section
            val_dataset_configs = sanitized_user_config.get("val_datasets", [])
        else:
            general_config = datasets_section.get("general", {})
            train_dataset_configs = datasets_section.get("train", [])
            val_dataset_configs = datasets_section.get("val", [])

        logger.info(f"Found {len(train_dataset_configs)} training dataset configs")
        train_blueprints = []
        for i, dataset_config in enumerate(train_dataset_configs):
            logger.info(f"Processing training dataset {i}: {dataset_config}")
            is_image_dataset = "image_directory" in dataset_config
            dataset_params_klass = (
                ImageDatasetParams if is_image_dataset else VideoDatasetParams
            )
            params = self.generate_params_by_fallbacks(
                dataset_params_klass,
                [dataset_config, general_config, argparse_config, runtime_params],
            )
            # is_val defaults to False in the dataclass so nothing special is needed
            train_blueprints.append(DatasetBlueprint(is_image_dataset, params))

        # Process validation datasets: mark them as validation.
        logger.info(f"Found {len(val_dataset_configs)} validation dataset configs")
        val_blueprints = []
        for i, dataset_config in enumerate(val_dataset_configs):
            logger.info(f"Processing validation dataset {i}: {dataset_config}")
            is_image_dataset = "image_directory" in dataset_config
            dataset_params_klass = (
                ImageDatasetParams if is_image_dataset else VideoDatasetParams
            )
            params = self.generate_params_by_fallbacks(
                dataset_params_klass,
                [dataset_config, general_config, argparse_config, runtime_params],
            )
            params.is_val = True  # mark as validation
            val_blueprints.append(DatasetBlueprint(is_image_dataset, params))

        train_dataset_group_blueprint = DatasetGroupBlueprint(train_blueprints)
        val_dataset_group_blueprint = DatasetGroupBlueprint(val_blueprints)

        logger.info(
            f"Created {len(train_blueprints)} training blueprints and {len(val_blueprints)} validation blueprints"
        )

        return Blueprint(
            train_dataset_group=train_dataset_group_blueprint,
            val_dataset_group=val_dataset_group_blueprint,
        )

    @staticmethod
    def generate_params_by_fallbacks(param_klass, fallbacks: Sequence[dict]):
        name_map = BlueprintGenerator.BLUEPRINT_PARAM_NAME_TO_CONFIG_OPTNAME
        search_value = BlueprintGenerator.search_value
        default_params = asdict(param_klass())
        param_names = default_params.keys()

        params = {
            name: search_value(
                name_map.get(name, name), fallbacks, default_params.get(name)
            )
            for name in param_names
        }

        return param_klass(**params)

    @staticmethod
    def search_value(
        key: str, fallbacks: Sequence[dict], default_value=None
    ):  # TODO: Missing type hint for default_value - REFACTOR: Add proper type hint (e.g., Optional[Any])
        for cand in fallbacks:
            value = cand.get(key)
            if value is not None:
                return value

        return default_value


def _format_dataset_info_table(
    datasets: List[Union[ImageDataset, VideoDataset]],
) -> str:
    """Format dataset information as a clean table for better readability."""
    if not datasets:
        return "No datasets configured."

    # Define column headers and widths
    headers = [
        "ID",
        "Type",
        "Val",
        "Resolution",
        "Batch",
        "Repeats",
        "Bucket",
        "Directory",
        "Cache Dir",
    ]

    # Calculate column widths based on content
    rows = []
    for i, dataset in enumerate(datasets):
        is_image_dataset = isinstance(dataset, ImageDataset)
        dataset_type = "Image" if is_image_dataset else "Video"
        validation = "Yes" if dataset.is_val else "No"
        resolution = f"{dataset.resolution[0]}x{dataset.resolution[1]}"

        # Get the appropriate directory path
        if is_image_dataset:
            directory = dataset.image_directory or "N/A"
        else:
            directory = dataset.video_directory or "N/A"

        # Truncate long paths for better table display
        if len(directory) > 30:
            directory = "..." + directory[-27:]

        cache_dir = dataset.cache_directory or "N/A"
        if len(cache_dir) > 20:
            cache_dir = "..." + cache_dir[-17:]

        bucket_info = "Yes" if dataset.enable_bucket else "No"
        if dataset.enable_bucket and dataset.bucket_no_upscale:
            bucket_info += "/NoUp"

        row = [
            str(i),
            dataset_type,
            validation,
            resolution,
            str(dataset.batch_size),
            str(dataset.num_repeats),
            bucket_info,
            directory,
            cache_dir,
        ]
        rows.append(row)

    # Calculate column widths
    col_widths = [len(header) for header in headers]
    for row in rows:
        for i, cell in enumerate(row):
            col_widths[i] = max(col_widths[i], len(str(cell)))

    # Build the table
    table_lines = []

    # Header
    header_line = " | ".join(
        header.ljust(col_widths[i]) for i, header in enumerate(headers)
    )
    table_lines.append(header_line)

    # Separator
    separator = "-+-".join("-" * width for width in col_widths)
    table_lines.append(separator)

    # Data rows
    for row in rows:
        row_line = " | ".join(
            str(cell).ljust(col_widths[i]) for i, cell in enumerate(row)
        )
        table_lines.append(row_line)

    return "\n".join(table_lines)


def _format_dataset_details_table(
    datasets: List[Union[ImageDataset, VideoDataset]],
) -> str:
    """Format detailed dataset information in a consistent table format for all dataset types."""
    if not datasets:
        return "No datasets configured."

    details_lines = []

    for i, dataset in enumerate(datasets):
        is_image_dataset = isinstance(dataset, ImageDataset)
        dataset_type = "Image" if is_image_dataset else "Video"

        # Common details for all datasets
        details_lines.append(
            f"üìÅ Dataset {i} ({dataset_type}) - Detailed Configuration:"
        )

        # Common parameters
        details_lines.append(f"‚îÇ Type                ‚îÇ {dataset_type}")
        details_lines.append(
            f"‚îÇ Validation Dataset  ‚îÇ {'Yes' if dataset.is_val else 'No'}"
        )
        details_lines.append(
            f"‚îÇ Resolution          ‚îÇ {dataset.resolution[0]} x {dataset.resolution[1]}"
        )
        details_lines.append(f"‚îÇ Batch Size          ‚îÇ {dataset.batch_size}")
        details_lines.append(f"‚îÇ Num Repeats         ‚îÇ {dataset.num_repeats}")
        details_lines.append(
            f"‚îÇ Enable Bucket       ‚îÇ {'Yes' if dataset.enable_bucket else 'No'}"
        )

        if dataset.enable_bucket:
            details_lines.append(
                f"‚îÇ Bucket No Upscale   ‚îÇ {'Yes' if dataset.bucket_no_upscale else 'No'}"
            )

        details_lines.append(
            f"‚îÇ Caption Extension   ‚îÇ {dataset.caption_extension or 'None'}"
        )
        details_lines.append(
            f"‚îÇ Debug Mode          ‚îÇ {'Yes' if dataset.debug_dataset else 'No'}"
        )

        # Type-specific parameters
        if is_image_dataset:
            details_lines.append(
                f"‚îÇ Image Directory     ‚îÇ {dataset.image_directory or 'N/A'}"
            )
        else:
            details_lines.append(
                f"‚îÇ Video Directory     ‚îÇ {dataset.video_directory or 'N/A'}"
            )
            details_lines.append(f"‚îÇ Target Frames       ‚îÇ {dataset.target_frames}")
            details_lines.append(
                f"‚îÇ Frame Extraction    ‚îÇ {dataset.frame_extraction or 'N/A'}"
            )
            details_lines.append(
                f"‚îÇ Frame Stride        ‚îÇ {dataset.frame_stride or 'N/A'}"
            )
            details_lines.append(
                f"‚îÇ Frame Sample        ‚îÇ {dataset.frame_sample or 'N/A'}"
            )
            details_lines.append(
                f"‚îÇ Max Frames          ‚îÇ {dataset.max_frames or 'N/A'}"
            )
            details_lines.append(
                f"‚îÇ Source FPS          ‚îÇ {dataset.source_fps or 'N/A'}"
            )

        details_lines.append(
            f"‚îÇ Cache Directory     ‚îÇ {dataset.cache_directory or 'N/A'}"
        )
        details_lines.append("")  # Empty line between datasets

    return "\n".join(details_lines)


# if training is True, it will return a dataset group for training, otherwise for caching
def generate_dataset_group_by_blueprint(
    dataset_group_blueprint: DatasetGroupBlueprint,
    training: bool = False,
    enable_control_lora: bool = False,
    prior_loss_weight: float = 1.0,
    num_timestep_buckets: Optional[int] = None,
) -> DatasetGroup:
    datasets: List[Union[ImageDataset, VideoDataset]] = []

    for i, dataset_blueprint in enumerate(dataset_group_blueprint.datasets):
        if dataset_blueprint.is_image_dataset:
            dataset_klass = ImageDataset
        else:
            dataset_klass = VideoDataset

        dataset = dataset_klass(**asdict(dataset_blueprint.params))
        datasets.append(dataset)

    # TODO: This assertion could be moved to a validation function for better separation of concerns
    cache_directories = [dataset.cache_directory for dataset in datasets]
    num_of_unique_cache_directories = len(set(cache_directories))
    if num_of_unique_cache_directories != len(cache_directories):
        raise ValueError(
            "cache directory should be unique for each dataset (note that cache directory is image/video directory if not specified)"
        )
    # REFACTOR: Extract cache directory validation to a separate method for better reusability

    # Display dataset information in a clean table format

    # Create and log the formatted table
    table_info = _format_dataset_info_table(datasets)
    logger.info("üìä Dataset Configuration Summary:")
    logger.info(f"\n{table_info}")

    # Log detailed configuration for all datasets
    detailed_info = _format_dataset_details_table(datasets)
    logger.info("üìã Detailed Dataset Configuration:")
    logger.info(f"\n{detailed_info}")

    # make buckets first because it determines the length of dataset
    # and set the same seed for all datasets
    seed = random.randint(
        0, 2**31
    )  # TODO: Magic number - consider making this configurable - REFACTOR: Extract to constant or config parameter
    for i, dataset in enumerate(datasets):
        # logger.info(f"[Dataset {i}]")  # TODO: Commented out logging - consider removing or enabling
        dataset.set_seed(seed)
        if training:
            if hasattr(dataset, "prepare_for_training"):
                import inspect

                sig = inspect.signature(dataset.prepare_for_training)
                kwargs = {}
                if "require_text_encoder_cache" in sig.parameters:
                    kwargs["require_text_encoder_cache"] = True
                if "load_pixels_for_control" in sig.parameters:
                    kwargs["load_pixels_for_control"] = enable_control_lora
                if "prior_loss_weight" in sig.parameters:
                    kwargs["prior_loss_weight"] = prior_loss_weight
                if "num_timestep_buckets" in sig.parameters:
                    kwargs["num_timestep_buckets"] = num_timestep_buckets
                dataset.prepare_for_training(**kwargs)
            else:
                pass
        else:
            # Skip prepare_for_training during caching mode to avoid error messages
            # about missing cache files (since we're creating them)
            logger.info(
                f"üì¶ [Dataset {i}] Skipping training preparation during caching mode"
            )
            pass
        # REFACTOR: Consider extracting dataset preparation logic to a separate method for better organization

    return DatasetGroup(datasets)


def load_user_config(
    config_file: str,
) -> dict:  # Fixed: renamed parameter to avoid shadowing
    file_path: Path = Path(config_file)  # Fixed: use different variable name
    if not file_path.is_file():
        raise ValueError(f"file not found: {file_path}")

    if file_path.name.lower().endswith(".toml"):
        try:
            config = toml.load(file_path)
        except Exception:
            logger.error(
                f"Error on parsing TOML config file. Please check the format: {file_path}"
            )
            raise
    else:
        raise ValueError(f"not supported config file format: {file_path}")

    return config


def validate_dataset_config(
    config_file: str,
    argparse_namespace: Optional[argparse.Namespace] = None,
    test_dataset_creation: bool = True,
) -> bool:
    """
    Validates a dataset configuration file by loading, sanitizing, and optionally testing dataset creation.

    Args:
        config_file: Path to the configuration file
        argparse_namespace: Optional argparse namespace for additional parameters
        test_dataset_creation: Whether to test actual dataset group creation

    Returns:
        True if validation passes, raises exception if validation fails
    """
    logger.info(f"Validating dataset config: {config_file}")

    # Create default argparse namespace if not provided
    if argparse_namespace is None:
        parser = argparse.ArgumentParser()
        parser.add_argument("--debug_dataset", action="store_true")
        argparse_namespace = parser.parse_args([])

    logger.info("[argparse_namespace]")
    logger.info(f"{vars(argparse_namespace)}")

    # Load and validate user config
    user_config = load_user_config(config_file)

    logger.info("üìù [user_config]")
    logger.info(f"{user_config}")

    # Sanitize the config
    sanitizer = ConfigSanitizer()
    sanitized_user_config = sanitizer.sanitize_user_config(user_config)

    logger.info("üßπ [sanitized_user_config]")
    logger.info(f"{sanitized_user_config}")

    # Generate blueprint
    blueprint = BlueprintGenerator(sanitizer).generate(user_config, argparse_namespace)

    logger.info("üìã [blueprint]")
    logger.info(f"{blueprint}")

    # Optionally test dataset creation
    if test_dataset_creation:
        logger.info("Testing dataset group creation...")

        train_dataset_group = generate_dataset_group_by_blueprint(
            blueprint.train_dataset_group
        )

        if len(blueprint.val_dataset_group.datasets) > 0:
            val_dataset_group = generate_dataset_group_by_blueprint(
                blueprint.val_dataset_group
            )
            dataset_group = DatasetGroup(
                train_dataset_group.datasets + val_dataset_group.datasets
            )
        else:
            dataset_group = train_dataset_group

        logger.info(
            f"Successfully created dataset group with {len(dataset_group.datasets)} datasets"
        )

    logger.info("Dataset config validation completed successfully!")
    return True
</file>

<file path="dataset/data_sources.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

import os
from typing import Optional, Callable
from typing import TYPE_CHECKING
import logging
import numpy as np
from PIL import Image

from dataset.buckets import BucketSelector
from dataset.datasource_utils import glob_images, glob_videos, load_video


from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

# Global set to track missing caption files we've already warned about
_missing_caption_warnings = set()
# Global flag to track if we've shown the general missing caption warning
_missing_caption_global_warning_shown = False


class ContentDataSource:
    # TODO: REFACTOR - This should be an abstract base class (ABC)
    # Consider using ABC and @abstractmethod decorators for better interface definition
    def __init__(self):
        self.caption_only = False
        self.load_control = False
        self.control_suffix = "_control"
        self.load_mask = False
        self.mask_path = None
        self.default_mask_file = None
        # Probability to drop the caption entirely (0.0-1.0). When >0, captions are set to '' with this probability.
        self.caption_dropout_rate: float = 0.0

    def set_caption_only(self, caption_only: bool):
        self.caption_only = caption_only

    def set_control_settings(
        self, load_control: bool, control_suffix: str = "_control"
    ):
        """Set control signal loading settings."""
        self.load_control = load_control
        self.control_suffix = control_suffix

    def set_caption_dropout_rate(self, rate: float):
        try:
            r = float(rate)
        except Exception:
            r = 0.0
        # clamp to [0, 1]
        self.caption_dropout_rate = 0.0 if r < 0.0 else (1.0 if r > 1.0 else r)

    def set_mask_settings(
        self,
        load_mask: bool,
        mask_path: Optional[str] = None,
        default_mask_file: Optional[str] = None,
    ):
        """Set mask loading settings for masked training. default_mask_file is deprecated and ignored."""
        self.load_mask = load_mask
        self.mask_path = mask_path
        self.default_mask_file = None

    @property
    def has_control(self) -> bool:
        """Check if control signals are available."""
        return self.load_control

    def is_indexable(self):
        raise NotImplementedError

    def get_caption(self, idx: int) -> tuple[str, str]:
        """
        Returns caption. May not be called if is_indexable() returns False.
        """
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def __iter__(self):
        raise NotImplementedError

    def __next__(self):
        raise NotImplementedError


class ImageDatasource(ContentDataSource):
    def __init__(self):
        super().__init__()

    def get_image_data(self, idx: int) -> tuple[str, Image.Image, str]:
        raise NotImplementedError


class ImageDirectoryDatasource(ImageDatasource):
    def __init__(self, image_directory: str, caption_extension: Optional[str] = None):
        # TODO: REFACTOR - Complex initialization logic should be split into separate methods
        # TODO: REFACTOR - Control image matching logic is complex and should be extracted
        super().__init__()
        self.image_directory = image_directory
        self.caption_extension = caption_extension
        self.current_idx = 0

        # glob images
        logger.info(f"glob images in {self.image_directory}")
        self.image_paths = glob_images(self.image_directory)
        logger.info(f"found {len(self.image_paths)} images")

    def is_indexable(self):
        return True

    def __len__(self):
        return len(self.image_paths)

    def get_image_data(
        self, idx: int
    ) -> tuple[str, Image.Image, str, Optional[Image.Image], Optional[Image.Image]]:
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")

        _, caption = self.get_caption(idx)

        # Load control image if enabled
        control_image = None
        if self.load_control:
            control_path = self._get_control_path(image_path)
            if os.path.exists(control_path):
                try:
                    control_image = Image.open(control_path).convert("RGB")
                    logger.debug(f"Loaded control image: {control_path}")
                except Exception as e:
                    logger.warning(f"Failed to load control image {control_path}: {e}")
            else:
                logger.debug(f"Control image not found: {control_path}")

        # Load mask image if enabled
        mask_image = None
        if self.load_mask:
            mask_path = self._get_mask_path(image_path)
            if os.path.exists(mask_path):
                try:
                    mask_image = Image.open(mask_path).convert(
                        "L"
                    )  # Convert to grayscale
                    logger.debug(f"Loaded mask image: {mask_path}")
                except Exception as e:
                    logger.warning(f"Failed to load mask image {mask_path}: {e}")
            else:
                logger.debug(f"Mask image not found: {mask_path}")

        return image_path, image, caption, control_image, mask_image

    def _get_control_path(self, image_path: str) -> str:
        """Get the path for the control image based on the original image path."""
        base_path = os.path.splitext(image_path)[0]
        return f"{base_path}{self.control_suffix}.png"

    def _get_mask_path(self, image_path: str) -> str:
        """Get the path for the mask image based on the original image path."""
        if self.mask_path is None:
            # If no mask path is specified, look for mask in same directory as image
            base_path = os.path.splitext(image_path)[0]
            return f"{base_path}_mask.png"
        else:
            # If mask path is specified, look for mask with same name in mask directory
            image_name = os.path.basename(image_path)
            base_name = os.path.splitext(image_name)[0]
            return os.path.join(self.mask_path, f"{base_name}.png")

    def get_caption(self, idx: int) -> tuple[str, str]:
        global _missing_caption_warnings, _missing_caption_global_warning_shown
        image_path = self.image_paths[idx]

        # If no caption extension is specified, return empty caption
        if not self.caption_extension:
            return image_path, ""

        caption_path = os.path.splitext(image_path)[0] + self.caption_extension

        # Check if caption file exists
        if os.path.exists(caption_path):
            try:
                with open(caption_path, "r", encoding="utf-8") as f:
                    caption = f.read().strip()
                # Apply caption dropout if configured
                if self.caption_dropout_rate > 0.0:
                    try:
                        import random as _random

                        if _random.random() < self.caption_dropout_rate:
                            caption = ""
                    except Exception:
                        pass
                return image_path, caption
            except Exception as e:
                # Only warn once per file for read errors
                if caption_path not in _missing_caption_warnings:
                    logger.warning(f"Failed to read caption file {caption_path}: {e}")
                    _missing_caption_warnings.add(caption_path)
                # Return empty string as fallback
                return image_path, ""
        else:
            # Show global warning only once
            if not _missing_caption_global_warning_shown:
                logger.warning(
                    "\nCaption files not found - using empty strings as captions"
                )
                logger.info("To disable this warning, either:")
                logger.info("  1. Create caption files for your images, or")
                logger.info(
                    "  2. Remove or comment out 'caption_extension' in your config"
                )
                _missing_caption_global_warning_shown = True

            # Return empty string as fallback
            return image_path, ""

    def __iter__(self):
        self.current_idx = 0
        return self

    def __next__(self) -> Callable:
        """
        Returns a fetcher function that returns image data.
        """
        if self.current_idx >= len(self.image_paths):
            raise StopIteration

        if self.caption_only:

            def create_caption_fetcher(index):
                return lambda: self.get_caption(index)

            fetcher = create_caption_fetcher(self.current_idx)
        else:

            def create_image_fetcher(index):
                return lambda: self.get_image_data(index)

            fetcher = create_image_fetcher(self.current_idx)

        self.current_idx += 1
        return fetcher


class VideoDatasource(ContentDataSource):
    def __init__(self):
        super().__init__()

        # None means all frames
        self.start_frame = None
        self.end_frame = None

        self.bucket_selector = None

        self.source_fps = None
        self.target_fps = None

    def __len__(self):
        raise NotImplementedError

    def get_video_data_from_path(
        self,
        video_path: str,
        start_frame: Optional[int] = None,
        end_frame: Optional[int] = None,
        bucket_selector: Optional[BucketSelector] = None,
    ) -> tuple[str, list[np.ndarray], str]:
        # this method can resize the video if bucket_selector is given to reduce the memory usage

        start_frame = start_frame if start_frame is not None else self.start_frame
        end_frame = end_frame if end_frame is not None else self.end_frame
        bucket_selector = (
            bucket_selector if bucket_selector is not None else self.bucket_selector
        )

        video = load_video(
            video_path,
            start_frame,
            end_frame,
            bucket_selector,
            source_fps=self.source_fps,
            target_fps=self.target_fps,
        )
        return video_path, video, ""

    def set_start_and_end_frame(
        self, start_frame: Optional[int], end_frame: Optional[int]
    ):
        self.start_frame = start_frame
        self.end_frame = end_frame

    def set_bucket_selector(self, bucket_selector: BucketSelector):
        self.bucket_selector = bucket_selector

    def set_source_and_target_fps(
        self, source_fps: Optional[float], target_fps: Optional[float]
    ):
        self.source_fps = source_fps
        self.target_fps = target_fps

    def __iter__(self):
        raise NotImplementedError

    def __next__(self):
        raise NotImplementedError


class VideoDirectoryDataSource(VideoDatasource):
    def __init__(
        self,
        video_directory: str,
        caption_extension: Optional[str] = None,
    ):
        super().__init__()
        self.video_directory = video_directory
        self.caption_extension = caption_extension
        self.current_idx = 0

        # glob videos
        logger.info(f"glob videos in {self.video_directory}")
        self.video_paths = glob_videos(self.video_directory)
        logger.info(f"found {len(self.video_paths)} videos")

    def is_indexable(self):
        return True

    def __len__(self):
        return len(self.video_paths)

    def get_video_data(
        self,
        idx: int,
        start_frame: Optional[int] = None,
        end_frame: Optional[int] = None,
        bucket_selector: Optional[BucketSelector] = None,
    ) -> tuple[
        str,
        list[np.ndarray],
        str,
        Optional[list[np.ndarray]],
        Optional[list[np.ndarray]],
    ]:
        video_path = self.video_paths[idx]
        _, video_frames, _ = self.get_video_data_from_path(
            video_path, start_frame, end_frame, bucket_selector
        )

        _, caption = self.get_caption(idx)

        # Load control video if enabled
        control_frames = None
        if self.load_control:
            control_path = self._get_control_path(video_path)
            if os.path.exists(control_path):
                try:
                    _, control_frames, _ = self.get_video_data_from_path(
                        control_path, start_frame, end_frame, bucket_selector
                    )
                    logger.debug(f"Loaded control video: {control_path}")
                except Exception as e:
                    logger.warning(f"Failed to load control video {control_path}: {e}")
            else:
                logger.debug(f"Control video not found: {control_path}")

        # Load mask video if enabled
        mask_frames = None
        if self.load_mask:
            mask_path = self._get_mask_path(video_path)
            if os.path.exists(mask_path):
                try:
                    _, mask_frames, _ = self.get_video_data_from_path(
                        mask_path, start_frame, end_frame, bucket_selector
                    )
                    # Convert to grayscale masks
                    mask_frames = [
                        frame[:, :, 0] if frame.ndim == 3 else frame
                        for frame in mask_frames
                    ]
                    logger.debug(f"Loaded mask video: {mask_path}")
                except Exception as e:
                    logger.warning(f"Failed to load mask video {mask_path}: {e}")
            else:
                logger.debug(f"Mask video not found: {mask_path}")

        return video_path, video_frames, caption, control_frames, mask_frames

    def _get_control_path(self, video_path: str) -> str:
        """Get the path for the control video based on the original video path."""
        base_path = os.path.splitext(video_path)[0]
        return f"{base_path}{self.control_suffix}.mp4"

    def _get_mask_path(self, video_path: str) -> str:
        """Get the path for the mask video based on the original video path."""
        if self.mask_path is None:
            # If no mask path is specified, look for mask in same directory as video
            base_path = os.path.splitext(video_path)[0]
            return f"{base_path}_mask.mp4"
        else:
            # If mask path is specified, look for mask with same name in mask directory
            video_name = os.path.basename(video_path)
            base_name = os.path.splitext(video_name)[0]
            return os.path.join(self.mask_path, f"{base_name}.mp4")

    def get_caption(self, idx: int) -> tuple[str, str]:
        global _missing_caption_warnings, _missing_caption_global_warning_shown
        video_path = self.video_paths[idx]

        # If no caption extension is specified, return empty caption
        if not self.caption_extension:
            return video_path, ""

        caption_path = os.path.splitext(video_path)[0] + self.caption_extension

        # Check if caption file exists
        if os.path.exists(caption_path):
            try:
                with open(caption_path, "r", encoding="utf-8") as f:
                    caption = f.read().strip()
                # Apply caption dropout if configured
                if self.caption_dropout_rate > 0.0:
                    try:
                        import random as _random

                        if _random.random() < self.caption_dropout_rate:
                            caption = ""
                    except Exception:
                        pass
                return video_path, caption
            except Exception as e:
                # Only warn once per file for read errors
                if caption_path not in _missing_caption_warnings:
                    logger.warning(f"Failed to read caption file {caption_path}: {e}")
                    _missing_caption_warnings.add(caption_path)
                # Return empty string as fallback
                return video_path, ""
        else:
            # Show global warning only once (shared with image datasets)
            if not _missing_caption_global_warning_shown:
                logger.warning(
                    "\nCaption files not found - using empty strings as captions"
                )
                logger.info("To disable this warning, either:")
                logger.info("  1. Create caption files for your videos, or")
                logger.info(
                    "  2. Remove or comment out 'caption_extension' in your config"
                )
                _missing_caption_global_warning_shown = True

            # Return empty string as fallback
            return video_path, ""

    def __iter__(self):
        self.current_idx = 0
        return self

    def __next__(self):
        if self.current_idx >= len(self.video_paths):
            raise StopIteration

        if self.caption_only:

            def create_caption_fetcher(index):
                return lambda: self.get_caption(index)

            fetcher = create_caption_fetcher(self.current_idx)

        else:

            def create_fetcher(index):
                return lambda: self.get_video_data(index)

            fetcher = create_fetcher(self.current_idx)

        self.current_idx += 1
        return fetcher
</file>

<file path="dataset/datasource_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

import glob
import os
from typing import Optional, Union
import logging
import numpy as np
from PIL import Image
import cv2
import av

from dataset.buckets import BucketSelector
from dataset.extensions import IMAGE_EXTENSIONS, VIDEO_EXTENSIONS


from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def glob_images(directory, base="*"):
    img_paths = []
    for ext in IMAGE_EXTENSIONS:
        if base == "*":
            img_paths.extend(
                glob.glob(os.path.join(glob.escape(directory), base + ext))
            )
        else:
            img_paths.extend(
                glob.glob(glob.escape(os.path.join(directory, base + ext)))
            )
    img_paths = list(set(img_paths))  # remove duplicates
    img_paths.sort()
    return img_paths


def glob_videos(directory, base="*"):
    video_paths = []
    for ext in VIDEO_EXTENSIONS:
        if base == "*":
            video_paths.extend(
                glob.glob(os.path.join(glob.escape(directory), base + ext))
            )
        else:
            video_paths.extend(
                glob.glob(glob.escape(os.path.join(directory, base + ext)))
            )
    video_paths = list(set(video_paths))  # remove duplicates
    video_paths.sort()
    return video_paths


def resize_image_to_bucket(
    image: Union[Image.Image, np.ndarray], bucket_reso: tuple[int, int]
) -> np.ndarray:
    """
    Resize the image to the bucket resolution.

    bucket_reso: **(width, height)**
    """
    is_pil_image = isinstance(image, Image.Image)
    if is_pil_image:
        image_width, image_height = image.size
    else:
        image_height, image_width = image.shape[:2]

    if bucket_reso == (image_width, image_height):
        return np.array(image) if is_pil_image else image

    bucket_width, bucket_height = bucket_reso

    # resize the image to the bucket resolution to match the short side
    scale_width = bucket_width / image_width
    scale_height = bucket_height / image_height
    scale = max(scale_width, scale_height)
    image_width = int(image_width * scale + 0.5)
    image_height = int(image_height * scale + 0.5)

    if scale > 1:
        image = Image.fromarray(image) if not is_pil_image else image

        image = image.resize((image_width, image_height), Image.Resampling.LANCZOS)  # type: ignore
        image = np.array(image)
    else:
        image = np.array(image) if is_pil_image else image
        image = cv2.resize(  # type: ignore
            image, (image_width, image_height), interpolation=cv2.INTER_AREA  # type: ignore
        )

    # crop the image to the bucket resolution
    crop_left = (image_width - bucket_width) // 2
    crop_top = (image_height - bucket_height) // 2
    image = image[
        crop_top : crop_top + bucket_height, crop_left : crop_left + bucket_width
    ]
    return image


def resize_image_to_bucket_lossless(
    image: Union[Image.Image, np.ndarray], bucket_reso: tuple[int, int]
) -> np.ndarray:
    """
    Resize the image to the bucket resolution using nearest-neighbor interpolation.
    Preserves exact pixel values when scaling by integer factors.

    Args:
        image: PIL Image or numpy array
        bucket_reso: Target resolution as (width, height)

    Returns:
        numpy.ndarray: Resized and cropped image

    Raises:
        ValueError: If image dimensions are invalid or bucket resolution is invalid
    """
    # Input validation
    if not isinstance(bucket_reso, tuple) or len(bucket_reso) != 2:
        raise ValueError("bucket_reso must be a tuple of (width, height)")

    bucket_width, bucket_height = bucket_reso
    if bucket_width <= 0 or bucket_height <= 0:
        raise ValueError("Bucket dimensions must be positive")

    is_pil_image = isinstance(image, Image.Image)
    if is_pil_image:
        image_width, image_height = image.size
    else:
        if not isinstance(image, np.ndarray):
            raise ValueError("Image must be PIL Image or numpy array")
        if len(image.shape) < 2:
            raise ValueError("Image must have at least 2 dimensions")
        image_height, image_width = image.shape[:2]

    # Validate image dimensions
    if image_width <= 0 or image_height <= 0:
        raise ValueError("Image dimensions must be positive")

    # If already at target size, return as numpy array
    if bucket_reso == (image_width, image_height):
        return np.array(image) if is_pil_image else image.copy()

    # Calculate scaling factors
    scale_width = bucket_width / image_width
    scale_height = bucket_height / image_height
    scale = max(scale_width, scale_height)

    # Calculate target dimensions preserving integer ratios where possible
    target_width = int(image_width * scale + 0.5)
    target_height = int(image_height * scale + 0.5)

    # Ensure minimum dimensions
    target_width = max(target_width, bucket_width)
    target_height = max(target_height, bucket_height)

    # Use nearest-neighbor interpolation for both upscaling and downscaling
    if is_pil_image:
        resized_image = image.resize(
            (target_width, target_height), Image.Resampling.NEAREST
        )
        resized_image = np.array(resized_image)
    else:
        resized_image = cv2.resize(
            image,
            (target_width, target_height),
            interpolation=cv2.INTER_NEAREST,
        )

    # Center crop to exact bucket size
    crop_left = max(0, (target_width - bucket_width) // 2)
    crop_top = max(0, (target_height - bucket_height) // 2)

    # Ensure crop coordinates are within bounds
    crop_left = min(crop_left, target_width - bucket_width)
    crop_top = min(crop_top, target_height - bucket_height)

    return resized_image[
        crop_top : crop_top + bucket_height, crop_left : crop_left + bucket_width
    ]


def load_video(
    video_path: str,
    start_frame: Optional[int] = None,
    end_frame: Optional[int] = None,
    bucket_selector: Optional[BucketSelector] = None,
    bucket_reso: Optional[tuple[int, int]] = None,
    source_fps: Optional[float] = None,
    target_fps: Optional[float] = None,
) -> list[np.ndarray]:
    # TODO: REFACTOR - This function is too complex and handles multiple responsibilities
    # Consider splitting into separate functions for video loading, frame extraction, and resizing
    # TODO: REFACTOR - Duplicate code blocks for video file vs directory handling should be extracted
    """
    bucket_reso: if given, resize the video to the bucket resolution, (width, height)
    """
    if source_fps is None or target_fps is None:
        if os.path.isfile(video_path):
            container = av.open(video_path)
            video = []
            for i, frame in enumerate(container.decode(video=0)):
                if start_frame is not None and i < start_frame:
                    continue
                if end_frame is not None and i >= end_frame:
                    break
                frame = frame.to_image()

                if bucket_selector is not None and bucket_reso is None:
                    bucket_reso = bucket_selector.get_bucket_resolution(
                        frame.size
                    )  # calc resolution from first frame

                if bucket_reso is not None:
                    frame = resize_image_to_bucket(frame, bucket_reso)
                else:
                    frame = np.array(frame)

                video.append(frame)
            container.close()
        else:
            # load images in the directory
            image_files = glob_images(video_path)
            image_files.sort()
            video = []
            for i in range(len(image_files)):
                if start_frame is not None and i < start_frame:
                    continue
                if end_frame is not None and i >= end_frame:
                    break

                image_file = image_files[i]
                image = Image.open(image_file).convert("RGB")

                if bucket_selector is not None and bucket_reso is None:
                    bucket_reso = bucket_selector.get_bucket_resolution(
                        image.size
                    )  # calc resolution from first frame
                image = np.array(image)
                if bucket_reso is not None:
                    image = resize_image_to_bucket(image, bucket_reso)

                video.append(image)
    else:
        # drop frames to match the target fps TODO commonize this code with the above if this works
        frame_index_delta = target_fps / source_fps  # example: 16 / 30 = 0.5333
        if os.path.isfile(video_path):
            container = av.open(video_path)
            video = []
            frame_index_with_fraction = 0.0
            previous_frame_index = -1
            for i, frame in enumerate(container.decode(video=0)):
                target_frame_index = int(frame_index_with_fraction)
                frame_index_with_fraction += frame_index_delta

                if target_frame_index == previous_frame_index:  # drop this frame
                    continue

                # accept this frame
                previous_frame_index = target_frame_index

                if start_frame is not None and target_frame_index < start_frame:
                    continue
                if end_frame is not None and target_frame_index >= end_frame:
                    break
                frame = frame.to_image()

                if bucket_selector is not None and bucket_reso is None:
                    bucket_reso = bucket_selector.get_bucket_resolution(
                        frame.size
                    )  # calc resolution from first frame

                if bucket_reso is not None:
                    frame = resize_image_to_bucket(frame, bucket_reso)
                else:
                    frame = np.array(frame)

                video.append(frame)
            container.close()
        else:
            # load images in the directory
            image_files = glob_images(video_path)
            image_files.sort()
            video = []
            frame_index_with_fraction = 0.0
            previous_frame_index = -1
            for i in range(len(image_files)):
                target_frame_index = int(frame_index_with_fraction)
                frame_index_with_fraction += frame_index_delta

                if target_frame_index == previous_frame_index:  # drop this frame
                    continue

                # accept this frame
                previous_frame_index = target_frame_index

                if start_frame is not None and target_frame_index < start_frame:
                    continue
                if end_frame is not None and target_frame_index >= end_frame:
                    break

                image_file = image_files[i]
                image = Image.open(image_file).convert("RGB")

                if bucket_selector is not None and bucket_reso is None:
                    bucket_reso = bucket_selector.get_bucket_resolution(
                        image.size
                    )  # calc resolution from first frame
                image = np.array(image)
                if bucket_reso is not None:
                    image = resize_image_to_bucket(image, bucket_reso)

                video.append(image)

    return video
</file>

<file path="dataset/extensions.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

import logging
from common.logger import get_logger
from common.dependencies import setup_pillow_extensions

logger = get_logger(__name__, level=logging.INFO)

# Base image extensions that are always supported
IMAGE_EXTENSIONS = [
    ".png",
    ".jpg",
    ".jpeg",
    ".webp",
    ".bmp",
    ".PNG",
    ".JPG",
    ".JPEG",
    ".WEBP",
    ".BMP",
]

# Use centralized optional dependency management to prevent duplicate warnings
pillow_extensions = setup_pillow_extensions()

# Add extensions based on what's available
if pillow_extensions["pillow_avif"] is not None:
    IMAGE_EXTENSIONS.extend([".avif", ".AVIF"])

if pillow_extensions["jxlpy"] is not None:
    IMAGE_EXTENSIONS.extend([".jxl", ".JXL"])

if pillow_extensions["pillow_jxl"] is not None and pillow_extensions["jxlpy"] is None:
    # Only add JXL extensions if jxlpy didn't already add them
    if ".jxl" not in IMAGE_EXTENSIONS:
        IMAGE_EXTENSIONS.extend([".jxl", ".JXL"])

VIDEO_EXTENSIONS = [
    ".mp4",
    ".webm",
    ".avi",
    ".mkv",
    ".mov",
    ".flv",
    ".wmv",
    ".m4v",
    ".mpg",
    ".mpeg",
    ".MP4",
    ".WEBM",
    ".AVI",
    ".MKV",
    ".MOV",
    ".FLV",
    ".WMV",
    ".M4V",
    ".MPG",
    ".MPEG",
]
</file>

<file path="dataset/frame_extraction.py">
from __future__ import annotations

from typing import List, Optional, Tuple

import numpy as np


def generate_crop_positions(
    frame_count: int,
    target_frames: Optional[List[int]],
    mode: str,
    frame_stride: Optional[int] = 1,
    frame_sample: Optional[int] = 1,
    max_frames: Optional[int] = None,
) -> List[Tuple[int, int]]:
    """
    Generate a list of (start_index, window_size) pairs for extracting frame windows.

    Args:
        frame_count: Total number of frames available in the video (F).
        target_frames: List of target window sizes (each is the number of frames in a clip).
        mode: Strategy name. Supported:
            - "head" (alias: "single_beginning")
            - "middle" (alias: "single_middle")
            - "chunk" (non-overlapping contiguous windows)
            - "slide" (sliding window with stride)
            - "uniform" (fixed count of evenly spaced starts)
            - "multiple_overlapping" (cover video with minimal number of windows, end-aligned)
            - "full" (use up to max_frames, rounded to N*4+1)
        frame_stride: Stride for "slide" mode.
        frame_sample: Number of samples for "uniform" mode.
        max_frames: Maximum frames for "full" mode.

    Returns:
        List of (start_index, window_size) tuples.
    """
    if not target_frames:
        return []

    crop_pos_and_frames: List[Tuple[int, int]] = []

    # Normalize aliases
    normalized_mode = mode
    if mode == "single_beginning":
        normalized_mode = "head"
    elif mode == "single_middle":
        normalized_mode = "middle"

    for target_frame in target_frames:
        if frame_count < target_frame:
            # Not enough frames to extract this window size
            continue

        if normalized_mode == "head":
            crop_pos_and_frames.append((0, target_frame))

        elif normalized_mode == "middle":
            start = (frame_count - target_frame) // 2
            crop_pos_and_frames.append((start, target_frame))

        elif normalized_mode == "chunk":
            for i in range(0, frame_count, target_frame):
                if i + target_frame <= frame_count:
                    crop_pos_and_frames.append((i, target_frame))

        elif normalized_mode == "slide":
            stride = frame_stride or 1
            for i in range(0, frame_count - target_frame + 1, stride):
                crop_pos_and_frames.append((i, target_frame))

        elif normalized_mode == "uniform":
            samples = frame_sample or 1
            starts = np.linspace(0, frame_count - target_frame, samples, dtype=int)
            for i in starts:
                crop_pos_and_frames.append((int(i), target_frame))

        elif normalized_mode == "multiple_overlapping":
            # Cover the whole video with minimal number of clips, end-aligned (may overlap)
            num_clips = ((frame_count - 1) // target_frame) + 1
            starts = np.linspace(0, frame_count - target_frame, num_clips, dtype=int)
            for i in starts:
                crop_pos_and_frames.append((int(i), target_frame))

        elif normalized_mode == "full":
            if max_frames is None or max_frames <= 0:
                use_frames = frame_count
            else:
                use_frames = min(frame_count, max_frames)
            # round to N*4+1 as per original implementation
            use_frames = (use_frames - 1) // 4 * 4 + 1
            crop_pos_and_frames.append((0, use_frames))

        else:
            raise ValueError(f"frame_extraction {mode} is not supported")

    return crop_pos_and_frames
</file>

<file path="dataset/hybrid_group.py">
import random
from typing import Optional, Sized, Protocol

import torch


class _LenGetItem(Protocol):
    def __len__(self) -> int: ...
    def __getitem__(self, idx: int): ...


class HybridDatasetGroup(torch.utils.data.Dataset):
    """A lightweight wrapper that mixes items from a main group and a correction group.

    - Non-invasive: does not alter underlying datasets
    - Probabilistic routing controlled by correction_ratio
    - Forwards epoch/step/max_step calls to both groups
    """

    def __init__(
        self,
        main_group: _LenGetItem,
        correction_group: _LenGetItem,
        correction_ratio: float = 0.2,
        rng: Optional[random.Random] = None,
    ) -> None:
        super().__init__()
        self.main_group = main_group
        self.correction_group = correction_group
        self.correction_ratio = max(0.0, min(1.0, float(correction_ratio)))
        self.rng = rng or random.Random()
        # Expose common metadata expected by trainer for logging
        try:
            self.num_train_items = getattr(self.main_group, "num_train_items")
        except Exception:
            try:
                self.num_train_items = len(self.main_group)
            except Exception:
                self.num_train_items = 0

        # Mirror dataset listing if available for log output
        self._datasets = getattr(self.main_group, "datasets", [])

    @property
    def datasets(self):  # for trainer logs
        return self._datasets

    def __len__(self) -> int:  # type: ignore[override]
        # Keep epoch length identical to main dataset to avoid altering scheduler math
        return len(self.main_group)

    def __getitem__(self, idx: int):  # type: ignore[override]
        use_correction = self.rng.random() < self.correction_ratio
        if use_correction and len(self.correction_group) > 0:
            # Map index into correction range to keep stochasticity but avoid OOB
            mapped_idx = idx % len(self.correction_group)
            return self.correction_group[mapped_idx]
        mapped_idx = idx % len(self.main_group)
        return self.main_group[mapped_idx]

    # Forwarders for Takenoko dataset lifecycle hooks
    def set_current_epoch(self, epoch, force_shuffle=None, reason=None):
        for ds in (self.main_group, self.correction_group):
            try:
                ds.set_current_epoch(epoch, force_shuffle=force_shuffle, reason=reason)  # type: ignore[attr-defined]
            except Exception:
                pass

    def set_current_step(self, step):
        for ds in (self.main_group, self.correction_group):
            try:
                ds.set_current_step(step)  # type: ignore[attr-defined]
            except Exception:
                pass

    def set_max_train_steps(self, max_train_steps):
        for ds in (self.main_group, self.correction_group):
            try:
                ds.set_max_train_steps(max_train_steps)  # type: ignore[attr-defined]
            except Exception:
                pass
</file>

<file path="dataset/image_video_dataset.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

from concurrent.futures import ThreadPoolExecutor
import glob
import os
import json
import random
import time
from typing import Any, Optional, Sequence, Tuple, Union
import logging
import numpy as np
import torch
from PIL import Image

from dataset.buckets import BucketBatchManager, BucketSelector
from dataset.data_sources import (
    ContentDataSource,
    ImageDirectoryDatasource,
    VideoDirectoryDataSource,
)
from dataset.datasource_utils import resize_image_to_bucket
from dataset.frame_extraction import generate_crop_positions

from dataset.item_info import ItemInfo
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

TARGET_FPS_WAN = 16


def get_cache_postfix(target_model: Optional[str] = None) -> Tuple[str, str]:
    """
    Get latent and text encoder cache postfixes based on target model.

    Args:
        target_model: Model type ("wan21", "wan22", etc.)

    Returns:
        Tuple of (latent_postfix, text_encoder_postfix)
    """
    if target_model == "wan21":
        return "wan21", "wan21_te"
    elif target_model == "wan22":
        return "wan22", "wan22_te"
    else:
        # Default fallback (for backwards compatibility)
        return "wan2x", "wan2x_te"


# Keep these for backwards compatibility, but they will be overridden by dynamic values
LATENT_CACHE_POSTFIX = "wan2x"
TEXT_ENCODER_CACHE_POSTFIX = "wan2x_te"


class BaseDataset(torch.utils.data.Dataset):
    def __init__(
        self,
        resolution: Tuple[int, int] = (960, 544),
        caption_extension: Optional[str] = None,
        caption_dropout_rate: float = 0.0,
        batch_size: int = 1,
        num_repeats: int = 1,
        enable_bucket: bool = False,
        bucket_no_upscale: bool = False,
        cache_directory: Optional[str] = None,
        debug_dataset: bool = False,
        is_val: bool = False,
        target_model: Optional[str] = None,
        is_reg: bool = False,
    ):
        # TODO: REFACTOR - Hardcoded default resolution should be configurable
        self.resolution = resolution
        self.caption_extension = caption_extension
        self.caption_dropout_rate = (
            float(caption_dropout_rate) if caption_dropout_rate is not None else 0.0
        )
        self.batch_size = batch_size
        self.num_repeats = num_repeats
        self.enable_bucket = enable_bucket
        self.bucket_no_upscale = bucket_no_upscale
        self.cache_directory = cache_directory
        self.debug_dataset = debug_dataset
        self.is_val = is_val
        self.target_model = target_model
        self.is_reg = is_reg
        self.seed = None
        self.current_epoch = 0
        self.dataset_index: Optional[int] = None  # Set by DatasetGroup when applicable

        # Get dynamic cache postfixes based on target model
        self.latent_cache_postfix, self.text_encoder_cache_postfix = get_cache_postfix(
            target_model
        )

        if not self.enable_bucket:
            self.bucket_no_upscale = False

    def get_dataset_type(self) -> str:
        """Get the type of this dataset for logging purposes."""
        return self.__class__.__name__

    def get_dataset_identifier(self) -> str:
        """Get a human-readable identifier for this dataset including type and index."""
        dataset_type = self.get_dataset_type()

        # Add validation suffix if this is a validation dataset
        if self.is_val:
            dataset_type += "(val)"

        # Add index if set (used in DatasetGroup)
        if self.dataset_index is not None:
            return f"{dataset_type}[{self.dataset_index}]"
        else:
            return dataset_type

    def get_dataset_details(self) -> str:
        """Get detailed information about this dataset for logging."""
        details = []

        # Add directory information if available
        if hasattr(self, "image_directory") and getattr(self, "image_directory", None):
            details.append(f"dir={os.path.basename(getattr(self, 'image_directory'))}")
        elif hasattr(self, "video_directory") and getattr(
            self, "video_directory", None
        ):
            details.append(f"dir={os.path.basename(getattr(self, 'video_directory'))}")

        # Add item count if available
        if hasattr(self, "num_train_items") and getattr(self, "num_train_items", 0) > 0:
            details.append(f"items={getattr(self, 'num_train_items')}")

        # Add batch size
        details.append(f"batch_size={self.batch_size}")

        if details:
            return f" ({', '.join(details)})"
        return ""

    def set_dataset_index(self, index: int):
        """Set the dataset index (used by DatasetGroup)."""
        self.dataset_index = index

    def get_metadata(self) -> dict:
        metadata = {
            "resolution": self.resolution,
            "caption_extension": self.caption_extension,
            "batch_size_per_device": self.batch_size,
            "num_repeats": self.num_repeats,
            "enable_bucket": bool(self.enable_bucket),
            "bucket_no_upscale": bool(self.bucket_no_upscale),
        }
        return metadata

    def get_all_latent_cache_files(self):
        return glob.glob(
            os.path.join(
                self.cache_directory, f"*_{self.latent_cache_postfix}.safetensors"  # type: ignore
            )  # type: ignore
        )

    def get_all_text_encoder_output_cache_files(self):
        return glob.glob(
            os.path.join(
                self.cache_directory, f"*_{self.text_encoder_cache_postfix}.safetensors"  # type: ignore
            )  # type: ignore
        )

    def get_latent_cache_path(self, item_info: ItemInfo) -> str:
        """
        Returns the cache path for the latent tensor.

        item_info: ItemInfo object

        Returns:
            str: cache path

        cache_path is based on the item_key and the resolution.
        """
        w, h = item_info.original_size
        basename = os.path.splitext(os.path.basename(item_info.item_key))[0]
        assert self.cache_directory is not None, "cache_directory is required"
        return os.path.join(
            self.cache_directory,
            f"{basename}_{w:04d}x{h:04d}_{self.latent_cache_postfix}.safetensors",
        )

    def get_text_encoder_output_cache_path(self, item_info: ItemInfo) -> str:
        basename = os.path.splitext(os.path.basename(item_info.item_key))[0]
        assert self.cache_directory is not None, "cache_directory is required"
        return os.path.join(
            self.cache_directory,
            f"{basename}_{self.text_encoder_cache_postfix}.safetensors",
        )

    def retrieve_latent_cache_batches(self, num_workers: int):
        raise NotImplementedError

    def retrieve_text_encoder_output_cache_batches(self, num_workers: int):
        raise NotImplementedError

    def set_seed(self, seed: int):
        self.seed = seed

    def set_current_epoch(self, epoch, force_shuffle=None, reason=None):
        """
        Set the current epoch for the dataset with conservative shuffling logic.

        Conservative Shuffling Philosophy:
        - Shuffling should be the EXCEPTION, not the rule
        - Only shuffle for true sequential epoch progression in training datasets
        - Never shuffle validation datasets automatically
        - Be conservative about all sync operations, checkpoint resumes, etc.

        Args:
            epoch: Target epoch number
            force_shuffle: If True, force bucket shuffling regardless of logic
                         If False, explicitly disable shuffling
                         If None, use conservative logic (default)
            reason: Optional string describing why epoch is being set (for better logging)
        """
        if self.current_epoch == epoch:
            return  # No change needed

        # Get dataset identifier for enhanced logging
        dataset_id = self.get_dataset_identifier()
        dataset_details = self.get_dataset_details()

        # Build context for logging
        context = f"[{dataset_id}{dataset_details}] (current_epoch: {self.current_epoch}, epoch: {epoch})"
        if reason:
            context += f" - {reason}"

        # Handle explicit shuffle control
        if force_shuffle is True:
            logger.info(f"Force shuffling buckets {context}")
            self.current_epoch = epoch
            self.shuffle_buckets()
            return

        if force_shuffle is False:
            logger.debug(f"Shuffling explicitly disabled {context}")
            self.current_epoch = epoch
            return

        # Conservative shuffling logic - shuffling should be rare and intentional
        is_sequential_increment = epoch == self.current_epoch + 1
        is_initialization = self.current_epoch == 0 and epoch > 0

        # CRITICAL: Validation datasets should NEVER shuffle automatically
        if self.is_val:
            logger.debug(
                f"Validation dataset epoch update {context} - no shuffle (validation datasets never shuffle)"
            )
            self.current_epoch = epoch
            return

        # CONSERVATIVE: Only shuffle in very specific training scenarios

        # 1. Normal sequential training progression (most common case)
        if is_sequential_increment:
            # Only shuffle if this appears to be genuine training progression
            if reason and any(
                keyword in reason.lower()
                for keyword in [
                    "sync",
                    "validation",
                    "collator",
                    "worker",
                    "dataloader",
                    "checkpoint",
                    "resume",
                ]
            ):
                # This is likely a sync operation, not genuine training progression
                logger.debug(
                    f"Sequential increment but sync operation detected {context} - no shuffle"
                )
                self.current_epoch = epoch
            else:
                # Genuine training epoch progression
                logger.info(f"Training epoch progression {context} - shuffling")
                self.current_epoch = epoch
                self.shuffle_buckets()
            return

        # 2. Initialization (first epoch)
        if is_initialization:
            # Only shuffle if this appears to be genuine training start
            if reason and any(
                keyword in reason.lower()
                for keyword in [
                    "sync",
                    "validation",
                    "collator",
                    "worker",
                    "dataloader",
                ]
            ):
                logger.debug(
                    f"Initialization but sync operation detected {context} - no shuffle"
                )
                self.current_epoch = epoch
            else:
                logger.info(f"Training initialization {context} - shuffling")
                self.current_epoch = epoch
                self.shuffle_buckets()
            return

        # 3. ALL OTHER SCENARIOS: Default to NO shuffling (conservative approach)

        # Classify the scenario for appropriate logging
        if epoch > self.current_epoch:
            if epoch - self.current_epoch > 1:
                # Large forward jump
                if reason and any(
                    keyword in reason.lower()
                    for keyword in ["resume", "checkpoint", "restore", "load"]
                ):
                    logger.info(
                        f"Checkpoint resume detected {context} - no shuffle (preserving reproducibility)"
                    )
                elif reason and any(
                    keyword in reason.lower()
                    for keyword in [
                        "validation",
                        "sync",
                        "collator",
                        "worker",
                        "dataloader",
                    ]
                ):
                    logger.debug(f"Sync operation large jump {context} - no shuffle")
                else:
                    logger.warning(
                        f"Large epoch jump {context} - no shuffle (use force_shuffle=True if needed)"
                    )
            else:
                # Small forward jump (shouldn't happen with good logic above, but handle gracefully)
                logger.debug(f"Small forward jump {context} - no shuffle")
        elif epoch < self.current_epoch:
            # Backward jump
            logger.warning(
                f"Backward epoch change {context} - no shuffle (unusual scenario)"
            )
        else:
            # Same epoch (already handled above, but defensive programming)
            logger.debug(f"Same epoch update {context} - no shuffle")

        # In all cases above, just update epoch without shuffling
        self.current_epoch = epoch

    def set_current_step(self, step):
        dataset_id = self.get_dataset_identifier()
        logger.debug(f"[{dataset_id}] Setting current step: {step}")
        self.current_step = step

    def set_max_train_steps(self, max_train_steps):
        dataset_id = self.get_dataset_identifier()
        logger.debug(f"[{dataset_id}] Setting max train steps: {max_train_steps}")
        self.max_train_steps = max_train_steps

    def shuffle_buckets(self):
        raise NotImplementedError

    def __len__(self):
        return NotImplementedError

    def __getitem__(self, idx):
        raise NotImplementedError

    def _default_retrieve_text_encoder_output_cache_batches(
        self, datasource: ContentDataSource, batch_size: int, num_workers: int
    ):
        # TODO: REFACTOR - The ItemInfo creation with dummy values (0,0) is questionable
        datasource.set_caption_only(True)
        executor = ThreadPoolExecutor(max_workers=num_workers)

        data: list[ItemInfo] = []
        futures = []

        def aggregate_future(consume_all: bool = False):
            while len(futures) >= num_workers or (consume_all and len(futures) > 0):
                completed_futures = [future for future in futures if future.done()]
                if len(completed_futures) == 0:
                    if (
                        len(futures) >= num_workers or consume_all
                    ):  # to avoid adding too many futures
                        time.sleep(0.1)
                        continue
                    else:
                        break  # submit batch if possible

                for future in completed_futures:
                    item_key, caption = future.result()
                    item_info = ItemInfo(item_key, caption, (0, 0), (0, 0), is_reg=self.is_reg)  # type: ignore
                    item_info.text_encoder_output_cache_path = (
                        self.get_text_encoder_output_cache_path(item_info)
                    )
                    data.append(item_info)

                    futures.remove(future)

        def submit_batch(flush: bool = False):
            nonlocal data
            if len(data) >= batch_size or (len(data) > 0 and flush):
                batch = data[0:batch_size]
                if len(data) > batch_size:
                    data = data[batch_size:]
                else:
                    data = []
                return batch
            return None

        for fetch_op in datasource:
            future = executor.submit(fetch_op)
            futures.append(future)
            aggregate_future()
            while True:
                batch = submit_batch()
                if batch is None:
                    break
                yield batch

        aggregate_future(consume_all=True)
        while True:
            batch = submit_batch(flush=True)
            if batch is None:
                break
            yield batch

        executor.shutdown()


class ImageDataset(BaseDataset):
    def __init__(
        self,
        resolution: Tuple[int, int],
        caption_extension: Optional[str],
        batch_size: int,
        num_repeats: int,
        enable_bucket: bool,
        bucket_no_upscale: bool,
        image_directory: Optional[str] = None,
        cache_directory: Optional[str] = None,
        debug_dataset: bool = False,
        is_val: bool = False,
        load_control: bool = False,
        control_suffix: str = "_control",
        target_model: Optional[str] = None,
        mask_path: Optional[str] = None,
        is_reg: bool = False,
        caption_dropout_rate: float = 0.0,
    ):
        super().__init__(
            resolution=resolution,
            caption_extension=caption_extension,
            caption_dropout_rate=caption_dropout_rate,
            batch_size=batch_size,
            num_repeats=num_repeats,
            enable_bucket=enable_bucket,
            bucket_no_upscale=bucket_no_upscale,
            cache_directory=cache_directory,
            debug_dataset=debug_dataset,
            is_val=is_val,
            target_model=target_model,
            is_reg=is_reg,
        )

        self.image_directory = image_directory
        self.load_control = load_control
        self.control_suffix = control_suffix
        self.mask_path = mask_path

        if image_directory is not None:
            self.datasource = ImageDirectoryDatasource(
                image_directory,
                caption_extension,
            )
            # propagate caption dropout rate to datasource
            if hasattr(self, "caption_dropout_rate"):
                try:
                    self.datasource.set_caption_dropout_rate(self.caption_dropout_rate)
                except Exception:
                    pass

        else:
            raise ValueError("‚õî image_directory must be specified")

        if self.cache_directory is None:
            self.cache_directory = self.image_directory

        self.batch_manager = None
        self.num_train_items = 0

        # Set control settings if provided
        if hasattr(self, "load_control") and hasattr(self, "control_suffix"):
            self.datasource.set_control_settings(self.load_control, self.control_suffix)

        # Set mask settings if provided
        if hasattr(self.datasource, "set_mask_settings"):
            self.datasource.set_mask_settings(
                load_mask=bool(self.mask_path),
                mask_path=self.mask_path,
                default_mask_file=None,
            )

        # Update control availability after settings have been applied
        self.has_control = self.datasource.has_control

    def get_metadata(self):
        metadata = super().get_metadata()
        if self.image_directory is not None:
            metadata["image_directory"] = os.path.basename(self.image_directory)
        metadata["has_control"] = self.has_control
        return metadata

    def get_total_image_count(self):
        return len(self.datasource) if self.datasource.is_indexable() else None

    def retrieve_latent_cache_batches(self, num_workers: int):
        buckset_selector = BucketSelector(
            self.resolution,
            self.enable_bucket,
            self.bucket_no_upscale,
        )
        executor = ThreadPoolExecutor(max_workers=num_workers)

        batches: dict[tuple[int, int], list[ItemInfo]] = (
            {}
        )  # (width, height) -> [ItemInfo]
        futures = []

        # aggregate futures and sort by bucket resolution
        def aggregate_future(consume_all: bool = False):
            while len(futures) >= num_workers or (consume_all and len(futures) > 0):
                completed_futures = [future for future in futures if future.done()]
                if len(completed_futures) == 0:
                    if (
                        len(futures) >= num_workers or consume_all
                    ):  # to avoid adding too many futures
                        time.sleep(0.1)
                        continue
                    else:
                        break  # submit batch if possible

                for future in completed_futures:
                    original_size, item_key, image, caption, controls, masks = (
                        future.result()
                    )
                    bucket_height, bucket_width = image.shape[:2]
                    bucket_reso = (bucket_width, bucket_height)

                    # Create ItemInfo with control content if available
                    item_info = ItemInfo(
                        item_key=item_key,
                        caption=caption,
                        original_size=original_size,
                        bucket_size=bucket_reso,  # This should be a tuple # type: ignore
                        content=image,
                        latent_cache_path=self.get_latent_cache_path(
                            ItemInfo(item_key, caption, original_size, bucket_reso)  # type: ignore
                        ),
                        weight=1.0,  # Placeholder, will be updated later
                        control_content=controls,  # Add control content
                        mask_content=masks,  # Add mask content
                        is_reg=self.is_reg,
                    )
                    item_info.latent_cache_path = self.get_latent_cache_path(item_info)

                    # Add control content if available
                    if controls is not None and len(controls) > 0:
                        # For now, use the first control image
                        item_info.control_content = controls[0]

                    if bucket_reso not in batches:
                        batches[bucket_reso] = []
                    batches[bucket_reso].append(item_info)

                    futures.remove(future)

        # submit batch if some bucket has enough items
        def submit_batch(flush: bool = False):
            for key in batches:
                if len(batches[key]) >= self.batch_size or flush:
                    batch = batches[key][0 : self.batch_size]
                    if len(batches[key]) > self.batch_size:
                        batches[key] = batches[key][self.batch_size :]
                    else:
                        del batches[key]
                    return key, batch
            return None, None

        for fetch_op in self.datasource:
            # fetch and resize image in a separate thread
            def fetch_and_resize(
                op: callable,  # type: ignore
            ) -> tuple[
                tuple[int, int],
                str,
                Image.Image,
                str,
                Optional[np.ndarray],
                Optional[np.ndarray],
            ]:
                image_key, image, caption, controls, masks = op()
                image: Image.Image
                image_size = image.size

                bucket_reso = buckset_selector.get_bucket_resolution(image_size)
                image = resize_image_to_bucket(image, bucket_reso)  # type: ignore # returns np.ndarray
                resized_controls = None
                resized_masks = None
                if controls is not None:
                    # Resize control signal to match the bucket resolution
                    if isinstance(controls, Image.Image):
                        controls = controls.resize(
                            bucket_reso, Image.Resampling.LANCZOS
                        )
                        controls = np.array(controls)
                    elif isinstance(controls, np.ndarray):
                        # Assuming controls is already in the right format
                        # You might need to resize it here if needed
                        pass
                    resized_controls = controls

                if masks is not None:
                    # Resize mask to match the bucket resolution
                    if isinstance(masks, Image.Image):
                        masks = masks.resize(bucket_reso, Image.Resampling.LANCZOS)
                        masks = np.array(masks)
                    elif isinstance(masks, np.ndarray):
                        # Assuming masks is already in the right format
                        # You might need to resize it here if needed
                        pass
                    resized_masks = masks

                return (
                    image_size,
                    image_key,
                    image,
                    caption,
                    resized_controls,
                    resized_masks,
                )

            future = executor.submit(fetch_and_resize, fetch_op)
            futures.append(future)
            aggregate_future()
            while True:
                key, batch = submit_batch()
                if key is None:
                    break
                yield key, batch

        aggregate_future(consume_all=True)
        while True:
            key, batch = submit_batch(flush=True)
            if key is None:
                break
            yield key, batch

        executor.shutdown()

    def retrieve_text_encoder_output_cache_batches(self, num_workers: int):
        return self._default_retrieve_text_encoder_output_cache_batches(
            self.datasource, self.batch_size, num_workers
        )

    def prepare_for_training(
        self,
        load_pixels_for_control=False,
        require_text_encoder_cache=True,
        prior_loss_weight: float = 1.0,
        num_timestep_buckets: Optional[int] = None,
    ):
        dataset_id = self.get_dataset_identifier()
        logger.info(
            f"[{dataset_id}] Preparing for training (load_pixels_for_control={load_pixels_for_control}, require_text_encoder_cache={require_text_encoder_cache})"
        )

        bucket_selector = BucketSelector(
            self.resolution,
            self.enable_bucket,
            self.bucket_no_upscale,
        )

        # glob cache files
        latent_cache_files = glob.glob(
            os.path.join(
                self.cache_directory, f"*_{self.latent_cache_postfix}.safetensors"  # type: ignore
            )  # type: ignore
        )

        logger.info(
            f"[{dataset_id}] Found {len(latent_cache_files)} latent cache files"
        )

        # Enhanced error handling for missing cache files
        if len(latent_cache_files) == 0:
            logger.error(
                f"[{dataset_id}] ‚ùå No latent cache files found in {self.cache_directory}! "
                f"Expected files matching pattern: *_{self.latent_cache_postfix}.safetensors"
            )
            logger.error(
                f"[{dataset_id}] This will result in 0 training items and training failure."
            )
            logger.info("üí° To fix this issue:")
            logger.info("   1. Run latent caching first using the cache_latents script")
            logger.info("   2. Ensure the cache_directory path is correct")
            logger.info(
                "   3. Verify your dataset configuration matches the cached data"
            )

        # assign cache files to item info
        bucketed_item_info: dict[tuple[int, int], list[ItemInfo]] = {}
        skipped_items = 0
        processed_items = 0

        for cache_file in latent_cache_files:
            tokens = os.path.basename(cache_file).split("_")

            image_size = tokens[-2]  # 0000x0000
            image_width, image_height = map(int, image_size.split("x"))
            image_size = (image_width, image_height)

            item_key = "_".join(tokens[:-2])
            text_encoder_output_cache_file = os.path.join(
                self.cache_directory,  # type: ignore
                f"{item_key}_{self.text_encoder_cache_postfix}.safetensors",
            )

            # Check text encoder cache existence based on requirement
            text_encoder_cache_exists = os.path.exists(text_encoder_output_cache_file)
            if require_text_encoder_cache and not text_encoder_cache_exists:
                logger.warning(
                    f"[{dataset_id}] Text encoder cache missing for {item_key} - skipping item"
                )
                skipped_items += 1
                continue
            elif not require_text_encoder_cache and not text_encoder_cache_exists:
                # During latent caching phase, text encoder cache may not exist yet
                logger.debug(
                    f"Text encoder cache not yet available: {text_encoder_output_cache_file}"
                )

            bucket_reso = bucket_selector.get_bucket_resolution(image_size)

            # Load actual pixels if requested (for control LoRA)
            content = None
            if load_pixels_for_control:
                try:
                    # Reconstruct original image path from cache file name
                    image_path = self._get_original_image_path_from_cache(
                        cache_file, item_key
                    )
                    if image_path and os.path.exists(image_path):
                        # Load and resize image
                        image = Image.open(image_path).convert("RGB")
                        content = resize_image_to_bucket(
                            image, bucket_reso
                        )  # returns np.ndarray
                        logger.debug(
                            f"Loaded pixels for control LoRA: {image_path}, shape: {content.shape}"
                        )
                    else:
                        logger.warning(
                            f"Could not find original image file for cache: {cache_file}"
                        )
                except Exception as e:
                    logger.warning(f"Failed to load pixels for {cache_file}: {e}")

            item_info = ItemInfo(
                item_key,
                "",
                image_size,
                bucket_reso,  # type: ignore
                content=content,
                latent_cache_path=cache_file,
                is_reg=self.is_reg,
            )
            # Only set text encoder cache path if it exists or is not required
            if text_encoder_cache_exists:
                item_info.text_encoder_output_cache_path = (
                    text_encoder_output_cache_file
                )
            else:
                item_info.text_encoder_output_cache_path = None

            bucket = bucketed_item_info.get(bucket_reso, [])
            for _ in range(self.num_repeats):
                bucket.append(item_info)
            bucketed_item_info[bucket_reso] = bucket
            processed_items += 1

        # Enhanced reporting
        if skipped_items > 0:
            logger.warning(
                f"[{dataset_id}] ‚ö†Ô∏è  Skipped {skipped_items} items due to missing text encoder cache files"
            )
            if require_text_encoder_cache:
                logger.info("üí° To include these items:")
                logger.info("   1. Run text encoder caching for the missing files")
                logger.info(
                    "   2. Or set require_text_encoder_cache=False (not recommended for training)"
                )

        # prepare batch manager
        self.batch_manager = BucketBatchManager(
            bucketed_item_info, self.batch_size, prior_loss_weight
        )
        # Store per-epoch timestep bucketing preference on the batch manager if supported
        try:
            if hasattr(self.batch_manager, "set_num_timestep_buckets"):
                self.batch_manager.set_num_timestep_buckets(num_timestep_buckets)  # type: ignore
            elif hasattr(self.batch_manager, "num_timestep_buckets"):
                setattr(self.batch_manager, "num_timestep_buckets", num_timestep_buckets)  # type: ignore
        except Exception:
            pass
        self.batch_manager.show_bucket_info()

        self.num_train_items = sum(
            [len(bucket) for bucket in bucketed_item_info.values()]
        )

        # Final validation
        if self.num_train_items == 0:
            logger.error(
                f"[{dataset_id}] ‚ùå Dataset preparation resulted in 0 training items! "
                f"Training will fail. Please check cache files and dataset configuration."
            )
        else:
            logger.info(
                f"[{dataset_id}] ‚úÖ Training preparation complete: {self.num_train_items} items across {len(bucketed_item_info)} buckets"
            )

    def shuffle_buckets(self):
        dataset_id = self.get_dataset_identifier()
        # set random seed for this epoch
        random.seed(self.seed + self.current_epoch)  # type: ignore
        logger.debug(
            f"[{dataset_id}] Shuffling buckets with seed={self.seed + self.current_epoch}"  # type: ignore
        )
        self.batch_manager.shuffle()  # type: ignore

    def __len__(self):
        if self.batch_manager is None:
            return 100  # dummy value
        return len(self.batch_manager)

    def __getitem__(self, idx):
        return self.batch_manager[idx]  # type: ignore

    def _get_original_image_path_from_cache(
        self, cache_file: str, item_key: str
    ) -> str:
        """Try to find the original image file that corresponds to a cache file."""
        # The item_key is derived from the original filename
        # Try common image extensions
        image_extensions = [".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tiff"]

        # Check in the image directory
        if self.image_directory:
            for ext in image_extensions:
                potential_path = os.path.join(self.image_directory, item_key + ext)
                if os.path.exists(potential_path):
                    return potential_path

        # If not found in image_directory, try looking in subdirectories
        # This handles cases where images might be in nested folders
        cache_dir = os.path.dirname(cache_file)
        for root, dirs, files in os.walk(cache_dir):
            for ext in image_extensions:
                potential_file = item_key + ext
                if potential_file in files:
                    return os.path.join(root, potential_file)

        # Last resort: search more broadly
        if self.image_directory:
            for root, dirs, files in os.walk(self.image_directory):
                for ext in image_extensions:
                    potential_file = item_key + ext
                    if potential_file in files:
                        return os.path.join(root, potential_file)

        return ""

    def _process_mask_image(
        self, mask_image: Optional[Image.Image]
    ) -> Optional[torch.Tensor]:
        """Process mask image and convert to tensor for training.

        Args:
            mask_image: PIL Image in grayscale (L mode)

        Returns:
            Tensor with values between 0 and 1, where:
            - 1.0 (white) means train on this pixel
            - 0.0 (black) means mask it out
            - Values in between become weights between 0 and 1
        """
        if mask_image is None:
            return None

        # Convert PIL image to tensor
        mask_tensor = torch.from_numpy(np.array(mask_image)).float()

        # Normalize to [0, 1] range
        if mask_tensor.max() > 1.0:
            mask_tensor = mask_tensor / 255.0

        return mask_tensor

    def _process_mask_video(
        self, mask_frames: Optional[list[np.ndarray]]
    ) -> Optional[torch.Tensor]:
        """Process mask video frames and convert to tensor for training.

        Args:
            mask_frames: List of numpy arrays representing mask frames

        Returns:
            Tensor with values between 0 and 1, where:
            - 1.0 (white) means train on this pixel
            - 0.0 (black) means mask it out
            - Values in between become weights between 0 and 1
        """
        if mask_frames is None:
            return None

        # Convert list of numpy arrays to tensor
        mask_tensor = torch.from_numpy(np.stack(mask_frames)).float()

        # Normalize to [0, 1] range
        if mask_tensor.max() > 1.0:
            mask_tensor = mask_tensor / 255.0

        return mask_tensor


class VideoDataset(BaseDataset):
    def __init__(
        self,
        resolution: Tuple[int, int],
        caption_extension: Optional[str],
        batch_size: int,
        num_repeats: int,
        enable_bucket: bool,
        bucket_no_upscale: bool,
        frame_extraction: Optional[str] = "head",
        frame_stride: Optional[int] = 1,
        frame_sample: Optional[int] = 1,
        target_frames: Optional[list[int]] = None,
        max_frames: Optional[int] = None,
        source_fps: Optional[float] = None,
        video_directory: Optional[str] = None,
        cache_directory: Optional[str] = None,
        debug_dataset: bool = False,
        is_val: bool = False,
        load_control: bool = False,
        control_suffix: str = "_control",
        target_model: Optional[str] = None,
        mask_path: Optional[str] = None,
        is_reg: bool = False,
        caption_dropout_rate: float = 0.0,
    ):
        super().__init__(
            resolution=resolution,
            caption_extension=caption_extension,
            caption_dropout_rate=caption_dropout_rate,
            batch_size=batch_size,
            num_repeats=num_repeats,
            enable_bucket=enable_bucket,
            bucket_no_upscale=bucket_no_upscale,
            cache_directory=cache_directory,
            debug_dataset=debug_dataset,
            is_val=is_val,
            target_model=target_model,
            is_reg=is_reg,
        )

        self.video_directory = video_directory
        self.frame_extraction = frame_extraction
        self.frame_stride = frame_stride
        self.frame_sample = frame_sample
        self.target_frames = target_frames
        self.max_frames = max_frames
        self.source_fps = source_fps
        self.target_fps = TARGET_FPS_WAN
        self.load_control = load_control
        self.control_suffix = control_suffix
        self.mask_path = mask_path

        if video_directory is not None:
            self.datasource = VideoDirectoryDataSource(
                video_directory, caption_extension
            )
            # propagate caption dropout rate to datasource
            if hasattr(self, "caption_dropout_rate"):
                try:
                    self.datasource.set_caption_dropout_rate(self.caption_dropout_rate)  # type: ignore
                except Exception:
                    pass
            # Set up control loading if enabled
            if self.load_control:
                self.datasource.set_control_settings(
                    self.load_control, self.control_suffix
                )

            # Set up mask loading if enabled
            if hasattr(self.datasource, "set_mask_settings"):
                self.datasource.set_mask_settings(
                    load_mask=bool(self.mask_path),
                    mask_path=self.mask_path,
                    default_mask_file=None,
                )

        if self.frame_extraction == "uniform" and self.frame_sample == 1:
            self.frame_extraction = "head"
            logger.warning(
                "frame_sample is set to 1 for frame_extraction=uniform. frame_extraction is changed to head."
            )
        if self.frame_extraction == "head":
            # head extraction. we can limit the number of frames to be extracted
            self.datasource.set_start_and_end_frame(0, max(self.target_frames))  # type: ignore

        if self.cache_directory is None:
            self.cache_directory = self.video_directory

        self.batch_manager = None
        self.num_train_items = 0
        self.has_control = self.datasource.has_control

    def get_metadata(self):
        metadata = super().get_metadata()
        if self.video_directory is not None:
            metadata["video_directory"] = os.path.basename(self.video_directory)

        metadata["frame_extraction"] = self.frame_extraction
        metadata["frame_stride"] = self.frame_stride
        metadata["frame_sample"] = self.frame_sample
        metadata["target_frames"] = self.target_frames
        metadata["max_frames"] = self.max_frames
        metadata["source_fps"] = self.source_fps

        return metadata

    def retrieve_latent_cache_batches(self, num_workers: int):
        # Keep mask loading enabled so that we can pre-cache masks to `_mask.safetensors` alongside latents.
        # This path will forward per-sample masks through `ItemInfo(mask_content=...)` so the collator can save them.
        buckset_selector = BucketSelector(self.resolution)
        self.datasource.set_bucket_selector(buckset_selector)
        if self.source_fps is not None:
            self.datasource.set_source_and_target_fps(self.source_fps, self.target_fps)
        else:
            self.datasource.set_source_and_target_fps(None, None)  # no conversion

        executor = ThreadPoolExecutor(max_workers=num_workers)

        # key: (width, height, frame_count) and optional latent_window_size, value: [ItemInfo]
        batches: dict[tuple[Any], list[ItemInfo]] = {}
        futures = []

        def aggregate_future(consume_all: bool = False):
            while len(futures) >= num_workers or (consume_all and len(futures) > 0):
                completed_futures = [future for future in futures if future.done()]
                if len(completed_futures) == 0:
                    if (
                        len(futures) >= num_workers or consume_all
                    ):  # to avoid adding too many futures
                        time.sleep(0.1)
                        continue
                    else:
                        break  # submit batch if possible

                for future in completed_futures:
                    original_frame_size, video_key, video, caption, control, mask = (
                        future.result()
                    )

                    frame_count = len(video)
                    video = np.stack(video, axis=0)
                    height, width = video.shape[1:3]
                    bucket_reso = (width, height)  # already resized

                    # process control/mask videos if available
                    control_video = control
                    mask_video = None
                    if mask is not None:
                        try:
                            mask = [
                                resize_image_to_bucket(frame, bucket_reso)
                                for frame in mask
                            ]
                            # Ensure grayscale (H,W). If frames are RGB, take first channel
                            mask = [f[:, :, 0] if f.ndim == 3 else f for f in mask]
                            mask_video = mask
                        except Exception as e:
                            logger.warning(
                                f"Failed to resize mask video for {video_key}: {e}"
                            )

                    # Frame window generation extracted to helper for readability and extensibility
                    crop_pos_and_frames = generate_crop_positions(
                        frame_count=frame_count,
                        target_frames=self.target_frames,
                        mode=self.frame_extraction,  # type: ignore
                        frame_stride=self.frame_stride,
                        frame_sample=self.frame_sample,
                        max_frames=self.max_frames,
                    )

                    for crop_pos, target_frame in crop_pos_and_frames:
                        cropped_video = video[crop_pos : crop_pos + target_frame]
                        body, ext = os.path.splitext(video_key)
                        item_key = f"{body}_{crop_pos:05d}-{target_frame:03d}{ext}"
                        batch_key = (
                            *bucket_reso,
                            target_frame,
                        )  # bucket_reso with frame_count

                        # crop control video if available
                        cropped_control = None
                        if control_video is not None:
                            cropped_control = control_video[
                                crop_pos : crop_pos + target_frame
                            ]
                            try:
                                cropped_control = np.stack(cropped_control, axis=0)
                            except Exception:
                                pass

                        # crop mask video if available
                        cropped_mask = None
                        if mask_video is not None:
                            try:
                                cropped_mask = mask_video[
                                    crop_pos : crop_pos + target_frame
                                ]
                                cropped_mask = np.stack(cropped_mask, axis=0)
                            except Exception:
                                cropped_mask = None

                        item_info = ItemInfo(
                            item_key,
                            caption,
                            original_frame_size,
                            batch_key,  # type: ignore
                            frame_count=target_frame,
                            content=cropped_video,
                            control_content=cropped_control,
                            mask_content=cropped_mask,
                            is_reg=self.is_reg,
                        )
                        item_info.latent_cache_path = self.get_latent_cache_path(
                            item_info
                        )

                        batch = batches.get(batch_key, [])  # type: ignore
                        batch.append(item_info)
                        batches[batch_key] = batch  # type: ignore

                    futures.remove(future)

        def submit_batch(flush: bool = False):
            for key in batches:
                if len(batches[key]) >= self.batch_size or flush:
                    batch = batches[key][0 : self.batch_size]
                    if len(batches[key]) > self.batch_size:
                        batches[key] = batches[key][self.batch_size :]
                    else:
                        del batches[key]
                    return key, batch
            return None, None

        for operator in self.datasource:

            def fetch_and_resize(
                op: callable,  # type: ignore
            ) -> tuple[
                tuple[int, int],
                str,
                list[np.ndarray],
                str,
                Optional[list[np.ndarray]],
                Optional[list[np.ndarray]],
            ]:
                try:
                    result = op()

                    if (
                        len(result) == 3
                    ):  # for backward compatibility TODO remove this in the future
                        video_key, video, caption = result
                        control = None
                    elif len(result) == 4:
                        video_key, video, caption, control = result
                    elif len(result) == 5:
                        video_key, video, caption, control, mask = result
                    else:
                        raise ValueError(
                            f"Unexpected number of values from datasource: {len(result)}"
                        )

                    # Validate that video is a list of numpy arrays
                    if not isinstance(video, list) or len(video) == 0:
                        raise ValueError(
                            f"Invalid video data for {video_key}: expected list of numpy arrays, got {type(video)}"
                        )

                    if not isinstance(video[0], np.ndarray):
                        raise ValueError(
                            f"Invalid video frame data for {video_key}: expected numpy array, got {type(video[0])}"
                        )

                    video: list[np.ndarray]
                    frame_size = (video[0].shape[1], video[0].shape[0])

                    # resize if necessary
                    bucket_reso = buckset_selector.get_bucket_resolution(frame_size)
                    video = [
                        resize_image_to_bucket(frame, bucket_reso) for frame in video
                    ]

                    # resize control if necessary
                    if control is not None:
                        control = [
                            resize_image_to_bucket(frame, bucket_reso)
                            for frame in control
                        ]

                    # resize mask if necessary
                    if "mask" in locals() and mask is not None:
                        mask = [
                            resize_image_to_bucket(frame, bucket_reso) for frame in mask
                        ]
                        # Ensure grayscale (H, W)
                        mask = [
                            frame[:, :, 0] if frame.ndim == 3 else frame
                            for frame in mask
                        ]

                    return (
                        frame_size,
                        video_key,
                        video,
                        caption,
                        control,
                        mask if "mask" in locals() else None,
                    )
                except Exception as e:
                    logger.error(f"Error processing video: {e}")
                    raise

            future = executor.submit(fetch_and_resize, operator)
            futures.append(future)
            aggregate_future()
            while True:
                key, batch = submit_batch()
                if key is None:
                    break
                yield key, batch

        aggregate_future(consume_all=True)
        while True:
            key, batch = submit_batch(flush=True)
            if key is None:
                break
            yield key, batch

        executor.shutdown()

    def retrieve_text_encoder_output_cache_batches(self, num_workers: int):
        return self._default_retrieve_text_encoder_output_cache_batches(
            self.datasource, self.batch_size, num_workers
        )

    def prepare_for_training(
        self,
        load_pixels_for_control=False,
        require_text_encoder_cache=True,
        prior_loss_weight: float = 1.0,
    ):
        dataset_id = self.get_dataset_identifier()
        logger.info(
            f"[{dataset_id}] Preparing for training (load_pixels_for_control={load_pixels_for_control}, require_text_encoder_cache={require_text_encoder_cache})"
        )

        bucket_selector = BucketSelector(
            self.resolution,
            self.enable_bucket,
            self.bucket_no_upscale,
        )

        # glob cache files
        latent_cache_files = glob.glob(
            os.path.join(
                self.cache_directory, f"*_{self.latent_cache_postfix}.safetensors"  # type: ignore
            )  # type: ignore
        )

        logger.info(
            f"[{dataset_id}] Found {len(latent_cache_files)} latent cache files"
        )

        # assign cache files to item info
        bucketed_item_info: dict[tuple[int, int, int], list[ItemInfo]] = (
            {}
        )  # (width, height, frame_count) -> [ItemInfo]
        for cache_file in latent_cache_files:
            tokens = os.path.basename(cache_file).split("_")

            image_size = tokens[-2]  # 0000x0000
            image_width, image_height = map(int, image_size.split("x"))
            image_size = (image_width, image_height)

            frame_pos, frame_count = tokens[-3].split("-")[
                :2
            ]  # "00000-000", or optional section index "00000-000-00"
            frame_pos, frame_count = int(frame_pos), int(frame_count)

            item_key = "_".join(tokens[:-3])
            text_encoder_output_cache_file = os.path.join(
                self.cache_directory,  # type: ignore
                f"{item_key}_{self.text_encoder_cache_postfix}.safetensors",
            )

            # Check text encoder cache existence based on requirement
            text_encoder_cache_exists = os.path.exists(text_encoder_output_cache_file)
            if require_text_encoder_cache and not text_encoder_cache_exists:
                logger.warning(
                    f"Text encoder output cache file not found: {text_encoder_output_cache_file}"
                )
                continue
            elif not require_text_encoder_cache and not text_encoder_cache_exists:
                # During latent caching phase, text encoder cache may not exist yet
                logger.debug(
                    f"Text encoder cache not yet available: {text_encoder_output_cache_file}"
                )

            bucket_reso = bucket_selector.get_bucket_resolution(image_size)
            bucket_reso = (*bucket_reso, frame_count)

            # Load actual video pixels if requested (for control LoRA)
            content = None
            if load_pixels_for_control:
                logger.debug(f"Loading video pixels for control LoRA: {cache_file}")
                try:
                    # Reconstruct original video path from cache file name
                    video_path = self._get_original_video_path_from_cache(
                        cache_file, item_key
                    )
                    logger.debug(f"Reconstructed video path: {video_path}")
                    logger.debug(
                        f"Video path exists: {os.path.exists(video_path) if video_path else False}"
                    )
                    if video_path and os.path.exists(video_path):
                        import decord

                        # Load and resize video frames
                        vr = decord.VideoReader(video_path)
                        # Extract the same frames that were cached (frame_pos to frame_pos+frame_count)
                        frames = vr[frame_pos : frame_pos + frame_count]

                        # Convert entire NDArray to numpy first, then resize each frame
                        frames_np = frames.asnumpy()  # Convert entire NDArray to numpy
                        video_frames = []
                        for i in range(frames_np.shape[0]):
                            frame_np = frames_np[i]  # Now we can index the numpy array
                            resized_frame = resize_image_to_bucket(
                                frame_np, bucket_reso[:2]
                            )
                            video_frames.append(resized_frame)

                        # Stack frames into single array: (F, H, W, C)
                        content = np.stack(video_frames, axis=0)
                        logger.debug(
                            f"Loaded video pixels for control LoRA: {video_path}, shape: {content.shape}"
                        )
                    else:
                        logger.warning(
                            f"Could not find original video file for cache: {cache_file}"
                        )
                except Exception as e:
                    logger.warning(f"Failed to load video pixels for {cache_file}: {e}")

            item_info = ItemInfo(
                item_key,
                "",
                image_size,
                bucket_reso,  # type: ignore
                frame_count=frame_count,
                content=content,
                latent_cache_path=cache_file,
                control_content=(
                    content if load_pixels_for_control and content is not None else None
                ),
                is_reg=self.is_reg,
            )
            # Only set text encoder cache path if it exists or is not required
            if text_encoder_cache_exists:
                item_info.text_encoder_output_cache_path = (
                    text_encoder_output_cache_file
                )
            else:
                item_info.text_encoder_output_cache_path = None

            bucket = bucketed_item_info.get(bucket_reso, [])
            for _ in range(self.num_repeats):
                bucket.append(item_info)
            bucketed_item_info[bucket_reso] = bucket

        # prepare batch manager
        self.batch_manager = BucketBatchManager(bucketed_item_info, self.batch_size, prior_loss_weight)  # type: ignore
        self.batch_manager.show_bucket_info()

        self.num_train_items = sum(
            [len(bucket) for bucket in bucketed_item_info.values()]
        )

        logger.info(
            f"[{dataset_id}] Training preparation complete: {self.num_train_items} items across {len(bucketed_item_info)} buckets"
        )

    def shuffle_buckets(self):
        dataset_id = self.get_dataset_identifier()
        # set random seed for this epoch
        random.seed(self.seed + self.current_epoch)  # type: ignore
        logger.debug(
            f"[{dataset_id}] Shuffling buckets with seed={self.seed + self.current_epoch}"  # type: ignore
        )
        self.batch_manager.shuffle()  # type: ignore

    def __len__(self):
        if self.batch_manager is None:
            return 100  # dummy value
        return len(self.batch_manager)

    def __getitem__(self, idx):
        return self.batch_manager[idx]  # type: ignore

    def _get_original_video_path_from_cache(
        self, cache_file: str, item_key: str
    ) -> str:
        """Try to find the original video file that corresponds to a cache file."""
        # Try common video extensions
        video_extensions = [".mp4", ".mkv", ".mov", ".avi", ".webm", ".m4v"]

        # Check in the video directory
        if self.video_directory:
            for ext in video_extensions:
                potential_path = os.path.join(self.video_directory, item_key + ext)
                if os.path.exists(potential_path):
                    return potential_path

        # If not found in video_directory, try looking in subdirectories
        cache_dir = os.path.dirname(cache_file)
        for root, dirs, files in os.walk(cache_dir):
            for ext in video_extensions:
                potential_file = item_key + ext
                if potential_file in files:
                    return os.path.join(root, potential_file)

        # Last resort: search more broadly
        if self.video_directory:
            for root, dirs, files in os.walk(self.video_directory):
                for ext in video_extensions:
                    potential_file = item_key + ext
                    if potential_file in files:
                        return os.path.join(root, potential_file)

        return ""

    def validate_resume_compatibility(
        self, expected_num_items: Optional[int] = None
    ) -> bool:
        """
        Validate if this dataset is compatible with a resumed training session.

        Args:
            expected_num_items: Expected number of training items from previous session

        Returns:
            True if compatible, False if major incompatibilities detected
        """
        dataset_id = self.get_dataset_identifier()

        if expected_num_items is None:
            logger.debug(
                f"[{dataset_id}] No expected item count provided - skipping compatibility check"
            )
            return True

        # Check if dataset has been prepared
        if not hasattr(self, "num_train_items") or self.num_train_items == 0:
            logger.error(
                f"[{dataset_id}] Dataset not prepared or has 0 items! "
                f"This will cause training failure. Please ensure cache files exist for this dataset."
            )
            return False

        # Check for significant size differences
        size_ratio = (
            self.num_train_items / expected_num_items if expected_num_items > 0 else 0
        )

        if size_ratio < 0.1 or size_ratio > 10.0:
            logger.error(
                f"[{dataset_id}] Major dataset size mismatch detected! "
                f"Expected ~{expected_num_items} items, but found {self.num_train_items} items "
                f"(ratio: {size_ratio:.2f}). This will break epoch/step calculations during resume."
            )
            return False
        elif size_ratio < 0.5 or size_ratio > 2.0:
            logger.warning(
                f"[{dataset_id}] Significant dataset size change detected! "
                f"Expected ~{expected_num_items} items, but found {self.num_train_items} items "
                f"(ratio: {size_ratio:.2f}). Training will continue but epoch/step calculations may be affected."
            )
        else:
            logger.info(
                f"[{dataset_id}] Dataset size compatible: {self.num_train_items} items "
                f"(expected ~{expected_num_items}, ratio: {size_ratio:.2f})"
            )

        return True

    def get_cache_compatibility_info(self) -> dict:
        """Get information about cache file compatibility for debugging."""
        dataset_id = self.get_dataset_identifier()
        info = {
            "dataset_id": dataset_id,
            "cache_directory": self.cache_directory,
            "latent_cache_files": 0,
            "text_encoder_cache_files": 0,
            "num_train_items": getattr(self, "num_train_items", 0),
            "has_batch_manager": self.batch_manager is not None,
        }

        if self.cache_directory and os.path.exists(self.cache_directory):
            # Count cache files
            latent_pattern = os.path.join(
                self.cache_directory, f"*_{self.latent_cache_postfix}.safetensors"
            )
            text_pattern = os.path.join(
                self.cache_directory, f"*_{self.text_encoder_cache_postfix}.safetensors"
            )

            info["latent_cache_files"] = len(glob.glob(latent_pattern))
            info["text_encoder_cache_files"] = len(glob.glob(text_pattern))

        return info


class DatasetGroup(torch.utils.data.ConcatDataset):
    # TODO: REFACTOR - Type annotation conflict with parent class
    # The parent class already has a 'datasets' attribute with different type
    def __init__(self, datasets: Sequence[Union[ImageDataset, VideoDataset]]):
        super().__init__(datasets)
        self.datasets: list[Union[ImageDataset, VideoDataset]] = datasets  # type: ignore
        self.num_train_items = 0

        # Set indices for each dataset to enable better logging
        for i, dataset in enumerate(self.datasets):
            dataset.set_dataset_index(i)
            self.num_train_items += dataset.num_train_items

        # Log initial group composition
        logger.info(f"üì¶ [DatasetGroup] Created with {len(self.datasets)} datasets:")
        for i, dataset in enumerate(self.datasets):
            dataset_id = dataset.get_dataset_identifier()
            dataset_details = dataset.get_dataset_details()
            logger.info(f"   {dataset_id}{dataset_details}")

    def get_dataset_identifier(self) -> str:
        """Get identifier for the dataset group."""
        return f"DatasetGroup({len(self.datasets)} datasets)"

    def set_current_epoch(self, epoch, force_shuffle=None, reason=None):
        """Set current epoch for all datasets in the group."""
        group_id = self.get_dataset_identifier()
        logger.debug(
            f"[{group_id}] Setting epoch {epoch} for all datasets"
            + (f" - {reason}" if reason else "")
        )

        for i, dataset in enumerate(self.datasets):
            dataset.set_current_epoch(epoch, force_shuffle=force_shuffle, reason=reason)

    def set_current_step(self, step):
        group_id = self.get_dataset_identifier()
        logger.debug(f"[{group_id}] Setting step {step} for all datasets")

        for i, dataset in enumerate(self.datasets):
            dataset.set_current_step(step)

    def set_max_train_steps(self, max_train_steps):
        group_id = self.get_dataset_identifier()
        logger.debug(
            f"[{group_id}] Setting max train steps {max_train_steps} for all datasets"
        )

        for i, dataset in enumerate(self.datasets):
            dataset.set_max_train_steps(max_train_steps)

    def validate_resume_compatibility(
        self, expected_dataset_info: Optional[dict] = None
    ) -> bool:
        """
        Validate if this dataset group is compatible with a resumed training session.

        Args:
            expected_dataset_info: Dictionary with expected dataset information from previous session

        Returns:
            True if compatible, False if major incompatibilities detected
        """
        group_id = self.get_dataset_identifier()

        if expected_dataset_info is None:
            logger.debug(
                f"[{group_id}] No expected dataset info provided - skipping compatibility check"
            )
            return True

        expected_total_items = expected_dataset_info.get("total_items", 0)
        expected_dataset_count = expected_dataset_info.get("dataset_count", 0)

        # Check dataset count mismatch
        if len(self.datasets) != expected_dataset_count:
            logger.error(
                f"[{group_id}] Dataset count mismatch! "
                f"Expected {expected_dataset_count} datasets, but found {len(self.datasets)} datasets. "
                f"This indicates a different dataset configuration."
            )
            return False

        # Validate individual datasets
        all_compatible = True
        expected_per_dataset = expected_dataset_info.get("per_dataset_items", [])

        for i, dataset in enumerate(self.datasets):
            expected_items = (
                expected_per_dataset[i] if i < len(expected_per_dataset) else None
            )
            if not dataset.validate_resume_compatibility(expected_items):  # type: ignore
                all_compatible = False

        # Check total items
        if expected_total_items > 0:
            total_ratio = self.num_train_items / expected_total_items
            if total_ratio < 0.1 or total_ratio > 10.0:
                logger.error(
                    f"[{group_id}] Major total item count mismatch! "
                    f"Expected ~{expected_total_items} total items, but found {self.num_train_items} items "
                    f"(ratio: {total_ratio:.2f})"
                )
                all_compatible = False

        if all_compatible:
            logger.info(f"[{group_id}] Resume compatibility validation passed ‚úÖ")
        else:
            logger.error(f"[{group_id}] Resume compatibility validation failed ‚ùå")

        return all_compatible

    def get_dataset_info_for_resume(self) -> dict:
        """Get dataset information that can be saved for resume validation."""
        return {
            "total_items": self.num_train_items,
            "dataset_count": len(self.datasets),
            "per_dataset_items": [
                getattr(ds, "num_train_items", 0) for ds in self.datasets
            ],
            "dataset_types": [ds.get_dataset_type() for ds in self.datasets],
            "cache_info": [ds.get_cache_compatibility_info() for ds in self.datasets],  # type: ignore
        }

    def log_dataset_compatibility_info(self):
        """Log detailed compatibility information for debugging."""
        group_id = self.get_dataset_identifier()
        logger.info(f"[{group_id}] üìä Dataset Compatibility Information:")

        for i, dataset in enumerate(self.datasets):
            cache_info = dataset.get_cache_compatibility_info()  # type: ignore
            dataset_id = cache_info["dataset_id"]

            logger.info(f"   Dataset {i}: {dataset_id}")
            logger.info(f"     Items: {cache_info['num_train_items']}")
            logger.info(f"     Latent caches: {cache_info['latent_cache_files']}")
            logger.info(
                f"     Text encoder caches: {cache_info['text_encoder_cache_files']}"
            )
            logger.info(f"     Cache directory: {cache_info['cache_directory']}")
            logger.info(f"     Ready for training: {cache_info['has_batch_manager']}")

        logger.info(f"   Total items across all datasets: {self.num_train_items}")


def save_dataset_metadata_for_resume(
    dataset_group: "DatasetGroup", output_dir: str, step: int
):
    """
    Save dataset metadata for resume validation.
    This should be called during checkpoint saving.
    """
    try:

        metadata = {
            "step": step,
            "dataset_info": dataset_group.get_dataset_info_for_resume(),
            "timestamp": time.time(),
        }

        metadata_path = os.path.join(output_dir, "dataset_metadata.json")
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

        logger.debug(
            f"üíæ Saved dataset metadata for resume validation: {metadata_path}"
        )

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to save dataset metadata for resume validation: {e}")


def load_dataset_metadata_for_resume(resume_dir: str) -> Optional[dict]:
    """
    Load dataset metadata for resume validation.
    This should be called during checkpoint resuming.
    """
    try:
        import json

        metadata_path = os.path.join(resume_dir, "dataset_metadata.json")
        if not os.path.exists(metadata_path):
            logger.debug(
                f"No dataset metadata found for resume validation: {metadata_path}"
            )
            return None

        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        logger.info(
            f"üìÇ Loaded dataset metadata for resume validation from: {metadata_path}"
        )
        return metadata.get("dataset_info")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to load dataset metadata for resume validation: {e}")
        return None


def validate_dataset_for_resume(
    dataset_group: "DatasetGroup",
    resume_dir: str,
    allow_dataset_change: bool = False,
    reset_training_state: bool = False,
    auto_handle_changes: bool = True,
) -> tuple[bool, bool, Optional[dict]]:
    """
    Intelligent dataset compatibility validation for resume.

    Args:
        dataset_group: The dataset group to validate
        resume_dir: Directory containing the resume state
        allow_dataset_change: If True, allow intentional dataset changes with warnings (legacy)
        reset_training_state: If True, suggest resetting epochs/steps for new dataset (legacy)
        auto_handle_changes: If True, automatically handle dataset changes intelligently (default)

    Returns:
        Tuple of (is_valid_to_continue, dataset_changed, expected_info)
        - is_valid_to_continue: Whether training can continue safely
        - dataset_changed: Whether the dataset is different from before
        - expected_info: Previous dataset information (if available)
    """
    expected_info = load_dataset_metadata_for_resume(resume_dir)
    if expected_info is None:
        logger.info(
            "üîç No previous dataset metadata found - assuming fresh training start"
        )
        return True, False, None

    logger.info("üîç Validating dataset compatibility for resume...")
    dataset_group.log_dataset_compatibility_info()

    is_compatible = dataset_group.validate_resume_compatibility(expected_info)

    if is_compatible:
        # Datasets are compatible - continue normally
        logger.info(
            "‚úÖ Dataset compatibility validation passed - resuming training normally"
        )
        logger.info("üìä Same dataset detected - preserving training progress")
        return True, False, expected_info

    # Datasets are different - handle automatically if enabled
    if auto_handle_changes:
        logger.info("ü§ñ AUTOMATIC DATASET CHANGE DETECTION:")
        logger.info(
            f"   Previous dataset: {expected_info.get('total_items', 'unknown')} total items, {expected_info.get('dataset_count', 'unknown')} datasets"
        )
        logger.info(
            f"   Current dataset: {dataset_group.num_train_items} total items, {len(dataset_group.datasets)} datasets"
        )

        logger.info(
            "üîÑ DATASET CHANGE STRATEGY: Always reset training state for safety"
        )
        logger.info("   ‚Ä¢ Different dataset detected ‚Üí Reset training progress")
        logger.info("   ‚Ä¢ This ensures proper epoch/step calculations")
        logger.info("   ‚Ä¢ LoRA weights and optimizer state will be preserved")

        logger.info("üéØ LoRA Resume with Different Dataset - What's preserved:")
        logger.info("   ‚úÖ LoRA network weights (your trained adaptations)")
        logger.info("   ‚úÖ Optimizer state (momentum, learning rate history)")
        logger.info("   ‚úÖ Training checkpoints and model state")
        logger.info("   üîÑ Training progress will be reset (epoch/step counters)")
        logger.info("   üîÑ This ensures proper training behavior with the new dataset")

        logger.info("")
        logger.info("üí° This conservative approach ensures:")
        logger.info("   ‚Ä¢ No training errors or crashes")
        logger.info("   ‚Ä¢ Proper learning rate scheduling")
        logger.info("   ‚Ä¢ Correct epoch/batch calculations")
        logger.info("   ‚Ä¢ Clean start with new data while keeping LoRA adaptations")

        return True, True, expected_info

    # Auto-handling disabled, use legacy behavior
    if allow_dataset_change:
        logger.warning("‚ö†Ô∏è  Dataset change detected - using legacy manual handling")
        return True, True, expected_info
    else:
        logger.error("‚ùå Dataset compatibility validation failed!")
        logger.info(
            "üí° The dataset has changed significantly from the previous training session."
        )
        logger.info(
            "üí° This is automatically handled in most cases, but validation failed."
        )
        logger.info("üí° Please check your dataset configuration or cache files.")
        return False, True, expected_info


def handle_dataset_change_for_resume_auto(
    expected_info: dict, actual_dataset_group: "DatasetGroup", global_step: int
) -> tuple[int, int, bool]:
    """
    Automatically handle training state adjustments when resuming with a different dataset.
    Always resets training state for safety when datasets differ.

    Args:
        expected_info: Previous dataset information
        actual_dataset_group: Current dataset group
        global_step: Current global step from checkpoint

    Returns:
        Tuple of (adjusted_global_step, suggested_epoch_start, reset_applied)
    """
    prev_total_items = expected_info.get("total_items", 0)
    curr_total_items = actual_dataset_group.num_train_items

    logger.info("üîß Handling dataset change for resume:")
    logger.info(f"   Previous dataset size: {prev_total_items} items")
    logger.info(f"   Current dataset size: {curr_total_items} items")
    logger.info(f"   Checkpoint global step: {global_step}")

    # Always reset training state when datasets are different
    logger.info("üîÑ RESETTING TRAINING STATE:")
    logger.info("   Strategy: Conservative reset for all dataset changes")

    adjusted_step = 0
    suggested_epoch = 0
    reset_applied = True

    logger.info("   - Global step reset to 0")
    logger.info("   - Epoch reset to 0")
    logger.info("   - LoRA weights preserved from checkpoint")
    logger.info("   - Optimizer state preserved from checkpoint")

    # Calculate size change info for user awareness
    if prev_total_items > 0 and curr_total_items > 0:
        size_ratio = curr_total_items / prev_total_items
        if size_ratio < 0.8:
            logger.info(f"   üìâ New dataset is smaller ({size_ratio:.2f}x)")
        elif size_ratio > 1.25:
            logger.info(f"   üìà New dataset is larger ({size_ratio:.2f}x)")
        else:
            logger.info(f"   üìä Similar dataset size ({size_ratio:.2f}x)")

    logger.info("‚úÖ Training state reset complete")
    logger.info("üí° Training will start fresh with your preserved LoRA adaptations")
    logger.info("üí° This ensures clean epoch/step calculations for the new dataset")

    return adjusted_step, suggested_epoch, reset_applied


def generate_safe_output_name(
    original_output_name: str,
    expected_info: dict,
    current_dataset_group: "DatasetGroup",
    strategy: str = "auto",
) -> str:
    """
    Generate a safe output name when dataset changes to prevent overwriting existing checkpoints.

    Args:
        original_output_name: The original output name from config
        expected_info: Previous dataset information
        current_dataset_group: Current dataset group
        strategy: Naming strategy ("auto", "timestamp", "sequential", "descriptive")

    Returns:
        Safe output name that won't conflict with existing checkpoints
    """
    import time
    from datetime import datetime

    if strategy == "timestamp":
        # Add timestamp suffix
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = f"{original_output_name}_dataset_change_{timestamp}"

    elif strategy == "sequential":
        # Add sequential version number
        base_name = original_output_name
        version = 2
        safe_name = f"{base_name}_v{version}"

        # Check if this name already exists and increment if needed
        # (This would need file system checking in real implementation)

    elif strategy == "descriptive":
        # Add descriptive suffix based on dataset characteristics
        prev_items = expected_info.get("total_items", 0)
        curr_items = current_dataset_group.num_train_items
        prev_datasets = expected_info.get("dataset_count", 0)
        curr_datasets = len(current_dataset_group.datasets)

        if curr_items > prev_items * 1.5:
            suffix = "expanded"
        elif curr_items < prev_items * 0.7:
            suffix = "subset"
        elif curr_datasets != prev_datasets:
            suffix = "modified"
        else:
            suffix = "adapted"

        safe_name = f"{original_output_name}_{suffix}"

    else:  # "auto" strategy (default)
        # Intelligent naming based on changes detected
        timestamp = datetime.now().strftime("%m%d_%H%M")
        safe_name = f"{original_output_name}_dataset_change_{timestamp}"

    return safe_name


def smart_resume_validation_with_protection(
    dataset_group: "DatasetGroup",
    resume_dir: str,
    current_output_name: str,
    auto_protect_checkpoints: bool = True,
    naming_strategy: str = "auto",
) -> tuple[bool, Optional[dict], Optional[str]]:
    """
    Smart resume validation with automatic checkpoint protection.

    This is the enhanced version that automatically protects against checkpoint overwrites.

    Args:
        dataset_group: The dataset group to validate
        resume_dir: Directory containing the resume state
        current_output_name: Current output name from config
        auto_protect_checkpoints: Whether to automatically rename output when datasets change
        naming_strategy: Strategy for generating safe names ("auto", "timestamp", "descriptive")

    Returns:
        Tuple of (can_continue, adjustment_info, safe_output_name)
        - can_continue: Whether training can continue safely
        - adjustment_info: Information about any adjustments made (or None)
        - safe_output_name: Protected output name (or None if no change needed)
    """
    is_valid, dataset_changed, expected_info = validate_dataset_for_resume(
        dataset_group=dataset_group, resume_dir=resume_dir, auto_handle_changes=True
    )

    if not is_valid:
        return False, None, None

    if not dataset_changed:
        # Same dataset - no protection needed
        logger.info("üìù Output name unchanged - same dataset detected")
        return True, None, None

    # Dataset changed - apply protection if enabled
    if auto_protect_checkpoints:
        safe_output_name = generate_safe_output_name(
            current_output_name, expected_info, dataset_group, naming_strategy  # type: ignore
        )

        logger.info("üõ°Ô∏è  AUTOMATIC CHECKPOINT PROTECTION ACTIVATED:")
        logger.info(f"   Dataset change detected - protecting existing checkpoints")
        logger.info(f"   Original output name: '{current_output_name}'")
        logger.info(f"   Protected output name: '{safe_output_name}'")
        logger.info("")
        logger.info("üìÅ This ensures:")
        logger.info("   ‚úÖ Your original checkpoints remain safe and untouched")
        logger.info("   ‚úÖ New training creates separate checkpoint files")
        logger.info("   ‚úÖ You can always go back to your original LoRA")
        logger.info("   ‚úÖ Clear separation between different training phases")
        logger.info("")
        logger.info("üí° If you want to use the original name, manually specify:")
        logger.info(f"   --output_name '{current_output_name}' (with caution)")

        # Prepare adjustment info
        adjustment_info = {
            "dataset_changed": True,
            "expected_info": expected_info,
            "requires_adjustment": True,
            "output_name_changed": True,
            "original_output_name": current_output_name,
            "safe_output_name": safe_output_name,
        }

        return True, adjustment_info, safe_output_name

    else:
        # Protection disabled - warn user about risks
        logger.warning("‚ö†Ô∏è  CHECKPOINT PROTECTION DISABLED:")
        logger.warning(f"   Dataset change detected but auto-protection is off")
        logger.warning(f"   Risk: New checkpoints may overwrite existing ones")
        logger.warning(f"   Original output name will be used: '{current_output_name}'")
        logger.warning("")
        logger.warning("üí° To enable automatic protection:")
        logger.warning("   Set auto_protect_checkpoints=True in your training config")

        adjustment_info = {
            "dataset_changed": True,
            "expected_info": expected_info,
            "requires_adjustment": True,
            "output_name_changed": False,
            "protection_disabled": True,
        }

        return True, adjustment_info, None


def apply_smart_resume_adjustments_with_protection(
    adjustment_info: dict, current_global_step: int, dataset_group: "DatasetGroup"
) -> tuple[int, int, dict]:
    """
    Apply smart adjustments for resumed training with dataset changes and checkpoint protection.

    Args:
        adjustment_info: Information returned from smart_resume_validation_with_protection
        current_global_step: Current global step from checkpoint
        dataset_group: Current dataset group

    Returns:
        Tuple of (adjusted_global_step, epoch_start, protection_info)
    """
    if not adjustment_info or not adjustment_info.get("requires_adjustment"):
        return current_global_step, 0, {}

    expected_info = adjustment_info["expected_info"]

    # Apply automatic training state adjustments
    adjusted_step, epoch_start, reset_applied = handle_dataset_change_for_resume_auto(
        expected_info, dataset_group, current_global_step
    )

    protection_info = {
        "training_state_reset": reset_applied,
        "adjusted_global_step": adjusted_step,
        "epoch_start": epoch_start,
        "output_name_changed": adjustment_info.get("output_name_changed", False),
        "safe_output_name": adjustment_info.get("safe_output_name"),
        "original_output_name": adjustment_info.get("original_output_name"),
    }

    if adjustment_info.get("output_name_changed"):
        logger.info("üéØ TRAINING ADJUSTMENTS SUMMARY:")
        logger.info(f"   ‚úÖ Global step: {current_global_step} ‚Üí {adjusted_step}")
        logger.info(f"   ‚úÖ Epoch start: {epoch_start}")
        logger.info(
            f"   ‚úÖ Output name: {adjustment_info['original_output_name']} ‚Üí {adjustment_info['safe_output_name']}"
        )
        logger.info(f"   ‚úÖ LoRA weights: Preserved from checkpoint")
        logger.info(
            f"   ‚úÖ Training state: {'Reset for new dataset' if reset_applied else 'Adjusted'}"
        )

    return adjusted_step, epoch_start, protection_info


def smart_resume_validation(
    dataset_group: "DatasetGroup",
    resume_dir: str,
    current_output_name: str = None,  # type: ignore
    auto_protect_checkpoints: bool = True,
) -> tuple[bool, Optional[dict], Optional[str]]:
    """
    Simple smart resume validation with automatic checkpoint protection.

    This is the main function you should use - it automatically protects your checkpoints!

    Args:
        dataset_group: The dataset group to validate
        resume_dir: Directory containing the resume state
        current_output_name: Current output name from config (optional, for protection)
        auto_protect_checkpoints: Whether to automatically rename output when datasets change

    Returns:
        Tuple of (can_continue, adjustment_info, safe_output_name)
        - can_continue: Whether training can continue safely
        - adjustment_info: Information about any adjustments made (or None)
        - safe_output_name: Protected output name (or None if no change needed)
    """
    if current_output_name is None:
        # Backward compatibility - no protection
        is_valid, dataset_changed, expected_info = validate_dataset_for_resume(
            dataset_group=dataset_group, resume_dir=resume_dir, auto_handle_changes=True
        )

        if not is_valid:
            return False, None, None

        if not dataset_changed:
            return True, None, None
        else:
            adjustment_info = {
                "dataset_changed": True,
                "expected_info": expected_info,
                "requires_adjustment": True,
                "output_name_changed": False,
            }
            return True, adjustment_info, None
    else:
        # Full protection enabled
        return smart_resume_validation_with_protection(
            dataset_group=dataset_group,
            resume_dir=resume_dir,
            current_output_name=current_output_name,
            auto_protect_checkpoints=auto_protect_checkpoints,
            naming_strategy="auto",
        )
</file>

<file path="dataset/item_info.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/dataset/image_video_dataset.py (Apache)

from typing import Any, Optional
import logging
import numpy as np

from common.logger import get_logger


logger = get_logger(__name__, level=logging.INFO)


class ItemInfo:
    # TODO: Consider using dataclass or Pydantic model for better type safety and validation
    # TODO: bucket_size type hint is too generic - should be tuple[int, int] or tuple[int, int, int]
    # TODO: Consider separating image and video item info into different classes
    def __init__(
        self,
        item_key: str,
        caption: str,
        original_size: tuple[int, int],
        bucket_size: Optional[tuple[Any]] = None,
        frame_count: Optional[int] = None,
        content: Optional[np.ndarray] = None,
        latent_cache_path: Optional[str] = None,
        weight: Optional[float] = 1.0,
        control_content: Optional[np.ndarray] = None,
        mask_content: Optional[np.ndarray] = None,
        is_reg: bool = False,
    ) -> None:
        self.item_key = item_key
        self.caption = caption
        self.original_size = original_size
        self.bucket_size = bucket_size
        self.frame_count = frame_count
        self.content = content
        self.latent_cache_path = latent_cache_path
        self.text_encoder_output_cache_path: Optional[str] = None
        self.weight = weight
        self.control_content = control_content
        self.mask_content = mask_content
        self.is_reg = is_reg

    def __str__(self) -> str:
        return (
            f"ItemInfo(item_key={self.item_key}, caption={self.caption}, "
            + f"original_size={self.original_size}, bucket_size={self.bucket_size}, "
            + f"frame_count={self.frame_count}, latent_cache_path={self.latent_cache_path}, content={self.content.shape if self.content is not None else None})"
        )
</file>

<file path="generation/sampling.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_generate_video.py (Apache)

import os
from typing import Union

import numpy as np
import torch
import torchvision
from transformers.models.llama import LlamaModel
import av
from einops import rearrange
from PIL import Image

from utils.vae_utils import load_vae

try:
    from lycoris.kohya import create_network_from_weights
except:
    pass

from utils.model_utils import str_to_dtype

import logging

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def clean_memory_on_device(device):
    if device.type == "cuda":
        torch.cuda.empty_cache()
    elif device.type == "cpu":
        pass
    elif device.type == "mps":  # not tested
        torch.mps.empty_cache()


def synchronize_device(device: torch.device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "xpu":
        torch.xpu.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()


def save_videos_grid(videos: torch.Tensor, path: str, rescale=False, n_rows=1, fps=24):
    """save videos by video tensor
       copy from https://github.com/guoyww/AnimateDiff/blob/e92bd5671ba62c0d774a32951453e328018b7c5b/animatediff/utils/util.py#L61

    Args:
        videos (torch.Tensor): video tensor predicted by the model
        path (str): path to save video
        rescale (bool, optional): rescale the video tensor from [-1, 1] to  . Defaults to False.
        n_rows (int, optional): Defaults to 1.
        fps (int, optional): video save fps. Defaults to 8.
    """
    # Safety check for empty or None path
    if not path or not path.strip():
        raise ValueError(
            f"Invalid video save path: '{path}'. Path cannot be empty or None."
        )

    logger.info(f"save_videos_grid called with path='{path}'")

    videos = rearrange(videos, "b c t h w -> t b c h w")
    outputs = []
    for x in videos:
        x = torchvision.utils.make_grid(x, nrow=n_rows)
        x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)
        if rescale:
            x = (x + 1.0) / 2.0  # -1,1 -> 0,1
        x = torch.clamp(x, 0, 1)
        x = (x * 255).numpy().astype(np.uint8)
        outputs.append(x)

    # Safety check for directory creation
    dirname = os.path.dirname(path)
    if dirname:  # Only create directory if dirname is not empty
        logger.info(f"Creating directory: {dirname}")
        os.makedirs(dirname, exist_ok=True)
    else:
        logger.warning(f"Empty directory name for path: {path}")

    # # save video with av
    # container = av.open(path, "w")
    # stream = container.add_stream("libx264", rate=fps)
    # for x in outputs:
    #     frame = av.VideoFrame.from_ndarray(x, format="rgb24")
    #     packet = stream.encode(frame)
    #     container.mux(packet)
    # packet = stream.encode(None)
    # container.mux(packet)
    # container.close()

    height, width, _ = outputs[0].shape

    # create output container
    container = av.open(path, mode="w")

    # create video stream
    codec = "libx264"
    pixel_format = "yuv420p"
    stream = container.add_stream(codec, rate=fps)
    stream.width = width  # type: ignore
    stream.height = height  # type: ignore
    stream.pix_fmt = pixel_format  # type: ignore
    stream.bit_rate = 4000000  # type: ignore # 4Mbit/s

    for frame_array in outputs:
        frame = av.VideoFrame.from_ndarray(frame_array, format="rgb24")
        packets = stream.encode(frame)  # type: ignore
        for packet in packets:
            container.mux(packet)

    for packet in stream.encode():  # type: ignore
        container.mux(packet)

    container.close()


def save_images_grid(
    videos: torch.Tensor,
    parent_dir: str,
    image_name: str,
    rescale: bool = False,
    n_rows: int = 1,
    create_subdir=True,
):
    # Safety check for empty or None paths
    if not parent_dir or not parent_dir.strip():
        raise ValueError(
            f"Invalid parent_dir: '{parent_dir}'. Parent directory cannot be empty or None."
        )
    if not image_name or not image_name.strip():
        raise ValueError(
            f"Invalid image_name: '{image_name}'. Image name cannot be empty or None."
        )

    logger.info(
        f"save_images_grid called with parent_dir='{parent_dir}', image_name='{image_name}'"
    )

    videos = rearrange(videos, "b c t h w -> t b c h w")
    outputs = []
    for x in videos:
        x = torchvision.utils.make_grid(x, nrow=n_rows)
        x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)
        if rescale:
            x = (x + 1.0) / 2.0  # -1,1 -> 0,1
        x = torch.clamp(x, 0, 1)
        x = (x * 255).numpy().astype(np.uint8)
        outputs.append(x)

    if create_subdir:
        output_dir = os.path.join(parent_dir, image_name)
    else:
        output_dir = parent_dir

    logger.info(f"Creating output directory: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    for i, x in enumerate(outputs):
        image_path = os.path.join(output_dir, f"{image_name}_{i:03d}.png")
        logger.info(f"Saving image to: {image_path}")
        image = Image.fromarray(x)
        image.save(image_path)


def prepare_vae(args, device):
    vae_dtype = (
        torch.float16 if args.vae_dtype is None else str_to_dtype(args.vae_dtype)
    )
    vae = load_vae(args, config=None, device=device, dtype=vae_dtype)
    vae.eval()
    # vae_kwargs = {"s_ratio": s_ratio, "t_ratio": t_ratio}

    # set chunk_size to CausalConv3d recursively
    chunk_size = args.vae_chunk_size
    if chunk_size is not None:
        vae.set_chunk_size_for_causal_conv_3d(chunk_size)  # type: ignore
        logger.info(f"Set chunk_size to {chunk_size} for CausalConv3d")

    if args.vae_spatial_tile_sample_min_size is not None:
        vae.enable_spatial_tiling(True)  # type: ignore
        vae.tile_sample_min_size = args.vae_spatial_tile_sample_min_size  # type: ignore
        vae.tile_latent_min_size = args.vae_spatial_tile_sample_min_size // 8  # type: ignore
    # elif args.vae_tiling:
    else:
        vae.enable_spatial_tiling(True)  # type: ignore

    return vae, vae_dtype


def encode_to_latents(args, video, device):
    vae, vae_dtype = prepare_vae(args, device)

    video = video.to(device=device, dtype=vae_dtype)
    video = video * 2 - 1  # 0, 1 -> -1, 1
    with torch.no_grad():
        latents = vae.encode(video).latent_dist.sample()  # type: ignore

    if hasattr(vae.config, "shift_factor") and vae.config.shift_factor:  # type: ignore
        latents = (latents - vae.config.shift_factor) * vae.config.scaling_factor  # type: ignore
    else:
        latents = latents * vae.config.scaling_factor  # type: ignore

    return latents


def decode_latents(args, latents, device):
    vae, vae_dtype = prepare_vae(args, device)

    expand_temporal_dim = False
    if len(latents.shape) == 4:
        latents = latents.unsqueeze(2)
        expand_temporal_dim = True
    elif len(latents.shape) == 5:
        pass
    else:
        raise ValueError(
            f"Only support latents with shape (b, c, h, w) or (b, c, f, h, w), but got {latents.shape}."
        )

    if hasattr(vae.config, "shift_factor") and vae.config.shift_factor:  # type: ignore
        latents = latents / vae.config.scaling_factor + vae.config.shift_factor  # type: ignore
    else:
        latents = latents / vae.config.scaling_factor  # type: ignore

    latents = latents.to(device=device, dtype=vae_dtype)
    with torch.no_grad():
        image = vae.decode(latents, return_dict=False)[0]  # type: ignore

    if expand_temporal_dim:
        image = image.squeeze(2)

    image = (image / 2 + 0.5).clamp(0, 1)
    # always cast to float32 as this does not cause significant overhead and is compatible with bfloa16
    image = image.cpu().float()

    return image


def check_inputs(args):
    height = args.video_size[0]
    width = args.video_size[1]
    video_length = args.video_length

    if height % 8 != 0 or width % 8 != 0:
        raise ValueError(
            f"`height` and `width` have to be divisible by 8 but are {height} and {width}."
        )
    return height, width, video_length
</file>

<file path="masking/mask_utils.py">
"""
Mask Utilities for Takenoko

This module provides utilities for creating, processing, and validating masks
for masked training in the Takenoko. Masked training allows you to train
the model only on specific parts of images or videos while ignoring other regions.

Mask Format:
- White (255) = Train on this pixel
- Black (0) = Ignore this pixel (mask it out)
- Gray values = Partial training weight (0.0 to 1.0)

Usage:
    from src.utils.mask_utils import create_center_mask, validate_mask

    # Create a center mask for 1024x1024 image
    mask = create_center_mask(1024, 1024, center_ratio=0.5)
    mask.save("center_mask.png")

    # Validate a mask
    is_valid = validate_mask("path/to/mask.png", (1024, 1024))
"""

import os
import numpy as np
from PIL import Image
from typing import Tuple, List, Optional, Union
import logging

logger = logging.getLogger(__name__)


def create_center_mask(
    width: int, height: int, center_ratio: float = 0.5
) -> Image.Image:
    """
    Create a mask that trains only on the center region of an image.

    Args:
        width: Image width in pixels
        height: Image height in pixels
        center_ratio: Ratio of center region to train on (0.0 to 1.0)

    Returns:
        PIL Image with white center region and black background

    Example:
        >>> mask = create_center_mask(1024, 1024, center_ratio=0.5)
        >>> mask.save("center_mask.png")
    """
    mask = np.zeros((height, width), dtype=np.uint8)

    # Calculate center region
    center_w = int(width * center_ratio)
    center_h = int(height * center_ratio)
    start_w = (width - center_w) // 2
    start_h = (height - center_h) // 2

    # Set center region to white (255)
    mask[start_h : start_h + center_h, start_w : start_w + center_w] = 255

    return Image.fromarray(mask)


def create_gradient_mask(
    width: int, height: int, gradient_type: str = "radial"
) -> Image.Image:
    """
    Create a gradient mask with smooth transitions.

    Args:
        width: Image width in pixels
        height: Image height in pixels
        gradient_type: Type of gradient ("radial", "linear_h", "linear_v")

    Returns:
        PIL Image with gradient mask

    Example:
        >>> mask = create_gradient_mask(1024, 1024, "radial")
        >>> mask.save("gradient_mask.png")
    """
    mask = np.zeros((height, width), dtype=np.uint8)

    if gradient_type == "radial":
        # Create radial gradient from center
        center_x, center_y = width // 2, height // 2
        for y in range(height):
            for x in range(width):
                distance = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
                max_distance = np.sqrt(center_x**2 + center_y**2)
                intensity = max(0, 255 * (1 - distance / max_distance))
                mask[y, x] = int(intensity)

    elif gradient_type == "linear_h":
        # Horizontal linear gradient
        for x in range(width):
            intensity = int(255 * (x / width))
            mask[:, x] = intensity

    elif gradient_type == "linear_v":
        # Vertical linear gradient
        for y in range(height):
            intensity = int(255 * (y / height))
            mask[y, :] = intensity

    return Image.fromarray(mask)


def create_selective_mask(
    width: int, height: int, regions: List[Tuple[int, int, int, int, int]]
) -> Image.Image:
    """
    Create a mask with specific regions to train on.

    Args:
        width: Image width in pixels
        height: Image height in pixels
        regions: List of (x1, y1, x2, y2, intensity) tuples

    Returns:
        PIL Image with selective regions

    Example:
        >>> regions = [
        ...     (100, 100, 300, 300, 255),  # White region
        ...     (400, 400, 500, 500, 128),  # Gray region
        ... ]
        >>> mask = create_selective_mask(1024, 1024, regions)
        >>> mask.save("selective_mask.png")
    """
    mask = np.zeros((height, width), dtype=np.uint8)

    for region in regions:
        x1, y1, x2, y2, intensity = region
        mask[y1:y2, x1:x2] = intensity

    return Image.fromarray(mask)


def create_border_mask(width: int, height: int, border_width: int = 50) -> Image.Image:
    """
    Create a mask that trains only on the border region.

    Args:
        width: Image width in pixels
        height: Image height in pixels
        border_width: Width of border region in pixels

    Returns:
        PIL Image with white border and black center
    """
    mask = np.zeros((height, width), dtype=np.uint8)

    # Set border regions to white
    mask[:border_width, :] = 255  # Top border
    mask[-border_width:, :] = 255  # Bottom border
    mask[:, :border_width] = 255  # Left border
    mask[:, -border_width:] = 255  # Right border

    return Image.fromarray(mask)


def create_corner_mask(width: int, height: int, corner_size: int = 200) -> Image.Image:
    """
    Create a mask that trains only on the four corners.

    Args:
        width: Image width in pixels
        height: Image height in pixels
        corner_size: Size of corner regions in pixels

    Returns:
        PIL Image with white corners and black center
    """
    mask = np.zeros((height, width), dtype=np.uint8)

    # Set corner regions to white
    mask[:corner_size, :corner_size] = 255  # Top-left
    mask[:corner_size, -corner_size:] = 255  # Top-right
    mask[-corner_size:, :corner_size] = 255  # Bottom-left
    mask[-corner_size:, -corner_size:] = 255  # Bottom-right

    return Image.fromarray(mask)


def validate_mask(
    mask_path: str, expected_size: Optional[Tuple[int, int]] = None
) -> bool:
    """
    Validate a mask image for training.

    Args:
        mask_path: Path to mask image
        expected_size: Expected (width, height) of mask

    Returns:
        True if mask is valid, False otherwise

    Example:
        >>> is_valid = validate_mask("mask.png", (1024, 1024))
        >>> print(f"Mask is valid: {is_valid}")
    """
    try:
        # Check if file exists
        if not os.path.exists(mask_path):
            logger.error(f"Mask file not found: {mask_path}")
            return False

        # Load and check image
        with Image.open(mask_path) as img:
            # Convert to grayscale
            img = img.convert("L")

            # Check size if specified
            if expected_size:
                if img.size != expected_size:
                    logger.error(
                        f"Mask size {img.size} doesn't match expected {expected_size}"
                    )
                    return False

            # Check value range
            img_array = np.array(img)
            min_val = img_array.min()
            max_val = img_array.max()

            if min_val < 0 or max_val > 255:
                logger.error(
                    f"Mask values out of range [0, 255]: [{min_val}, {max_val}]"
                )
                return False

            logger.info(f"Mask validation passed: {mask_path}")
            return True

    except Exception as e:
        logger.error(f"Error validating mask {mask_path}: {e}")
        return False


def batch_create_masks(
    image_directory: str, mask_directory: str, mask_type: str = "center", **kwargs
) -> None:
    """
    Batch create masks for all images in a directory.

    Args:
        image_directory: Directory containing training images
        mask_directory: Directory to save mask images
        mask_type: Type of mask to create ("center", "gradient", "border", "corner")
        **kwargs: Additional arguments for mask creation functions

    Example:
        >>> batch_create_masks(
        ...     "path/to/images",
        ...     "path/to/masks",
        ...     mask_type="center",
        ...     center_ratio=0.5
        ... )
    """
    # Create mask directory if it doesn't exist
    os.makedirs(mask_directory, exist_ok=True)

    # Supported image extensions
    image_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"}

    # Get all image files
    image_files = []
    for filename in os.listdir(image_directory):
        if any(filename.lower().endswith(ext) for ext in image_extensions):
            image_files.append(filename)

    logger.info(f"Found {len(image_files)} images to process")

    for filename in image_files:
        # Get image path and determine mask path
        image_path = os.path.join(image_directory, filename)
        base_name = os.path.splitext(filename)[0]
        mask_path = os.path.join(mask_directory, f"{base_name}.png")

        try:
            # Get image dimensions
            with Image.open(image_path) as img:
                width, height = img.size

            # Create mask based on type
            if mask_type == "center":
                mask = create_center_mask(width, height, **kwargs)
            elif mask_type == "gradient":
                mask = create_gradient_mask(width, height, **kwargs)
            elif mask_type == "border":
                mask = create_border_mask(width, height, **kwargs)
            elif mask_type == "corner":
                mask = create_corner_mask(width, height, **kwargs)
            else:
                logger.error(f"Unknown mask type: {mask_type}")
                continue

            # Save mask
            mask.save(mask_path)
            logger.info(f"Created mask: {mask_path}")

        except Exception as e:
            logger.error(f"Error creating mask for {filename}: {e}")


def analyze_mask_coverage(mask_path: str) -> dict:
    """
    Analyze mask coverage and statistics.

    Args:
        mask_path: Path to mask image

    Returns:
        Dictionary with mask statistics

    Example:
        >>> stats = analyze_mask_coverage("mask.png")
        >>> print(f"Coverage: {stats['coverage']:.2%}")
    """
    try:
        with Image.open(mask_path) as img:
            img = img.convert("L")
            img_array = np.array(img)

            # Calculate statistics
            total_pixels = img_array.size
            white_pixels = np.sum(img_array == 255)
            black_pixels = np.sum(img_array == 0)
            gray_pixels = total_pixels - white_pixels - black_pixels

            coverage = white_pixels / total_pixels
            avg_intensity = img_array.mean()

            return {
                "coverage": coverage,
                "total_pixels": total_pixels,
                "white_pixels": white_pixels,
                "black_pixels": black_pixels,
                "gray_pixels": gray_pixels,
                "avg_intensity": avg_intensity,
                "min_intensity": img_array.min(),
                "max_intensity": img_array.max(),
            }

    except Exception as e:
        logger.error(f"Error analyzing mask {mask_path}: {e}")
        return {}


def create_mask_from_alpha(image_path: str, threshold: int = 128) -> Image.Image:
    """
    Create a mask from the alpha channel of an image.

    Args:
        image_path: Path to image with alpha channel
        threshold: Threshold for alpha values (0-255)

    Returns:
        PIL Image mask based on alpha channel
    """
    with Image.open(image_path) as img:
        if img.mode in ("RGBA", "LA"):
            # Extract alpha channel
            alpha = img.split()[-1]
            # Convert to binary mask based on threshold
            mask_array = np.array(alpha) > threshold
            mask_array = mask_array.astype(np.uint8) * 255
            return Image.fromarray(mask_array)
        else:
            raise ValueError(f"Image {image_path} doesn't have an alpha channel")


def augment_mask(
    mask: np.ndarray, augmentation_factor: float = 0.1, noise_type: str = "gaussian"
) -> np.ndarray:
    """
    Apply augmentation to a mask.

    Args:
        mask: Mask as numpy array
        augmentation_factor: Strength of augmentation (0.0 to 1.0)
        noise_type: Type of noise ("gaussian", "uniform")

    Returns:
        Augmented mask as numpy array
    """
    if noise_type == "gaussian":
        noise = np.random.normal(0, augmentation_factor * 255, mask.shape)
    elif noise_type == "uniform":
        noise = np.random.uniform(
            -augmentation_factor * 255, augmentation_factor * 255, mask.shape
        )
    else:
        raise ValueError(f"Unknown noise type: {noise_type}")

    augmented_mask = np.clip(mask.astype(float) + noise, 0, 255)
    return augmented_mask.astype(np.uint8)


# Example usage and testing functions
def create_example_masks(output_directory: str = "example_masks") -> None:
    """
    Create example masks for demonstration.

    Args:
        output_directory: Directory to save example masks
    """
    os.makedirs(output_directory, exist_ok=True)

    # Create various example masks
    masks = [
        ("center_mask.png", create_center_mask(1024, 1024, 0.5)),
        ("gradient_radial.png", create_gradient_mask(1024, 1024, "radial")),
        ("gradient_linear_h.png", create_gradient_mask(1024, 1024, "linear_h")),
        ("border_mask.png", create_border_mask(1024, 1024, 100)),
        ("corner_mask.png", create_corner_mask(1024, 1024, 200)),
    ]

    for filename, mask in masks:
        mask_path = os.path.join(output_directory, filename)
        mask.save(mask_path)
        logger.info(f"Created example mask: {mask_path}")

        # Analyze the mask
        stats = analyze_mask_coverage(mask_path)
        logger.info(f"Coverage: {stats['coverage']:.2%}")
</file>

<file path="modules/custom_offloading_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/modules/custom_offloading_utils.py (Apache)

from concurrent.futures import ThreadPoolExecutor
import gc
import time
from typing import Optional
import torch
import torch.nn as nn

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def clean_memory_on_device(device: torch.device):
    """
    Clean memory on the specified device, will be called from training scripts.
    """
    gc.collect()

    # device may "cuda" or "cuda:0", so we need to check the type of device
    if device.type == "cuda":
        torch.cuda.empty_cache()
    if device.type == "xpu":
        torch.xpu.empty_cache()
    if device.type == "mps":
        torch.mps.empty_cache()


def synchronize_device(device: torch.device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "xpu":
        torch.xpu.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()


def swap_weight_devices_cuda(
    device: torch.device, layer_to_cpu: nn.Module, layer_to_cuda: nn.Module
):
    assert layer_to_cpu.__class__ == layer_to_cuda.__class__

    weight_swap_jobs = []

    # This is not working for all cases (e.g. SD3), so we need to find the corresponding modules
    # for module_to_cpu, module_to_cuda in zip(layer_to_cpu.modules(), layer_to_cuda.modules()):
    #     print(module_to_cpu.__class__, module_to_cuda.__class__)
    #     if hasattr(module_to_cpu, "weight") and module_to_cpu.weight is not None:
    #         weight_swap_jobs.append((module_to_cpu, module_to_cuda, module_to_cpu.weight.data, module_to_cuda.weight.data))

    modules_to_cpu = {k: v for k, v in layer_to_cpu.named_modules()}
    for module_to_cuda_name, module_to_cuda in layer_to_cuda.named_modules():
        if hasattr(module_to_cuda, "weight") and module_to_cuda.weight is not None:
            module_to_cpu = modules_to_cpu.get(module_to_cuda_name, None)
            if (
                module_to_cpu is not None
                and module_to_cpu.weight.shape == module_to_cuda.weight.shape
            ):
                weight_swap_jobs.append(
                    (
                        module_to_cpu,
                        module_to_cuda,
                        module_to_cpu.weight.data,
                        module_to_cuda.weight.data,
                    )
                )
            else:
                if module_to_cuda.weight.data.device.type != device.type:
                    # print(
                    #     f"Module {module_to_cuda_name} not found in CPU model or shape mismatch, so not swapping and moving to device"
                    # )
                    module_to_cuda.weight.data = module_to_cuda.weight.data.to(device)  # type: ignore

    torch.cuda.current_stream().synchronize()  # this prevents the illegal loss value

    stream = torch.cuda.Stream()
    with torch.cuda.stream(stream):  # type: ignore
        # cuda to cpu
        for (
            module_to_cpu,
            module_to_cuda,
            cuda_data_view,
            cpu_data_view,
        ) in weight_swap_jobs:
            cuda_data_view.record_stream(stream)
            module_to_cpu.weight.data = cuda_data_view.data.to("cpu", non_blocking=True)

        stream.synchronize()

        # cpu to cuda
        for (
            module_to_cpu,
            module_to_cuda,
            cuda_data_view,
            cpu_data_view,
        ) in weight_swap_jobs:
            cuda_data_view.copy_(module_to_cuda.weight.data, non_blocking=True)
            module_to_cuda.weight.data = cuda_data_view

    stream.synchronize()
    torch.cuda.current_stream().synchronize()  # this prevents the illegal loss value


def swap_weight_devices_no_cuda(
    device: torch.device, layer_to_cpu: nn.Module, layer_to_cuda: nn.Module
):
    """
    not tested
    """
    assert layer_to_cpu.__class__ == layer_to_cuda.__class__

    weight_swap_jobs = []
    for module_to_cpu, module_to_cuda in zip(
        layer_to_cpu.modules(), layer_to_cuda.modules()
    ):
        if hasattr(module_to_cpu, "weight") and module_to_cpu.weight is not None:
            weight_swap_jobs.append(
                (
                    module_to_cpu,
                    module_to_cuda,
                    module_to_cpu.weight.data,
                    module_to_cuda.weight.data,
                )
            )

    # device to cpu
    for (
        module_to_cpu,
        module_to_cuda,
        cuda_data_view,
        cpu_data_view,
    ) in weight_swap_jobs:
        module_to_cpu.weight.data = cuda_data_view.data.to("cpu", non_blocking=True)

    synchronize_device(device)

    # cpu to device
    for (
        module_to_cpu,
        module_to_cuda,
        cuda_data_view,
        cpu_data_view,
    ) in weight_swap_jobs:
        cuda_data_view.copy_(module_to_cuda.weight.data, non_blocking=True)
        module_to_cuda.weight.data = cuda_data_view

    synchronize_device(device)


def weighs_to_device(layer: nn.Module, device: torch.device):
    for module in layer.modules():
        if hasattr(module, "weight") and module.weight is not None:
            module.weight.data = module.weight.data.to(device, non_blocking=True)  # type: ignore


class Offloader:
    """
    common offloading class
    """

    def __init__(
        self,
        block_type: str,
        num_blocks: int,
        blocks_to_swap: int,
        device: torch.device,
        debug: bool = False,
    ):
        self.block_type = block_type
        self.num_blocks = num_blocks
        self.blocks_to_swap = blocks_to_swap
        self.device = device
        self.debug = debug

        self.thread_pool = ThreadPoolExecutor(max_workers=1)
        self.futures = {}
        self.cuda_available = device.type == "cuda"

    def swap_weight_devices(self, block_to_cpu: nn.Module, block_to_cuda: nn.Module):
        if self.cuda_available:
            swap_weight_devices_cuda(self.device, block_to_cpu, block_to_cuda)
        else:
            swap_weight_devices_no_cuda(self.device, block_to_cpu, block_to_cuda)

    def _submit_move_blocks(self, blocks, block_idx_to_cpu, block_idx_to_cuda):
        def move_blocks(bidx_to_cpu, block_to_cpu, bidx_to_cuda, block_to_cuda):
            if self.debug:
                start_time = time.perf_counter()
                print(
                    f"[{self.block_type}] Move block {bidx_to_cpu} to CPU and block {bidx_to_cuda} to {'CUDA' if self.cuda_available else 'device'}"
                )

            self.swap_weight_devices(block_to_cpu, block_to_cuda)

            if self.debug:
                print(
                    f"[{self.block_type}] Moved blocks {bidx_to_cpu} and {bidx_to_cuda} in {time.perf_counter()-start_time:.2f}s"
                )
            return bidx_to_cpu, bidx_to_cuda  # , event

        block_to_cpu = blocks[block_idx_to_cpu]
        block_to_cuda = blocks[block_idx_to_cuda]

        self.futures[block_idx_to_cuda] = self.thread_pool.submit(
            move_blocks,
            block_idx_to_cpu,
            block_to_cpu,
            block_idx_to_cuda,
            block_to_cuda,
        )

    def _wait_blocks_move(self, block_idx):
        if block_idx not in self.futures:
            return

        if self.debug:
            print(f"[{self.block_type}] Wait for block {block_idx}")
            start_time = time.perf_counter()

        future = self.futures.pop(block_idx)
        _, bidx_to_cuda = future.result()

        assert (
            block_idx == bidx_to_cuda
        ), f"Block index mismatch: {block_idx} != {bidx_to_cuda}"

        if self.debug:
            print(
                f"[{self.block_type}] Waited for block {block_idx}: {time.perf_counter()-start_time:.2f}s"
            )


class ModelOffloader(Offloader):
    """
    supports forward offloading
    """

    def __init__(
        self,
        block_type: str,
        blocks: list[nn.Module],
        num_blocks: int,
        blocks_to_swap: int,
        supports_backward: bool,
        device: torch.device,
        debug: bool = False,
    ):
        super().__init__(block_type, num_blocks, blocks_to_swap, device, debug)

        self.supports_backward = supports_backward
        self.forward_only = (
            not supports_backward
        )  # forward only offloading: can be changed to True for inference

        if self.supports_backward:
            # register backward hooks
            self.remove_handles = []
            for i, block in enumerate(blocks):
                hook = self.create_backward_hook(blocks, i)
                if hook is not None:
                    handle = block.register_full_backward_hook(hook)
                    self.remove_handles.append(handle)

    def set_forward_only(self, forward_only: bool):
        self.forward_only = forward_only

    def __del__(self):
        if self.supports_backward:
            for handle in self.remove_handles:
                handle.remove()

    def create_backward_hook(
        self, blocks: list[nn.Module], block_index: int
    ) -> Optional[callable]:  # type: ignore
        # -1 for 0-based index
        num_blocks_propagated = self.num_blocks - block_index - 1
        swapping = (
            num_blocks_propagated > 0 and num_blocks_propagated <= self.blocks_to_swap
        )
        waiting = block_index > 0 and block_index <= self.blocks_to_swap

        if not swapping and not waiting:
            return None

        # create  hook
        block_idx_to_cpu = self.num_blocks - num_blocks_propagated
        block_idx_to_cuda = self.blocks_to_swap - num_blocks_propagated
        block_idx_to_wait = block_index - 1

        def backward_hook(module, grad_input, grad_output):
            if self.debug:
                print(f"Backward hook for block {block_index}")

            if swapping:
                self._submit_move_blocks(blocks, block_idx_to_cpu, block_idx_to_cuda)
            if waiting:
                self._wait_blocks_move(block_idx_to_wait)
            return None

        return backward_hook

    def prepare_block_devices_before_forward(self, blocks: list[nn.Module]):
        if self.blocks_to_swap is None or self.blocks_to_swap == 0:
            return

        if self.debug:
            print(f"[{self.block_type}] Prepare block devices before forward")

        for b in blocks[0 : self.num_blocks - self.blocks_to_swap]:
            b.to(self.device)
            weighs_to_device(b, self.device)  # make sure weights are on device

        for b in blocks[self.num_blocks - self.blocks_to_swap :]:
            b.to(
                self.device
            )  # move block to device first. this makes sure that buffers (non weights) are on the device
            weighs_to_device(b, "cpu")  # type: ignore # make sure weights are on cpu

        synchronize_device(self.device)
        clean_memory_on_device(self.device)

    def wait_for_block(self, block_idx: int):
        if self.blocks_to_swap is None or self.blocks_to_swap == 0:
            return
        self._wait_blocks_move(block_idx)

    def submit_move_blocks_forward(self, blocks: list[nn.Module], block_idx: int):
        # check if blocks_to_swap is enabled
        if self.blocks_to_swap is None or self.blocks_to_swap == 0:
            return

        # if backward is enabled, we do not swap blocks in forward pass more than blocks_to_swap, because it should be on GPU
        if not self.forward_only and block_idx >= self.blocks_to_swap:
            return

        block_idx_to_cpu = block_idx
        block_idx_to_cuda = self.num_blocks - self.blocks_to_swap + block_idx
        block_idx_to_cuda = (
            block_idx_to_cuda % self.num_blocks
        )  # this works for forward-only offloading
        self._submit_move_blocks(blocks, block_idx_to_cpu, block_idx_to_cuda)
</file>

<file path="modules/fp8_optimization_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/modules/fp8_optimization_utils.py (Apache)

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from typing import List, Union
import logging
from tqdm import tqdm
from common.logger import get_logger
from utils.safetensors_utils import MemoryEfficientSafeOpen

logger = get_logger(__name__, level=logging.INFO)

from utils.device_utils import clean_memory_on_device


def calculate_fp8_maxval(exp_bits=4, mantissa_bits=3, sign_bits=1):
    """
    Calculate the maximum representable value in FP8 format.
    Default is E4M3 format (4-bit exponent, 3-bit mantissa, 1-bit sign).

    Args:
        exp_bits (int): Number of exponent bits
        mantissa_bits (int): Number of mantissa bits
        sign_bits (int): Number of sign bits (0 or 1)

    Returns:
        float: Maximum value representable in FP8 format
    """
    assert exp_bits + mantissa_bits + sign_bits == 8, "Total bits must be 8"

    # Calculate exponent bias
    bias = 2 ** (exp_bits - 1) - 1

    # Calculate maximum mantissa value
    mantissa_max = 1.0
    for i in range(mantissa_bits - 1):
        mantissa_max += 2 ** -(i + 1)

    # Calculate maximum value
    max_value = mantissa_max * (2 ** (2**exp_bits - 1 - bias))

    return max_value


def quantize_tensor_to_fp8(
    tensor,
    scale,
    exp_bits=4,
    mantissa_bits=3,
    sign_bits=1,
    max_value=None,
    min_value=None,
):
    """
    Quantize a tensor to FP8 format.

    Args:
        tensor (torch.Tensor): Tensor to quantize
        scale (float or torch.Tensor): Scale factor
        exp_bits (int): Number of exponent bits
        mantissa_bits (int): Number of mantissa bits
        sign_bits (int): Number of sign bits

    Returns:
        tuple: (quantized_tensor, scale_factor)
    """
    # Create scaled tensor
    scaled_tensor = tensor / scale

    # Calculate FP8 parameters
    bias = 2 ** (exp_bits - 1) - 1

    if max_value is None:
        # Calculate max and min values
        max_value = calculate_fp8_maxval(exp_bits, mantissa_bits, sign_bits)
        min_value = -max_value if sign_bits > 0 else 0.0

    # Clamp tensor to range
    clamped_tensor = torch.clamp(scaled_tensor, min_value, max_value)

    # Quantization process
    abs_values = torch.abs(clamped_tensor)
    nonzero_mask = abs_values > 0

    # Calculate log scales (only for non-zero elements)
    log_scales = torch.zeros_like(clamped_tensor)
    if nonzero_mask.any():
        log_scales[nonzero_mask] = torch.floor(
            torch.log2(abs_values[nonzero_mask]) + bias
        ).detach()

    # Limit log scales and calculate quantization factor
    log_scales = torch.clamp(log_scales, min=1.0)
    quant_factor = 2.0 ** (log_scales - mantissa_bits - bias)

    # Quantize and dequantize
    quantized = torch.round(clamped_tensor / quant_factor) * quant_factor

    return quantized, scale


def optimize_state_dict_with_fp8(
    state_dict,
    calc_device,
    target_layer_keys=None,
    exclude_layer_keys=None,
    exp_bits=4,
    mantissa_bits=3,
    move_to_device=False,
):
    """
    Optimize Linear layer weights in a model's state dict to FP8 format.

    Args:
        state_dict (dict): State dict to optimize, replaced in-place
        calc_device (str): Device to quantize tensors on
        target_layer_keys (list, optional): Layer key patterns to target (None for all Linear layers)
        exclude_layer_keys (list, optional): Layer key patterns to exclude
        exp_bits (int): Number of exponent bits
        mantissa_bits (int): Number of mantissa bits
        move_to_device (bool): Move optimized tensors to the calculating device

    Returns:
        dict: FP8 optimized state dict
    """
    if exp_bits == 4 and mantissa_bits == 3:
        fp8_dtype = torch.float8_e4m3fn
    elif exp_bits == 5 and mantissa_bits == 2:
        fp8_dtype = torch.float8_e5m2
    else:
        raise ValueError(f"‚õî Unsupported FP8 format: E{exp_bits}M{mantissa_bits}")

    # Calculate FP8 max value
    max_value = calculate_fp8_maxval(exp_bits, mantissa_bits)
    min_value = -max_value  # this function supports only signed FP8

    # Create optimized state dict
    optimized_count = 0

    # Enumerate tarket keys
    target_state_dict_keys = []
    for key in state_dict.keys():
        # Check if it's a weight key and matches target patterns
        is_target = (
            target_layer_keys is None
            or any(pattern in key for pattern in target_layer_keys)
        ) and key.endswith(".weight")
        is_excluded = exclude_layer_keys is not None and any(
            pattern in key for pattern in exclude_layer_keys
        )
        is_target = is_target and not is_excluded

        if is_target and isinstance(state_dict[key], torch.Tensor):
            target_state_dict_keys.append(key)

    # Process each key
    for key in tqdm(target_state_dict_keys):
        value = state_dict[key]

        # Save original device and dtype
        original_device = value.device
        original_dtype = value.dtype

        # Move to calculation device
        if calc_device is not None:
            value = value.to(calc_device)

        # Calculate scale factor
        scale = torch.max(torch.abs(value.flatten())) / max_value
        # print(f"Optimizing {key} with scale: {scale}")

        # Quantize weight to FP8
        quantized_weight, _ = quantize_tensor_to_fp8(
            value, scale, exp_bits, mantissa_bits, 1, max_value, min_value
        )

        # Add to state dict using original key for weight and new key for scale
        fp8_key = key  # Maintain original key
        scale_key = key.replace(".weight", ".scale_weight")

        quantized_weight = quantized_weight.to(fp8_dtype)

        if not move_to_device:
            quantized_weight = quantized_weight.to(original_device)

        scale_tensor = torch.tensor(
            [scale], dtype=original_dtype, device=quantized_weight.device
        )

        state_dict[fp8_key] = quantized_weight
        state_dict[scale_key] = scale_tensor

        optimized_count += 1

        if calc_device is not None:  # optimized_count % 10 == 0 and
            # free memory on calculation device
            clean_memory_on_device(calc_device)

    logger.info(f"Number of optimized Linear layers: {optimized_count}")
    return state_dict


def load_safetensors_with_fp8_optimization(
    model_files: List[str],
    calc_device: Union[str, torch.device],
    target_layer_keys=None,
    exclude_layer_keys=None,
    exp_bits=4,
    mantissa_bits=3,
    move_to_device=False,
    weight_hook=None,
):
    """
    Load weight tensors from safetensors files and merge LoRA weights into the state dict with explicit FP8 optimization.

    Args:
        model_files (list[str]): List of model files to load
        calc_device (str or torch.device): Device to quantize tensors on
        target_layer_keys (list, optional): Layer key patterns to target for optimization (None for all Linear layers)
        exclude_layer_keys (list, optional): Layer key patterns to exclude from optimization
        exp_bits (int): Number of exponent bits
        mantissa_bits (int): Number of mantissa bits
        move_to_device (bool): Move optimized tensors to the calculating device
        weight_hook (callable, optional): Function to apply to each weight tensor before optimization

    Returns:
        dict: FP8 optimized state dict
    """
    if exp_bits == 4 and mantissa_bits == 3:
        fp8_dtype = torch.float8_e4m3fn
    elif exp_bits == 5 and mantissa_bits == 2:
        fp8_dtype = torch.float8_e5m2
    else:
        raise ValueError(f"Unsupported FP8 format: E{exp_bits}M{mantissa_bits}")

    # Calculate FP8 max value
    max_value = calculate_fp8_maxval(exp_bits, mantissa_bits)
    min_value = -max_value  # this function supports only signed FP8

    # Define function to determine if a key is a target key. target means fp8 optimization, not for weight hook.
    def is_target_key(key):
        # Check if weight key matches target patterns and does not match exclude patterns
        is_target = (
            target_layer_keys is None
            or any(pattern in key for pattern in target_layer_keys)
        ) and key.endswith(".weight")
        is_excluded = exclude_layer_keys is not None and any(
            pattern in key for pattern in exclude_layer_keys
        )
        return is_target and not is_excluded

    # Create optimized state dict
    optimized_count = 0

    # Process each file
    state_dict = {}
    for model_file in model_files:
        with MemoryEfficientSafeOpen(model_file) as f:
            keys = f.keys()
            for key in tqdm(
                keys, desc=f"Loading {os.path.basename(model_file)}", unit="key"
            ):
                value = f.get_tensor(key)
                if weight_hook is not None:
                    # Apply weight hook if provided
                    value = weight_hook(key, value)

                if not is_target_key(key):
                    state_dict[key] = value
                    continue

                # Save original device and dtype
                original_device = value.device
                original_dtype = value.dtype

                # Move to calculation device
                if calc_device is not None:
                    value = value.to(calc_device)

                # Calculate scale factor
                scale = torch.max(torch.abs(value.flatten())) / max_value
                # print(f"Optimizing {key} with scale: {scale}")

                # Quantize weight to FP8
                quantized_weight, _ = quantize_tensor_to_fp8(
                    value, scale, exp_bits, mantissa_bits, 1, max_value, min_value
                )

                # Add to state dict using original key for weight and new key for scale
                fp8_key = key  # Maintain original key
                scale_key = key.replace(".weight", ".scale_weight")
                assert fp8_key != scale_key, "FP8 key and scale key must be different"

                quantized_weight = quantized_weight.to(fp8_dtype)

                if not move_to_device:
                    quantized_weight = quantized_weight.to(original_device)

                scale_tensor = torch.tensor(
                    [scale], dtype=original_dtype, device=quantized_weight.device
                )

                state_dict[fp8_key] = quantized_weight
                state_dict[scale_key] = scale_tensor

                optimized_count += 1

                if calc_device is not None and optimized_count % 10 == 0:
                    # free memory on calculation device
                    clean_memory_on_device(calc_device)

    logger.info(f"Number of optimized Linear layers: {optimized_count}")
    return state_dict


def fp8_linear_forward_patch(self: nn.Linear, x, use_scaled_mm=False, max_value=None):
    """
    Patched forward method for Linear layers with FP8 weights.

    Args:
        self: Linear layer instance
        x (torch.Tensor): Input tensor
        use_scaled_mm (bool): Use scaled_mm for FP8 Linear layers, requires SM 8.9+ (RTX 40 series)
        max_value (float): Maximum value for FP8 quantization. If None, no quantization is applied for input tensor.

    Returns:
        torch.Tensor: Result of linear transformation
    """
    if use_scaled_mm:
        input_dtype = x.dtype
        original_weight_dtype = self.scale_weight.dtype
        weight_dtype = self.weight.dtype
        target_dtype = torch.float8_e5m2
        assert (
            weight_dtype == torch.float8_e4m3fn
        ), "Only FP8 E4M3FN format is supported"
        assert x.ndim == 3, "Input tensor must be 3D (batch_size, seq_len, hidden_dim)"

        if max_value is None:
            # no input quantization
            scale_x = torch.tensor(1.0, dtype=torch.float32, device=x.device)
        else:
            # calculate scale factor for input tensor
            scale_x = (torch.max(torch.abs(x.flatten())) / max_value).to(torch.float32)

            # quantize input tensor to FP8: this seems to consume a lot of memory
            x, _ = quantize_tensor_to_fp8(x, scale_x, 5, 2, 1, max_value, -max_value)

        original_shape = x.shape
        x = x.reshape(-1, x.shape[2]).to(target_dtype)

        weight = self.weight.t()
        scale_weight = self.scale_weight.to(torch.float32)

        if self.bias is not None:
            # float32 is not supported with bias in scaled_mm
            o = torch._scaled_mm(
                x,
                weight,
                out_dtype=original_weight_dtype,  # type: ignore
                bias=self.bias,
                scale_a=scale_x,
                scale_b=scale_weight,  # type: ignore
            )
        else:
            o = torch._scaled_mm(
                x, weight, out_dtype=input_dtype, scale_a=scale_x, scale_b=scale_weight  # type: ignore
            )

        return o.reshape(original_shape[0], original_shape[1], -1).to(input_dtype)

    else:
        # Dequantize the weight
        original_dtype = self.scale_weight.dtype
        dequantized_weight = self.weight.to(original_dtype) * self.scale_weight  # type: ignore

        # Perform linear transformation
        if self.bias is not None:
            output = F.linear(x, dequantized_weight, self.bias)
        else:
            output = F.linear(x, dequantized_weight)

        return output


def apply_fp8_monkey_patch(model, optimized_state_dict, use_scaled_mm=False):
    """
    Apply monkey patching to a model using FP8 optimized state dict.

    Args:
        model (nn.Module): Model instance to patch
        optimized_state_dict (dict): FP8 optimized state dict
        use_scaled_mm (bool): Use scaled_mm for FP8 Linear layers, requires SM 8.9+ (RTX 40 series)

    Returns:
        nn.Module: The patched model (same instance, modified in-place)
    """
    # # Calculate FP8 float8_e5m2 max value
    # max_value = calculate_fp8_maxval(5, 2)
    max_value = None  # do not quantize input tensor

    # Find all scale keys to identify FP8-optimized layers
    scale_keys = [k for k in optimized_state_dict.keys() if k.endswith(".scale_weight")]

    # Enumerate patched layers
    patched_module_paths = set()
    for scale_key in scale_keys:
        # Extract module path from scale key (remove .scale_weight)
        module_path = scale_key.rsplit(".scale_weight", 1)[0]
        patched_module_paths.add(module_path)

    patched_count = 0

    # Apply monkey patch to each layer with FP8 weights
    for name, module in model.named_modules():
        # Check if this module has a corresponding scale_weight
        has_scale = name in patched_module_paths

        # Apply patch if it's a Linear layer with FP8 scale
        if isinstance(module, nn.Linear) and has_scale:
            # register the scale_weight as a buffer to load the state_dict
            module.register_buffer(
                "scale_weight", torch.tensor(1.0, dtype=module.weight.dtype)
            )

            # Create a new forward method with the patched version.
            def new_forward(self, x):
                return fp8_linear_forward_patch(self, x, use_scaled_mm, max_value)

            # Bind method to module
            module.forward = new_forward.__get__(module, type(module))

            patched_count += 1

    logger.info(f"Number of monkey-patched Linear layers: {patched_count}")
    return model
</file>

<file path="modules/scheduling_flow_match_discrete.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/modules/scheduling_flow_match_discrete.py (Apache)

# Copyright 2024 Stability AI, Katherine Crowson and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================

from dataclasses import dataclass
from typing import Optional, Tuple, Union

import numpy as np
import torch

from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.utils.outputs import BaseOutput
from diffusers.schedulers.scheduling_utils import SchedulerMixin

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


@dataclass
class FlowMatchDiscreteSchedulerOutput(BaseOutput):
    """
    Output class for the scheduler's `step` function output.

    Args:
        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):
            Computed sample `(x_{t-1})` of previous timestep. `prev_sample` should be used as next model input in the
            denoising loop.
    """

    prev_sample: torch.FloatTensor


class FlowMatchDiscreteScheduler(SchedulerMixin, ConfigMixin):
    """
    Euler scheduler.

    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic
    methods the library implements for all schedulers such as loading and saving.

    Args:
        num_train_timesteps (`int`, defaults to 1000):
            The number of diffusion steps to train the model.
        timestep_spacing (`str`, defaults to `"linspace"`):
            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and
            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.
        shift (`float`, defaults to 1.0):
            The shift value for the timestep schedule.
        reverse (`bool`, defaults to `True`):
            Whether to reverse the timestep schedule.
    """

    _compatibles = []
    order = 1

    @register_to_config
    def __init__(
        self,
        num_train_timesteps: int = 1000,
        shift: float = 1.0,
        reverse: bool = True,
        solver: str = "euler",
        n_tokens: Optional[int] = None,
    ):
        sigmas = torch.linspace(1, 0, num_train_timesteps + 1)

        if not reverse:
            sigmas = sigmas.flip(0)

        self.sigmas = sigmas
        # the value fed to model
        self.timesteps = (sigmas[:-1] * num_train_timesteps).to(dtype=torch.float32)

        self._step_index = None
        self._begin_index = None

        self.supported_solver = ["euler"]
        if solver not in self.supported_solver:
            raise ValueError(
                f"Solver {solver} not supported. Supported solvers: {self.supported_solver}"
            )

    @property
    def step_index(self):
        """
        The index counter for current timestep. It will increase 1 after each scheduler step.
        """
        return self._step_index

    @property
    def begin_index(self):
        """
        The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
        """
        return self._begin_index

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.set_begin_index
    def set_begin_index(self, begin_index: int = 0):
        """
        Sets the begin index for the scheduler. This function should be run from pipeline before the inference.

        Args:
            begin_index (`int`):
                The begin index for the scheduler.
        """
        self._begin_index = begin_index

    def _sigma_to_t(self, sigma):
        return sigma * self.config.num_train_timesteps  # type: ignore

    def set_timesteps(
        self,
        num_inference_steps: int,
        device: Union[str, torch.device] = None,  # type: ignore
        n_tokens: int = None,  # type: ignore
    ):
        """
        Sets the discrete timesteps used for the diffusion chain (to be run before inference).

        Args:
            num_inference_steps (`int`):
                The number of diffusion steps used when generating samples with a pre-trained model.
            device (`str` or `torch.device`, *optional*):
                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
            n_tokens (`int`, *optional*):
                Number of tokens in the input sequence.
        """
        self.num_inference_steps = num_inference_steps

        sigmas = torch.linspace(1, 0, num_inference_steps + 1)
        sigmas = self.sd3_time_shift(sigmas)

        if not self.config.reverse:  # type: ignore
            sigmas = 1 - sigmas

        self.sigmas = sigmas
        self.timesteps = (sigmas[:-1] * self.config.num_train_timesteps).to(  # type: ignore
            dtype=torch.float32, device=device
        )

        # Reset step index
        self._step_index = None

    def index_for_timestep(self, timestep, schedule_timesteps=None):
        if schedule_timesteps is None:
            schedule_timesteps = self.timesteps

        indices = (schedule_timesteps == timestep).nonzero()

        # The sigma index that is taken for the **very** first `step`
        # is always the second index (or the last index if there is only 1)
        # This way we can ensure we don't accidentally skip a sigma in
        # case we start in the middle of the denoising schedule (e.g. for image-to-image)
        pos = 1 if len(indices) > 1 else 0

        return indices[pos].item()

    def _init_step_index(self, timestep):
        if self.begin_index is None:
            if isinstance(timestep, torch.Tensor):
                timestep = timestep.to(self.timesteps.device)
            self._step_index = self.index_for_timestep(timestep)
        else:
            self._step_index = self._begin_index

    def scale_model_input(
        self, sample: torch.Tensor, timestep: Optional[int] = None
    ) -> torch.Tensor:
        return sample

    def sd3_time_shift(self, t: torch.Tensor):
        return (self.config.shift * t) / (1 + (self.config.shift - 1) * t)  # type: ignore

    def step(
        self,
        model_output: torch.FloatTensor,
        timestep: Union[float, torch.FloatTensor],
        sample: torch.FloatTensor,
        return_dict: bool = True,
    ) -> Union[FlowMatchDiscreteSchedulerOutput, Tuple]:
        """
        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion
        process from the learned model outputs (most often the predicted noise).

        Args:
            model_output (`torch.FloatTensor`):
                The direct output from learned diffusion model.
            timestep (`float`):
                The current discrete timestep in the diffusion chain.
            sample (`torch.FloatTensor`):
                A current instance of a sample created by the diffusion process.
            generator (`torch.Generator`, *optional*):
                A random number generator.
            n_tokens (`int`, *optional*):
                Number of tokens in the input sequence.
            return_dict (`bool`):
                Whether or not to return a [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] or
                tuple.

        Returns:
            [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] or `tuple`:
                If return_dict is `True`, [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] is
                returned, otherwise a tuple is returned where the first element is the sample tensor.
        """

        if (
            isinstance(timestep, int)
            or isinstance(timestep, torch.IntTensor)
            or isinstance(timestep, torch.LongTensor)
        ):
            raise ValueError(
                (
                    "‚õî Passing integer indices (e.g. from `enumerate(timesteps)`) as timesteps to"
                    " `EulerDiscreteScheduler.step()` is not supported. Make sure to pass"
                    " one of the `scheduler.timesteps` as a timestep."
                ),
            )

        if self.step_index is None:
            self._init_step_index(timestep)

        # Upcast to avoid precision issues when computing prev_sample
        sample = sample.to(torch.float32)  # type: ignore

        dt = self.sigmas[self.step_index + 1] - self.sigmas[self.step_index]  # type: ignore

        if self.config.solver == "euler":  # type: ignore
            prev_sample = sample + model_output.to(torch.float32) * dt
        else:
            raise ValueError(
                f"Solver {self.config.solver} not supported. Supported solvers: {self.supported_solver}"  # type: ignore
            )

        # upon completion increase step index by one
        self._step_index += 1  # type: ignore

        if not return_dict:
            return (prev_sample,)

        return FlowMatchDiscreteSchedulerOutput(prev_sample=prev_sample)  # type: ignore

    def __len__(self):
        return self.config.num_train_timesteps  # type: ignore
</file>

<file path="modules/unet_causal_3d_blocks.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/modules/unet_causal_3d_blocks.py (Apache)

# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================

from typing import Optional, Tuple, Union

import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange

from diffusers.models.activations import get_activation
from diffusers.models.attention_processor import SpatialNorm
from diffusers.models.attention_processor import Attention
from diffusers.models.normalization import AdaGroupNorm
from diffusers.models.normalization import RMSNorm

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def prepare_causal_attention_mask(
    n_frame: int, n_hw: int, dtype, device, batch_size: int = None  # type: ignore
):
    seq_len = n_frame * n_hw
    mask = torch.full((seq_len, seq_len), float("-inf"), dtype=dtype, device=device)
    for i in range(seq_len):
        i_frame = i // n_hw
        mask[i, : (i_frame + 1) * n_hw] = 0
    if batch_size is not None:
        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)
    return mask


class CausalConv3d(nn.Module):
    """
    Implements a causal 3D convolution layer where each position only depends on previous timesteps and current spatial locations.
    This maintains temporal causality in video generation tasks.
    """

    def __init__(
        self,
        chan_in,
        chan_out,
        kernel_size: Union[int, Tuple[int, int, int]],
        stride: Union[int, Tuple[int, int, int]] = 1,
        dilation: Union[int, Tuple[int, int, int]] = 1,
        pad_mode="replicate",
        chunk_size=0,
        **kwargs,
    ):
        super().__init__()

        self.pad_mode = pad_mode
        padding = (
            kernel_size // 2,  # type: ignore
            kernel_size // 2,  # type: ignore
            kernel_size // 2,  # type: ignore
            kernel_size // 2,  # type: ignore
            kernel_size - 1,  # type: ignore
            0,
        )  # W, H, T
        self.time_causal_padding = padding
        self.chunk_size = chunk_size

        self.conv = nn.Conv3d(
            chan_in, chan_out, kernel_size, stride=stride, dilation=dilation, **kwargs
        )

    def original_forward(self, x):
        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)
        return self.conv(x)

    def forward(self, x):
        if self.chunk_size == 0:
            return self.original_forward(x)

        # if not large, call original forward
        if x.shape[4] < self.chunk_size * 1.5:
            return self.original_forward(x)

        # # debug: verify the original forward is the same as chunked forward
        # orig_forwarded_value = None
        # if x.shape[4] < self.chunk_size * 4:
        #     orig_forwarded_value = self.original_forward(x)

        # get the kernel size
        kernel_size = self.conv.kernel_size[0]  # assume cubic kernel
        assert (
            kernel_size == self.conv.kernel_size[1] == self.conv.kernel_size[2]
        ), "Only cubic kernels are supported"
        padding_size = kernel_size // 2  # 1 for kernel_size=3, 0 for kernel_size=1

        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)

        B, C, D, H, W = orig_shape = x.shape
        chunk_size = self.chunk_size
        chunk_size -= (
            chunk_size % self.conv.stride[2]
        )  # make sure the chunk size is divisible by stride
        # print(f"chunked forward: {x.shape}, chunk_size: {chunk_size}")

        # calculate the indices for chunking with overlap and padding by kernel size and stride
        indices = []
        i = 0
        while i < W - padding_size:
            start_idx = i - padding_size
            end_idx = min(i + chunk_size + padding_size, W)
            if i == 0:
                start_idx = 0
                end_idx += (
                    padding_size  # to make sure the first chunk is divisible by stride
                )
            if W - end_idx < chunk_size // 2:  # small chunk at the end
                end_idx = W
            indices.append((start_idx, end_idx))
            i = end_idx - padding_size
        # print(f"chunked forward: {x.shape}, chunked indices: {indices}")

        chunks = []
        for start_idx, end_idx in indices:
            chunk = x[:, :, :, :, start_idx:end_idx]
            chunk_output = self.conv(chunk)
            # print(chunk.shape, chunk_output.shape)
            chunks.append(chunk_output)

        # concatenate the chunks
        x = torch.cat(chunks, dim=4)

        assert (
            x.shape[2]
            == ((D - padding_size * 2) + self.conv.stride[0] - 1) // self.conv.stride[0]
        ), f"Invalid shape: {x.shape}, {orig_shape}, {padding_size}, {self.conv.stride}"
        assert (
            x.shape[3]
            == ((H - padding_size * 2) + self.conv.stride[1] - 1) // self.conv.stride[1]
        ), f"Invalid shape: {x.shape}, {orig_shape}, {padding_size}, {self.conv.stride}"
        assert (
            x.shape[4]
            == ((W - padding_size * 2) + self.conv.stride[2] - 1) // self.conv.stride[2]
        ), f"Invalid shape: {x.shape}, {orig_shape}, {padding_size}, {self.conv.stride}"

        # # debug: verify the original forward is the same as chunked forward
        # if orig_forwarded_value is not None:
        #     assert torch.allclose(
        #         orig_forwarded_value, x, rtol=1e-4, atol=1e-2
        #     ), f"Chunked forward is different from original forward. {x.shape}, {orig_shape}, {padding_size}, {self.conv.stride}, {self.conv.kernel_size}"

        return x


class UpsampleCausal3D(nn.Module):
    """
    A 3D upsampling layer with an optional convolution.
    """

    def __init__(
        self,
        channels: int,
        use_conv: bool = False,
        use_conv_transpose: bool = False,
        out_channels: Optional[int] = None,
        name: str = "conv",
        kernel_size: Optional[int] = None,
        padding=1,
        norm_type=None,
        eps=None,
        elementwise_affine=None,
        bias=True,
        interpolate=True,
        upsample_factor=(2, 2, 2),
    ):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name
        self.interpolate = interpolate
        self.upsample_factor = upsample_factor

        if norm_type == "ln_norm":
            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)  # type: ignore
        elif norm_type == "rms_norm":
            self.norm = RMSNorm(channels, eps, elementwise_affine)  # type: ignore
        elif norm_type is None:
            self.norm = None
        else:
            raise ValueError(f"unknown norm_type: {norm_type}")

        conv = None
        if use_conv_transpose:
            raise NotImplementedError
        elif use_conv:
            if kernel_size is None:
                kernel_size = 3
            conv = CausalConv3d(
                self.channels, self.out_channels, kernel_size=kernel_size, bias=bias
            )

        if name == "conv":
            self.conv = conv
        else:
            self.Conv2d_0 = conv

    def forward(
        self,
        hidden_states: torch.FloatTensor,
        output_size: Optional[int] = None,
        scale: float = 1.0,
    ) -> torch.FloatTensor:
        assert hidden_states.shape[1] == self.channels

        if self.norm is not None:
            raise NotImplementedError

        if self.use_conv_transpose:
            return self.conv(hidden_states)  # type: ignore

        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16
        dtype = hidden_states.dtype
        if dtype == torch.bfloat16:
            hidden_states = hidden_states.to(torch.float32)  # type: ignore

        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
        if hidden_states.shape[0] >= 64:
            hidden_states = hidden_states.contiguous()  # type: ignore

        # if `output_size` is passed we force the interpolation output
        # size and do not make use of `scale_factor=2`
        if self.interpolate:
            B, C, T, H, W = hidden_states.shape
            first_h, other_h = hidden_states.split((1, T - 1), dim=2)
            if output_size is None:
                if T > 1:
                    other_h = F.interpolate(
                        other_h, scale_factor=self.upsample_factor, mode="nearest"
                    )

                first_h = first_h.squeeze(2)
                first_h = F.interpolate(
                    first_h, scale_factor=self.upsample_factor[1:], mode="nearest"
                )
                first_h = first_h.unsqueeze(2)
            else:
                raise NotImplementedError

            if T > 1:
                hidden_states = torch.cat((first_h, other_h), dim=2)  # type: ignore
            else:
                hidden_states = first_h

        # If the input is bfloat16, we cast back to bfloat16
        if dtype == torch.bfloat16:
            hidden_states = hidden_states.to(dtype)  # type: ignore

        if self.use_conv:
            if self.name == "conv":
                hidden_states = self.conv(hidden_states)  # type: ignore
            else:
                hidden_states = self.Conv2d_0(hidden_states)  # type: ignore

        return hidden_states


class DownsampleCausal3D(nn.Module):
    """
    A 3D downsampling layer with an optional convolution.
    """

    def __init__(
        self,
        channels: int,
        use_conv: bool = False,
        out_channels: Optional[int] = None,
        padding: int = 1,
        name: str = "conv",
        kernel_size=3,
        norm_type=None,
        eps=None,
        elementwise_affine=None,
        bias=True,
        stride=2,
    ):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.padding = padding
        stride = stride
        self.name = name

        if norm_type == "ln_norm":
            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)  # type: ignore
        elif norm_type == "rms_norm":
            self.norm = RMSNorm(channels, eps, elementwise_affine)  # type: ignore
        elif norm_type is None:
            self.norm = None
        else:
            raise ValueError(f"unknown norm_type: {norm_type}")

        if use_conv:
            conv = CausalConv3d(
                self.channels,
                self.out_channels,
                kernel_size=kernel_size,
                stride=stride,
                bias=bias,
            )
        else:
            raise NotImplementedError

        if name == "conv":
            self.Conv2d_0 = conv
            self.conv = conv
        elif name == "Conv2d_0":
            self.conv = conv
        else:
            self.conv = conv

    def forward(
        self, hidden_states: torch.FloatTensor, scale: float = 1.0
    ) -> torch.FloatTensor:
        assert hidden_states.shape[1] == self.channels

        if self.norm is not None:
            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(
                0, 3, 1, 2
            )

        assert hidden_states.shape[1] == self.channels

        hidden_states = self.conv(hidden_states)

        return hidden_states


class ResnetBlockCausal3D(nn.Module):
    r"""
    A Resnet block.
    """

    def __init__(
        self,
        *,
        in_channels: int,
        out_channels: Optional[int] = None,
        conv_shortcut: bool = False,
        dropout: float = 0.0,
        temb_channels: int = 512,
        groups: int = 32,
        groups_out: Optional[int] = None,
        pre_norm: bool = True,
        eps: float = 1e-6,
        non_linearity: str = "swish",
        skip_time_act: bool = False,
        # default, scale_shift, ada_group, spatial
        time_embedding_norm: str = "default",
        kernel: Optional[torch.FloatTensor] = None,
        output_scale_factor: float = 1.0,
        use_in_shortcut: Optional[bool] = None,
        up: bool = False,
        down: bool = False,
        conv_shortcut_bias: bool = True,
        conv_3d_out_channels: Optional[int] = None,
    ):
        super().__init__()
        self.pre_norm = pre_norm
        self.pre_norm = True
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.up = up
        self.down = down
        self.output_scale_factor = output_scale_factor
        self.time_embedding_norm = time_embedding_norm
        self.skip_time_act = skip_time_act

        linear_cls = nn.Linear

        if groups_out is None:
            groups_out = groups

        if self.time_embedding_norm == "ada_group":
            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)
        elif self.time_embedding_norm == "spatial":
            self.norm1 = SpatialNorm(in_channels, temb_channels)
        else:
            self.norm1 = torch.nn.GroupNorm(
                num_groups=groups, num_channels=in_channels, eps=eps, affine=True
            )

        self.conv1 = CausalConv3d(in_channels, out_channels, kernel_size=3, stride=1)

        if temb_channels is not None:
            if self.time_embedding_norm == "default":
                self.time_emb_proj = linear_cls(temb_channels, out_channels)
            elif self.time_embedding_norm == "scale_shift":
                self.time_emb_proj = linear_cls(temb_channels, 2 * out_channels)
            elif (
                self.time_embedding_norm == "ada_group"
                or self.time_embedding_norm == "spatial"
            ):
                self.time_emb_proj = None
            else:
                raise ValueError(
                    f"Unknown time_embedding_norm : {self.time_embedding_norm} "
                )
        else:
            self.time_emb_proj = None

        if self.time_embedding_norm == "ada_group":
            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)
        elif self.time_embedding_norm == "spatial":
            self.norm2 = SpatialNorm(out_channels, temb_channels)
        else:
            self.norm2 = torch.nn.GroupNorm(
                num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True
            )

        self.dropout = torch.nn.Dropout(dropout)
        conv_3d_out_channels = conv_3d_out_channels or out_channels
        self.conv2 = CausalConv3d(
            out_channels, conv_3d_out_channels, kernel_size=3, stride=1
        )

        self.nonlinearity = get_activation(non_linearity)

        self.upsample = self.downsample = None
        if self.up:
            self.upsample = UpsampleCausal3D(in_channels, use_conv=False)
        elif self.down:
            self.downsample = DownsampleCausal3D(in_channels, use_conv=False, name="op")

        self.use_in_shortcut = (
            self.in_channels != conv_3d_out_channels
            if use_in_shortcut is None
            else use_in_shortcut
        )

        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = CausalConv3d(
                in_channels,
                conv_3d_out_channels,
                kernel_size=1,
                stride=1,
                bias=conv_shortcut_bias,
            )

    def forward(
        self,
        input_tensor: torch.FloatTensor,
        temb: torch.FloatTensor,
        scale: float = 1.0,
    ) -> torch.FloatTensor:
        hidden_states = input_tensor

        if (
            self.time_embedding_norm == "ada_group"
            or self.time_embedding_norm == "spatial"
        ):
            hidden_states = self.norm1(hidden_states, temb)
        else:
            hidden_states = self.norm1(hidden_states)

        hidden_states = self.nonlinearity(hidden_states)

        if self.upsample is not None:
            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
            if hidden_states.shape[0] >= 64:
                input_tensor = input_tensor.contiguous()  # type: ignore
                hidden_states = hidden_states.contiguous()
            input_tensor = self.upsample(input_tensor, scale=scale)
            hidden_states = self.upsample(hidden_states, scale=scale)
        elif self.downsample is not None:
            input_tensor = self.downsample(input_tensor, scale=scale)
            hidden_states = self.downsample(hidden_states, scale=scale)

        hidden_states = self.conv1(hidden_states)

        if self.time_emb_proj is not None:
            if not self.skip_time_act:
                temb = self.nonlinearity(temb)
            temb = self.time_emb_proj(temb, scale)[:, :, None, None]

        if temb is not None and self.time_embedding_norm == "default":
            hidden_states = hidden_states + temb

        if (
            self.time_embedding_norm == "ada_group"
            or self.time_embedding_norm == "spatial"
        ):
            hidden_states = self.norm2(hidden_states, temb)
        else:
            hidden_states = self.norm2(hidden_states)

        if temb is not None and self.time_embedding_norm == "scale_shift":
            scale, shift = torch.chunk(temb, 2, dim=1)  # type: ignore
            hidden_states = hidden_states * (1 + scale) + shift

        hidden_states = self.nonlinearity(hidden_states)

        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)

        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)

        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor

        return output_tensor


def get_down_block3d(
    down_block_type: str,
    num_layers: int,
    in_channels: int,
    out_channels: int,
    temb_channels: int,
    add_downsample: bool,
    downsample_stride: int,
    resnet_eps: float,
    resnet_act_fn: str,
    transformer_layers_per_block: int = 1,
    num_attention_heads: Optional[int] = None,
    resnet_groups: Optional[int] = None,
    cross_attention_dim: Optional[int] = None,
    downsample_padding: Optional[int] = None,
    dual_cross_attention: bool = False,
    use_linear_projection: bool = False,
    only_cross_attention: bool = False,
    upcast_attention: bool = False,
    resnet_time_scale_shift: str = "default",
    attention_type: str = "default",
    resnet_skip_time_act: bool = False,
    resnet_out_scale_factor: float = 1.0,
    cross_attention_norm: Optional[str] = None,
    attention_head_dim: Optional[int] = None,
    downsample_type: Optional[str] = None,
    dropout: float = 0.0,
):
    # If attn head dim is not defined, we default it to the number of heads
    if attention_head_dim is None:
        logger.warn(
            f"It is recommended to provide `attention_head_dim` when calling `get_down_block`. Defaulting `attention_head_dim` to {num_attention_heads}."
        )
        attention_head_dim = num_attention_heads

    down_block_type = (
        down_block_type[7:]
        if down_block_type.startswith("UNetRes")
        else down_block_type
    )
    if down_block_type == "DownEncoderBlockCausal3D":
        return DownEncoderBlockCausal3D(
            num_layers=num_layers,
            in_channels=in_channels,
            out_channels=out_channels,
            dropout=dropout,
            add_downsample=add_downsample,
            downsample_stride=downsample_stride,
            resnet_eps=resnet_eps,
            resnet_act_fn=resnet_act_fn,
            resnet_groups=resnet_groups,  # type: ignore
            downsample_padding=downsample_padding,  # type: ignore
            resnet_time_scale_shift=resnet_time_scale_shift,
        )
    raise ValueError(f"{down_block_type} does not exist.")


def get_up_block3d(
    up_block_type: str,
    num_layers: int,
    in_channels: int,
    out_channels: int,
    prev_output_channel: int,
    temb_channels: int,
    add_upsample: bool,
    upsample_scale_factor: Tuple,
    resnet_eps: float,
    resnet_act_fn: str,
    resolution_idx: Optional[int] = None,
    transformer_layers_per_block: int = 1,
    num_attention_heads: Optional[int] = None,
    resnet_groups: Optional[int] = None,
    cross_attention_dim: Optional[int] = None,
    dual_cross_attention: bool = False,
    use_linear_projection: bool = False,
    only_cross_attention: bool = False,
    upcast_attention: bool = False,
    resnet_time_scale_shift: str = "default",
    attention_type: str = "default",
    resnet_skip_time_act: bool = False,
    resnet_out_scale_factor: float = 1.0,
    cross_attention_norm: Optional[str] = None,
    attention_head_dim: Optional[int] = None,
    upsample_type: Optional[str] = None,
    dropout: float = 0.0,
) -> nn.Module:
    # If attn head dim is not defined, we default it to the number of heads
    if attention_head_dim is None:
        logger.warn(
            f"It is recommended to provide `attention_head_dim` when calling `get_up_block`. Defaulting `attention_head_dim` to {num_attention_heads}."
        )
        attention_head_dim = num_attention_heads

    up_block_type = (
        up_block_type[7:] if up_block_type.startswith("UNetRes") else up_block_type
    )
    if up_block_type == "UpDecoderBlockCausal3D":
        return UpDecoderBlockCausal3D(
            num_layers=num_layers,
            in_channels=in_channels,
            out_channels=out_channels,
            resolution_idx=resolution_idx,
            dropout=dropout,
            add_upsample=add_upsample,
            upsample_scale_factor=upsample_scale_factor,
            resnet_eps=resnet_eps,
            resnet_act_fn=resnet_act_fn,
            resnet_groups=resnet_groups,  # type: ignore
            resnet_time_scale_shift=resnet_time_scale_shift,
            temb_channels=temb_channels,
        )
    raise ValueError(f"{up_block_type} does not exist.")


class UNetMidBlockCausal3D(nn.Module):
    """
    A 3D UNet mid-block [`UNetMidBlockCausal3D`] with multiple residual blocks and optional attention blocks.
    """

    def __init__(
        self,
        in_channels: int,
        temb_channels: int,
        dropout: float = 0.0,
        num_layers: int = 1,
        resnet_eps: float = 1e-6,
        resnet_time_scale_shift: str = "default",  # default, spatial
        resnet_act_fn: str = "swish",
        resnet_groups: int = 32,
        attn_groups: Optional[int] = None,
        resnet_pre_norm: bool = True,
        add_attention: bool = True,
        attention_head_dim: int = 1,
        output_scale_factor: float = 1.0,
    ):
        super().__init__()
        resnet_groups = (
            resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        )
        self.add_attention = add_attention

        if attn_groups is None:
            attn_groups = (
                resnet_groups if resnet_time_scale_shift == "default" else None
            )

        # there is always at least one resnet
        resnets = [
            ResnetBlockCausal3D(
                in_channels=in_channels,
                out_channels=in_channels,
                temb_channels=temb_channels,
                eps=resnet_eps,
                groups=resnet_groups,
                dropout=dropout,
                time_embedding_norm=resnet_time_scale_shift,
                non_linearity=resnet_act_fn,
                output_scale_factor=output_scale_factor,
                pre_norm=resnet_pre_norm,
            )
        ]
        attentions = []

        if attention_head_dim is None:
            logger.warn(
                f"It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}."
            )
            attention_head_dim = in_channels

        for _ in range(num_layers):
            if self.add_attention:
                attentions.append(
                    Attention(
                        in_channels,
                        heads=in_channels // attention_head_dim,
                        dim_head=attention_head_dim,
                        rescale_output_factor=output_scale_factor,
                        eps=resnet_eps,
                        norm_num_groups=attn_groups,
                        spatial_norm_dim=(
                            temb_channels
                            if resnet_time_scale_shift == "spatial"
                            else None
                        ),
                        residual_connection=True,
                        bias=True,
                        upcast_softmax=True,
                        _from_deprecated_attn_block=True,
                    )
                )
            else:
                attentions.append(None)

            resnets.append(
                ResnetBlockCausal3D(
                    in_channels=in_channels,
                    out_channels=in_channels,
                    temb_channels=temb_channels,
                    eps=resnet_eps,
                    groups=resnet_groups,
                    dropout=dropout,
                    time_embedding_norm=resnet_time_scale_shift,
                    non_linearity=resnet_act_fn,
                    output_scale_factor=output_scale_factor,
                    pre_norm=resnet_pre_norm,
                )
            )

        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(
        self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None
    ) -> torch.FloatTensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if attn is not None:
                B, C, T, H, W = hidden_states.shape
                hidden_states = rearrange(hidden_states, "b c f h w -> b (f h w) c")
                attention_mask = prepare_causal_attention_mask(
                    T, H * W, hidden_states.dtype, hidden_states.device, batch_size=B
                )
                hidden_states = attn(
                    hidden_states, temb=temb, attention_mask=attention_mask
                )
                hidden_states = rearrange(
                    hidden_states, "b (f h w) c -> b c f h w", f=T, h=H, w=W
                )
            hidden_states = resnet(hidden_states, temb)

        return hidden_states


class DownEncoderBlockCausal3D(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        dropout: float = 0.0,
        num_layers: int = 1,
        resnet_eps: float = 1e-6,
        resnet_time_scale_shift: str = "default",
        resnet_act_fn: str = "swish",
        resnet_groups: int = 32,
        resnet_pre_norm: bool = True,
        output_scale_factor: float = 1.0,
        add_downsample: bool = True,
        downsample_stride: int = 2,
        downsample_padding: int = 1,
    ):
        super().__init__()
        resnets = []

        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(
                ResnetBlockCausal3D(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    temb_channels=None,  # type: ignore
                    eps=resnet_eps,
                    groups=resnet_groups,
                    dropout=dropout,
                    time_embedding_norm=resnet_time_scale_shift,
                    non_linearity=resnet_act_fn,
                    output_scale_factor=output_scale_factor,
                    pre_norm=resnet_pre_norm,
                )
            )

        self.resnets = nn.ModuleList(resnets)

        if add_downsample:
            self.downsamplers = nn.ModuleList(
                [
                    DownsampleCausal3D(
                        out_channels,
                        use_conv=True,
                        out_channels=out_channels,
                        padding=downsample_padding,
                        name="op",
                        stride=downsample_stride,
                    )
                ]
            )
        else:
            self.downsamplers = None

    def forward(
        self, hidden_states: torch.FloatTensor, scale: float = 1.0
    ) -> torch.FloatTensor:
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=None, scale=scale)

        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, scale)

        return hidden_states


class UpDecoderBlockCausal3D(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        resolution_idx: Optional[int] = None,
        dropout: float = 0.0,
        num_layers: int = 1,
        resnet_eps: float = 1e-6,
        resnet_time_scale_shift: str = "default",  # default, spatial
        resnet_act_fn: str = "swish",
        resnet_groups: int = 32,
        resnet_pre_norm: bool = True,
        output_scale_factor: float = 1.0,
        add_upsample: bool = True,
        upsample_scale_factor=(2, 2, 2),
        temb_channels: Optional[int] = None,
    ):
        super().__init__()
        resnets = []

        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels

            resnets.append(
                ResnetBlockCausal3D(
                    in_channels=input_channels,
                    out_channels=out_channels,
                    temb_channels=temb_channels,  # type: ignore
                    eps=resnet_eps,
                    groups=resnet_groups,
                    dropout=dropout,
                    time_embedding_norm=resnet_time_scale_shift,
                    non_linearity=resnet_act_fn,
                    output_scale_factor=output_scale_factor,
                    pre_norm=resnet_pre_norm,
                )
            )

        self.resnets = nn.ModuleList(resnets)

        if add_upsample:
            self.upsamplers = nn.ModuleList(
                [
                    UpsampleCausal3D(
                        out_channels,
                        use_conv=True,
                        out_channels=out_channels,
                        upsample_factor=upsample_scale_factor,
                    )
                ]
            )
        else:
            self.upsamplers = None

        self.resolution_idx = resolution_idx

    def forward(
        self,
        hidden_states: torch.FloatTensor,
        temb: Optional[torch.FloatTensor] = None,
        scale: float = 1.0,
    ) -> torch.FloatTensor:
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=temb, scale=scale)

        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)

        return hidden_states
</file>

<file path="networks/control_lora_wan.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/networks/lora.py (Apache)
## Based on: https://github.com/spacepxl/WanTraining/blob/main/train_wan_lora.py (Apache)

# Control LoRA network module for training with control signals
# This extends the standard LoRA functionality to support control signal training

import ast
from typing import Dict, List, Optional, Type, Union
from transformers import CLIPTextModel
import torch
import torch.nn as nn

import logging
from common.logger import get_logger
import os

logger = get_logger(__name__, level=logging.INFO)

# Import the base LoRA module
from .lora_wan import (
    LoRAModule,
    LoRANetwork,
)

WAN_TARGET_REPLACE_MODULES: list[str] = ["WanAttentionBlock"]


class ControlLoRAModule(LoRAModule):
    """
    Control LoRA module that extends the base LoRA functionality
    to support control signal training.
    """

    def __init__(
        self,
        lora_name,
        org_module: torch.nn.Module,
        multiplier=1.0,
        lora_dim=4,
        alpha=1,
        dropout=None,
        rank_dropout=None,
        module_dropout=None,
        split_dims: Optional[List[int]] = None,
        control_config: Optional[Dict] = None,
    ):
        super().__init__(
            lora_name,
            org_module,
            multiplier,
            lora_dim,
            alpha,
            dropout,
            rank_dropout,
            module_dropout,
            split_dims,
        )

        self.control_config = control_config or {}
        self.control_enabled = self.control_config.get("enabled", False)

        if self.control_enabled:
            logger.info(
                f"Control LoRA enabled for {lora_name} with config: {self.control_config}"
            )

    def forward(self, x):
        # Standard LoRA forward pass
        org_forwarded = self.org_forward(x)

        # module dropout
        if self.module_dropout is not None and self.training:
            if torch.rand(1) < self.module_dropout:
                return org_forwarded

        if self.split_dims is None:
            lx = self.lora_down(x)

            # normal dropout
            if self.dropout is not None and self.training:
                lx = torch.nn.functional.dropout(lx, p=self.dropout)

            # rank dropout
            if self.rank_dropout is not None and self.training:
                mask = (
                    torch.rand((lx.size(0), self.lora_dim), device=lx.device)
                    > self.rank_dropout
                )
                if len(lx.size()) == 3:
                    mask = mask.unsqueeze(1)  # for Text Encoder
                elif len(lx.size()) == 4:
                    mask = mask.unsqueeze(-1).unsqueeze(-1)  # for Conv2d
                lx = lx * mask

                # scaling for rank dropout: treat as if the rank is changed
                scale = self.scale * (
                    1.0 / (1.0 - self.rank_dropout)
                )  # redundant for readability
            else:
                scale = self.scale

            lx = self.lora_up(lx)

            return org_forwarded + lx * self.multiplier * scale
        else:
            lxs = [lora_down(x) for lora_down in self.lora_down]  # type: ignore

            # normal dropout
            if self.dropout is not None and self.training:
                lxs = [torch.nn.functional.dropout(lx, p=self.dropout) for lx in lxs]

            # rank dropout
            if self.rank_dropout is not None and self.training:
                masks = [
                    torch.rand((lx.size(0), self.lora_dim), device=lx.device)
                    > self.rank_dropout
                    for lx in lxs
                ]
                for i in range(len(lxs)):
                    if len(lxs[i].size()) == 3:
                        masks[i] = masks[i].unsqueeze(1)
                    elif len(lxs[i].size()) == 4:
                        masks[i] = masks[i].unsqueeze(-1).unsqueeze(-1)
                    lxs[i] = lxs[i] * masks[i]

                # scaling for rank dropout: treat as if the rank is changed
                scale = self.scale * (
                    1.0 / (1.0 - self.rank_dropout)
                )  # redundant for readability
            else:
                scale = self.scale

            lxs = [lora_up(lx) for lora_up, lx in zip(self.lora_up, lxs)]  # type: ignore

            return org_forwarded + torch.cat(lxs, dim=-1) * self.multiplier * scale


class ControlLoRAInfModule(ControlLoRAModule):
    """
    Control LoRA inference module that extends the base inference functionality.
    """

    def __init__(
        self,
        lora_name,
        org_module: torch.nn.Module,
        multiplier=1.0,
        lora_dim=4,
        alpha=1,
        control_config: Optional[Dict] = None,
        **kwargs,
    ):
        # no dropout for inference
        super().__init__(
            lora_name,
            org_module,
            multiplier,
            lora_dim,
            alpha,
            control_config=control_config,
        )

        self.org_module_ref = [org_module]  # for reference
        self.enabled = True
        self.network: Optional[ControlLoRANetwork] = None

    def set_network(self, network):
        self.network = network

    def default_forward(self, x):
        return super().forward(x)

    def forward(self, x):
        if not self.enabled:
            return self.org_forward(x)
        return self.default_forward(x)


class ControlLoRANetwork(LoRANetwork):
    """
    Control LoRA network that extends the base LoRA network
    to support control signal training.
    """

    def __init__(
        self,
        target_replace_modules: List[str],
        prefix: str,
        text_encoders: Union[List[CLIPTextModel], CLIPTextModel],
        unet: nn.Module,
        multiplier: float = 1.0,
        lora_dim: int = 4,
        alpha: float = 1,
        dropout: Optional[float] = None,
        rank_dropout: Optional[float] = None,
        module_dropout: Optional[float] = None,
        conv_lora_dim: Optional[int] = None,
        conv_alpha: Optional[float] = None,
        module_class: Type[object] = ControlLoRAModule,
        modules_dim: Optional[Dict[str, int]] = None,
        modules_alpha: Optional[Dict[str, int]] = None,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        verbose: Optional[bool] = False,
        control_config: Optional[Dict] = None,
    ) -> None:
        # Store control config for use in module creation
        self.control_config = control_config or {}

        # Call parent constructor with modified module_class
        super().__init__(
            target_replace_modules,
            prefix,
            text_encoders,
            unet,
            multiplier,
            lora_dim,
            alpha,
            dropout,
            rank_dropout,
            module_dropout,
            conv_lora_dim,
            conv_alpha,
            module_class,
            modules_dim,
            modules_alpha,
            exclude_patterns,
            include_patterns,
            verbose,
        )

        # Override module creation to pass control_config
        self._create_modules_with_control_config()

    def _create_modules_with_control_config(self):
        """Recreate modules with control config passed to each module."""
        # This is a simplified approach - in practice, you'd want to modify
        # the module creation logic in the parent class
        for lora in self.text_encoder_loras + self.unet_loras:
            if hasattr(lora, "control_config"):
                lora.control_config = self.control_config  # type: ignore

    def load_state_dict(self, state_dict, strict=True):
        """
        Custom load_state_dict that handles missing keys gracefully.
        This is needed because the network structure might change between
        saving and loading, especially with control LoRA configurations.
        """
        # Get the current state dict keys
        current_keys = set(self.state_dict().keys())
        saved_keys = set(state_dict.keys())

        # Analyze the key differences
        missing_from_saved = current_keys - saved_keys
        missing_from_current = saved_keys - current_keys
        matching_keys = current_keys & saved_keys

        # Log detailed information about the key differences
        logger.info(f"üîç State dict analysis:")
        logger.info(f"   Current network has {len(current_keys)} keys")
        logger.info(f"   Saved state has {len(saved_keys)} keys")
        logger.info(f"   Matching keys: {len(matching_keys)}")
        logger.info(f"   Missing from saved: {len(missing_from_saved)}")
        logger.info(f"   Missing from current: {len(missing_from_current)}")

        # Extract module names from keys for better analysis
        def extract_module_name(key):
            if "." in key:
                return key.split(".")[0]
            return key

        current_modules = set(extract_module_name(key) for key in current_keys)
        saved_modules = set(extract_module_name(key) for key in saved_keys)

        logger.info(f"üìä Module analysis:")
        logger.info(f"   Current modules: {len(current_modules)}")
        logger.info(f"   Saved modules: {len(saved_modules)}")
        logger.info(f"   Common modules: {len(current_modules & saved_modules)}")

        # Show sample module names for debugging
        sample_current = list(current_modules)[:5]
        sample_saved = list(saved_modules)[:5]
        logger.info(f"   Sample current modules: {sample_current}")
        logger.info(f"   Sample saved modules: {sample_saved}")

        # Filter the input state dict to only include keys that exist in the current network
        filtered_state_dict = {}
        ignored_keys = []

        for key, value in state_dict.items():
            if key in current_keys:
                filtered_state_dict[key] = value
            else:
                ignored_keys.append(key)

        # Log information about the filtering
        if ignored_keys:
            logger.warning(
                f"ControlLoRANetwork: {len(ignored_keys)} keys from saved state "
                f"are not present in current network structure and will be ignored."
            )

            # Group ignored keys by module for better understanding
            ignored_modules = {}
            for key in ignored_keys:
                module_name = extract_module_name(key)
                if module_name not in ignored_modules:
                    ignored_modules[module_name] = []
                ignored_modules[module_name].append(key)

            logger.info(f"üóëÔ∏è  Ignored keys by module:")
            for module_name, keys in list(ignored_modules.items())[
                :10
            ]:  # Show first 10 modules
                logger.info(f"   {module_name}: {len(keys)} keys")
                if len(keys) <= 3:
                    logger.info(f"      {keys}")
                else:
                    logger.info(f"      {keys[:3]}... (and {len(keys)-3} more)")

        # Check if we have a reasonable number of matching keys
        if len(filtered_state_dict) == 0:
            logger.error(
                "‚ùå No matching keys found between saved state and current network!"
            )
            logger.error("This indicates a fundamental mismatch in network structure.")
            return super().load_state_dict({}, strict=False)

        match_ratio = len(filtered_state_dict) / len(current_keys)
        logger.info(
            f"‚úÖ Loading {len(filtered_state_dict)} keys (match ratio: {match_ratio:.2%})"
        )

        if match_ratio < 0.5:
            logger.warning(
                f"‚ö†Ô∏è  Low match ratio ({match_ratio:.2%}). This may indicate significant "
                f"network structure changes between save and load."
            )

        # Call parent's load_state_dict with filtered state dict
        result = super().load_state_dict(filtered_state_dict, strict=False)

        # Log the final result
        if result.missing_keys:
            logger.info(
                f"üìã Final result: {len(result.missing_keys)} keys still missing from current network"
            )

            # Group missing keys by module
            missing_modules = {}
            for key in result.missing_keys:
                module_name = extract_module_name(key)
                if module_name not in missing_modules:
                    missing_modules[module_name] = []
                missing_modules[module_name].append(key)

            logger.info(f"üìã Missing keys by module:")
            for module_name, keys in list(missing_modules.items())[
                :5
            ]:  # Show first 5 modules
                logger.info(f"   {module_name}: {len(keys)} keys")

        if result.unexpected_keys:
            logger.info(
                f"üìã Final result: {len(result.unexpected_keys)} unexpected keys"
            )

        # Final success/failure assessment
        total_expected = len(current_keys)
        total_loaded = len(filtered_state_dict) - len(result.missing_keys)
        load_success_ratio = total_loaded / total_expected if total_expected > 0 else 0

        logger.info(
            f"üéØ Load success: {total_loaded}/{total_expected} keys ({load_success_ratio:.2%})"
        )

        if load_success_ratio >= 0.8:
            logger.info("‚úÖ Control LoRA state loading successful!")
        elif load_success_ratio >= 0.5:
            logger.warning("‚ö†Ô∏è  Partial control LoRA state loading - some keys missing")
        else:
            logger.error(
                "‚ùå Control LoRA state loading largely failed - most keys missing"
            )

        return result

    def save_weights(self, file, dtype, metadata):
        """
        Enhanced save_weights method that includes control LoRA configuration metadata.
        This helps with proper resumption of control LoRA training.
        """
        # Add control LoRA specific metadata
        if self.control_config and self.control_config.get("enabled", False):
            logger.info("üéØ Saving control LoRA weights with configuration metadata")

            # Add control LoRA configuration to metadata
            if metadata is None:
                metadata = {}

            metadata["control_lora_enabled"] = "True"
            metadata["control_lora_type"] = str(
                self.control_config.get("control_lora_type", "unknown")
            )
            metadata["control_preprocessing"] = str(
                self.control_config.get("control_preprocessing", "unknown")
            )
            metadata["control_blur_kernel_size"] = str(
                self.control_config.get("control_blur_kernel_size", 0)
            )
            metadata["control_blur_sigma"] = str(
                self.control_config.get("control_blur_sigma", 0.0)
            )
            metadata["control_scale_factor"] = str(
                self.control_config.get("control_scale_factor", 1.0)
            )
            metadata["input_lr_scale"] = str(
                self.control_config.get("input_lr_scale", 1.0)
            )
            metadata["control_concatenation_dim"] = str(
                self.control_config.get("control_concatenation_dim", 0)
            )

            # Log what we're saving
            logger.info(f"üì¶ Control LoRA config being saved:")
            for key, value in metadata.items():
                if key.startswith("control_"):
                    logger.info(f"   {key}: {value}")

            # Log network structure information
            state_dict = self.state_dict()
            lora_modules = []
            for key in state_dict.keys():
                if "lora_" in key and ("down" in key or "up" in key or "alpha" in key):
                    module_name = key.split(".")[0]
                    if module_name not in lora_modules:
                        lora_modules.append(module_name)

            logger.info(f"üéØ LoRA modules being saved: {len(lora_modules)} modules")
            logger.info(f"   Module names: {lora_modules[:10]}...")  # Show first 10

            # Add module count to metadata
            metadata["control_lora_module_count"] = str(len(lora_modules))
            metadata["control_lora_modules"] = str(
                lora_modules[:20]
            )  # Save first 20 module names
        else:
            logger.info("üì¶ Saving standard LoRA weights (control LoRA not enabled)")

        # Call parent's save_weights method
        super().save_weights(file, dtype, metadata)

    def load_weights(self, file):
        """
        Enhanced load_weights method that validates control LoRA configuration from metadata.
        This helps ensure proper resumption of control LoRA training.
        """
        if os.path.splitext(file)[1] == ".safetensors":
            from safetensors.torch import load_file

            weights_sd = load_file(file)

            # Check for control LoRA metadata
            # For now, we'll skip metadata validation for safetensors files
            # as the metadata access is complex and varies by safetensors version
            metadata = None
            logger.info("üì¶ Loading LoRA weights from safetensors format")

            # Skip metadata validation for now
            if False:  # metadata:
                if metadata.get("control_lora_enabled", False):
                    logger.info(
                        "üéØ Loading control LoRA weights with configuration validation"
                    )

                    # Log saved configuration
                    logger.info(f"üì¶ Saved control LoRA config:")
                    for key, value in metadata.items():
                        if key.startswith("control_"):
                            logger.info(f"   {key}: {value}")

                    # Validate configuration compatibility
                    if self.control_config and self.control_config.get(
                        "enabled", False
                    ):
                        saved_type = metadata.get("control_lora_type", "unknown")
                        current_type = self.control_config.get(
                            "control_lora_type", "unknown"
                        )

                        if saved_type != current_type:
                            logger.warning(
                                f"‚ö†Ô∏è  Control LoRA type mismatch: saved={saved_type}, current={current_type}"
                            )

                        saved_preprocessing = metadata.get(
                            "control_preprocessing", "unknown"
                        )
                        current_preprocessing = self.control_config.get(
                            "control_preprocessing", "unknown"
                        )

                        if saved_preprocessing != current_preprocessing:
                            logger.warning(
                                f"‚ö†Ô∏è  Control preprocessing mismatch: saved={saved_preprocessing}, current={current_preprocessing}"
                            )

                        # Log module count comparison
                        saved_module_count = metadata.get(
                            "control_lora_module_count", 0
                        )
                        current_state_dict = self.state_dict()
                        current_modules = []
                        for key in current_state_dict.keys():
                            if "lora_" in key and (
                                "down" in key or "up" in key or "alpha" in key
                            ):
                                module_name = key.split(".")[0]
                                if module_name not in current_modules:
                                    current_modules.append(module_name)

                        logger.info(
                            f"üéØ Module count comparison: saved={saved_module_count}, current={len(current_modules)}"
                        )

                        if saved_module_count != len(current_modules):
                            logger.warning(
                                f"‚ö†Ô∏è  Module count mismatch: saved={saved_module_count}, current={len(current_modules)}"
                            )
                    else:
                        logger.warning(
                            "‚ö†Ô∏è  Loading control LoRA weights but current config has control LoRA disabled"
                        )
                else:
                    logger.info(
                        "üì¶ Loading standard LoRA weights (no control LoRA metadata found)"
                    )
            else:
                logger.info("üì¶ Loading LoRA weights (no metadata available)")
        else:
            weights_sd = torch.load(file, map_location="cpu")
            logger.info("üì¶ Loading LoRA weights from torch format")

        info = self.load_state_dict(weights_sd, False)
        return info


def create_arch_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: Optional[List[nn.Module]],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    control_config: Optional[Dict] = None,
    **kwargs,
):
    """Create a control LoRA network with the specified configuration."""

    # add default exclude patterns
    exclude_patterns = kwargs.get("exclude_patterns", None)
    if exclude_patterns is None:
        exclude_patterns = []
    else:
        exclude_patterns = ast.literal_eval(exclude_patterns)

    # For control LoRA, we need to train patch_embedding, so exclude it from exclusion patterns
    if control_config and control_config.get("enabled", False):
        # Exclude everything except patch_embedding
        exclude_patterns.append(
            r".*(text_embedding|time_embedding|time_projection|norm|head).*"
        )
        logger.info("Control LoRA enabled: patch_embedding will be trained")

        # Validate that patch_embedding is not excluded
        patch_embedding_excluded = any(
            "patch_embedding" in pattern for pattern in exclude_patterns
        )
        if patch_embedding_excluded:
            logger.warning(
                "patch_embedding appears to be excluded from training! "
                "This will prevent control LoRA from working properly."
            )
        else:
            logger.info("patch_embedding confirmed to be included in training")
    else:
        # Standard exclusion including patch_embedding
        exclude_patterns.append(
            r".*(patch_embedding|text_embedding|time_embedding|time_projection|norm|head).*"
        )

    kwargs["exclude_patterns"] = exclude_patterns

    network = create_control_network(
        WAN_TARGET_REPLACE_MODULES,
        "control_lora_unet",
        multiplier,
        network_dim,
        network_alpha,
        vae,
        text_encoders or [],  # Convert None to empty list
        unet,
        neuron_dropout=neuron_dropout,
        control_config=control_config,
        **kwargs,
    )

    # After network creation, log what modules are actually being trained
    if control_config and control_config.get("enabled", False):
        logger.info("Control LoRA network created. Training modules:")
        for name, module in network.named_modules():
            if hasattr(module, "lora_name"):
                logger.info(f"  - {module.lora_name}")

    return network


def create_control_arch_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: Optional[List[nn.Module]],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    control_config: Optional[Dict] = None,
    **kwargs,
):
    """Create a control LoRA network with the specified configuration. (Legacy function name)"""
    return create_arch_network(
        multiplier,
        network_dim,
        network_alpha,
        vae,
        text_encoders,
        unet,
        neuron_dropout=neuron_dropout,
        control_config=control_config,
        **kwargs,
    )


def create_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: Optional[List[nn.Module]],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    control_config: Optional[Dict] = None,
    **kwargs,
):
    """Create a control LoRA network. (Fallback compatibility function)"""
    return create_arch_network(
        multiplier,
        network_dim,
        network_alpha,
        vae,
        text_encoders,
        unet,
        neuron_dropout=neuron_dropout,
        control_config=control_config,
        **kwargs,
    )


def create_control_network(
    target_replace_modules: List[str],
    prefix: str,
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: Optional[List[nn.Module]],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    control_config: Optional[Dict] = None,
    **kwargs,
):
    """Create a control LoRA network."""

    # Set defaults for None values
    if network_dim is None:
        network_dim = 4  # default
    if network_alpha is None:
        network_alpha = 1.0
    if text_encoders is None:
        text_encoders = []

    # Extract control-specific arguments
    conv_dim = kwargs.get("conv_dim", None)
    conv_alpha = kwargs.get("conv_alpha", None)
    rank_dropout = kwargs.get("rank_dropout", None)
    module_dropout = kwargs.get("module_dropout", None)
    exclude_patterns = kwargs.get("exclude_patterns", None)
    include_patterns = kwargs.get("include_patterns", None)
    verbose = kwargs.get("verbose", False)

    # too many arguments ( ^œâ^)ÔΩ•ÔΩ•ÔΩ•
    network = ControlLoRANetwork(
        target_replace_modules,
        prefix,
        text_encoders,  # type: ignore
        unet,  # type: ignore
        multiplier=multiplier,
        lora_dim=network_dim,
        alpha=network_alpha,
        dropout=neuron_dropout,
        rank_dropout=rank_dropout,
        module_dropout=module_dropout,
        conv_lora_dim=conv_dim,
        conv_alpha=conv_alpha,
        exclude_patterns=exclude_patterns,
        include_patterns=include_patterns,
        verbose=verbose,
        control_config=control_config,
    )

    loraplus_lr_ratio = kwargs.get("loraplus_lr_ratio", None)
    loraplus_lr_ratio = (
        float(loraplus_lr_ratio) if loraplus_lr_ratio is not None else None
    )
    if loraplus_lr_ratio is not None:
        network.set_loraplus_lr_ratio(loraplus_lr_ratio)

    return network


# Create network from weights for inference, weights are not loaded here (because can be merged)
def create_control_network_from_weights(
    target_replace_modules: List[str],
    multiplier: float,
    weights_sd: Dict[str, torch.Tensor],
    text_encoders: Optional[List[nn.Module]] = None,
    unet: Optional[nn.Module] = None,
    for_inference: bool = False,
    control_config: Optional[Dict] = None,
    **kwargs,
) -> ControlLoRANetwork:
    """Create a control LoRA network from weights."""

    # get dim/alpha mapping
    modules_dim = {}
    modules_alpha = {}
    for key, value in weights_sd.items():
        if "." not in key:
            continue

        lora_name = key.split(".")[0]
        if "alpha" in key:
            modules_alpha[lora_name] = value
        elif "lora_down" in key:
            dim = value.shape[0]
            modules_dim[lora_name] = dim

    module_class = ControlLoRAInfModule if for_inference else ControlLoRAModule

    network = ControlLoRANetwork(
        target_replace_modules,
        "control_lora_unet",
        text_encoders,  # type: ignore
        unet,  # type: ignore
        multiplier=multiplier,
        modules_dim=modules_dim,
        modules_alpha=modules_alpha,
        module_class=module_class,
        control_config=control_config,
    )
    return network
</file>

<file path="networks/controlnet_wan.py">
## Based on https://github.com/TheDenk/wan2.1-dilated-controlnet/blob/main/train/train_controlnet.py (Apache)

"""Modular ControlNet for WAN, integrated with Takenoko's network system.

This implementation provides a lightweight ControlNet that is compatible with
Takenoko's training pipeline. It is intentionally self-contained (no external
Diffusers dependency) and focuses on producing per-layer control states that
are injected into the main `WanModel` at configurable intervals.

Design notes:
- The network encodes the control hint (e.g., edges) via a small 3D CNN stack
  to downscale and aggregate spatiotemporal information.
- The control hint features are concatenated with the current noisy latents and
  patch-embedded into token space with the same inner dimension as the WAN
  transformer (num_heads * head_dim).
- A set of zero-initialized linear layers (one per injected layer) maps token
  embeddings to per-layer residual states that are added to the main model.

The result is an effective, zero-disturbance-at-init control signal path that
learns to steer generation during training.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn

from common.logger import get_logger

logger = get_logger(__name__)


def _zero_module(module: nn.Module) -> nn.Module:
    for p in module.parameters():
        nn.init.zeros_(p)
    return module


@dataclass
class ControlNetWanConfig:
    """Configuration for ControlNetWanNetwork.

    Attributes:
        patch_size: 3D patch size used by the main WAN model.
        num_attention_heads: Number of attention heads in WAN model.
        attention_head_dim: Dimension per attention head in WAN model.
        vae_channels: Number of channels in WAN latents (in_dim of WanModel).
        control_in_channels: Number of channels in the control hint (e.g., 1 or 3).
        num_layers: Number of per-layer control outputs to produce.
        downscale_coef: Spatial downscale factor for the control encoder.
        out_proj_dim: Dimension of per-token hidden state in WAN (heads * head_dim).
    """

    patch_size: Tuple[int, int, int]
    num_attention_heads: int
    attention_head_dim: int
    vae_channels: int
    control_in_channels: int = 3
    num_layers: int = 20
    downscale_coef: int = 8
    out_proj_dim: Optional[int] = None

    def resolve(self) -> None:
        if self.out_proj_dim is None:
            self.out_proj_dim = self.num_attention_heads * self.attention_head_dim


class ControlNetWanNetwork(nn.Module):
    """Lightweight ControlNet producing per-layer control states for WAN.

    Forward signature mirrors the reference shape expectations:
    - hidden_states: (B, vae_channels, F, H, W)
    - controlnet_states: (B, C_control, F, H, W)
    - encoder_hidden_states: (B, L_text, C_text) [currently unused, reserved]
    - timestep: (B,) int64 [currently unused, reserved]

    Returns a tuple(list) of per-layer token tensors:
      ( (B, L_tokens, out_proj_dim), ... ) with `num_layers` elements.
    """

    def __init__(self, cfg: ControlNetWanConfig) -> None:
        super().__init__()
        cfg.resolve()
        self.cfg = cfg

        start_channels = cfg.control_in_channels * (cfg.downscale_coef**2)
        channels = [start_channels, start_channels // 2, start_channels // 4]

        self.control_encoder = nn.ModuleList(
            [
                nn.Sequential(
                    nn.Conv3d(
                        cfg.control_in_channels,
                        channels[0],
                        kernel_size=(3, cfg.downscale_coef + 1, cfg.downscale_coef + 1),
                        stride=(1, cfg.downscale_coef, cfg.downscale_coef),
                        padding=(1, cfg.downscale_coef // 2, cfg.downscale_coef // 2),
                    ),
                    nn.GELU(approximate="tanh"),
                    nn.GroupNorm(2, channels[0]),
                ),
                nn.Sequential(
                    nn.Conv3d(
                        channels[0],
                        channels[1],
                        kernel_size=3,
                        stride=(2, 1, 1),
                        padding=1,
                    ),
                    nn.GELU(approximate="tanh"),
                    nn.GroupNorm(2, channels[1]),
                ),
                nn.Sequential(
                    nn.Conv3d(
                        channels[1],
                        channels[2],
                        kernel_size=3,
                        stride=(2, 1, 1),
                        padding=1,
                    ),
                    nn.GELU(approximate="tanh"),
                    nn.GroupNorm(2, channels[2]),
                ),
            ]
        )

        inner_dim = cfg.num_attention_heads * cfg.attention_head_dim
        self.patch_embedding = nn.Conv3d(
            cfg.vae_channels + channels[2],
            inner_dim,
            kernel_size=cfg.patch_size,
            stride=cfg.patch_size,
        )

        # One zero-initialized projection per injection layer
        self.controlnet_blocks = nn.ModuleList(
            [_zero_module(nn.Linear(inner_dim, cfg.out_proj_dim)) for _ in range(cfg.num_layers)]  # type: ignore[arg-type]
        )

        self.gradient_checkpointing: bool = False

    def enable_gradient_checkpointing(self, enabled: bool = True) -> None:
        self.gradient_checkpointing = enabled

    def forward(
        self,
        *,
        hidden_states: torch.Tensor,
        timestep: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        controlnet_states: torch.Tensor,
        return_dict: bool = False,
    ) -> Tuple[Tuple[torch.Tensor, ...]] | Tuple[torch.Tensor, ...]:
        # Encode control video
        x_control = controlnet_states
        for enc in self.control_encoder:
            x_control = enc(x_control)

        # Concatenate with latents and patch-embed to tokens
        x = torch.cat([hidden_states, x_control], dim=1)
        x = self.patch_embedding(x)  # (B, inner_dim, T', H', W')
        x = x.flatten(2).transpose(1, 2)  # (B, L_tokens, inner_dim)

        # Produce per-layer residuals
        outputs: List[torch.Tensor] = []
        for proj in self.controlnet_blocks:
            outputs.append(proj(x))  # (B, L_tokens, out_proj_dim)

        result: Tuple[torch.Tensor, ...] = tuple(outputs)
        if return_dict:
            # Keep parity with diffusers-like API shape without strict typing
            return (result,)
        return (result,)

    # Takenoko network API compatibility ------------------------------------
    def prepare_optimizer_params(
        self, unet_lr: float, **_: Any
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """Return optimizer param group for this ControlNet.

        Args:
            unet_lr: Base learning rate for WAN model; reused here.
        """
        trainable = [p for p in self.parameters() if p.requires_grad]
        if not trainable:
            return [], []
        return ([{"params": trainable, "lr": float(unet_lr)}], ["controlnet"])


def create_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: Optional[nn.Module],
    text_encoders: Optional[List[nn.Module]],
    unet: nn.Module,
    **kwargs: Any,
) -> ControlNetWanNetwork:
    """Factory for ControlNetWanNetwork matching Takenoko's create_network API.

    Unused args are kept for signature compatibility with other network modules.
    ControlNet-specific settings are expected in kwargs, e.g.:
      - controlnet_transformer_num_layers: int
      - downscale_coef: int
      - control_in_channels: int
    """
    # Extract WAN backbone properties
    num_heads: int = int(getattr(unet, "num_heads", 16))
    dim: int = int(getattr(unet, "dim", num_heads * 128))
    head_dim: int = dim // num_heads
    vae_channels: int = int(getattr(unet, "in_dim", 16))
    patch_size: Tuple[int, int, int] = tuple(getattr(unet, "patch_size", (1, 2, 2)))  # type: ignore[assignment]

    # ControlNet-specific params
    num_layers = int(kwargs.get("controlnet_transformer_num_layers", 20))
    downscale_coef = int(kwargs.get("downscale_coef", 8))
    control_in_channels = int(kwargs.get("control_in_channels", 3))
    out_proj_dim = int(kwargs.get("out_proj_dim", dim))

    cfg = ControlNetWanConfig(
        patch_size=patch_size,
        num_attention_heads=num_heads,
        attention_head_dim=head_dim,
        vae_channels=vae_channels,
        control_in_channels=control_in_channels,
        num_layers=num_layers,
        downscale_coef=downscale_coef,
        out_proj_dim=out_proj_dim,
    )

    net = ControlNetWanNetwork(cfg)
    logger.info(
        "Created ControlNetWanNetwork: heads=%d, head_dim=%d, layers=%d, vae_ch=%d, out_proj_dim=%d",
        num_heads,
        head_dim,
        num_layers,
        vae_channels,
        cfg.out_proj_dim or -1,
    )
    return net


# Optional alias for clarity when imported from model manager
def create_controlnet(*args: Any, **kwargs: Any) -> ControlNetWanNetwork:
    return create_network(*args, **kwargs)
</file>

<file path="networks/lora_wan.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/networks/lora.py (Apache)

# LoRA network module: currently conv2d is not fully supported
# reference:
# https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
# https://github.com/cloneofsimo/lora/blob/master/lora_diffusion/lora.py

import ast
import math
import os
import re
from typing import Dict, List, Optional, Type, Union, cast
from torch import Tensor
from transformers import CLIPTextModel
import numpy as np
import torch
import torch.nn as nn

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


WAN_TARGET_REPLACE_MODULES: list[str] = ["WanAttentionBlock"]


class LoRAModule(torch.nn.Module):
    """
    replaces forward method of the original Linear, instead of replacing the original Linear module.
    """

    def __init__(
        self,
        lora_name,
        org_module: torch.nn.Module,
        multiplier=1.0,
        lora_dim=4,
        alpha=1,
        dropout=None,
        rank_dropout=None,
        module_dropout=None,
        split_dims: Optional[List[int]] = None,
        # LoRA-GGPO parameters (optional)
        ggpo_sigma: Optional[float] = None,
        ggpo_beta: Optional[float] = None,
    ):
        """
        if alpha == 0 or None, alpha is rank (no scaling).

        split_dims is used to mimic the split qkv of multi-head attention.
        """
        super().__init__()
        self.lora_name = lora_name

        if org_module.__class__.__name__ == "Conv2d":
            in_dim = org_module.in_channels
            out_dim = org_module.out_channels
        else:
            in_dim = org_module.in_features
            out_dim = org_module.out_features

        self.lora_dim = lora_dim
        self.split_dims = split_dims

        if split_dims is None:
            if org_module.__class__.__name__ == "Conv2d":
                kernel_size = org_module.kernel_size
                stride = org_module.stride
                padding = org_module.padding
                self.lora_down = torch.nn.Conv2d(
                    in_dim, self.lora_dim, kernel_size, stride, padding, bias=False  # type: ignore
                )
                self.lora_up = torch.nn.Conv2d(
                    self.lora_dim, out_dim, (1, 1), (1, 1), bias=False  # type: ignore
                )
            else:
                self.lora_down = torch.nn.Linear(in_dim, self.lora_dim, bias=False)  # type: ignore
                self.lora_up = torch.nn.Linear(self.lora_dim, out_dim, bias=False)  # type: ignore

            torch.nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))
            torch.nn.init.zeros_(self.lora_up.weight)
        else:
            # conv2d not supported
            assert (
                sum(split_dims) == out_dim
            ), "sum of split_dims must be equal to out_dim"
            assert (
                org_module.__class__.__name__ == "Linear"
            ), "split_dims is only supported for Linear"
            # print(f"split_dims: {split_dims}")
            self.lora_down = torch.nn.ModuleList(
                [
                    torch.nn.Linear(in_dim, self.lora_dim, bias=False)  # type: ignore
                    for _ in range(len(split_dims))
                ]
            )
            self.lora_up = torch.nn.ModuleList(
                [
                    torch.nn.Linear(self.lora_dim, split_dim, bias=False)
                    for split_dim in split_dims
                ]
            )
            for lora_down in self.lora_down:
                torch.nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))  # type: ignore
            for lora_up in self.lora_up:
                torch.nn.init.zeros_(lora_up.weight)  # type: ignore

        if type(alpha) == torch.Tensor:
            alpha = alpha.detach().float().numpy()  # without casting, bf16 causes error
        alpha = self.lora_dim if alpha is None or alpha == 0 else alpha
        self.scale = alpha / self.lora_dim
        self.register_buffer("alpha", torch.tensor(alpha))  # for save/load

        # same as microsoft's
        self.multiplier = multiplier
        self.org_module = org_module  # remove in applying
        self.dropout = dropout
        self.rank_dropout = rank_dropout
        self.module_dropout = module_dropout

        # === GGPO setup ===
        self.ggpo_sigma: Optional[float] = (
            float(ggpo_sigma) if ggpo_sigma is not None else None
        )
        self.ggpo_beta: Optional[float] = (
            float(ggpo_beta) if ggpo_beta is not None else None
        )
        # Only enable GGPO for Linear layers where shapes are well-defined
        self._ggpo_enabled: bool = (
            self.ggpo_sigma is not None
            and self.ggpo_beta is not None
            and org_module.__class__.__name__ == "Linear"
        )
        self._org_module_shape: Optional[torch.Size] = None
        self._perturbation_norm_factor: Optional[float] = None
        self._org_weight_row_norm_estimate: Optional[Tensor] = None
        self.combined_weight_norms: Optional[Tensor] = None
        self.grad_norms: Optional[Tensor] = None

        if self._ggpo_enabled:
            try:
                self._org_module_shape = org_module.weight.shape  # type: ignore[attr-defined]
                # Normalization by sqrt(num_rows)
                if self._org_module_shape is None:
                    raise RuntimeError(
                        "Missing org module shape for GGPO initialization"
                    )
                rows0 = int(self._org_module_shape[0])
                self._perturbation_norm_factor = 1.0 / math.sqrt(float(rows0))
                self._initialize_org_weight_norm_estimate(org_module.weight)  # type: ignore[arg-type]
            except Exception:
                # Fail-closed if any unexpected module structure
                self._ggpo_enabled = False

    def apply_to(self):
        self.org_forward = self.org_module.forward
        self.org_module.forward = self.forward
        del self.org_module

    @property
    def device(self) -> torch.device:
        return next(self.parameters()).device

    @property
    def dtype(self) -> torch.dtype:
        return next(self.parameters()).dtype

    @torch.no_grad()
    def _initialize_org_weight_norm_estimate(self, org_weight: Tensor) -> None:
        """Estimate average per-row L2 norm of the frozen base weight (Linear only).

        Uses a random subset of rows to avoid scanning large matrices every step.
        """
        try:
            num_rows: int = int(org_weight.shape[0])
            sample_size: int = min(1024, num_rows)
            if sample_size <= 0:
                return
            row_indices = torch.randperm(num_rows, device=org_weight.device)[
                :sample_size
            ]
            sampled_rows = org_weight.index_select(0, row_indices).to(
                dtype=torch.float32
            )
            # Flatten across input dimension for per-row L2 norm
            sampled_norms = torch.linalg.vector_norm(
                sampled_rows.flatten(1), ord=2, dim=1
            )
            estimate = sampled_norms.mean()
            # Keep as 1-element tensor on module device for broadcasting
            self._org_weight_row_norm_estimate = estimate.detach().to(
                device=self.device, dtype=torch.float32
            )
        except Exception:
            # Non-fatal: disable GGPO if estimation fails
            self._ggpo_enabled = False

    def forward(self, x):
        org_forwarded = self.org_forward(x)

        # module dropout
        if self.module_dropout is not None and self.training:
            if torch.rand(1) < self.module_dropout:
                return org_forwarded

        if self.split_dims is None:
            lx = self.lora_down(x)

            # normal dropout
            if self.dropout is not None and self.training:
                lx = torch.nn.functional.dropout(lx, p=self.dropout)

            # rank dropout
            if self.rank_dropout is not None and self.training:
                mask = (
                    torch.rand((lx.size(0), self.lora_dim), device=lx.device)
                    > self.rank_dropout
                )
                if len(lx.size()) == 3:
                    mask = mask.unsqueeze(1)  # for Text Encoder
                elif len(lx.size()) == 4:
                    mask = mask.unsqueeze(-1).unsqueeze(-1)  # for Conv2d
                lx = lx * mask

                # scaling for rank dropout: treat as if the rank is changed
                scale = self.scale * (
                    1.0 / (1.0 - self.rank_dropout)
                )  # redundant for readability
            else:
                scale = self.scale

            lx = self.lora_up(lx)

            # GGPO perturbation (Linear only), applied in training
            if (
                self.training
                and self._ggpo_enabled
                and self._org_module_shape is not None
                and self._perturbation_norm_factor is not None
                and self.combined_weight_norms is not None
                and self.grad_norms is not None
            ):
                try:
                    with torch.no_grad():
                        # per-row scale: sigma*||W'|| + beta*||grad||
                        sigma = cast(float, self.ggpo_sigma)
                        beta = cast(float, self.ggpo_beta)
                        perturbation_scale: Tensor = (
                            sigma * self.combined_weight_norms + beta * self.grad_norms
                        ).to(device=self.device, dtype=torch.float32)
                        # normalize by sqrt(num_rows)
                        norm_factor = cast(float, self._perturbation_norm_factor)
                        perturbation_scale = perturbation_scale * norm_factor
                        # Random Gaussian perturbation with per-row scaling
                        shape = cast(torch.Size, self._org_module_shape)
                        pert = torch.randn(
                            shape,
                            device=self.device,
                            dtype=self.dtype,
                        )
                        # Broadcast per-row factor to full weight shape (out, in)
                        pert = pert * perturbation_scale.to(self.dtype)
                    # Apply linear with perturbation as weight
                    perturbation_out = torch.nn.functional.linear(x, pert)
                    return (
                        org_forwarded + lx * self.multiplier * scale + perturbation_out
                    )
                except Exception:
                    # On any failure, fall back to standard forward
                    return org_forwarded + lx * self.multiplier * scale
            else:
                return org_forwarded + lx * self.multiplier * scale
        else:
            lxs = [lora_down(x) for lora_down in self.lora_down]  # type: ignore

            # normal dropout
            if self.dropout is not None and self.training:
                lxs = [torch.nn.functional.dropout(lx, p=self.dropout) for lx in lxs]

            # rank dropout
            if self.rank_dropout is not None and self.training:
                masks = [
                    torch.rand((lx.size(0), self.lora_dim), device=lx.device)
                    > self.rank_dropout
                    for lx in lxs
                ]
                for i in range(len(lxs)):
                    if len(lx.size()) == 3:  # type: ignore
                        masks[i] = masks[i].unsqueeze(1)
                    elif len(lx.size()) == 4:  # type: ignore
                        masks[i] = masks[i].unsqueeze(-1).unsqueeze(-1)
                    lxs[i] = lxs[i] * masks[i]

                # scaling for rank dropout: treat as if the rank is changed
                scale = self.scale * (
                    1.0 / (1.0 - self.rank_dropout)
                )  # redundant for readability
            else:
                scale = self.scale

            lxs = [lora_up(lx) for lora_up, lx in zip(self.lora_up, lxs)]  # type: ignore

            return org_forwarded + torch.cat(lxs, dim=-1) * self.multiplier * scale


class LoRAInfModule(LoRAModule):
    def __init__(
        self,
        lora_name,
        org_module: torch.nn.Module,
        multiplier=1.0,
        lora_dim=4,
        alpha=1,
        **kwargs,
    ):
        # no dropout for inference
        super().__init__(lora_name, org_module, multiplier, lora_dim, alpha)

        self.org_module_ref = [org_module]  # for reference
        self.enabled = True
        self.network: LoRANetwork = None  # type: ignore

    def set_network(self, network):
        self.network = network

    # merge weight to org_module
    # def merge_to(self, sd, dtype, device, non_blocking=False):
    #     if torch.cuda.is_available():
    #         stream = torch.cuda.Stream(device=device)
    #         with torch.cuda.stream(stream):
    #             print(f"merge_to {self.lora_name}")
    #             self._merge_to(sd, dtype, device, non_blocking)
    #             torch.cuda.synchronize(device=device)
    #             print(f"merge_to {self.lora_name} done")
    #         torch.cuda.empty_cache()
    #     else:
    #         self._merge_to(sd, dtype, device, non_blocking)

    def merge_to(self, sd, dtype, device, non_blocking=False):
        # extract weight from org_module
        org_sd = self.org_module.state_dict()
        weight = org_sd["weight"]
        org_dtype = weight.dtype
        org_device = weight.device
        weight = weight.to(
            device, dtype=torch.float, non_blocking=non_blocking
        )  # for calculation

        if dtype is None:
            dtype = org_dtype
        if device is None:
            device = org_device

        if self.split_dims is None:
            # get up/down weight
            down_weight = sd["lora_down.weight"].to(
                device, dtype=torch.float, non_blocking=non_blocking
            )
            up_weight = sd["lora_up.weight"].to(
                device, dtype=torch.float, non_blocking=non_blocking
            )

            # merge weight
            if len(weight.size()) == 2:
                # linear
                weight = (
                    weight + self.multiplier * (up_weight @ down_weight) * self.scale
                )
            elif down_weight.size()[2:4] == (1, 1):
                # conv2d 1x1
                weight = (
                    weight
                    + self.multiplier
                    * (
                        up_weight.squeeze(3).squeeze(2)
                        @ down_weight.squeeze(3).squeeze(2)
                    )
                    .unsqueeze(2)
                    .unsqueeze(3)
                    * self.scale
                )
            else:
                # conv2d 3x3
                conved = torch.nn.functional.conv2d(
                    down_weight.permute(1, 0, 2, 3), up_weight
                ).permute(1, 0, 2, 3)
                # logger.info(conved.size(), weight.size(), module.stride, module.padding)
                weight = weight + self.multiplier * conved * self.scale

            # set weight to org_module
            org_sd["weight"] = weight.to(
                org_device, dtype=dtype
            )  # back to CPU without non_blocking
            self.org_module.load_state_dict(org_sd)
        else:
            # split_dims
            total_dims = sum(self.split_dims)
            for i in range(len(self.split_dims)):
                # get up/down weight
                down_weight = sd[f"lora_down.{i}.weight"].to(
                    device, torch.float, non_blocking=non_blocking
                )  # (rank, in_dim)
                up_weight = sd[f"lora_up.{i}.weight"].to(
                    device, torch.float, non_blocking=non_blocking
                )  # (split dim, rank)

                # pad up_weight -> (total_dims, rank)
                padded_up_weight = torch.zeros(
                    (total_dims, up_weight.size(0)), device=device, dtype=torch.float
                )
                padded_up_weight[
                    sum(self.split_dims[:i]) : sum(self.split_dims[: i + 1])
                ] = up_weight

                # merge weight
                weight = (
                    weight + self.multiplier * (up_weight @ down_weight) * self.scale
                )

            # set weight to org_module
            org_sd["weight"] = weight.to(
                org_device, dtype
            )  # back to CPU without non_blocking
            self.org_module.load_state_dict(org_sd)

    # return weight for merge
    def get_weight(self, multiplier=None):
        if multiplier is None:
            multiplier = self.multiplier

        # get up/down weight from module
        up_weight = self.lora_up.weight.to(torch.float)
        down_weight = self.lora_down.weight.to(torch.float)

        # pre-calculated weight
        if len(down_weight.size()) == 2:  # type: ignore
            # linear
            weight = self.multiplier * (up_weight @ down_weight) * self.scale  # type: ignore
        elif down_weight.size()[2:4] == (1, 1):  # type: ignore
            # conv2d 1x1
            weight = (
                self.multiplier
                * (up_weight.squeeze(3).squeeze(2) @ down_weight.squeeze(3).squeeze(2))  # type: ignore
                .unsqueeze(2)
                .unsqueeze(3)
                * self.scale
            )
        else:
            # conv2d 3x3
            conved = torch.nn.functional.conv2d(
                down_weight.permute(1, 0, 2, 3), up_weight  # type: ignore
            ).permute(
                1, 0, 2, 3
            )  # type: ignore
            weight = self.multiplier * conved * self.scale

        return weight

    def default_forward(self, x):
        # logger.info(f"default_forward {self.lora_name} {x.size()}")
        if self.split_dims is None:
            lx = self.lora_down(x)
            lx = self.lora_up(lx)
            return self.org_forward(x) + lx * self.multiplier * self.scale
        else:
            lxs = [lora_down(x) for lora_down in self.lora_down]  # type: ignore
            lxs = [lora_up(lx) for lora_up, lx in zip(self.lora_up, lxs)]  # type: ignore
            return (
                self.org_forward(x)
                + torch.cat(lxs, dim=-1) * self.multiplier * self.scale
            )

    def forward(self, x):
        if not self.enabled:
            return self.org_forward(x)
        return self.default_forward(x)


def create_arch_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: List[nn.Module],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    **kwargs,
):
    # add default exclude patterns
    exclude_patterns = kwargs.get("exclude_patterns", None)
    if exclude_patterns is None:
        exclude_patterns = []
    else:
        exclude_patterns = ast.literal_eval(exclude_patterns)

    # exclude if 'img_mod', 'txt_mod' or 'modulation' in the name
    exclude_patterns.append(
        r".*(patch_embedding|text_embedding|time_embedding|time_projection|norm|head).*"
    )

    kwargs["exclude_patterns"] = exclude_patterns

    return create_network(
        WAN_TARGET_REPLACE_MODULES,
        "lora_unet",
        multiplier,
        network_dim,
        network_alpha,
        vae,
        text_encoders,
        unet,
        neuron_dropout=neuron_dropout,
        **kwargs,
    )


def create_network(
    target_replace_modules: List[str],
    prefix: str,
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: nn.Module,
    text_encoders: List[nn.Module],
    unet: nn.Module,
    neuron_dropout: Optional[float] = None,
    **kwargs,
):
    if network_dim is None:
        network_dim = 4  # default
    if network_alpha is None:
        network_alpha = 1.0

    # extract dim/alpha for conv2d, and block dim
    conv_dim = kwargs.get("conv_dim", None)
    conv_alpha = kwargs.get("conv_alpha", None)
    if conv_dim is not None:
        conv_dim = int(conv_dim)
        if conv_alpha is None:
            conv_alpha = 1.0
        else:
            conv_alpha = float(conv_alpha)

    # TODO generic rank/dim setting with regular expression

    # rank/module dropout
    rank_dropout = kwargs.get("rank_dropout", None)
    if rank_dropout is not None:
        rank_dropout = float(rank_dropout)
    module_dropout = kwargs.get("module_dropout", None)
    if module_dropout is not None:
        module_dropout = float(module_dropout)

    # verbose
    verbose = kwargs.get("verbose", False)
    if verbose is not None:
        verbose = True if verbose == "True" else False

    # regular expression for module selection: exclude and include
    exclude_patterns = kwargs.get("exclude_patterns", None)
    if exclude_patterns is not None and isinstance(exclude_patterns, str):
        exclude_patterns = ast.literal_eval(exclude_patterns)
    include_patterns = kwargs.get("include_patterns", None)
    if include_patterns is not None and isinstance(include_patterns, str):
        include_patterns = ast.literal_eval(include_patterns)

    # Parse GGPO parameters (may arrive as strings via network_args)
    ggpo_sigma = kwargs.get("ggpo_sigma", None)
    ggpo_beta = kwargs.get("ggpo_beta", None)
    try:
        ggpo_sigma = float(ggpo_sigma) if ggpo_sigma is not None else None
    except Exception:
        ggpo_sigma = None
    try:
        ggpo_beta = float(ggpo_beta) if ggpo_beta is not None else None
    except Exception:
        ggpo_beta = None

    network = LoRANetwork(
        target_replace_modules,
        prefix,
        text_encoders,  # type: ignore
        unet,
        multiplier=multiplier,
        lora_dim=network_dim,
        alpha=network_alpha,
        dropout=neuron_dropout,
        rank_dropout=rank_dropout,
        module_dropout=module_dropout,
        conv_lora_dim=conv_dim,
        conv_alpha=conv_alpha,
        exclude_patterns=exclude_patterns,
        include_patterns=include_patterns,
        verbose=verbose,
        ggpo_sigma=cast(Optional[float], ggpo_sigma),
        ggpo_beta=cast(Optional[float], ggpo_beta),
    )

    loraplus_lr_ratio = kwargs.get("loraplus_lr_ratio", None)
    # loraplus_unet_lr_ratio = kwargs.get("loraplus_unet_lr_ratio", None)
    # loraplus_text_encoder_lr_ratio = kwargs.get("loraplus_text_encoder_lr_ratio", None)
    loraplus_lr_ratio = (
        float(loraplus_lr_ratio) if loraplus_lr_ratio is not None else None
    )
    # loraplus_unet_lr_ratio = float(loraplus_unet_lr_ratio) if loraplus_unet_lr_ratio is not None else None
    # loraplus_text_encoder_lr_ratio = float(loraplus_text_encoder_lr_ratio) if loraplus_text_encoder_lr_ratio is not None else None
    if (
        loraplus_lr_ratio is not None
    ):  # or loraplus_unet_lr_ratio is not None or loraplus_text_encoder_lr_ratio is not None:
        network.set_loraplus_lr_ratio(
            loraplus_lr_ratio
        )  # , loraplus_unet_lr_ratio, loraplus_text_encoder_lr_ratio)

    return network


class LoRANetwork(torch.nn.Module):
    # only supports U-Net (DiT), Text Encoders are not supported

    def __init__(
        self,
        target_replace_modules: List[str],
        prefix: str,
        text_encoders: Union[List[CLIPTextModel], CLIPTextModel],
        unet: nn.Module,
        multiplier: float = 1.0,
        lora_dim: int = 4,
        alpha: float = 1,
        dropout: Optional[float] = None,
        rank_dropout: Optional[float] = None,
        module_dropout: Optional[float] = None,
        conv_lora_dim: Optional[int] = None,
        conv_alpha: Optional[float] = None,
        module_class: Type[object] = LoRAModule,
        modules_dim: Optional[Dict[str, int]] = None,
        modules_alpha: Optional[Dict[str, int]] = None,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        verbose: Optional[bool] = False,
        # LoRA-GGPO parameters
        ggpo_sigma: Optional[float] = None,
        ggpo_beta: Optional[float] = None,
    ) -> None:
        super().__init__()
        self.multiplier = multiplier

        self.lora_dim = lora_dim
        self.alpha = alpha
        self.conv_lora_dim = conv_lora_dim
        self.conv_alpha = conv_alpha
        self.dropout = dropout
        self.rank_dropout = rank_dropout
        self.module_dropout = module_dropout
        self.target_replace_modules = target_replace_modules
        self.prefix = prefix

        # Store GGPO params to pass into modules
        self.ggpo_sigma: Optional[float] = (
            float(ggpo_sigma) if ggpo_sigma is not None else None
        )
        self.ggpo_beta: Optional[float] = (
            float(ggpo_beta) if ggpo_beta is not None else None
        )

        self.loraplus_lr_ratio = None
        # self.loraplus_unet_lr_ratio = None
        # self.loraplus_text_encoder_lr_ratio = None

        if modules_dim is not None:
            logger.info(f"create LoRA network from weights")
        else:
            logger.info(
                f"create LoRA network. base dim (rank): {lora_dim}, alpha: {alpha}"
            )
            logger.info(
                f"neuron dropout: p={self.dropout}, rank dropout: p={self.rank_dropout}, module dropout: p={self.module_dropout}"
            )
            # if self.conv_lora_dim is not None:
            #     logger.info(
            #         f"apply LoRA to Conv2d with kernel size (3,3). dim (rank): {self.conv_lora_dim}, alpha: {self.conv_alpha}"
            #     )
        # if train_t5xxl:
        #     logger.info(f"train T5XXL as well")

        # compile regular expression if specified
        exclude_re_patterns = []
        if exclude_patterns is not None:
            for pattern in exclude_patterns:
                try:
                    re_pattern = re.compile(pattern)
                except re.error as e:
                    logger.error(f"Invalid exclude pattern '{pattern}': {e}")
                    continue
                exclude_re_patterns.append(re_pattern)

        include_re_patterns = []
        if include_patterns is not None:
            for pattern in include_patterns:
                try:
                    re_pattern = re.compile(pattern)
                except re.error as e:
                    logger.error(f"Invalid include pattern '{pattern}': {e}")
                    continue
                include_re_patterns.append(re_pattern)

        # create module instances
        def create_modules(
            is_unet: bool,
            pfx: str,
            root_module: torch.nn.Module,
            target_replace_mods: Optional[List[str]] = None,
            filter: Optional[str] = None,
            default_dim: Optional[int] = None,
        ) -> List[LoRAModule]:
            loras = []
            skipped = []
            for name, module in root_module.named_modules():
                if (
                    target_replace_mods is None
                    or module.__class__.__name__ in target_replace_mods
                ):
                    if target_replace_mods is None:  # dirty hack for all modules
                        module = root_module  # search all modules

                    for child_name, child_module in module.named_modules():
                        is_linear = child_module.__class__.__name__ == "Linear"
                        is_conv2d = child_module.__class__.__name__ == "Conv2d"
                        is_conv2d_1x1 = is_conv2d and child_module.kernel_size == (1, 1)

                        if is_linear or is_conv2d:
                            original_name = (name + "." if name else "") + child_name
                            lora_name = f"{pfx}.{original_name}".replace(".", "_")

                            # exclude/include filter
                            excluded = False
                            for pattern in exclude_re_patterns:
                                if pattern.match(original_name):
                                    excluded = True
                                    break
                            included = False
                            for pattern in include_re_patterns:
                                if pattern.match(original_name):
                                    included = True
                                    break
                            if excluded and not included:
                                if verbose:
                                    logger.info(f"exclude: {original_name}")
                                continue

                            # filter by name (not used in the current implementation)
                            if filter is not None and not filter in lora_name:
                                continue

                            dim = None
                            alpha = None

                            if modules_dim is not None:
                                # Module specification exists
                                if lora_name in modules_dim:
                                    dim = modules_dim[lora_name]
                                    alpha = modules_alpha[lora_name]  # type: ignore
                            else:
                                # Normally, target all
                                if is_linear or is_conv2d_1x1:
                                    dim = (
                                        default_dim
                                        if default_dim is not None
                                        else self.lora_dim
                                    )
                                    alpha = self.alpha
                                elif self.conv_lora_dim is not None:
                                    dim = self.conv_lora_dim
                                    alpha = self.conv_alpha

                            if dim is None or dim == 0:
                                # Output skipped information
                                if (
                                    is_linear
                                    or is_conv2d_1x1
                                    or (self.conv_lora_dim is not None)
                                ):
                                    skipped.append(lora_name)
                                continue

                            lora = module_class(
                                lora_name,  # type: ignore
                                child_module,
                                self.multiplier,
                                dim,
                                alpha,
                                dropout=dropout,
                                rank_dropout=rank_dropout,
                                module_dropout=module_dropout,
                                ggpo_sigma=self.ggpo_sigma,
                                ggpo_beta=self.ggpo_beta,
                            )
                            loras.append(lora)

                if target_replace_mods is None:
                    break  # all modules are searched
            return loras, skipped  # type: ignore

        # # create LoRA for text encoder
        # # it is redundant to create LoRA modules even if they are not used

        self.text_encoder_loras: List[Union[LoRAModule, LoRAInfModule]] = []
        # skipped_te = []
        # for i, text_encoder in enumerate(text_encoders):
        #     index = i
        #     if not train_t5xxl and index > 0:  # 0: CLIP, 1: T5XXL, so we skip T5XXL if train_t5xxl is False
        #         break
        #     logger.info(f"create LoRA for Text Encoder {index+1}:")
        #     text_encoder_loras, skipped = create_modules(False, index, text_encoder, LoRANetwork.TEXT_ENCODER_TARGET_REPLACE_MODULE)
        #     logger.info(f"create LoRA for Text Encoder {index+1}: {len(text_encoder_loras)} modules.")
        #     self.text_encoder_loras.extend(text_encoder_loras)
        #     skipped_te += skipped

        # create LoRA for U-Net
        self.unet_loras: List[Union[LoRAModule, LoRAInfModule]]
        self.unet_loras, skipped_un = create_modules(  # type: ignore
            True, prefix, unet, target_replace_modules
        )

        logger.info(f"create LoRA for U-Net/DiT: {len(self.unet_loras)} modules.")
        if verbose:
            for lora in self.unet_loras:
                logger.info(f"\t{lora.lora_name:50} {lora.lora_dim}, {lora.alpha}")

        skipped = skipped_un
        if verbose and len(skipped) > 0:  # type: ignore
            logger.warning(
                f"because dim (rank) is 0, {len(skipped)} LoRA modules are skipped"  # type: ignore
            )
            for name in skipped:  # type: ignore
                logger.info(f"\t{name}")

        # assertion
        names = set()
        for lora in self.text_encoder_loras + self.unet_loras:
            assert (
                lora.lora_name not in names
            ), f"duplicated lora name: {lora.lora_name}"
            names.add(lora.lora_name)

    def prepare_network(self, args):
        """
        called after the network is created
        """
        pass

    def set_multiplier(self, multiplier):
        self.multiplier = multiplier
        for lora in self.text_encoder_loras + self.unet_loras:
            lora.multiplier = self.multiplier

    def set_enabled(self, is_enabled):
        for lora in self.text_encoder_loras + self.unet_loras:
            lora.enabled = is_enabled

    def load_weights(self, file):
        if os.path.splitext(file)[1] == ".safetensors":
            from safetensors.torch import load_file

            weights_sd = load_file(file)
        else:
            weights_sd = torch.load(file, map_location="cpu")

        info = self.load_state_dict(weights_sd, False)
        return info

    def apply_to(
        self,
        text_encoders: Optional[nn.Module],
        unet: Optional[nn.Module],
        apply_text_encoder: bool = True,
        apply_unet: bool = True,
    ):
        if apply_text_encoder:
            logger.info(
                f"enable LoRA for text encoder: {len(self.text_encoder_loras)} modules"
            )
        else:
            self.text_encoder_loras = []

        if apply_unet:
            logger.info(f"enable LoRA for U-Net: {len(self.unet_loras)} modules")
        else:
            self.unet_loras = []

        for lora in self.text_encoder_loras + self.unet_loras:
            lora.apply_to()
            self.add_module(lora.lora_name, lora)

    def is_mergeable(self):
        return True

    def merge_to(
        self,
        text_encoders,
        unet,
        weights_sd,
        dtype=None,
        device=None,
        non_blocking=False,
    ):
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=2) as executor:  # 2 workers is enough
            futures = []
            for lora in self.text_encoder_loras + self.unet_loras:
                sd_for_lora = {}
                for key in weights_sd.keys():
                    if key.startswith(lora.lora_name):
                        sd_for_lora[key[len(lora.lora_name) + 1 :]] = weights_sd[key]
                if len(sd_for_lora) == 0:
                    logger.info(f"no weight for {lora.lora_name}")
                    continue

                # lora.merge_to(sd_for_lora, dtype, device)
                futures.append(
                    executor.submit(
                        lora.merge_to, sd_for_lora, dtype, device, non_blocking  # type: ignore
                    )
                )

        for future in futures:
            future.result()

        logger.info(f"weights are merged")

    def set_loraplus_lr_ratio(
        self, loraplus_lr_ratio
    ):  # , loraplus_unet_lr_ratio, loraplus_text_encoder_lr_ratio):
        self.loraplus_lr_ratio = loraplus_lr_ratio

        logger.info(f"LoRA+ UNet LR Ratio: {self.loraplus_lr_ratio}")
        # logger.info(f"LoRA+ Text Encoder LR Ratio: {self.loraplus_text_encoder_lr_ratio or self.loraplus_lr_ratio}")

    def prepare_optimizer_params(
        self, unet_lr: float = 1e-4, input_lr_scale: float = 1.0, **kwargs
    ):
        self.requires_grad_(True)

        all_params = []
        lr_descriptions = []

        def assemble_params(loras, lr, loraplus_ratio, input_lr_scale):
            param_groups = {"lora": {}, "plus": {}, "patch_embedding": {}}
            for lora in loras:
                for name, param in lora.named_parameters():
                    if "patch_embedding" in name:
                        param_groups["patch_embedding"][
                            f"{lora.lora_name}.{name}"
                        ] = param
                    elif loraplus_ratio is not None and "lora_up" in name:
                        param_groups["plus"][f"{lora.lora_name}.{name}"] = param
                    else:
                        param_groups["lora"][f"{lora.lora_name}.{name}"] = param

            params = []
            descriptions = []
            for key in param_groups.keys():
                param_data = {"params": param_groups[key].values()}

                if len(param_data["params"]) == 0:
                    continue

                if lr is not None:
                    if key == "plus":
                        param_data["lr"] = lr * loraplus_ratio
                    elif key == "patch_embedding":
                        param_data["lr"] = lr * input_lr_scale
                    else:
                        param_data["lr"] = lr

                if (
                    param_data.get("lr", None) == 0
                    or param_data.get("lr", None) is None
                ):
                    logger.info("NO LR skipping!")
                    continue

                params.append(param_data)
                descriptions.append(key if key != "lora" else "")

            return params, descriptions

        if self.unet_loras:
            params, descriptions = assemble_params(
                self.unet_loras, unet_lr, self.loraplus_lr_ratio, input_lr_scale
            )
            all_params.extend(params)
            lr_descriptions.extend(
                ["unet" + (" " + d if d else "") for d in descriptions]
            )

        return all_params, lr_descriptions

    def enable_gradient_checkpointing(self):
        # not supported
        pass

    def prepare_grad_etc(self, unet):
        self.requires_grad_(True)

    def on_epoch_start(self, unet):
        self.train()

    def on_step_start(self):
        pass

    def get_trainable_params(self):
        return self.parameters()

    # ===== GGPO Orchestration =====
    @torch.no_grad()
    def update_norms(self) -> None:
        for lora in getattr(self, "text_encoder_loras", []) + getattr(self, "unet_loras", []):  # type: ignore
            if hasattr(lora, "update_norms"):
                lora.update_norms()

    @torch.no_grad()
    def update_grad_norms(self) -> None:
        for lora in getattr(self, "text_encoder_loras", []) + getattr(self, "unet_loras", []):  # type: ignore
            if hasattr(lora, "update_grad_norms"):
                lora.update_grad_norms()

    @torch.no_grad()
    def grad_norms(self) -> Optional[Tensor]:
        values: List[Tensor] = []
        for lora in getattr(self, "text_encoder_loras", []) + getattr(self, "unet_loras", []):  # type: ignore
            val = getattr(lora, "grad_norms", None)
            if isinstance(val, Tensor):
                values.append(val)
        if not values:
            return None
        return torch.cat(values, dim=0).mean()

    @torch.no_grad()
    def combined_weight_norms(self) -> Optional[Tensor]:
        values: List[Tensor] = []
        for lora in getattr(self, "text_encoder_loras", []) + getattr(self, "unet_loras", []):  # type: ignore
            val = getattr(lora, "combined_weight_norms", None)
            if isinstance(val, Tensor):
                values.append(val)
        if not values:
            return None
        return torch.cat(values, dim=0).mean()

    def load_state_dict(self, state_dict, strict=True):
        """
        Custom load_state_dict that handles missing keys gracefully.
        This is needed because the network structure might change between
        saving and loading, especially when resuming training.
        """
        # Get the current state dict keys
        current_keys = set(self.state_dict().keys())
        saved_keys = set(state_dict.keys())

        # Analyze the key differences
        missing_from_saved = current_keys - saved_keys
        missing_from_current = saved_keys - current_keys
        matching_keys = current_keys & saved_keys

        # Log detailed information about the key differences
        logger.info(f"üîç LoRA state dict analysis:")
        logger.info(f"   Current network has {len(current_keys)} keys")
        logger.info(f"   Saved state has {len(saved_keys)} keys")
        logger.info(f"   Matching keys: {len(matching_keys)}")
        logger.info(f"   Missing from saved: {len(missing_from_saved)}")
        logger.info(f"   Missing from current: {len(missing_from_current)}")

        # Filter the input state dict to only include keys that exist in the current network
        filtered_state_dict = {}
        ignored_keys = []

        for key, value in state_dict.items():
            if key in current_keys:
                filtered_state_dict[key] = value
            else:
                ignored_keys.append(key)

        # Log information about the filtering
        if ignored_keys:
            logger.warning(
                f"LoRANetwork: {len(ignored_keys)} keys from saved state "
                f"are not present in current network structure and will be ignored."
            )

            # Show first few ignored keys for debugging
            sample_ignored = ignored_keys[:10]
            logger.info(f"   Sample ignored keys: {sample_ignored}")

        # Check if we have a reasonable number of matching keys
        if len(filtered_state_dict) == 0:
            logger.error(
                "‚ùå No matching keys found between saved state and current network!"
            )
            logger.error("This indicates a fundamental mismatch in network structure.")
            return super().load_state_dict({}, strict=False)

        match_ratio = len(filtered_state_dict) / len(current_keys)
        logger.info(
            f"‚úÖ Loading {len(filtered_state_dict)} keys (match ratio: {match_ratio:.2%})"
        )

        if match_ratio < 0.5:
            logger.warning(
                f"‚ö†Ô∏è  Low match ratio ({match_ratio:.2%}). This may indicate significant "
                f"network structure changes between save and load."
            )

        # Call parent's load_state_dict with filtered state dict
        result = super().load_state_dict(filtered_state_dict, strict=False)

        # Log the result
        if result.missing_keys:
            logger.info(
                f"üìã Final result: {len(result.missing_keys)} keys still missing from current network"
            )

        if result.unexpected_keys:
            logger.info(
                f"üìã Final result: {len(result.unexpected_keys)} unexpected keys"
            )

        # Final success/failure assessment
        total_expected = len(current_keys)
        total_loaded = len(filtered_state_dict) - len(result.missing_keys)
        load_success_ratio = total_loaded / total_expected if total_expected > 0 else 0

        logger.info(
            f"üéØ LoRA load success: {total_loaded}/{total_expected} keys ({load_success_ratio:.2%})"
        )

        if load_success_ratio >= 0.8:
            logger.info("‚úÖ LoRA state loading successful!")
        elif load_success_ratio >= 0.5:
            logger.warning("‚ö†Ô∏è  Partial LoRA state loading - some keys missing")
        else:
            logger.error("‚ùå LoRA state loading largely failed - most keys missing")

        return result

    def save_weights(self, file, dtype, metadata):
        if metadata is not None and len(metadata) == 0:
            metadata = None

        state_dict = self.state_dict()

        if dtype is not None:
            for key in list(state_dict.keys()):
                v = state_dict[key]
                v = v.detach().clone().to("cpu").to(dtype)
                state_dict[key] = v

        if os.path.splitext(file)[1] == ".safetensors":
            from safetensors.torch import save_file
            from utils import model_utils

            # Precalculate model hashes to save time on indexing
            if metadata is None:
                metadata = {}
            model_hash, legacy_hash = model_utils.precalculate_safetensors_hashes(
                state_dict, metadata
            )
            metadata["sshs_model_hash"] = model_hash
            metadata["sshs_legacy_hash"] = legacy_hash

            save_file(state_dict, file, metadata)
        else:
            torch.save(state_dict, file)

    def backup_weights(self):
        loras: List[LoRAInfModule] = self.text_encoder_loras + self.unet_loras  # type: ignore
        for lora in loras:
            org_module = lora.org_module_ref[0]
            if not hasattr(org_module, "_lora_org_weight"):
                sd = org_module.state_dict()
                org_module._lora_org_weight = sd["weight"].detach().clone()
                org_module._lora_restored = True  # type: ignore

    def restore_weights(self):
        loras: List[LoRAInfModule] = self.text_encoder_loras + self.unet_loras  # type: ignore
        for lora in loras:
            org_module = lora.org_module_ref[0]
            if not org_module._lora_restored:
                sd = org_module.state_dict()
                sd["weight"] = org_module._lora_org_weight
                org_module.load_state_dict(sd)
                org_module._lora_restored = True  # type: ignore

    def pre_calculation(self):
        loras: List[LoRAInfModule] = self.text_encoder_loras + self.unet_loras  # type: ignore
        for lora in loras:
            org_module = lora.org_module_ref[0]
            sd = org_module.state_dict()

            org_weight = sd["weight"]
            lora_weight = lora.get_weight().to(
                org_weight.device, dtype=org_weight.dtype
            )
            sd["weight"] = org_weight + lora_weight
            assert sd["weight"].shape == org_weight.shape
            org_module.load_state_dict(sd)

            org_module._lora_restored = False  # type: ignore
            lora.enabled = False

    def apply_max_norm_regularization(self, max_norm_value, device):
        downkeys = []
        upkeys = []
        alphakeys = []
        norms = []
        keys_scaled = 0

        state_dict = self.state_dict()
        for key in state_dict.keys():
            if "lora_down" in key and "weight" in key:
                downkeys.append(key)
                upkeys.append(key.replace("lora_down", "lora_up"))
                alphakeys.append(key.replace("lora_down.weight", "alpha"))

        for i in range(len(downkeys)):
            down = state_dict[downkeys[i]].to(device)
            up = state_dict[upkeys[i]].to(device)
            alpha = state_dict[alphakeys[i]].to(device)
            dim = down.shape[0]
            scale = alpha / dim

            if up.shape[2:] == (1, 1) and down.shape[2:] == (1, 1):
                updown = (
                    (up.squeeze(2).squeeze(2) @ down.squeeze(2).squeeze(2))
                    .unsqueeze(2)
                    .unsqueeze(3)
                )
            elif up.shape[2:] == (3, 3) or down.shape[2:] == (3, 3):
                updown = torch.nn.functional.conv2d(
                    down.permute(1, 0, 2, 3), up
                ).permute(1, 0, 2, 3)
            else:
                updown = up @ down

            updown *= scale

            norm = updown.norm().clamp(min=max_norm_value / 2)
            desired = torch.clamp(norm, max=max_norm_value)
            ratio = desired.cpu() / norm.cpu()
            sqrt_ratio = ratio**0.5
            if ratio != 1:
                keys_scaled += 1
                state_dict[upkeys[i]] *= sqrt_ratio
                state_dict[downkeys[i]] *= sqrt_ratio
            scalednorm = updown.norm() * ratio
            norms.append(scalednorm.item())

        return keys_scaled, sum(norms) / len(norms), max(norms)


def create_arch_network_from_weights(
    multiplier: float,
    weights_sd: Dict[str, torch.Tensor],
    text_encoders: Optional[List[nn.Module]] = None,
    unet: Optional[nn.Module] = None,
    for_inference: bool = False,
    **kwargs,
) -> LoRANetwork:
    return create_network_from_weights(
        WAN_TARGET_REPLACE_MODULES,
        multiplier,
        weights_sd,
        text_encoders,
        unet,
        for_inference,
        **kwargs,
    )


# Create network from weights for inference, weights are not loaded here (because can be merged)
def create_network_from_weights(
    target_replace_modules: List[str],
    multiplier: float,
    weights_sd: Dict[str, torch.Tensor],
    text_encoders: Optional[List[nn.Module]] = None,
    unet: Optional[nn.Module] = None,
    for_inference: bool = False,
    **kwargs,
) -> LoRANetwork:
    # get dim/alpha mapping
    modules_dim = {}
    modules_alpha = {}
    for key, value in weights_sd.items():
        if "." not in key:
            continue

        lora_name = key.split(".")[0]
        if "alpha" in key:
            modules_alpha[lora_name] = value
        elif "lora_down" in key:
            dim = value.shape[0]
            modules_dim[lora_name] = dim
            # logger.info(lora_name, value.size(), dim)

    module_class = LoRAInfModule if for_inference else LoRAModule

    network = LoRANetwork(
        target_replace_modules,
        "lora_unet",
        text_encoders,  # type: ignore
        unet,  # type: ignore
        multiplier=multiplier,
        modules_dim=modules_dim,
        modules_alpha=modules_alpha,
        module_class=module_class,
    )
    return network
</file>

<file path="networks/reward_lora.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/scripts/wan2.1_fun/train_reward_lora.py (Apache)

"""Reward LoRA network for WAN.

This module intentionally reuses the standard LoRA implementation for WAN
(`lora_wan.LoRANetwork`) because the architectural changes for Reward LoRA
are in the training loop (reward-based objective), not in the adapter blocks
themselves. We provide a thin wrapper class and factory functions to align
with the project's network loading conventions.

Factory functions exported:
- create_arch_network
- create_arch_network_from_weights
- create_network_from_weights

These mirror the signatures used across Takenoko for consistency.
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn

from common.logger import get_logger

# Reuse the base WAN LoRA implementation
from .lora_wan import (
    LoRANetwork,
    LoRAModule,
    LoRAInfModule,
    create_arch_network as _base_create_arch_network,
    create_arch_network_from_weights as _base_create_arch_network_from_weights,
    create_network_from_weights as _base_create_network_from_weights,
    WAN_TARGET_REPLACE_MODULES,
)


logger = get_logger(__name__)


class RewardLoRANetwork(LoRANetwork):
    """Thin wrapper around LoRANetwork for naming and future extensions."""

    def __init__(
        self,
        target_replace_modules: List[str],
        prefix: str,
        text_encoders: Optional[List[nn.Module]],
        unet: nn.Module,
        multiplier: float = 1.0,
        lora_dim: int = 4,
        alpha: float = 1.0,
        dropout: Optional[float] = None,
        rank_dropout: Optional[float] = None,
        module_dropout: Optional[float] = None,
        conv_lora_dim: Optional[int] = None,
        conv_alpha: Optional[float] = None,
        module_class: type = LoRAModule,
        modules_dim: Optional[Dict[str, int]] = None,
        modules_alpha: Optional[Dict[str, int]] = None,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        verbose: bool = False,
    ) -> None:
        super().__init__(
            target_replace_modules,
            prefix,
            text_encoders,  # type: ignore[arg-type]
            unet,
            multiplier,
            lora_dim,
            alpha,
            dropout,
            rank_dropout,
            module_dropout,
            conv_lora_dim,
            conv_alpha,
            module_class,
            modules_dim,
            modules_alpha,
            exclude_patterns,
            include_patterns,
            verbose,
        )

    # No behavior change; we keep this class for clearer logs and future tweaks.


def create_arch_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: Optional[nn.Module],
    text_encoders: Optional[List[nn.Module]],
    transformer: nn.Module,
    neuron_dropout: Optional[float] = None,
    verbose: bool = False,
    **kwargs: Any,
) -> RewardLoRANetwork:
    """Create a RewardLoRANetwork with standard WAN target modules.

    Parameters
    - multiplier: float ‚Äî LoRA scale multiplier
    - network_dim: Optional[int] ‚Äî LoRA rank
    - network_alpha: Optional[float] ‚Äî LoRA alpha
    - vae: unused (kept for interface compatibility)
    - text_encoders: Optional[List[nn.Module]] ‚Äî not applied for WAN
    - transformer: nn.Module ‚Äî the WAN DiT model to adapt
    - neuron_dropout: Optional[float] ‚Äî LoRA neuron dropout
    - verbose: bool ‚Äî log extra info
    - kwargs: Any ‚Äî propagated for compatibility
    """
    _ = vae  # unused - kept for signature compatibility

    lora_dim = int(network_dim) if network_dim is not None else 4
    alpha = float(network_alpha) if network_alpha is not None else float(lora_dim)

    # Use the base factory to ensure target module discovery remains unified.
    # Ensure text_encoders is a list for the base factory typing
    te_list: List[nn.Module] = (
        list(text_encoders) if isinstance(text_encoders, list) else []
    )

    base_net: LoRANetwork = _base_create_arch_network(
        multiplier,
        lora_dim,
        alpha,
        vae,  # type: ignore[arg-type]
        te_list,
        transformer,
        neuron_dropout=neuron_dropout,
        verbose=verbose,
        **kwargs,
    )

    # Re-wrap to label as RewardLoRANetwork for clearer logs
    reward_net = RewardLoRANetwork(
        target_replace_modules=WAN_TARGET_REPLACE_MODULES,
        prefix=base_net.prefix,
        text_encoders=text_encoders,  # type: ignore[arg-type]
        unet=transformer,
        multiplier=multiplier,
        lora_dim=lora_dim,
        alpha=alpha,
        dropout=neuron_dropout,
        rank_dropout=getattr(base_net, "rank_dropout", None),
        module_dropout=getattr(base_net, "module_dropout", None),
        conv_lora_dim=getattr(base_net, "conv_lora_dim", None),
        conv_alpha=getattr(base_net, "conv_alpha", None),
        modules_dim=getattr(base_net, "modules_dim", None),
        modules_alpha=getattr(base_net, "modules_alpha", None),
        exclude_patterns=getattr(base_net, "exclude_patterns", None),
        include_patterns=getattr(base_net, "include_patterns", None),
        verbose=verbose,
    )

    # Copy created submodules from base network
    for name, module in base_net.named_children():
        reward_net.add_module(name, module)

    logger.info(
        "Created RewardLoRANetwork (WAN) with rank=%s alpha=%s", lora_dim, alpha
    )
    return reward_net


def create_arch_network_from_weights(
    multiplier: float,
    weights_sd: Dict[str, torch.Tensor],
    text_encoders: Optional[List[nn.Module]] = None,
    unet: Optional[nn.Module] = None,
    for_inference: bool = False,
    **kwargs: Any,
) -> Tuple[RewardLoRANetwork, Dict[str, Any]]:
    """Create RewardLoRANetwork from weights (dim/alpha inferred from state dict)."""
    # Ensure text_encoders is a list for the base factory typing
    te_list: List[nn.Module] = (
        list(text_encoders) if isinstance(text_encoders, list) else []
    )

    base_net: LoRANetwork = _base_create_arch_network_from_weights(
        multiplier,
        weights_sd,
        text_encoders=te_list,
        unet=unet,
        for_inference=for_inference,
        **kwargs,
    )

    # Wrap base into RewardLoRANetwork for consistent type naming
    reward_net = RewardLoRANetwork(
        target_replace_modules=WAN_TARGET_REPLACE_MODULES,
        prefix=base_net.prefix,
        text_encoders=text_encoders,  # type: ignore[arg-type]
        unet=unet if isinstance(unet, nn.Module) else None,  # type: ignore[arg-type]
        multiplier=multiplier,
        lora_dim=getattr(base_net, "lora_dim", 4),
        alpha=getattr(base_net, "alpha", 1.0),
        dropout=getattr(base_net, "dropout", None),
        rank_dropout=getattr(base_net, "rank_dropout", None),
        module_dropout=getattr(base_net, "module_dropout", None),
        conv_lora_dim=getattr(base_net, "conv_lora_dim", None),
        conv_alpha=getattr(base_net, "conv_alpha", None),
        modules_dim=getattr(base_net, "modules_dim", None),
        modules_alpha=getattr(base_net, "modules_alpha", None),
        exclude_patterns=getattr(base_net, "exclude_patterns", None),
        include_patterns=getattr(base_net, "include_patterns", None),
        verbose=getattr(base_net, "verbose", False),
    )
    for name, module in base_net.named_children():
        reward_net.add_module(name, module)

    info: Dict[str, Any] = {"inferred_dim_alpha": True}
    return reward_net, info


def create_network_from_weights(
    target_replace_modules: List[str],
    multiplier: float,
    weights_sd: Dict[str, torch.Tensor],
    text_encoders: Optional[List[nn.Module]] = None,
    unet: Optional[nn.Module] = None,
    for_inference: bool = False,
    **kwargs: Any,
) -> RewardLoRANetwork:
    base_net: LoRANetwork = _base_create_network_from_weights(
        target_replace_modules,
        multiplier,
        weights_sd,
        text_encoders=text_encoders,
        unet=unet,
        for_inference=for_inference,
        **kwargs,
    )

    reward_net = RewardLoRANetwork(
        target_replace_modules=target_replace_modules,
        prefix=base_net.prefix,
        text_encoders=text_encoders,  # type: ignore[arg-type]
        unet=unet if isinstance(unet, nn.Module) else None,  # type: ignore[arg-type]
        multiplier=multiplier,
        lora_dim=getattr(base_net, "lora_dim", 4),
        alpha=getattr(base_net, "alpha", 1.0),
        dropout=getattr(base_net, "dropout", None),
        rank_dropout=getattr(base_net, "rank_dropout", None),
        module_dropout=getattr(base_net, "module_dropout", None),
        conv_lora_dim=getattr(base_net, "conv_lora_dim", None),
        conv_alpha=getattr(base_net, "conv_alpha", None),
        modules_dim=getattr(base_net, "modules_dim", None),
        modules_alpha=getattr(base_net, "modules_alpha", None),
        exclude_patterns=getattr(base_net, "exclude_patterns", None),
        include_patterns=getattr(base_net, "include_patterns", None),
        verbose=getattr(base_net, "verbose", False),
    )
    for name, module in base_net.named_children():
        reward_net.add_module(name, module)
    return reward_net
</file>

<file path="networks/vae_wan.py">
"""VAE training network for WAN models.

This module enables direct training of the VAE model, useful for:
- Fine-tuning VAE on specific datasets
- Domain adaptation
- Improving reconstruction quality
"""

import argparse
from typing import Any, Dict, List, Optional, Tuple, Union
import torch
import torch.nn as nn
from wan.modules.vae import WanVAE

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class VaeWanNetwork(torch.nn.Module):
    """Network module for training VAE directly."""

    def __init__(
        self,
        vae: Any,  # WanVAE, but using Any to avoid linter issues
        multiplier: float = 1.0,
        training_mode: str = "full",  # "full", "decoder_only", "encoder_only"
        **kwargs,
    ):
        super().__init__()
        self.multiplier = multiplier
        self.training_mode = training_mode
        self.vae = vae

        # Freeze/unfreeze based on training mode
        self._setup_training_mode()

        logger.info(f"VAE training mode: {training_mode}")
        logger.info(
            f"Trainable parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}"
        )

    def _setup_training_mode(self):
        """Setup which parts of VAE to train based on training_mode."""
        # First freeze everything
        for param in self.vae.parameters():
            param.requires_grad = False

        if self.training_mode == "full":
            # Train entire VAE
            for param in self.vae.parameters():
                param.requires_grad = True
        elif self.training_mode == "decoder_only":
            # Only train decoder
            if hasattr(self.vae, "decoder"):
                for param in self.vae.decoder.parameters():
                    param.requires_grad = True
        elif self.training_mode == "encoder_only":
            # Only train encoder
            if hasattr(self.vae, "encoder"):
                for param in self.vae.encoder.parameters():
                    param.requires_grad = True
        else:
            raise ValueError(f"Unknown training mode: {self.training_mode}")

    def prepare_optimizer_params(
        self, unet_lr: float, input_lr_scale: float = 1.0, **kwargs
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """Prepare optimizer parameters for VAE training."""
        params = []
        lr_descriptions = []

        # Get trainable parameters
        trainable_params = [p for p in self.vae.parameters() if p.requires_grad]

        if trainable_params:
            params.append(
                {
                    "params": trainable_params,
                    "lr": unet_lr * self.multiplier,
                }
            )
            lr_descriptions.append(f"vae_{self.training_mode}")

        logger.info(f"VAE optimizer: {len(trainable_params):,} trainable parameters")
        return params, lr_descriptions

    def get_trainable_params(self) -> List[torch.nn.Parameter]:
        """Get all trainable parameters."""
        return [p for p in self.vae.parameters() if p.requires_grad]

    def apply_max_norm_regularization(
        self, max_norm_value: float, device: torch.device
    ) -> Tuple[int, float, float]:
        """Apply max norm regularization to trainable parameters."""
        if max_norm_value <= 0:
            return 0, 0.0, 0.0

        params = self.get_trainable_params()
        if not params:
            return 0, 0.0, 0.0

        total_norm = torch.norm(
            torch.stack([torch.norm(p.data) for p in params])
        ).item()
        max_norm = max_norm_value

        if total_norm > max_norm:
            clip_coef = max_norm / (total_norm + 1e-6)
            for p in params:
                p.data.mul_(clip_coef)
            return len(params), total_norm, max_norm

        return 0, total_norm, total_norm

    def on_epoch_start(self, transformer: Any) -> None:
        """Called at the start of each epoch."""
        pass

    def on_step_start(self) -> None:
        """Called at the start of each step."""
        pass

    def forward(self, *args, **kwargs):
        """Forward pass - not used in VAE training."""
        raise NotImplementedError(
            "VAE network doesn't use forward pass during training"
        )


def create_network(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: Any,  # WanVAE, but using Any to avoid linter issues
    transformer: Optional[Any] = None,
    **kwargs,
) -> VaeWanNetwork:
    """Create VAE network for training."""

    # Parse training mode from network_args if provided
    training_mode = kwargs.get("training_mode", "full")

    logger.info(f"Creating VAE network with training_mode={training_mode}")

    network = VaeWanNetwork(
        vae=vae,
        multiplier=multiplier,
        training_mode=training_mode,
        **kwargs,
    )

    return network


def create_network_from_weights(
    multiplier: float,
    network_dim: Optional[int],
    network_alpha: Optional[float],
    vae: Any,  # WanVAE, but using Any to avoid linter issues
    weights_sd: Optional[Dict[str, torch.Tensor]] = None,
    transformer: Optional[Any] = None,
    **kwargs,
) -> VaeWanNetwork:
    """Create VAE network and load weights."""

    network = create_network(
        multiplier, network_dim, network_alpha, vae, transformer, **kwargs
    )

    if weights_sd is not None:
        # Load VAE weights
        missing_keys, unexpected_keys = network.vae.load_state_dict(
            weights_sd, strict=False
        )
        if missing_keys:
            logger.warning(f"Missing keys when loading VAE weights: {missing_keys}")
        if unexpected_keys:
            logger.warning(
                f"Unexpected keys when loading VAE weights: {unexpected_keys}"
            )

    return network
</file>

<file path="optimizers/adafactor.py">
## Based on: https://github.com/ostris/ai-toolkit/blob/main/toolkit/optimizers/adafactor.py (MIT)

import math
from typing import List
import torch

from optimum.quanto import QBytesTensor
import random

from optimizers.optimizer_utils import copy_stochastic, stochastic_grad_accummulation


class Adafactor(torch.optim.Optimizer):
    """
    Adafactor implementation with stochastic rounding accumulation and stochastic rounding on apply.
    Modified from transformers Adafactor implementation to support stochastic rounding accumulation and apply.

    AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
    https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py

    Paper: *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost* https://arxiv.org/abs/1804.04235 Note that
    this optimizer internally adjusts the learning rate depending on the `scale_parameter`, `relative_step` and
    `warmup_init` options. To use a manual (external) learning rate schedule you should set `scale_parameter=False` and
    `relative_step=False`.

    Arguments:
        params (`Iterable[nn.parameter.Parameter]`):
            Iterable of parameters to optimize or dictionaries defining parameter groups.
        lr (`float`, *optional*):
            The external learning rate.
        eps (`Tuple[float, float]`, *optional*, defaults to `(1e-30, 0.001)`):
            Regularization constants for square gradient and parameter scale respectively
        clip_threshold (`float`, *optional*, defaults to 1.0):
            Threshold of root mean square of final gradient update
        decay_rate (`float`, *optional*, defaults to -0.8):
            Coefficient used to compute running averages of square
        beta1 (`float`, *optional*):
            Coefficient used for computing running averages of gradient
        weight_decay (`float`, *optional*, defaults to 0.0):
            Weight decay (L2 penalty)
        scale_parameter (`bool`, *optional*, defaults to `True`):
            If True, learning rate is scaled by root mean square
        relative_step (`bool`, *optional*, defaults to `True`):
            If True, time-dependent learning rate is computed instead of external learning rate
        warmup_init (`bool`, *optional*, defaults to `False`):
            Time-dependent learning rate computation depends on whether warm-up initialization is being used

    This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested.

    Recommended T5 finetuning settings (https://discuss.huggingface.co/t/t5-finetuning-tips/684/3):

        - Training without LR warmup or clip_threshold is not recommended.

           - use scheduled LR warm-up to fixed LR
           - use clip_threshold=1.0 (https://arxiv.org/abs/1804.04235)
        - Disable relative updates
        - Use scale_parameter=False
        - Additional optimizer operations like gradient clipping should not be used alongside Adafactor

    Example:

    ```python
    Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)
    ```

    Others reported the following combination to work well:

    ```python
    Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
    ```

    When using `lr=None` with [`Trainer`] you will most likely need to use [`~optimization.AdafactorSchedule`]
    scheduler as following:

    ```python
    from transformers.optimization import Adafactor, AdafactorSchedule

    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
    lr_scheduler = AdafactorSchedule(optimizer)
    trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))
    ```

    Usage:

    ```python
    # replace AdamW with Adafactor
    optimizer = Adafactor(
        model.parameters(),
        lr=1e-3,
        eps=(1e-30, 1e-3),
        clip_threshold=1.0,
        decay_rate=-0.8,
        beta1=None,
        weight_decay=0.0,
        relative_step=False,
        scale_parameter=False,
        warmup_init=False,
    )
    ```"""

    def __init__(
        self,
        params,
        lr=None,
        eps=(1e-30, 1e-3),
        clip_threshold=1.0,
        decay_rate=-0.8,
        beta1=None,
        weight_decay=0.0,
        scale_parameter=True,
        relative_step=True,
        warmup_init=False,
        do_paramiter_swapping=False,
        paramiter_swapping_factor=0.1,
        stochastic_accumulation=True,
        stochastic_rounding=True,
    ):
        self.stochastic_rounding = stochastic_rounding
        if lr is not None and relative_step:
            raise ValueError(
                "Cannot combine manual `lr` and `relative_step=True` options"
            )
        if warmup_init and not relative_step:
            raise ValueError("`warmup_init=True` requires `relative_step=True`")

        defaults = {
            "lr": lr,
            "eps": eps,
            "clip_threshold": clip_threshold,
            "decay_rate": decay_rate,
            "beta1": beta1,
            "weight_decay": weight_decay,
            "scale_parameter": scale_parameter,
            "relative_step": relative_step,
            "warmup_init": warmup_init,
        }
        super().__init__(params, defaults)

        self.base_lrs: List[float] = [lr for group in self.param_groups]  # type: ignore

        self.is_stochastic_rounding_accumulation = False

        # setup stochastic grad accum hooks
        if stochastic_accumulation:
            for group in self.param_groups:
                for param in group["params"]:
                    if param.requires_grad and param.dtype != torch.float32:
                        self.is_stochastic_rounding_accumulation = True
                        param.register_post_accumulate_grad_hook(
                            stochastic_grad_accummulation
                        )

        self.do_paramiter_swapping = do_paramiter_swapping
        self.paramiter_swapping_factor = paramiter_swapping_factor
        self._total_paramiter_size = 0
        # count total paramiters
        for group in self.param_groups:
            for param in group["params"]:
                self._total_paramiter_size += torch.numel(param)
        # pretty print total paramiters with comma seperation
        print(f"Total training paramiters: {self._total_paramiter_size:,}")

        # needs to be enabled to count paramiters
        if self.do_paramiter_swapping:
            self.enable_paramiter_swapping(self.paramiter_swapping_factor)

    def enable_paramiter_swapping(self, paramiter_swapping_factor=0.1):
        self.do_paramiter_swapping = True
        self.paramiter_swapping_factor = paramiter_swapping_factor
        # call it an initial time
        self.swap_paramiters()

    def swap_paramiters(self):
        all_params = []
        # deactivate all paramiters
        for group in self.param_groups:
            for param in group["params"]:
                param.requires_grad_(False)
                # remove any grad
                param.grad = None
                all_params.append(param)
        # shuffle all paramiters
        random.shuffle(all_params)

        # keep activating paramiters until we are going to go over the target paramiters
        target_paramiters = int(
            self._total_paramiter_size * self.paramiter_swapping_factor
        )
        total_paramiters = 0
        for param in all_params:
            total_paramiters += torch.numel(param)
            if total_paramiters >= target_paramiters:
                break
            else:
                param.requires_grad_(True)

    @staticmethod
    def _get_lr(param_group, param_state):
        rel_step_sz = param_group["lr"]
        if param_group["relative_step"]:
            min_step = (
                1e-6 * param_state["step"] if param_group["warmup_init"] else 1e-2
            )
            rel_step_sz = min(min_step, 1.0 / math.sqrt(param_state["step"]))
        param_scale = 1.0
        if param_group["scale_parameter"]:
            param_scale = max(param_group["eps"][1], param_state["RMS"])
        return param_scale * rel_step_sz

    @staticmethod
    def _get_options(param_group, param_shape):
        factored = len(param_shape) >= 2
        use_first_moment = param_group["beta1"] is not None
        return factored, use_first_moment

    @staticmethod
    def _rms(tensor):
        return tensor.norm(2) / (tensor.numel() ** 0.5)

    @staticmethod
    def _approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col):
        # copy from fairseq's adafactor implementation:
        # https://github.com/huggingface/transformers/blob/8395f14de6068012787d83989c3627c3df6a252b/src/transformers/optimization.py#L505
        r_factor = (
            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True))
            .rsqrt_()
            .unsqueeze(-1)
        )
        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()
        return torch.mul(r_factor, c_factor)

    def step_hook(self):
        if not self.is_stochastic_rounding_accumulation:
            return
        # copy over stochastically rounded grads
        for group in self.param_groups:
            for param in group["params"]:
                if param.requires_grad and hasattr(param, "_accum_grad"):
                    param.grad = param._accum_grad
                    del param._accum_grad

    # adafactor manages its own lr
    def get_learning_rates(self):
        lrs = [
            self._get_lr(group, self.state[group["params"][0]])
            for group in self.param_groups
            if group["params"][0].grad is not None
        ]
        if len(lrs) == 0:
            lrs = self.base_lrs  # if called before stepping
        return lrs

    @torch.no_grad()
    def step(self, closure=None):
        """
        Performs a single optimization step

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        self.step_hook()
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None or not p.requires_grad:
                    continue

                grad = p.grad
                if grad.dtype != torch.float32:
                    grad = grad.to(torch.float32)
                if grad.is_sparse:
                    raise RuntimeError("Adafactor does not support sparse gradients.")

                # if p has atts _scale then it is quantized. We need to divide the grad by the scale
                # if hasattr(p, "_scale"):
                #     grad = grad / p._scale

                state = self.state[p]
                grad_shape = grad.shape

                factored, use_first_moment = self._get_options(group, grad_shape)
                # State Initialization
                if len(state) == 0:
                    state["step"] = 0

                    if use_first_moment:
                        # Exponential moving average of gradient values
                        state["exp_avg"] = torch.zeros_like(grad)
                    if factored:
                        state["exp_avg_sq_row"] = torch.zeros(grad_shape[:-1]).to(grad)
                        state["exp_avg_sq_col"] = torch.zeros(
                            grad_shape[:-2] + grad_shape[-1:]
                        ).to(grad)
                    else:
                        state["exp_avg_sq"] = torch.zeros_like(grad)

                    state["RMS"] = 0
                else:
                    if use_first_moment:
                        state["exp_avg"] = state["exp_avg"].to(grad)
                    if factored:
                        state["exp_avg_sq_row"] = state["exp_avg_sq_row"].to(grad)
                        state["exp_avg_sq_col"] = state["exp_avg_sq_col"].to(grad)
                    else:
                        state["exp_avg_sq"] = state["exp_avg_sq"].to(grad)

                p_data_fp32 = p

                if isinstance(p_data_fp32, QBytesTensor):
                    p_data_fp32 = p_data_fp32.dequantize()
                if p.dtype != torch.float32:
                    p_data_fp32 = p_data_fp32.clone().float()  # type: ignore

                state["step"] += 1
                state["RMS"] = self._rms(p_data_fp32)
                lr = self._get_lr(group, state)

                beta2t = 1.0 - math.pow(state["step"], group["decay_rate"])
                eps = group["eps"]
                if isinstance(eps, tuple) or isinstance(eps, list):
                    eps = eps[0]
                update = (grad**2) + eps
                if factored:
                    exp_avg_sq_row = state["exp_avg_sq_row"]
                    exp_avg_sq_col = state["exp_avg_sq_col"]

                    exp_avg_sq_row.mul_(beta2t).add_(
                        update.mean(dim=-1), alpha=(1.0 - beta2t)
                    )
                    exp_avg_sq_col.mul_(beta2t).add_(
                        update.mean(dim=-2), alpha=(1.0 - beta2t)
                    )

                    # Approximation of exponential moving average of square of gradient
                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)
                    update.mul_(grad)
                else:
                    exp_avg_sq = state["exp_avg_sq"]

                    exp_avg_sq.mul_(beta2t).add_(update, alpha=(1.0 - beta2t))
                    update = exp_avg_sq.rsqrt().mul_(grad)

                update.div_(
                    (self._rms(update) / group["clip_threshold"]).clamp_(min=1.0)
                )
                update.mul_(lr)

                if use_first_moment:
                    exp_avg = state["exp_avg"]
                    exp_avg.mul_(group["beta1"]).add_(
                        update, alpha=(1 - group["beta1"])
                    )
                    update = exp_avg

                if group["weight_decay"] != 0:
                    p_data_fp32.add_(p_data_fp32, alpha=(-group["weight_decay"] * lr))  # type: ignore

                p_data_fp32.add_(-update)  # type: ignore

                if p.dtype != torch.float32 and self.stochastic_rounding:
                    # apply stochastic rounding
                    copy_stochastic(p, p_data_fp32)  # type: ignore

        return loss
</file>

<file path="optimizers/adamw_8bit_kahan.py">
## Based on: https://github.com/tdrussell/diffusion-pipe/blob/main/optimizers/adamw_8bit.py (MIT)

import torch
import bitsandbytes
import bitsandbytes.functional as F


class AdamW8bitKahan(bitsandbytes.optim.AdamW8bit):
    def __init__(self, *args, stabilize=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.stabilize = stabilize

    @torch.no_grad()
    def init_state(self, group, p, gindex, pindex):
        super().init_state(group, p, gindex, pindex)
        self.state[p]["shift"] = self.get_state_buffer(p, dtype=p.dtype)

    @torch.no_grad()
    def update_step(self, group, p, gindex, pindex):
        # avoid update error from non-contiguous memory layout
        p.data = p.data.contiguous()
        p.grad = p.grad.contiguous()

        state = self.state[p]
        grad = p.grad

        config = self.get_config(gindex, pindex, group)

        state["step"] += 1
        step = state["step"]

        if config["percentile_clipping"] < 100:
            current_gnorm, clip_value, gnorm_scale = F.percentile_clipping(
                grad,
                state["gnorm_vec"],
                step,
                config["percentile_clipping"],
            )
        else:
            gnorm_scale = 1.0

        shift = state["shift"]

        # StableAdamW
        if self.stabilize:
            exp_avg_sq = state["state2"]
            eps_sq = torch.tensor(
                config["eps"] ** 2, dtype=exp_avg_sq.dtype, device=exp_avg_sq.device
            )
            rms = grad.pow(2).div_(exp_avg_sq.maximum(eps_sq)).mean().sqrt()
            lr = config["lr"] / max(1, rms.item())
        else:
            lr = config["lr"]

        if state["state1"].dtype == torch.float:
            F.optimizer_update_32bit(
                self.optimizer_name,
                grad,
                shift,
                state["state1"],
                config["betas"][0],
                config["eps"],
                step,
                lr,
                state["state2"],
                config["betas"][1],
                config["betas"][2] if len(config["betas"]) >= 3 else 0.0,
                config["alpha"],
                config["weight_decay"],
                gnorm_scale,  # type: ignore
                state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
                skip_zeros=config["skip_zeros"],
            )

        elif state["state1"].dtype == torch.uint8 and not config["block_wise"]:
            F.optimizer_update_8bit(
                self.optimizer_name,
                grad,
                shift,
                state["state1"],
                state["state2"],
                config["betas"][0],
                config["betas"][1],
                config["eps"],
                step,
                lr,
                state["qmap1"],
                state["qmap2"],
                state["max1"],
                state["max2"],
                state["new_max1"],
                state["new_max2"],
                config["weight_decay"],
                gnorm_scale=gnorm_scale,  # type: ignore
                unorm_vec=state["unorm_vec"] if config["max_unorm"] > 0.0 else None,
                max_unorm=config["max_unorm"],
            )

            # swap maxes
            state["max1"], state["new_max1"] = state["new_max1"], state["max1"]
            state["max2"], state["new_max2"] = state["new_max2"], state["max2"]
        elif state["state1"].dtype == torch.uint8 and config["block_wise"]:
            F.optimizer_update_8bit_blockwise(
                self.optimizer_name,
                grad,
                shift,
                state["state1"],
                state["state2"],
                config["betas"][0],
                config["betas"][1],
                config["betas"][2] if len(config["betas"]) >= 3 else 0.0,
                config["alpha"],
                config["eps"],
                step,
                lr,
                state["qmap1"],
                state["qmap2"],
                state["absmax1"],
                state["absmax2"],
                config["weight_decay"],
                gnorm_scale=gnorm_scale,  # type: ignore
                skip_zeros=config["skip_zeros"],
            )

        buffer = p.clone()
        p.add_(shift)
        shift.add_(buffer.sub_(p))
</file>

<file path="optimizers/automagic.py">
## Based on: https://github.com/tdrussell/diffusion-pipe/blob/main/optimizers/automagic.py (MIT)

# Copied from AI Toolkit.
# I added Kahan summation for bfloat16 parameters.

# MIT License

# Copyright (c) 2024 Ostris, LLC

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


from typing import List
import torch
from optimizers.optimizer_utils import (
    Auto8bitTensor,
    copy_stochastic,
    stochastic_grad_accummulation,
)
from optimum.quanto import QBytesTensor
import random


class Automagic(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr=1e-6,  # lr is start lr
        min_lr=1e-7,
        max_lr=1e-3,
        lr_bump=1e-6,  # amount to bump the lr when adjusting
        eps=(1e-30, 1e-3),
        clip_threshold=1.0,
        beta2=0.999,
        weight_decay=0.0,
        do_paramiter_swapping=False,
        paramiter_swapping_factor=0.1,
    ):
        self.lr = lr
        if self.lr > 1e-3:
            print(
                f"Warning! Start lr is very high: {self.lr}. Forcing to 1e-6. this does not work like prodigy"
            )
            self.lr = 1e-6
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.lr_bump = lr_bump

        defaults = {
            "lr": lr,
            "eps": eps,
            "clip_threshold": clip_threshold,
            "beta2": beta2,
            "weight_decay": weight_decay,
        }
        super().__init__(params, defaults)

        self.base_lrs: List[float] = [lr for group in self.param_groups]

        self.is_stochastic_rounding_accumulation = False

        # setup stochastic grad accum hooks

        # for group in self.param_groups:
        #     for param in group["params"]:
        #         if param.requires_grad and param.dtype != torch.float32:
        #             self.is_stochastic_rounding_accumulation = True
        #             param.register_post_accumulate_grad_hook(
        #                 stochastic_grad_accummulation
        #             )

        self.do_paramiter_swapping = do_paramiter_swapping
        self.paramiter_swapping_factor = paramiter_swapping_factor
        self._total_paramiter_size = 0
        # count total paramiters
        for group in self.param_groups:
            for param in group["params"]:
                self._total_paramiter_size += torch.numel(param)
        # pretty print total paramiters with comma seperation
        print(f"Total training paramiters: {self._total_paramiter_size:,}")

        # needs to be enabled to count paramiters
        if self.do_paramiter_swapping:
            self.enable_paramiter_swapping(self.paramiter_swapping_factor)

    def enable_paramiter_swapping(self, paramiter_swapping_factor=0.1):
        self.do_paramiter_swapping = True
        self.paramiter_swapping_factor = paramiter_swapping_factor
        # call it an initial time
        self.swap_paramiters()

    def swap_paramiters(self):
        all_params = []
        # deactivate all paramiters
        for group in self.param_groups:
            for param in group["params"]:
                param.requires_grad_(False)
                # remove any grad
                param.grad = None
                all_params.append(param)
        # shuffle all paramiters
        random.shuffle(all_params)

        # keep activating paramiters until we are going to go over the target paramiters
        target_paramiters = int(
            self._total_paramiter_size * self.paramiter_swapping_factor
        )
        total_paramiters = 0
        for param in all_params:
            total_paramiters += torch.numel(param)
            if total_paramiters >= target_paramiters:
                break
            else:
                param.requires_grad_(True)

    @staticmethod
    def _get_lr(param_group, param_state):
        if "avg_lr" in param_state:
            lr = param_state["avg_lr"]
        else:
            lr = 0.0
        return lr

    def _get_group_lr(self, group):
        group_lrs = []
        for p in group["params"]:
            group_lrs.append(self._get_lr(group, self.state[p]))
        # return avg
        if len(group_lrs) == 0:
            return self.lr
        return sum(group_lrs) / len(group_lrs)

    @staticmethod
    def _rms(tensor):
        return tensor.norm(2) / (tensor.numel() ** 0.5)

    @staticmethod
    def _approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col):
        r_factor = (
            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True))
            .rsqrt_()
            .unsqueeze(-1)
        )
        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()
        return torch.mul(r_factor, c_factor)

    def step_hook(self):
        if not self.is_stochastic_rounding_accumulation:
            return
        # copy over stochastically rounded grads
        for group in self.param_groups:
            for param in group["params"]:
                if param.requires_grad and hasattr(param, "_accum_grad"):
                    param.grad = param._accum_grad
                    del param._accum_grad

    # automagic manages its own lr
    def get_learning_rates(self):
        lrs = [self._get_group_lr(group) for group in self.param_groups]
        if len(lrs) == 0:
            lrs = self.base_lrs  # if called before stepping
        return lrs

    def get_avg_learning_rate(self):
        lrs = self.get_learning_rates()
        return sum(lrs) / len(lrs)

    @torch.no_grad()
    def step(self, closure=None):
        """
        Performs a single optimization step

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        self.step_hook()
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None or not p.requires_grad:
                    continue

                grad = p.grad
                if grad.dtype != torch.float32:
                    grad = grad.to(torch.float32)
                if grad.is_sparse:
                    raise RuntimeError("Automagic does not support sparse gradients.")

                state = self.state[p]
                grad_shape = grad.shape

                factored = len(grad_shape) >= 2
                # State Initialization
                if len(state) == 0:
                    self.initialize_state(p)
                else:
                    # Check if exp_avg_sq_row and exp_avg_sq_col exist for factored case
                    if factored:
                        if (
                            "exp_avg_sq_row" not in state
                            or "exp_avg_sq_col" not in state
                        ):
                            state["exp_avg_sq_row"] = torch.zeros(p.shape[:-1]).to(grad)
                            state["exp_avg_sq_col"] = torch.zeros(
                                p.shape[:-2] + p.shape[-1:]
                            ).to(grad)
                        else:
                            state["exp_avg_sq_row"] = state["exp_avg_sq_row"].to(grad)
                            state["exp_avg_sq_col"] = state["exp_avg_sq_col"].to(grad)
                    # Check if exp_avg_sq exists for non-factored case
                    else:
                        if "exp_avg_sq" not in state:
                            state["exp_avg_sq"] = torch.zeros_like(grad)
                        else:
                            state["exp_avg_sq"] = state["exp_avg_sq"].to(grad)

                p_data_fp32 = p

                if isinstance(p_data_fp32, QBytesTensor):
                    p_data_fp32 = p_data_fp32.dequantize()
                if p.dtype != torch.float32:
                    p_data_fp32 = p_data_fp32.clone().float()  # type: ignore

                # Initialize step if it doesn't exist
                if "step" not in state:
                    state["step"] = 0
                state["step"] += 1
                state["RMS"] = self._rms(p_data_fp32)

                # Use fixed beta2 from group instead of decay_rate calculation
                beta2 = group["beta2"]
                eps = group["eps"]
                if isinstance(eps, tuple) or isinstance(eps, list):
                    eps = eps[0]
                update = (grad**2) + eps
                if factored:
                    exp_avg_sq_row = state["exp_avg_sq_row"]
                    exp_avg_sq_col = state["exp_avg_sq_col"]

                    exp_avg_sq_row.mul_(beta2).add_(
                        update.mean(dim=-1), alpha=(1.0 - beta2)
                    )
                    exp_avg_sq_col.mul_(beta2).add_(
                        update.mean(dim=-2), alpha=(1.0 - beta2)
                    )

                    # Approximation of exponential moving average of square of gradient
                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)
                    update.mul_(grad)
                else:
                    exp_avg_sq = state["exp_avg_sq"]

                    exp_avg_sq.mul_(beta2).add_(update, alpha=(1.0 - beta2))
                    update = exp_avg_sq.rsqrt().mul_(grad)

                update.div_(
                    (self._rms(update) / group["clip_threshold"]).clamp_(min=1.0)
                )

                # Ensure state is properly initialized
                if "last_polarity" not in state or "lr_mask" not in state:
                    self.initialize_state(p)

                # Get signs of current last update and updates
                last_polarity = state["last_polarity"]
                current_polarity = (update > 0).to(torch.bool)
                sign_agreement = torch.where(last_polarity == current_polarity, 1, -1)
                state["last_polarity"] = current_polarity

                lr_mask = state["lr_mask"].to(torch.float32)

                # Update learning rate mask based on sign agreement
                new_lr = torch.where(
                    sign_agreement > 0,
                    lr_mask + self.lr_bump,  # Increase lr
                    lr_mask - self.lr_bump,  # Decrease lr
                )

                # Clip learning rates to bounds
                new_lr = torch.clamp(new_lr, min=self.min_lr, max=self.max_lr)

                # Apply the learning rate mask to the update
                update.mul_(new_lr)

                state["lr_mask"] = Auto8bitTensor(new_lr)
                state["avg_lr"] = torch.mean(new_lr)

                if group["weight_decay"] != 0:
                    # Apply weight decay with per-parameter learning rates
                    # Instead of using add_ with a tensor alpha (which isn't supported),
                    # we'll use element-wise multiplication to apply the weight decay
                    weight_decay_update = (
                        p_data_fp32 * (-group["weight_decay"]) * new_lr
                    )
                    # p_data_fp32.add_(weight_decay_update)
                else:
                    weight_decay_update = None

                if p.dtype == torch.bfloat16:
                    # Kahan summation for bfloat16
                    update.mul_(-1)
                    if weight_decay_update is not None:
                        update.add_(weight_decay_update)
                    shift = state["shift"]
                    shift.add_(update)
                    # Use grad as temp buffer
                    grad.copy_(p.detach())
                    p.add_(shift)
                    shift.add_(grad.sub_(p))
                else:
                    if weight_decay_update is not None:
                        p_data_fp32.add_(weight_decay_update)  # type: ignore

                    p_data_fp32.add_(-update)  # type: ignore
                    if p.dtype != torch.float32:
                        # apply stochastic rounding
                        copy_stochastic(p, p_data_fp32)  # type: ignore

        return loss

    def initialize_state(self, p):
        state = self.state[p]
        state["step"] = 0

        # store the lr mask
        if "lr_mask" not in state:
            state["lr_mask"] = Auto8bitTensor(
                torch.ones(p.shape).to(p.device, dtype=torch.float32) * self.lr
            )
        state["avg_lr"] = torch.mean(state["lr_mask"].to(torch.float32))
        if "last_polarity" not in state:
            state["last_polarity"] = torch.zeros(
                p.shape, dtype=torch.bool, device=p.device
            )

        factored = len(p.shape) >= 2
        if factored:
            state["exp_avg_sq_row"] = torch.zeros(p.shape[:-1]).to(p)
            state["exp_avg_sq_col"] = torch.zeros(p.shape[:-2] + p.shape[-1:]).to(p)
        else:
            state["exp_avg_sq"] = torch.zeros_like(p)

        state["RMS"] = 0
        # For Kahan summation.
        if p.dtype == torch.bfloat16:
            state["shift"] = torch.zeros_like(p)

    # override the state_dict to save the lr_mask
    def state_dict(self, *args, **kwargs):
        orig_state_dict = super().state_dict(*args, **kwargs)
        # convert the state to quantized tensor to scale and quantized
        new_sace_state = {}
        for p, state in orig_state_dict["state"].items():
            save_state = {k: v for k, v in state.items() if k != "lr_mask"}

            # Check if lr_mask exists in the state before trying to access it
            if "lr_mask" in state:
                save_state["lr_mask"] = state["lr_mask"].state_dict()

            new_sace_state[p] = save_state

        orig_state_dict["state"] = new_sace_state

        return orig_state_dict

    def load_state_dict(self, state_dict, strict=True):
        # Validate that the state_dict is from an Automagic optimizer
        is_valid_automagic_state = False

        # Check if state_dict has the expected structure
        if "state" in state_dict and isinstance(state_dict["state"], dict):
            # Check if at least one state entry has an lr_mask, which is specific to Automagic
            for param_id, param_state in state_dict["state"].items():
                if isinstance(param_state, dict) and "lr_mask" in param_state:
                    is_valid_automagic_state = True
                    break

        if not is_valid_automagic_state:
            return

        # First, call the parent class's load_state_dict to load the basic optimizer state
        # We'll handle the lr_mask separately
        state_dict_copy = {"state": {}, "param_groups": state_dict["param_groups"]}

        # Copy all state entries except lr_mask
        for param_id, param_state in state_dict["state"].items():
            state_dict_copy["state"][param_id] = {
                k: v for k, v in param_state.items() if k != "lr_mask"
            }

        # Call parent class load_state_dict with the modified state dict
        super().load_state_dict(state_dict_copy)

        # Now handle the lr_mask separately
        # We need to map the saved parameters to the current parameters
        # This is tricky because the parameter IDs might be different

        # Get all current parameters that require gradients
        current_params = []
        for group in self.param_groups:
            for p in group["params"]:
                if p.requires_grad:
                    current_params.append(p)

        # If the number of parameters doesn't match, we can't reliably map them
        if len(current_params) != len(state_dict["param_groups"][0]["params"]):
            print(
                f"WARNING: Number of parameters doesn't match between saved state ({len(state_dict['param_groups'][0]['params'])}) "
                f"and current model ({len(current_params)}). Learning rate masks may not be correctly loaded."
            )

        # Map parameters by their position in the param_groups
        # This assumes the order of parameters is preserved between saving and loading
        saved_param_ids = list(state_dict["state"].keys())

        for i, current_param in enumerate(current_params):
            if i >= len(saved_param_ids):
                break

            saved_param_id = saved_param_ids[i]
            saved_state = state_dict["state"][saved_param_id]

            # Skip if this saved state doesn't have an lr_mask
            if "lr_mask" not in saved_state:
                continue

            # Initialize the state for this parameter if it doesn't exist
            if current_param not in self.state:
                self.initialize_state(current_param)

            # Get the current state for this parameter
            current_state = self.state[current_param]

            # Load the lr_mask from the saved state
            saved_lr_mask = saved_state["lr_mask"]

            # Reconstruct the Auto8bitTensor from its state dict
            try:
                # Make sure the shapes match
                if (
                    "quantized" in saved_lr_mask
                    and saved_lr_mask["quantized"].shape == current_param.shape
                ):
                    saved_lr_mask["quantized"] = saved_lr_mask["quantized"].to(
                        current_param.device
                    )
                    current_state["lr_mask"] = Auto8bitTensor(saved_lr_mask)
                else:
                    print(
                        f"WARNING: Shape mismatch for parameter {i}. "
                        f"Expected {current_param.shape}, got {saved_lr_mask['quantized'].shape if 'quantized' in saved_lr_mask else 'unknown'}. "
                        f"Initializing new lr_mask."
                    )
                    # Initialize a new lr_mask
                    current_state["lr_mask"] = Auto8bitTensor(
                        torch.ones(current_param.shape).to(
                            current_param.device, dtype=torch.float32
                        )
                        * self.lr
                    )
            except Exception as e:
                print(f"ERROR: Failed to load lr_mask for parameter {i}: {e}")
                # Initialize a new lr_mask
                current_state["lr_mask"] = Auto8bitTensor(
                    torch.ones(current_param.shape).to(
                        current_param.device, dtype=torch.float32
                    )
                    * self.lr
                )
</file>

<file path="optimizers/enhanced_logging.py">
## Based on: https://github.com/ostris/ai-toolkit/blob/main/toolkit/optimizers/prodigy_8bit.py (MIT)
## Based on: https://github.com/tdrussell/diffusion-pipe/blob/main/optimizers/automagic.py (MIT)

"""Enhanced optimizer logging functionality.

This module provides enhanced logging capabilities for specific optimizers that have
internal state or adaptive learning rates, such as Prodigy and Automagic optimizers.
"""

import logging
from typing import Any, Dict, List, Optional, Tuple, Union
import torch
from torch.optim import Optimizer

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class EnhancedOptimizerLogger:
    """Enhanced logging functionality for specific optimizers.

    This class provides additional metrics and visualizations for optimizers that have
    internal state or adaptive learning rates, such as Prodigy and Automagic optimizers.
    """

    def __init__(self):
        """Initialize the enhanced optimizer logger."""
        self.supported_optimizers = {
            "Prodigy": self._log_prodigy_metrics,
            "Prodigy8bit": self._log_prodigy_metrics,
            "Automagic": self._log_automagic_metrics,
        }

    def get_prodigy_d(self, optimizer: Optimizer) -> float:
        """Extract the 'd' parameter from Prodigy optimizer for logging.

        Args:
            optimizer: The optimizer instance

        Returns:
            Average 'd' value across all parameter groups
        """
        try:
            if not hasattr(optimizer, "param_groups"):
                return 0.0

            d_values = []
            for group in optimizer.param_groups:
                if "d" in group:
                    d_values.append(group["d"])

            if not d_values:
                return 0.0

            return sum(d_values) / len(d_values)
        except Exception as e:
            logger.debug(f"Failed to extract Prodigy 'd' parameter: {e}")
            return 0.0

    def get_automagic_lrs(self, optimizer: Optimizer) -> Tuple[torch.Tensor, float]:
        """Extract ALL individual, per-element learning rates from Automagic optimizer.

        Args:
            optimizer: The Automagic optimizer instance

        Returns:
            Tuple of (all_learning_rates_tensor, average_learning_rate)
        """
        try:
            # Handle Accelerate-wrapped optimizers
            if hasattr(optimizer, "optimizer"):
                # This is an AcceleratedOptimizer, get the underlying optimizer
                actual_optimizer = optimizer.optimizer  # type: ignore
            else:
                actual_optimizer = optimizer

            all_lrs_tensors = []
            total_params = 0
            params_with_state = 0
            params_with_lr_mask = 0

            for group in actual_optimizer.param_groups:
                for p in group["params"]:
                    if p.requires_grad:
                        total_params += 1

                        # The critical check: does this parameter have a state and the lr_mask?
                        if p in actual_optimizer.state:
                            params_with_state += 1
                            state = actual_optimizer.state[p]

                            if "lr_mask" in state:
                                params_with_lr_mask += 1
                                lr_mask = state["lr_mask"]

                                # The lr_mask is a custom Auto8bitTensor. We need to get the float tensor.
                                # Based on the optimizer code, it has a `.to(torch.float32)` method.
                                if hasattr(lr_mask, "to"):
                                    float_lrs = lr_mask.to(torch.float32)
                                else:
                                    # Fallback just in case
                                    float_lrs = lr_mask

                                # Add the full tensor for this parameter
                                all_lrs_tensors.append(float_lrs.flatten())

            # If we didn't find any parameters with an lr_mask, return empty.
            if not all_lrs_tensors:
                return torch.tensor([]), 0.0

            # Concatenate all learning rate tensors into a single flat tensor
            full_lrs_tensor = torch.cat(all_lrs_tensors)

            if full_lrs_tensor.numel() == 0:
                return torch.tensor([]), 0.0

            # Calculate the true average from the full tensor
            avg_lr = full_lrs_tensor.mean().item()

            return full_lrs_tensor, avg_lr
        except Exception as e:
            print(f"[DEBUG] Error extracting Automagic learning rates: {e}")
            logger.error(
                f"Error extracting Automagic learning rates: {e}", exc_info=True
            )
            return torch.tensor([]), 0.0

    def _log_prodigy_metrics(self, optimizer: Optimizer) -> Dict[str, float]:
        """Log Prodigy optimizer metrics.

        Args:
            optimizer: The Prodigy optimizer instance

        Returns:
            Dict containing Prodigy-specific metrics
        """
        metrics = {}
        try:
            prodigy_d = self.get_prodigy_d(optimizer)
            metrics["train/prodigy_d"] = prodigy_d
        except Exception as e:
            logger.debug(f"Failed to log Prodigy metrics: {e}")
        return metrics

    def _log_automagic_metrics(self, optimizer: Optimizer) -> Dict[str, float]:
        """Log Automagic optimizer metrics.

        Args:
            optimizer: The Automagic optimizer instance

        Returns:
            Dict containing Automagic-specific metrics
        """
        metrics = {}
        try:
            lrs_tensor, avg_lr = self.get_automagic_lrs(optimizer)
            if len(lrs_tensor) > 0:
                metrics["train/automagic_avg_lr"] = avg_lr
                logger.debug(
                    f"Automagic logging: {len(lrs_tensor)} parameters, avg_lr={avg_lr:.2e}"
                )
            else:
                logger.debug("Automagic logging: No learning rates found")
        except Exception as e:
            logger.debug(f"Failed to log Automagic metrics: {e}")
        return metrics

    def get_enhanced_metrics(self, optimizer: Optimizer) -> Dict[str, float]:
        """Get enhanced metrics for the given optimizer.

        Args:
            optimizer: The optimizer instance

        Returns:
            Dict containing enhanced metrics for supported optimizers
        """
        # Handle Accelerate-wrapped optimizers
        if hasattr(optimizer, "optimizer"):
            # This is an AcceleratedOptimizer, get the underlying optimizer
            actual_optimizer = optimizer.optimizer  # type: ignore
            optimizer_name = actual_optimizer.__class__.__name__
        else:
            optimizer_name = optimizer.__class__.__name__

        if optimizer_name in self.supported_optimizers:
            return self.supported_optimizers[optimizer_name](optimizer)

        return {}

    def get_histogram_data(
        self, optimizer: Optimizer
    ) -> Optional[Tuple[str, torch.Tensor]]:
        """Get histogram data for optimizers that support it.

        Args:
            optimizer: The optimizer instance

        Returns:
            Tuple of (metric_name, tensor_data) if supported, None otherwise
        """
        # Handle Accelerate-wrapped optimizers
        if hasattr(optimizer, "optimizer"):
            # This is an AcceleratedOptimizer, get the underlying optimizer
            actual_optimizer = optimizer.optimizer  # type: ignore
            optimizer_name = actual_optimizer.__class__.__name__
        else:
            optimizer_name = optimizer.__class__.__name__

        if optimizer_name == "Automagic":
            try:
                lrs_tensor, _ = self.get_automagic_lrs(optimizer)
                if len(lrs_tensor) > 0:
                    return ("train/automagic_lrs_histogram", lrs_tensor)
            except Exception as e:
                logger.debug(f"Failed to get Automagic histogram data: {e}")

        return None

    def is_supported(self, optimizer: Optimizer) -> bool:
        """Check if the optimizer supports enhanced logging.

        Args:
            optimizer: The optimizer instance

        Returns:
            True if enhanced logging is supported, False otherwise
        """
        # Handle Accelerate-wrapped optimizers
        if hasattr(optimizer, "optimizer"):
            # This is an AcceleratedOptimizer, get the underlying optimizer
            actual_optimizer = optimizer.optimizer  # type: ignore
            optimizer_name = actual_optimizer.__class__.__name__
        else:
            optimizer_name = optimizer.__class__.__name__

        return optimizer_name in self.supported_optimizers

    def get_supported_optimizers(self) -> List[str]:
        """Get list of supported optimizer names.

        Returns:
            List of optimizer class names that support enhanced logging
        """
        return list(self.supported_optimizers.keys())

    def test_enhanced_logging(self, optimizer: Optimizer) -> Dict[str, Any]:
        """Test the enhanced optimizer logging functionality.

        Args:
            optimizer: The optimizer to test

        Returns:
            Dict with test results
        """
        test_results = {}

        try:
            optimizer_name = optimizer.__class__.__name__
            test_results["optimizer_name"] = optimizer_name
            test_results["is_supported"] = self.is_supported(optimizer)

            if not self.is_supported(optimizer):
                test_results["message"] = (
                    f"Optimizer {optimizer_name} not supported for enhanced logging"
                )
                return test_results

            # Get enhanced metrics
            metrics = self.get_enhanced_metrics(optimizer)
            test_results["metrics"] = metrics

            # Get histogram data if available
            histogram_data = self.get_histogram_data(optimizer)
            if histogram_data:
                metric_name, tensor_data = histogram_data
                test_results["histogram_metric"] = metric_name
                test_results["histogram_data_shape"] = list(tensor_data.shape)
                test_results["histogram_data_count"] = len(tensor_data)

            # Log test results
            if optimizer_name in ["Prodigy", "Prodigy8bit"]:
                prodigy_d = self.get_prodigy_d(optimizer)
                logger.info(f"Prodigy 'd' parameter: {prodigy_d}")

            elif optimizer_name == "Automagic":
                lrs_tensor, avg_lr = self.get_automagic_lrs(optimizer)
                logger.info(
                    f"Automagic learning rates: {len(lrs_tensor)} parameters, avg: {avg_lr:.2e}"
                )

        except Exception as e:
            test_results["error"] = str(e)
            logger.error(f"Error testing enhanced optimizer logging: {e}")

        return test_results


# Global instance for easy access
enhanced_logger = EnhancedOptimizerLogger()


def get_prodigy_d(optimizer: Optimizer) -> float:
    """Extract the 'd' parameter from Prodigy optimizer for logging.

    Args:
        optimizer: The optimizer instance

    Returns:
        Average 'd' value across all parameter groups
    """
    return enhanced_logger.get_prodigy_d(optimizer)


def get_automagic_lrs(optimizer: Optimizer) -> Tuple[torch.Tensor, float]:
    """Extract learning rates from Automagic optimizer for logging.

    Args:
        optimizer: The Automagic optimizer instance

    Returns:
        Tuple of (learning_rates_tensor, average_learning_rate)
    """
    return enhanced_logger.get_automagic_lrs(optimizer)


def get_enhanced_metrics(optimizer: Optimizer) -> Dict[str, float]:
    """Get enhanced metrics for the given optimizer.

    Args:
        optimizer: The optimizer instance

    Returns:
        Dict containing enhanced metrics for supported optimizers
    """
    return enhanced_logger.get_enhanced_metrics(optimizer)


def get_histogram_data(optimizer: Optimizer) -> Optional[Tuple[str, torch.Tensor]]:
    """Get histogram data for optimizers that support it.

    Args:
        optimizer: The optimizer instance

    Returns:
        Tuple of (metric_name, tensor_data) if supported, None otherwise
    """
    return enhanced_logger.get_histogram_data(optimizer)


def is_supported(optimizer: Optimizer) -> bool:
    """Check if the optimizer supports enhanced logging.

    Args:
        optimizer: The optimizer instance

    Returns:
        True if enhanced logging is supported, False otherwise
    """
    return enhanced_logger.is_supported(optimizer)
</file>

<file path="optimizers/fira_optimizer.py">
"""Fira optimizer implementation for WAN network trainer.

This module provides Fira and FiraPT optimizer implementations with proper
parameter group handling and Fira-specific parameter management.
"""

import logging
from typing import Any, Dict, List, Optional, Tuple, Union
import torch

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class FiraOptimizerManager:
    """Handles Fira optimizer creation and configuration."""

    @staticmethod
    def create_fira_optimizer(
        args: Any,
        transformer: torch.nn.Module,  # We still receive the transformer for context
        trainable_params: List[torch.nn.Parameter],
        lr: float,
        optimizer_kwargs: Dict[str, Any],
    ) -> Tuple[Any, Dict[str, Any]]:
        """Create Fira optimizer using fira.FiraAdamW with enhanced logging and parameter handling."""
        from fira import FiraAdamW, divide_params

        optimizer_class = FiraAdamW

        # ====================================================================
        # The `transformer` is frozen. Gradients are only computed for the LoRA
        # `network` parameters, which are passed in via `trainable_params`.
        #
        # The Fira documentation example performs full fine-tuning, so all model
        # parameters get gradients. Here, we must only give the optimizer the
        # parameters that are actually trainable.
        #
        # The `divide_params` function adds Fira-specific attributes to the
        # parameters. To use it correctly in a LoRA context, we must run it on
        # the trainable network module, NOT the frozen base model.
        # ====================================================================

        # Enhanced logging from original method
        logger.info(f"Model type: {type(transformer).__name__}")
        logger.info(f"Learning rate: {lr}")
        logger.info(f"Network dimension: {args.network_dim}")

        # Simple and Correct Fix:
        # Use the already prepared trainable_params. The Fira optimizer will apply its
        # logic to these parameters. We will add the fira-specific args to the groups.
        param_groups = trainable_params

        # Fira-specific parameters (similar to FiraPT method)
        fira_params = {
            "rank": args.network_dim,
            "update_proj_gap": 50,
            "alpha": 1.0,
            "proj_type": "std",
        }

        # Add Fira-specific parameters to each group
        for group in param_groups:
            if isinstance(group, dict):
                group.update(fira_params)  # type: ignore

        logger.info(f"Fira parameters configured: {fira_params}")
        logger.info(
            f"Configuring Fira with {len(param_groups)} trainable parameter groups."
        )

        # Enhanced parameter group logging (from original method)
        total_params = 0
        if param_groups:
            for i, group in enumerate(param_groups):
                if isinstance(group, dict) and "params" in group:
                    # Handle the params correctly - they might be a dict_values object
                    params_obj = group.get("params", [])

                    # Convert to list of parameters if it's a dict_values or dict
                    if hasattr(params_obj, "values"):
                        # It's a dict or dict_values object
                        params_list = list(params_obj.values())
                    elif isinstance(params_obj, list):
                        # It's already a list
                        params_list = params_obj
                    else:
                        # Fallback
                        params_list = (
                            list(params_obj)
                            if hasattr(params_obj, "__iter__")
                            else [params_obj]
                        )

                    # Update the group with the corrected params list
                    if isinstance(group, dict):
                        group["params"] = params_list  # type: ignore
                    total_params += len(params_list)

                    logger.info(f"  Group {i}: {len(params_list)} parameters")
                    logger.info(
                        f"  Group {i} keys: {list(group.keys()) if isinstance(group, dict) else 'N/A'}"
                    )

                    # Log parameter types for debugging
                    if params_list:
                        param_types = set(type(p).__name__ for p in params_list)
                        logger.info(f"  Group {i} parameter types: {param_types}")
                else:
                    logger.info(
                        f"  Group {i}: {type(group)} - {len(group) if hasattr(group, '__len__') else 'unknown'}"
                    )

        logger.info(f"Total trainable parameters: {total_params}")

        # Create optimizer with proper arguments
        try:
            optimizer = optimizer_class(param_groups, lr=lr, **optimizer_kwargs)  # type: ignore
            logger.info(f"Successfully created {optimizer_class.__name__} optimizer")
        except Exception as e:
            logger.error(f"Failed to create FiraAdamW optimizer: {e}")
            raise e

        # Initialize Fira-specific state if needed (from original method)
        init_state_method = getattr(optimizer, "init_state", None)
        if init_state_method is not None and callable(init_state_method):
            logger.info("Initializing Fira-specific state...")
            init_state_method()
            logger.info("Fira state initialization completed")

        # Log optimizer state for debugging
        logger.info(f"Optimizer state keys: {list(optimizer.state_dict().keys())}")
        logger.info(f"Optimizer parameter groups: {len(optimizer.param_groups)}")

        return optimizer, {"train_fn": lambda: None, "eval_fn": lambda: None}
</file>

<file path="optimizers/fourier_loss.py">
## Based on: https://github.com/hinablue/sd-scripts/blob/sd3-hina/library/fourier_loss.py (Apache)

"""
Fourier loss function module

This module provides various Fourier domain loss functions to enhance the frequency domain representation capability of deep learning models.
Main features include:
- Basic Fourier loss
- Frequency-weighted Fourier loss
- Multi-scale Fourier loss
- Adaptive Fourier loss
- Composite loss functions
- Settings and utility tools
"""

import torch
import logging
from typing import Optional, List, Dict, Any, Tuple
import json
from torch.fft import fftfreq, fftn
import torch.nn.functional as F


logger = logging.getLogger(__name__)


def create_frequency_weight_mask(
    height: int,
    width: int,
    high_freq_weight: float = 2.0,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
) -> torch.Tensor:
    """
    Create a frequency weight mask, giving higher weights to high-frequency components

    Args:
        height: Height dimension
        width: Width dimension
        high_freq_weight: High-frequency weight multiplier
        device: Tensor device
        dtype: Tensor data type

    Returns:
        Frequency weight mask tensor
    """
    # Limit weight range to prevent excessive amplification
    high_freq_weight = max(1.0, min(high_freq_weight, 3.0))

    # Create frequency coordinates
    freq_h_kwargs = {}
    freq_w_kwargs = {}
    if device is not None:
        freq_h_kwargs["device"] = device
        freq_w_kwargs["device"] = device
    if dtype is not None:
        freq_h_kwargs["dtype"] = dtype
        freq_w_kwargs["dtype"] = dtype
    freq_h = fftfreq(height, **freq_h_kwargs)
    freq_w = fftfreq(width, **freq_w_kwargs)

    # Create 2D frequency grid
    freq_h_grid, freq_w_grid = torch.meshgrid(freq_h, freq_w, indexing="ij")

    # Calculate frequency magnitude
    freq_magnitude = torch.sqrt(freq_h_grid**2 + freq_w_grid**2)

    # Normalize to [0, 1] range, add smoothing
    max_freq = freq_magnitude.max()
    if max_freq > 0:
        freq_magnitude = freq_magnitude / max_freq
    else:
        freq_magnitude = torch.zeros_like(freq_magnitude)

    # Use a smoother weighting (sigmoid function instead of linear)
    # This reduces extreme weight values
    sigmoid_factor = 4.0  # Controls the steepness of the transition
    freq_sigmoid = torch.sigmoid(sigmoid_factor * (freq_magnitude - 0.5))

    # Create weights: low frequency is 1.0, high frequency gradually increases to high_freq_weight
    weight_mask = 1.0 + (high_freq_weight - 1.0) * freq_sigmoid

    return weight_mask


def compute_fourier_magnitude_spectrum(
    tensor: torch.Tensor,
    dims: tuple = (-2, -1),
    eps: float = 1e-8,
    normalize: bool = True,
) -> torch.Tensor:
    """
    Compute the Fourier magnitude spectrum of a tensor

    Args:
        tensor: Input tensor
        dims: Dimensions for FFT
        eps: Numerical stability constant
        normalize: Whether to normalize the magnitude spectrum

    Returns:
        Fourier magnitude spectrum
    """
    # Compute multi-dimensional FFT
    fft_result = fftn(tensor, dim=dims)

    # Compute magnitude spectrum and add numerical stability
    magnitude = torch.abs(fft_result) + eps

    if normalize:
        # Normalize: divide by the square root of tensor size and max value
        tensor_numel = 1
        for dim in dims:
            tensor_numel *= tensor.shape[dim]

        # Normalize by tensor size (similar to FFTW normalization)
        magnitude = magnitude / (tensor_numel**0.5)

        # Further normalize by the input tensor's value range
        input_scale = torch.std(tensor) + eps
        magnitude = magnitude / input_scale

    return magnitude


def fourier_latent_loss_basic(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    norm_type: str = "l2",
    dims: tuple = (-2, -1),
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Basic Fourier latent loss calculation

    Args:
        model_pred: Model prediction (z_SR)
        target: Target (z_HR)
        norm_type: Loss norm type ("l1" or "l2")
        dims: FFT calculation dimensions
        eps: Numerical stability constant

    Returns:
        Fourier feature loss
    """
    # Compute Fourier magnitude spectrum (normalized)
    mag_pred = compute_fourier_magnitude_spectrum(model_pred, dims, eps, normalize=True)
    mag_target = compute_fourier_magnitude_spectrum(target, dims, eps, normalize=True)

    # Compute loss
    if norm_type == "l1":
        loss = torch.mean(torch.abs(mag_target - mag_pred))
    elif norm_type == "l2":
        loss = torch.mean((mag_target - mag_pred) ** 2)
    else:
        raise ValueError(f"Unsupported norm_type: {norm_type}")

    # Further constrain loss range to prevent outliers
    loss = torch.clamp(
        loss, max=torch.tensor(10.0, device=loss.device, dtype=loss.dtype)
    )

    return loss


def fourier_latent_loss_weighted(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    high_freq_weight: float = 2.0,
    dims: tuple = (-2, -1),
    norm_type: str = "l2",
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Frequency-weighted Fourier latent loss

    Args:
        model_pred: Model prediction (z_SR)
        target: Target (z_HR)
        high_freq_weight: High-frequency component weight multiplier
        dims: FFT calculation dimensions
        norm_type: Loss norm type
        eps: Numerical stability constant

    Returns:
        Weighted Fourier feature loss
    """
    # Limit high-frequency weight range to prevent excessive amplification
    high_freq_weight = torch.clamp(
        torch.tensor(high_freq_weight), min=1.0, max=3.0
    ).item()

    # Compute Fourier magnitude spectrum (normalized)
    mag_pred = compute_fourier_magnitude_spectrum(model_pred, dims, eps, normalize=True)
    mag_target = compute_fourier_magnitude_spectrum(target, dims, eps, normalize=True)

    # Create frequency weight mask
    height, width = model_pred.shape[dims[0]], model_pred.shape[dims[1]]
    weight_mask = create_frequency_weight_mask(
        height,
        width,
        high_freq_weight,
        device=model_pred.device,
        dtype=model_pred.dtype,
    )

    # Expand weight mask to match tensor shape
    while weight_mask.dim() < mag_pred.dim():
        weight_mask = weight_mask.unsqueeze(0)

    # Compute weighted difference
    diff = mag_target - mag_pred
    if norm_type == "l1":
        weighted_diff = torch.abs(diff) * weight_mask
    elif norm_type == "l2":
        weighted_diff = (diff**2) * weight_mask
    else:
        raise ValueError(f"Unsupported norm_type: {norm_type}")

    # Compute weighted average loss
    loss = torch.mean(weighted_diff)

    # Further constrain loss range to prevent outliers
    loss = torch.clamp(
        loss, max=torch.tensor(10.0, device=loss.device, dtype=loss.dtype)
    )

    return loss


def fourier_latent_loss_multiscale(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    scales: Optional[List[int]] = None,
    scale_weights: Optional[List[float]] = None,
    dims: tuple = (-2, -1),
    norm_type: str = "l2",
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Multi-scale Fourier latent loss

    Args:
        model_pred: Model prediction (z_SR)
        target: Target (z_HR)
        scales: Multiple scaling factors
        scale_weights: Weights for each scale (if None, calculated automatically)
        dims: FFT calculation dimensions
        norm_type: Loss norm type
        eps: Numerical stability constant

    Returns:
        Multi-scale Fourier feature loss
    """
    if scales is None:
        scales = [1, 2]

    if scale_weights is None:
        scale_weights = [1.0 / scale for scale in scales]

    if len(scale_weights) != len(scales):
        raise ValueError("scale_weights length must match scales length")

    total_loss = 0.0
    total_weight = 0.0

    for scale, weight in zip(scales, scale_weights):
        if scale == 1:
            pred_scaled = model_pred
            target_scaled = target
        else:
            # Check if tensor dimensions are sufficient for pooling
            if (
                model_pred.dim() >= 4
                and model_pred.shape[-1] >= scale
                and model_pred.shape[-2] >= scale
            ):
                # Use average pooling for downsampling
                pred_scaled = F.avg_pool2d(model_pred, scale)
                target_scaled = F.avg_pool2d(target, scale)
            else:
                # Skip invalid scale
                continue

        # Compute Fourier loss at this scale
        scale_loss = fourier_latent_loss_basic(
            pred_scaled, target_scaled, norm_type, dims, eps
        )

        total_loss += weight * scale_loss
        total_weight += weight

    # Prevent division by zero
    if total_weight == 0:
        return torch.tensor(0.0, device=model_pred.device, dtype=model_pred.dtype)

    # Constrain final loss value
    final_loss = total_loss / total_weight
    if not isinstance(final_loss, torch.Tensor):
        final_loss = torch.tensor(
            final_loss, device=model_pred.device, dtype=model_pred.dtype
        )
    final_loss = torch.clamp(
        final_loss,
        max=torch.tensor(10.0, device=final_loss.device, dtype=final_loss.dtype),
    )

    return final_loss


def fourier_latent_loss_adaptive(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    current_step: int,
    total_steps: int,
    max_weight: float = 2.0,
    min_weight: float = 0.5,
    dims: tuple = (-2, -1),
    norm_type: str = "l2",
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Adaptive Fourier latent loss (weight adjusts with training progress)

    Args:
        model_pred: Model prediction (z_SR)
        target: Target (z_HR)
        current_step: Current training step
        total_steps: Total training steps
        max_weight: Maximum high-frequency weight
        min_weight: Minimum high-frequency weight
        dims: FFT calculation dimensions
        norm_type: Loss norm type
        eps: Numerical stability constant

    Returns:
        Adaptive Fourier feature loss
    """
    # Limit weight range
    max_weight = max(1.0, min(max_weight, 3.0))
    min_weight = max(0.5, min(min_weight, max_weight))

    # Calculate training progress (0.0 to 1.0)
    progress = min(current_step / max(total_steps, 1), 1.0)

    # Early training emphasizes high frequency, later gradually balances
    high_freq_weight = max_weight - (max_weight - min_weight) * progress

    return fourier_latent_loss_weighted(
        model_pred, target, high_freq_weight, dims, norm_type, eps
    )


def conditional_loss_with_fourier(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    loss_type: str,
    reduction: str,
    huber_c: Optional[torch.Tensor] = None,
    current_step: int = 0,
    total_steps: int = 1000,
    # Fourier feature loss parameters
    fourier_weight: float = 0.05,
    fourier_mode: str = "weighted",  # "basic", "weighted", "multiscale", "adaptive"
    fourier_norm: str = "l2",
    fourier_dims: tuple = (-2, -1),
    fourier_high_freq_weight: float = 2.0,
    fourier_scales: Optional[List[int]] = None,
    fourier_scale_weights: Optional[List[float]] = None,
    fourier_adaptive_max_weight: float = 2.0,
    fourier_adaptive_min_weight: float = 0.5,
    fourier_eps: float = 1e-8,
    fourier_warmup_steps: int = 200,
) -> torch.Tensor:
    """
    Enhanced conditional_loss supporting Fourier feature loss

    Args:
        model_pred: Model prediction tensor
        target: Target tensor
        loss_type: Base loss type ("fourier")
        reduction: Loss reduction method ("mean", "sum", "none")
        huber_c: Huber loss parameter
        current_step: Current training step
        total_steps: Total training steps

        # Fourier feature loss parameters
        fourier_weight: Fourier loss weight
        fourier_mode: Fourier loss mode
        fourier_norm: Fourier loss norm ("l1" or "l2")
        fourier_dims: FFT calculation dimensions
        fourier_high_freq_weight: High-frequency weight multiplier (weighted mode)
        fourier_scales: Multi-scale list (multiscale mode)
        fourier_scale_weights: Scale weight list (multiscale mode)
        fourier_adaptive_max_weight: Adaptive max weight (adaptive mode)
        fourier_adaptive_min_weight: Adaptive min weight (adaptive mode)
        fourier_eps: Numerical stability constant
        fourier_warmup_steps: Fourier loss warmup steps

    Returns:
        Composite loss value
    """

    # Compute base loss
    if fourier_norm == "l1":
        base_loss = torch.nn.functional.l1_loss(model_pred, target, reduction=reduction)
    else:
        base_loss = torch.nn.functional.mse_loss(
            model_pred, target, reduction=reduction
        )

    # If not fourier loss or weight is 0, return base loss directly
    if loss_type != "fourier" or fourier_weight <= 0.0:
        return base_loss

    # If weight is 0, return base loss directly
    if fourier_weight <= 0.0:
        return base_loss

    # If within warmup period, return base loss directly
    if current_step < fourier_warmup_steps:
        return base_loss

    # Check if tensor dimensions are sufficient
    if model_pred.dim() < 3 or target.dim() < 3:
        logger = logging.getLogger(__name__)
        logger.warning(
            f"Fourier loss requires at least 3D tensors, got {model_pred.dim()}D and {target.dim()}D tensors, skipping Fourier loss calculation"
        )
        return base_loss

    # Ensure tensor shapes match
    if model_pred.shape != target.shape:
        logger = logging.getLogger(__name__)
        logger.warning(
            f"model_pred and target shapes do not match: {model_pred.shape} vs {target.shape}, skipping Fourier loss calculation"
        )
        return base_loss

    try:
        # Compute Fourier loss based on mode
        if fourier_mode == "basic":
            fourier_loss = fourier_latent_loss_basic(
                model_pred, target, fourier_norm, fourier_dims, fourier_eps
            )
        elif fourier_mode == "weighted":
            fourier_loss = fourier_latent_loss_weighted(
                model_pred,
                target,
                fourier_high_freq_weight,
                fourier_dims,
                fourier_norm,
                fourier_eps,
            )
        elif fourier_mode == "multiscale":
            if fourier_scales is None:
                fourier_scales = [1, 2]
            fourier_loss = fourier_latent_loss_multiscale(
                model_pred,
                target,
                fourier_scales,
                fourier_scale_weights,
                fourier_dims,
                fourier_norm,
                fourier_eps,
            )
        elif fourier_mode == "adaptive":
            fourier_loss = fourier_latent_loss_adaptive(
                model_pred,
                target,
                current_step,
                total_steps,
                fourier_adaptive_max_weight,
                fourier_adaptive_min_weight,
                fourier_dims,
                fourier_norm,
                fourier_eps,
            )
        elif fourier_mode == "unified":
            # Use unified mode, support extra parameters
            unified_kwargs = {}
            if fourier_scales is not None:
                unified_kwargs["scales"] = fourier_scales
            if fourier_scale_weights is not None:
                unified_kwargs["scale_weights"] = fourier_scale_weights

            fourier_loss = fourier_latent_loss_unified(
                model_pred,
                target,
                dims=fourier_dims,
                norm_type=fourier_norm,
                eps=fourier_eps,
                high_freq_weight=fourier_high_freq_weight,
                current_step=current_step,
                total_steps=total_steps,
                max_weight=fourier_adaptive_max_weight,
                min_weight=fourier_adaptive_min_weight,
                **unified_kwargs,
            )
        elif fourier_mode in [
            "unified_basic",
            "unified_balanced",
            "unified_detail",
            "unified_adaptive",
        ]:
            # Use simplified unified mode
            mode_map = {
                "unified_basic": "basic",
                "unified_balanced": "balanced",
                "unified_detail": "detail",
                "unified_adaptive": "adaptive",
            }
            fourier_loss = fourier_latent_loss_unified_simple(
                model_pred,
                target,
                mode=mode_map[fourier_mode],
                current_step=current_step,
                total_steps=total_steps,
            )
        else:
            raise ValueError(f"Unsupported fourier_mode: {fourier_mode}")

        # Dynamically adjust Fourier loss weight to avoid large gap with base loss
        # Safely convert loss to scalar for comparison
        try:
            # Ensure base loss is scalar
            if base_loss.numel() > 1:
                base_loss_magnitude = base_loss.detach().mean().item()
            else:
                base_loss_magnitude = base_loss.detach().item()

            # Ensure Fourier loss is scalar
            if fourier_loss.numel() > 1:
                fourier_loss_magnitude = fourier_loss.detach().mean().item()
            else:
                fourier_loss_magnitude = fourier_loss.detach().item()

            # Compute adaptive weight to ensure Fourier loss does not overwhelm base loss
            adaptive_weight = fourier_weight
            if (
                fourier_loss_magnitude > 0
                and base_loss_magnitude > 0
                and not (
                    torch.isnan(torch.tensor(fourier_loss_magnitude))
                    or torch.isnan(torch.tensor(base_loss_magnitude))
                )
            ):
                ratio = fourier_loss_magnitude / base_loss_magnitude
                if ratio > 10.0:  # If Fourier loss is too large, reduce weight
                    adaptive_weight = fourier_weight / (ratio / 10.0)
                    adaptive_weight = max(adaptive_weight, fourier_weight * 0.1)
        except (RuntimeError, ValueError, AttributeError) as e:
            # If unable to get scalar value, use original weight
            logger = logging.getLogger(__name__)
            logger.debug(
                f"Unable to compute adaptive weight, using original weight: {e}"
            )
            adaptive_weight = fourier_weight

        # Combine base loss and Fourier loss
        total_loss = base_loss + adaptive_weight * fourier_loss

        return total_loss

    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(
            f"Fourier loss calculation failed: {e}, falling back to base loss"
        )
        return base_loss


def fourier_latent_loss_unified(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    # Basic parameters
    dims: tuple = (-2, -1),
    norm_type: str = "l2",
    eps: float = 1e-8,
    # Multi-scale parameters
    scales: Optional[List[int]] = None,
    scale_weights: Optional[List[float]] = None,
    enable_multiscale: bool = True,
    # Frequency weighting parameters
    enable_frequency_weighting: bool = True,
    high_freq_weight: float = 2.0,
    freq_weight_per_scale: Optional[List[float]] = None,  # Frequency weight per scale
    # Adaptive parameters
    enable_adaptive: bool = True,
    current_step: int = 0,
    total_steps: int = 1000,
    adaptive_mode: str = "linear",  # "linear", "cosine", "exponential"
    max_weight: float = 2.5,
    min_weight: float = 0.8,
    # Integration strategy parameters
    multiscale_weight: float = 0.6,  # Multi-scale loss weight
    weighted_weight: float = 0.4,  # Weighted loss weight
    adaptive_scaling: bool = True,  # Whether to adaptively scale weights
) -> torch.Tensor:
    """
    Unified Fourier latent loss calculation

    Combines multi-scale, frequency weighting, and adaptive strategies in a unified implementation

    Args:
        model_pred: Model prediction tensor
        target: Target tensor
        dims: FFT calculation dimensions
        norm_type: Loss norm type
        eps: Numerical stability constant

        # Multi-scale parameters
        scales: Multi-scale list
        scale_weights: Weights for each scale
        enable_multiscale: Whether to enable multi-scale

        # Frequency weighting parameters
        enable_frequency_weighting: Whether to enable frequency weighting
        high_freq_weight: Base high-frequency weight
        freq_weight_per_scale: Frequency weight override per scale

        # Adaptive parameters
        enable_adaptive: Whether to enable adaptation
        current_step: Current step
        total_steps: Total steps
        adaptive_mode: Adaptive mode
        max_weight: Maximum weight
        min_weight: Minimum weight

        # Integration strategy parameters
        multiscale_weight: Multi-scale component weight
        weighted_weight: Weighted component weight
        adaptive_scaling: Whether to adaptively scale combined weights

    Returns:
        Integrated Fourier loss
    """

    # Parameter validation and default settings
    if scales is None:
        scales = [1, 2, 4] if enable_multiscale else [1]

    if scale_weights is None:
        # Dynamically calculate scale weights, larger scales get smaller weights
        scale_weights = [1.0 / (scale**0.5) for scale in scales]
        # Normalize weights
        total_scale_weight = sum(scale_weights)
        scale_weights = [w / total_scale_weight for w in scale_weights]

    if freq_weight_per_scale is None:
        freq_weight_per_scale = [high_freq_weight] * len(scales)
    elif len(freq_weight_per_scale) != len(scales):
        # Extend or truncate to correct length
        freq_weight_per_scale = (
            freq_weight_per_scale + [high_freq_weight] * len(scales)
        )[: len(scales)]

    # Compute adaptive weight factor
    adaptive_factor = 1.0
    if enable_adaptive:
        # Calculate training progress
        progress = min(current_step / max(total_steps, 1), 1.0)

        # Compute adaptive factor based on different modes
        if adaptive_mode == "linear":
            # Linear decay: from max_weight to min_weight
            adaptive_factor = max_weight - (max_weight - min_weight) * progress
        elif adaptive_mode == "cosine":
            # Cosine decay: smoother transition
            import math

            adaptive_factor = min_weight + (max_weight - min_weight) * 0.5 * (
                1 + math.cos(math.pi * progress)
            )
        elif adaptive_mode == "exponential":
            # Exponential decay: fast drop early, slow later
            import math

            adaptive_factor = min_weight + (max_weight - min_weight) * math.exp(
                -5 * progress
            )
        else:
            raise ValueError(f"Unsupported adaptive_mode: {adaptive_mode}")

    # Compute multi-scale loss component
    multiscale_loss = 0.0
    if enable_multiscale and len(scales) > 1:
        total_loss = 0.0
        total_weight = 0.0

        for i, (scale, scale_weight) in enumerate(zip(scales, scale_weights)):
            # Get tensor at this scale
            if scale == 1:
                pred_scaled = model_pred
                target_scaled = target
            else:
                # Check tensor dimensions
                if (
                    model_pred.dim() >= 4
                    and model_pred.shape[-1] >= scale
                    and model_pred.shape[-2] >= scale
                ):
                    pred_scaled = F.avg_pool2d(model_pred, scale)
                    target_scaled = F.avg_pool2d(target, scale)
                else:
                    continue

            # Compute frequency-weighted loss at this scale
            if enable_frequency_weighting:
                current_freq_weight = freq_weight_per_scale[i] * adaptive_factor
                scale_loss = fourier_latent_loss_weighted(
                    pred_scaled,
                    target_scaled,
                    current_freq_weight,
                    dims,
                    norm_type,
                    eps,
                )
            else:
                scale_loss = fourier_latent_loss_basic(
                    pred_scaled, target_scaled, norm_type, dims, eps
                )

            total_loss += scale_weight * scale_loss
            total_weight += scale_weight

        if total_weight > 0:
            multiscale_loss = total_loss / total_weight

    # Compute single-scale weighted loss component (base scale)
    weighted_loss = 0.0
    if enable_frequency_weighting:
        current_freq_weight = high_freq_weight * adaptive_factor
        weighted_loss = fourier_latent_loss_weighted(
            model_pred, target, current_freq_weight, dims, norm_type, eps
        )
    else:
        weighted_loss = fourier_latent_loss_basic(
            model_pred, target, norm_type, dims, eps
        )

    # Combine losses
    if enable_multiscale and len(scales) > 1:
        # Adaptively adjust combined weights
        if adaptive_scaling:
            # Adjust multi-scale and weighted loss ratio based on training progress
            progress = min(current_step / max(total_steps, 1), 1.0)
            # Early: emphasize multi-scale, later: emphasize detail
            current_multiscale_weight = multiscale_weight * (
                1.0 + 0.5 * (1.0 - progress)
            )
            current_weighted_weight = weighted_weight * (1.0 + 0.5 * progress)

            # Normalize weights
            total_weight = current_multiscale_weight + current_weighted_weight
            current_multiscale_weight /= total_weight
            current_weighted_weight /= total_weight
        else:
            current_multiscale_weight = multiscale_weight
            current_weighted_weight = weighted_weight

        final_loss = (
            current_multiscale_weight * multiscale_loss
            + current_weighted_weight * weighted_loss
        )
    else:
        # If no multi-scale, use only weighted loss
        final_loss = weighted_loss

    # Final constraint
    final_loss = torch.clamp(
        final_loss,
        max=torch.tensor(10.0, device=final_loss.device, dtype=final_loss.dtype),
    )

    return final_loss


def get_fourier_loss_unified_config(mode: str = "balanced") -> Dict[str, Any]:
    """
    Get default unified Fourier loss settings
    """
    # Default configs
    configs = {
        "basic": {
            "enable_multiscale": False,
            "enable_frequency_weighting": True,
            "enable_adaptive": True,
            "high_freq_weight": 1.5,
            "adaptive_mode": "linear",
            "max_weight": 2.0,
            "min_weight": 1.0,
        },
        "balanced": {
            "enable_multiscale": True,
            "enable_frequency_weighting": True,
            "enable_adaptive": True,
            "scales": [1, 2],
            "high_freq_weight": 2.0,
            "adaptive_mode": "linear",
            "max_weight": 2.5,
            "min_weight": 0.8,
            "multiscale_weight": 0.6,
            "weighted_weight": 0.4,
        },
        "detail": {
            "enable_multiscale": True,
            "enable_frequency_weighting": True,
            "enable_adaptive": True,
            "scales": [1, 2, 4],
            "high_freq_weight": 2.5,
            "freq_weight_per_scale": [2.0, 2.5, 3.0],
            "adaptive_mode": "cosine",
            "max_weight": 3.0,
            "min_weight": 1.0,
            "multiscale_weight": 0.7,
            "weighted_weight": 0.3,
        },
        "adaptive": {
            "enable_multiscale": True,
            "enable_frequency_weighting": True,
            "enable_adaptive": True,
            "scales": [1, 2],
            "adaptive_mode": "exponential",
            "max_weight": 2.8,
            "min_weight": 0.5,
            "adaptive_scaling": True,
        },
    }

    if mode not in configs:
        raise ValueError(f"Unknown mode: {mode}. Available: {list(configs.keys())}")

    return configs[mode]


def fourier_latent_loss_unified_simple(
    model_pred: torch.Tensor,
    target: torch.Tensor,
    mode: str = "balanced",
    current_step: int = 0,
    total_steps: int = 1000,
    **kwargs,
) -> torch.Tensor:
    """
    Simplified unified Fourier loss with default configs

    Args:
        model_pred: Model prediction tensor
        target: Target tensor
        mode: Default mode
            - "basic": Basic mode, mainly uses single-scale weighting
            - "balanced": Balanced mode, combines multi-scale and weighting
            - "detail": Detail mode, emphasizes high frequency and multi-scale
            - "adaptive": Adaptive mode, emphasizes dynamic adjustment
        current_step: Current step
        total_steps: Total steps
        **kwargs: Other parameter overrides

    Returns:
        Fourier loss
    """

    # Merge config and user parameters
    config = get_fourier_loss_unified_config(mode).copy()
    config.update(kwargs)

    return fourier_latent_loss_unified(
        model_pred, target, current_step=current_step, total_steps=total_steps, **config
    )


# Convenience function: default settings


def get_fourier_loss_config(mode: str = "balanced") -> Dict[str, Any]:
    """
    Get default Fourier loss settings

    Args:
        mode: Setting mode
            - "conservative": Conservative setting, smaller Fourier weight
            - "balanced": Balanced setting, medium Fourier weight
            - "aggressive": Aggressive setting, larger Fourier weight
            - "super_resolution": For super-resolution tasks
            - "fine_detail": Focus on detail enhancement

    Returns:
        Settings dictionary
    """
    configs = {
        "conservative": {
            "fourier_weight": 0.01,
            "fourier_mode": "basic",
            "fourier_norm": "l2",
            "fourier_high_freq_weight": 1.5,
            "fourier_warmup_steps": 500,
        },
        "balanced": {
            "fourier_weight": 0.05,
            "fourier_mode": "weighted",
            "fourier_norm": "l2",
            "fourier_high_freq_weight": 2.0,
            "fourier_warmup_steps": 300,
        },
        "aggressive": {
            "fourier_weight": 0.1,
            "fourier_mode": "multiscale",
            "fourier_norm": "l1",
            "fourier_scales": [1, 2, 4],
            "fourier_warmup_steps": 200,
        },
        "super_resolution": {
            "fourier_weight": 0.08,
            "fourier_mode": "adaptive",
            "fourier_norm": "l2",
            "fourier_adaptive_max_weight": 3.0,
            "fourier_adaptive_min_weight": 1.0,
            "fourier_warmup_steps": 400,
        },
        "fine_detail": {
            "fourier_weight": 0.12,
            "fourier_mode": "weighted",
            "fourier_norm": "l1",
            "fourier_high_freq_weight": 2.5,
            "fourier_warmup_steps": 100,
        },
        "unified_balanced": {
            "fourier_weight": 0.06,
            "fourier_mode": "unified_balanced",
            "fourier_norm": "l2",
            "fourier_warmup_steps": 250,
        },
        "unified_detail": {
            "fourier_weight": 0.08,
            "fourier_mode": "unified_detail",
            "fourier_norm": "l2",
            "fourier_warmup_steps": 200,
        },
        "unified_adaptive": {
            "fourier_weight": 0.07,
            "fourier_mode": "unified_adaptive",
            "fourier_norm": "l2",
            "fourier_warmup_steps": 300,
        },
        "unified_custom": {
            "fourier_weight": 0.05,
            "fourier_mode": "unified",
            "fourier_norm": "l2",
            "fourier_high_freq_weight": 2.0,
            "fourier_scales": [1, 2, 4],
            "fourier_adaptive_max_weight": 2.5,
            "fourier_adaptive_min_weight": 0.8,
            "fourier_warmup_steps": 250,
        },
    }

    if mode not in configs:
        raise ValueError(
            f"Unknown mode: {mode}. Available modes: {list(configs.keys())}"
        )

    return configs[mode]


# Convenience function for training scripts


def apply_fourier_loss_to_args(args, quick_mode: str = "balanced"):
    """
    Apply Fourier loss settings to training arguments

    Args:
        args: Training arguments object
        quick_mode: Setting mode
    """
    quick_mode = (
        quick_mode
        if quick_mode
        in [
            "conservative",
            "balanced",
            "aggressive",
            "super_resolution",
            "fine_detail",
            "unified_balanced",
            "unified_detail",
            "unified_adaptive",
            "unified_custom",
        ]
        else "balanced"
    )

    config = get_fourier_loss_config(quick_mode)

    # Set loss type to fourier
    args.loss_type = "fourier"

    # Set Fourier-related parameters
    for key, value in config.items():
        setattr(args, key, value)

    # Set to default if parameter does not exist
    if hasattr(args, "fourier_weight") is False:
        args.fourier_weight = 0.05
    if hasattr(args, "fourier_mode") is False:
        args.fourier_mode = "weighted"
    if hasattr(args, "fourier_norm") is False:
        args.fourier_norm = "l2"
    if hasattr(args, "fourier_dims") is False:
        args.fourier_dims = (-2, -1)
    if hasattr(args, "fourier_high_freq_weight") is False:
        args.fourier_high_freq_weight = 2.0
    if hasattr(args, "fourier_scales") is False:
        args.fourier_scales = None
    if hasattr(args, "fourier_scale_weights") is False:
        args.fourier_scale_weights = None
    if hasattr(args, "fourier_adaptive_max_weight") is False:
        args.fourier_adaptive_max_weight = 2.0
    if hasattr(args, "fourier_adaptive_min_weight") is False:
        args.fourier_adaptive_min_weight = 0.5
    if hasattr(args, "fourier_eps") is False:
        args.fourier_eps = 1e-8
    if hasattr(args, "fourier_warmup_steps") is False:
        args.fourier_warmup_steps = 300

    if args.fourier_mode in [
        "unified_basic",
        "unified_balanced",
        "unified_detail",
        "unified_adaptive",
    ]:
        # Use simplified unified mode
        mode_map = {
            "unified_basic": "basic",
            "unified_balanced": "balanced",
            "unified_detail": "detail",
            "unified_adaptive": "adaptive",
        }
        args.fourier_unified_config = json.dumps(
            get_fourier_loss_unified_config(mode_map[args.fourier_mode])
        )

    return args
</file>

<file path="optimizers/gradient_release.py">
## Based on: https://github.com/tdrussell/diffusion-pipe/blob/main/optimizers/gradient_release.py (MIT)

import torch


# Simple wrapper for use with gradient release. Grad hooks do the optimizer steps, so this no-ops
# the step() and zero_grad() methods. It also handles state_dict.
class GradientReleaseOptimizerWrapper(torch.optim.Optimizer):
    def __init__(self, optimizers):
        self.optimizers = optimizers

    @property
    def param_groups(self):
        ret = []
        for opt in self.optimizers:
            ret.extend(opt.param_groups)
        return ret

    def state_dict(self):
        return {i: opt.state_dict() for i, opt in enumerate(self.optimizers)}

    def load_state_dict(self, state_dict):
        for i, sd in state_dict.items():
            self.optimizers[i].load_state_dict(sd)

    def step(self):
        pass

    def zero_grad(self, set_to_none=True):
        pass
</file>

<file path="optimizers/hina_adaptive.py">
## Based on: https://github.com/hinablue/sd-scripts/blob/sd3-hina/library/hina_adaptive.py (Apache)

import torch
import torch.nn as nn
from typing import Any, Dict, List, Tuple, Optional
import math
import array
import random
import concurrent.futures
from threading import Thread
from collections import defaultdict
import time

import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class MemoryMonitor:
    """Dynamic memory monitor"""

    def __init__(self, target_vram_gb: float = 16):
        self.target_vram = target_vram_gb * 1024**3
        self.current_usage = 0

    def check_memory_pressure(self) -> float:
        """Check memory pressure ratio"""
        if torch.cuda.is_available():
            current_allocated = torch.cuda.memory_allocated()
            pressure_ratio = current_allocated / self.target_vram
            return pressure_ratio
        return 0.0

    def suggest_optimizations(self, pressure_ratio: float) -> Dict[str, bool]:
        """Suggest optimization strategies based on memory pressure"""
        if pressure_ratio > 0.9:
            return {
                "reduce_buffer_pool": True,
                "increase_gc_frequency": True,
                "use_checkpoint_offload": True,
                "reduce_precision": True,
            }
        elif pressure_ratio > 0.7:
            return {
                "reduce_buffer_pool": True,
                "increase_gc_frequency": False,
                "use_checkpoint_offload": False,
                "reduce_precision": False,
            }
        return {
            "reduce_buffer_pool": False,
            "increase_gc_frequency": False,
            "use_checkpoint_offload": False,
            "reduce_precision": False,
        }


class EnhancedBufferPool:
    """Enhanced buffer pool for intelligent memory management"""

    def __init__(self, max_total_memory_mb: int = 500):
        self._buffer_pool = {}
        self._usage_stats = defaultdict(int)
        self._max_total_memory = max_total_memory_mb * 1024 * 1024
        self._current_memory = 0

    def get_buffer_with_priority(
        self,
        shape: Tuple,
        dtype: torch.dtype,
        device: torch.device,
        priority: str = "normal",
    ) -> torch.Tensor:
        """Get buffer based on priority"""
        key = (shape, dtype, device)
        self._usage_stats[key] += 1

        if key in self._buffer_pool and self._buffer_pool[key]:
            return self._buffer_pool[key].pop()

        # Check memory budget
        tensor_size = torch.prod(torch.tensor(shape)).item() * self._get_dtype_size(
            dtype
        )
        if (
            self._current_memory + tensor_size > self._max_total_memory
            and priority != "critical"
        ):
            # Not enough memory and not critical priority, return new tensor (do not add to pool)
            return torch.empty(shape, dtype=dtype, device=device)

        return torch.empty(shape, dtype=dtype, device=device)

    def return_buffer(self, tensor: torch.Tensor, priority: str = "normal"):
        """Return buffer to pool"""
        key = (tuple(tensor.shape), tensor.dtype, tensor.device)

        if key not in self._buffer_pool:
            self._buffer_pool[key] = []

        # Decide whether to keep based on usage frequency
        usage_freq = self._usage_stats.get(key, 0)
        max_buffers = max(
            1, min(3, usage_freq // 10)
        )  # Dynamically adjust buffer count

        if len(self._buffer_pool[key]) < max_buffers:
            tensor.zero_()
            self._buffer_pool[key].append(tensor)
            tensor_size = torch.prod(
                torch.tensor(tensor.shape)
            ).item() * self._get_dtype_size(tensor.dtype)
            self._current_memory += tensor_size

    def smart_cleanup(self, memory_pressure: float):
        """Smartly clean up buffer pool"""
        if memory_pressure > 0.8:
            # Clean up buffers with low usage frequency
            keys_to_clean = sorted(
                self._usage_stats.keys(), key=lambda k: self._usage_stats[k]
            )[: len(self._usage_stats) // 2]
            for key in keys_to_clean:
                if key in self._buffer_pool:
                    del self._buffer_pool[key]
            self._current_memory = 0  # Reset counter

    @staticmethod
    def _get_dtype_size(dtype: torch.dtype) -> int:
        """Get data type size"""
        size_map = {
            torch.float32: 4,
            torch.float16: 2,
            torch.bfloat16: 2,
            torch.int32: 4,
            torch.int16: 2,
            torch.int8: 1,
            torch.bool: 1,
        }
        return size_map.get(dtype, 4)


class CompactStateDict:
    """Compact state storage"""

    __slots__ = ["tensor_states", "scalar_states", "bool_states", "string_states"]

    def __init__(self):
        self.tensor_states = {}
        self.scalar_states = {}
        self.bool_states = {}
        self.string_states = {}

    def set_tensor(
        self, key: str, value: torch.Tensor, use_half_precision: bool = False
    ):
        """Set tensor state, optionally using half precision"""
        if use_half_precision and value.dtype == torch.float32:
            value = value.to(torch.bfloat16)
        self.tensor_states[key] = value

    def get_tensor(
        self,
        key: str,
        target_dtype: torch.dtype = None,  # type: ignore
        target_device: torch.device = None,  # type: ignore
    ) -> torch.Tensor:
        """Get tensor state, optionally converting to target dtype and device"""
        tensor = self.tensor_states.get(key)
        if tensor is not None:
            if target_dtype is not None and tensor.dtype != target_dtype:
                tensor = tensor.to(target_dtype)
            if target_device is not None and tensor.device != target_device:
                tensor = tensor.to(target_device)
        return tensor  # type: ignore

    def set_scalar(self, key: str, value: float):
        """Set scalar state"""
        self.scalar_states[key] = value

    def get_scalar(self, key: str, default: float = 0.0) -> float:
        """Get scalar state"""
        return self.scalar_states.get(key, default)


class CompressedRelationships:
    """Compressed parameter relationship storage"""

    def __init__(self):
        self.param_pairs = []
        self.compatibility_scores = torch.tensor([], dtype=torch.float16)
        self.interaction_types = []
        self._type_pool = {
            "matmul_12": 0,
            "matmul_21": 1,
            "matmul_12t": 2,
            "matmul_1t2": 3,
            "norm_based": 4,
        }
        self._reverse_type_pool = {v: k for k, v in self._type_pool.items()}

    def add_relationship(
        self,
        param1_id: int,
        param2_id: int,
        compatibility: float,
        interaction_type: str,
    ):
        """Add parameter relationship"""
        self.param_pairs.append((param1_id, param2_id))

        # Expand compatibility score tensor
        new_score = torch.tensor([compatibility], dtype=torch.float16)
        self.compatibility_scores = torch.cat([self.compatibility_scores, new_score])

        # Use type pool
        type_id = self._type_pool.get(interaction_type, 4)
        self.interaction_types.append(type_id)

    def get_relationship(self, param1_id: int) -> Optional[Dict]:
        """Get parameter relationship"""
        for i, (p1_id, p2_id) in enumerate(self.param_pairs):
            if p1_id == param1_id:
                return {
                    "partner_id": p2_id,
                    "compatibility": self.compatibility_scores[i].item(),
                    "interaction_type": self._reverse_type_pool[
                        self.interaction_types[i]
                    ],
                }
        return None


@torch.jit.script
def quantize_importance_score(score: float) -> int:
    """Quantize importance score to int16"""
    return int(torch.clamp(torch.round(torch.tensor(score * 6553.5)), 0, 65535).item())


@torch.jit.script
def dequantize_importance_score(quantized: int) -> float:
    """Dequantize importance score"""
    return float(quantized) / 6553.5


@torch.jit.script
def compute_lr_mask_update_core(
    lr_mask: torch.Tensor,
    sign_agree: torch.Tensor,
    lr_bump: float,
    min_lr: float,
    max_lr: float,
) -> torch.Tensor:
    """JIT-compiled core logic for lr_mask update"""
    lr_adjustment = torch.where(sign_agree > 0, lr_bump, -lr_bump)
    new_lr_mask = lr_mask + lr_adjustment
    return torch.clamp(new_lr_mask, min=min_lr, max=max_lr)


@torch.jit.script
def orthogonal_gradient_core_optimized(
    grad_flat: torch.Tensor, param_flat: torch.Tensor, eps: float
) -> torch.Tensor:
    """Optimized orthogonal gradient projection core computation"""
    grad_norm = torch.norm(grad_flat, p=2)
    if grad_norm <= eps:
        return grad_flat

    dot_product = torch.dot(param_flat, grad_flat)
    param_norm_sq = torch.dot(param_flat, param_flat) + eps
    proj_coeff = dot_product / param_norm_sq

    orthogonal_grad_flat = grad_flat - proj_coeff * param_flat
    orth_norm = torch.norm(orthogonal_grad_flat, p=2) + eps
    scale_factor = grad_norm / orth_norm

    return orthogonal_grad_flat * scale_factor


class AsyncComputeManager:
    """Asynchronous computation manager"""

    def __init__(self, max_workers: int = 2):
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.pending_futures = []

    def submit_async_task(self, func, *args, **kwargs):
        """Submit asynchronous task"""
        future = self.executor.submit(func, *args, **kwargs)
        self.pending_futures.append(future)
        return future

    def collect_completed_tasks(self, timeout: float = 0.001):
        """Collect completed tasks"""
        completed = []
        remaining = []

        for future in self.pending_futures:
            if future.done():
                try:
                    result = future.result(timeout=timeout)
                    completed.append(result)
                except concurrent.futures.TimeoutError:
                    remaining.append(future)
                except Exception as e:
                    logger.warning(f"Asynchronous task execution failed: {e}")
                    remaining.append(future)
            else:
                remaining.append(future)

        self.pending_futures = remaining
        return completed

    def shutdown(self):
        """Shutdown executor"""
        self.executor.shutdown(wait=True)


class HinaAdaptive(torch.optim.Optimizer):
    """
    Memory-optimized version of the adaptive HinaAdaptive optimizer

    Main optimization features:
    1. Precision grading: critical states keep high precision, secondary states use low precision
    2. Intelligent buffer pool: dynamic memory management
    3. Compressed state storage: reduce Python object overhead
    4. Asynchronous computation: non-critical computations run asynchronously
    5. Adaptive memory management: adjust strategies based on memory pressure
    6. Edge overfitting control:
       - Edge suppression: detect and suppress edge gradients to prevent edge overfitting
       - Frequency awareness: control high-frequency noise for training stability
       - Spatial awareness: regularization based on spatial variance
       - LoRA low-rank regularization: rank penalty mechanism for LoRA layers
    """

    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: Tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 1e-2,
        amsgrad: bool = False,
        optim_bits: int = 32,
        args: Any = None,
        percentile_clipping: int = 100,
        block_wise: bool = True,
        is_paged: bool = False,
        # Enhanced features configuration
        use_spd: bool = True,
        spd_lambda: float = 0.06,
        use_cautious: bool = True,
        use_orthogonal_grad: bool = False,
        use_adopt_stability: bool = True,
        use_grams: bool = True,
        use_agr: bool = True,
        use_tam: bool = True,
        tam_beta: float = 0.999,
        # Dynamic adaptive learning rate functionality
        use_dynamic_adaptation: bool = True,
        adaptation_strength: float = 1.0,
        relationship_discovery_interval: int = 100,
        importance_decay: float = 0.95,
        compatibility_threshold: float = 0.3,
        # lr_mask mechanism configuration
        use_lr_mask: bool = True,
        lr_bump: float = 3e-6,
        min_lr: float = 1e-7,
        max_lr: float = 1e-3,
        warmup_steps: int = 500,
        # Dynamic weight decay configuration
        dynamic_weight_decay: bool = True,
        wd_transition_steps: int = 1000,
        wd_decay_factor: float = 0.7,
        wd_min_ratio: float = 0.1,
        # Memory optimization configuration
        memory_efficient: bool = True,
        vram_budget_gb: float = 16.0,
        cpu_offload_states: bool = True,
        reduce_precision: bool = True,
        adaptive_features: bool = True,
        emergency_simplify: bool = True,
        max_buffer_memory_mb: int = 500,
        # Edge overfitting control parameters
        edge_suppression: bool = False,
        edge_penalty: float = 0.1,
        # Spatial awareness
        spatial_awareness: bool = False,
        frequency_penalty: float = 0.05,
        detail_preservation: float = 0.8,
        edge_threshold: float = 0.6,
        # LoRA low-rank regularization
        lora_rank_penalty: bool = False,
        rank_penalty_strength: float = 0.01,
        low_rank_emphasis: float = 1.2,
        **kwargs,
    ):
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            amsgrad=amsgrad,
            optim_bits=optim_bits,
            args=args,
            percentile_clipping=percentile_clipping,
            block_wise=block_wise,
            is_paged=is_paged,
        )

        super().__init__(params, defaults)

        # Original feature switches
        self.use_spd = use_spd
        self.spd_lambda = spd_lambda
        self.use_cautious = use_cautious
        self.use_orthogonal_grad = use_orthogonal_grad
        self.use_adopt_stability = use_adopt_stability
        self.use_grams = use_grams
        self.use_agr = use_agr
        self.use_tam = use_tam
        self.tam_beta = tam_beta

        # Dynamic adaptive functionality configuration
        self.use_dynamic_adaptation = use_dynamic_adaptation
        self.adaptation_strength = adaptation_strength
        self.relationship_discovery_interval = relationship_discovery_interval
        self.importance_decay = importance_decay
        self.compatibility_threshold = compatibility_threshold

        # lr_mask mechanism configuration
        self.use_lr_mask = use_lr_mask
        self.lr_bump = lr_bump
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.warmup_steps = warmup_steps

        # Dynamic weight decay configuration
        self.dynamic_weight_decay = dynamic_weight_decay
        self.wd_transition_steps = wd_transition_steps
        self.wd_decay_factor = wd_decay_factor
        self.wd_min_ratio = wd_min_ratio

        # Memory optimization configuration
        self.memory_efficient = memory_efficient
        self.vram_budget_gb = vram_budget_gb
        self.cpu_offload_states = cpu_offload_states
        self.reduce_precision = reduce_precision
        self.adaptive_features = adaptive_features
        self.emergency_simplify = emergency_simplify

        # Edge overfitting control configuration
        self.edge_suppression = edge_suppression
        self.edge_penalty = edge_penalty
        self.spatial_awareness = spatial_awareness
        self.frequency_penalty = frequency_penalty
        self.detail_preservation = detail_preservation
        self.edge_threshold = edge_threshold
        self.lora_rank_penalty = lora_rank_penalty
        self.rank_penalty_strength = rank_penalty_strength
        self.low_rank_emphasis = low_rank_emphasis

        # Initialize memory management components
        self.memory_monitor = MemoryMonitor(vram_budget_gb)
        self.buffer_pool = EnhancedBufferPool(max_buffer_memory_mb)
        self.async_manager = AsyncComputeManager()

        # Initialize edge overfitting control components
        if self.edge_suppression:
            self.edge_cache = {}  # Edge computation cache

        # Compressed state storage
        self.compressed_relationships = CompressedRelationships()
        self.quantized_importance_scores = {}  # param_id -> int16
        self.last_relationship_update = 0

        # Initialize parameter group metadata
        self._initialize_adaptive_metadata()

        # Store initial parameters (for SPD)
        if self.use_spd:
            self._store_initial_parameters()

        # Enable PyTorch optimizations
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True

        logger.info(f"HinaAdaptive initialized, VRAM budget: {vram_budget_gb}GB")

    def _initialize_adaptive_metadata(self):
        """Initialize adaptive version metadata (memory optimized version)"""
        self.param_groups_metadata = {}

        for group_idx, group in enumerate(self.param_groups):
            self.param_groups_metadata[group_idx] = {
                "param_count": len(group["params"]),
                "param_list": list(group["params"]),
                "compact_states": {},  # Use compact state storage
            }

            # Initialize tracking information for each parameter
            for param in group["params"]:
                param_id = id(param)

                # Use quantized importance score
                self.quantized_importance_scores[param_id] = quantize_importance_score(
                    1.0
                )

                # Use compact state dictionary
                compact_state = CompactStateDict()
                compact_state.set_scalar("initial_norm", 0.0)
                compact_state.set_scalar("change_rate", 0.0)
                compact_state.set_scalar("stability", 1.0)

                # lr_mask state initialization (using half precision)
                if self.use_lr_mask:
                    device = param.device if hasattr(param, "device") else "cuda"
                    shape = param.shape

                    if self.reduce_precision:
                        lr_mask = (
                            torch.ones(shape, device=device, dtype=torch.bfloat16)
                            * self.defaults["lr"]
                        )
                    else:
                        lr_mask = (
                            torch.ones(shape, device=device, dtype=torch.float32)
                            * self.defaults["lr"]
                        )

                    compact_state.set_tensor(
                        "lr_mask", lr_mask, use_half_precision=self.reduce_precision
                    )
                    compact_state.set_tensor(
                        "last_polarity",
                        torch.zeros(shape, dtype=torch.bool, device=device),
                    )
                    compact_state.set_scalar("lr_max", self.defaults["lr"])
                    compact_state.set_scalar("avg_lr", self.defaults["lr"])
                    compact_state.set_scalar(
                        "warmup_complete", 0.0
                    )  # 0.0 = False, 1.0 = True

                # Edge overfitting control state initialization
                if self.edge_suppression:
                    device = param.device if hasattr(param, "device") else "cuda"
                    shape = param.shape

                    # Edge history tracking
                    compact_state.set_tensor(
                        "edge_history",
                        torch.zeros(shape, device=device, dtype=torch.float32),
                    )
                    compact_state.set_tensor(
                        "edge_momentum",
                        torch.zeros(shape, device=device, dtype=torch.float32),
                    )
                    compact_state.set_scalar("edge_strength", 0.0)

                if self.spatial_awareness:
                    device = param.device if hasattr(param, "device") else "cuda"
                    shape = param.shape

                    # Spatial awareness state
                    compact_state.set_tensor(
                        "spatial_variance",
                        torch.ones(shape, device=device, dtype=torch.float32),
                    )
                    compact_state.set_tensor(
                        "detail_tracker",
                        torch.zeros(shape, device=device, dtype=torch.float32),
                    )
                    compact_state.set_scalar("spatial_activity", 0.0)

                if self.lora_rank_penalty and len(param.shape) == 2:
                    device = param.device if hasattr(param, "device") else "cuda"
                    min_dim = min(param.shape)

                    # LoRA low-rank tracking
                    compact_state.set_tensor(
                        "rank_tracker",
                        torch.zeros(min_dim, device=device, dtype=torch.float32),
                    )
                    compact_state.set_scalar("rank_penalty_history", 0.0)

                self.param_groups_metadata[group_idx]["compact_states"][
                    param_id
                ] = compact_state

    def _store_initial_parameters(self):
        """Store initial parameters for SPD (memory optimized version)"""
        self.initial_params = {}
        for group_idx, group in enumerate(self.param_groups):
            for param in group["params"]:
                if param.requires_grad:
                    if self.cpu_offload_states:
                        # Store initial parameters on CPU to save VRAM
                        self.initial_params[param] = param.data.clone().detach().cpu()
                    else:
                        self.initial_params[param] = param.data.clone().detach()

    def _get_optimized_buffer(
        self,
        shape: Tuple,
        dtype: torch.dtype,
        device: torch.device,
        priority: str = "normal",
    ) -> torch.Tensor:
        """Get optimized buffer"""
        return self.buffer_pool.get_buffer_with_priority(shape, dtype, device, priority)

    def _return_optimized_buffer(self, tensor: torch.Tensor, priority: str = "normal"):
        """Return optimized buffer"""
        self.buffer_pool.return_buffer(tensor, priority)

    def _check_memory_and_adapt(self):
        """Check memory and adapt"""
        if not self.memory_efficient:
            return

        memory_pressure = self.memory_monitor.check_memory_pressure()
        optimizations = self.memory_monitor.suggest_optimizations(memory_pressure)

        if optimizations["reduce_buffer_pool"]:
            self.buffer_pool.smart_cleanup(memory_pressure)

        if optimizations["increase_gc_frequency"]:
            torch.cuda.empty_cache()

        if optimizations["reduce_precision"] and not self.reduce_precision:
            logger.warning(
                "Memory pressure too high, consider enabling reduce_precision=True"
            )

        if optimizations["use_checkpoint_offload"] and hasattr(
            self, "_enable_emergency_offload"
        ):
            self._enable_emergency_offload()  # type: ignore

    def _compute_parameter_contribution_score_optimized(
        self, param, compact_state, param_id
    ):
        """
        Optimized parameter contribution score calculation
        Use batch operations and reduce memory allocation
        """
        # 1. Gradient-related contribution analysis
        grad_contribution = 0.0
        if param.grad is not None:
            current_grad_norm = torch.norm(param.grad).item()
            grad_contribution = current_grad_norm

        # 2. Parameter change-related contribution analysis
        change_contribution = 0.0
        initial_norm = compact_state.get_scalar("initial_norm")

        if initial_norm == 0.0:
            # First record
            initial_norm = torch.norm(param.data).item()
            compact_state.set_scalar("initial_norm", initial_norm)
        else:
            # Calculate relative change rate
            current_norm = torch.norm(param.data).item()
            if initial_norm > 0:
                change_rate = abs(current_norm - initial_norm) / initial_norm
                compact_state.set_scalar("change_rate", change_rate)
                change_contribution = change_rate

        # 3. Parameter intrinsic characteristic analysis (sample calculation to save time)
        if random.random() < 0.1:  # 10% probability for full calculation
            param_variance = torch.var(param.data).item()
            param_sparsity = (param.data.abs() < 1e-6).float().mean().item()
            intrinsic_contribution = param_variance * (1.0 - param_sparsity)
        else:
            # Use quick estimation
            intrinsic_contribution = 0.5

        # Overall contribution score
        total_contribution = (
            grad_contribution * 0.4
            + change_contribution * 0.3
            + intrinsic_contribution * 0.3
        )

        return max(0.01, total_contribution)

    def _discover_parameter_relationships_async(self, group_metadata):
        """Asynchronously discover parameter relationships"""

        def discover_relationships():
            param_list = group_metadata["param_list"]
            new_relationships = []

            # Limit the number of parameter pairs to control computation
            max_pairs = min(100, len(param_list) * (len(param_list) - 1) // 2)
            pair_count = 0

            for i, param1 in enumerate(param_list):
                if pair_count >= max_pairs:
                    break
                if param1.dim() != 2:
                    continue

                for j, param2 in enumerate(param_list[i + 1 :], i + 1):
                    if pair_count >= max_pairs:
                        break
                    if param2.dim() != 2:
                        continue

                    compatibility = self._compute_parameter_compatibility_fast(
                        param1, param2
                    )
                    if compatibility > self.compatibility_threshold:
                        param1_id = id(param1)
                        param2_id = id(param2)
                        interaction_type = self._determine_interaction_type_fast(
                            param1, param2
                        )

                        new_relationships.append(
                            {
                                "param1_id": param1_id,
                                "param2_id": param2_id,
                                "compatibility": compatibility,
                                "interaction_type": interaction_type,
                            }
                        )
                        pair_count += 1

            return new_relationships

        # Submit asynchronous task
        return self.async_manager.submit_async_task(discover_relationships)

    def _compute_parameter_compatibility_fast(self, param1, param2):
        """Fast version of parameter compatibility calculation"""
        if param1.dim() != 2 or param2.dim() != 2:
            return 0.0

        shape1, shape2 = param1.shape, param2.shape

        # Check matrix multiplication possibility
        multiplication_checks = [
            shape1[1] == shape2[0],
            shape1[0] == shape2[1],
            shape1[1] == shape2[1],
            shape1[0] == shape2[0],
        ]

        if not any(multiplication_checks):
            return 0.0

        # Simplified correlation calculation (sampled version)
        try:
            # Use only the first 1000 elements for correlation calculation
            flat1 = param1.data.flatten()[:1000]
            flat2 = param2.data.flatten()[:1000]

            min_size = min(len(flat1), len(flat2))
            if min_size > 1:
                flat1 = flat1[:min_size]
                flat2 = flat2[:min_size]

                correlation = torch.corrcoef(torch.stack([flat1, flat2]))[0, 1]
                if torch.isnan(correlation):
                    correlation = 0.0
                else:
                    correlation = abs(correlation.item())
            else:
                correlation = 0.0

            shape_compatibility = sum(multiplication_checks) / len(
                multiplication_checks
            )
            total_compatibility = shape_compatibility * 0.7 + correlation * 0.3

            return total_compatibility

        except Exception as e:
            logger.debug(f"Fast compatibility calculation failed: {e}")
            return 0.0

    @staticmethod
    def _determine_interaction_type_fast(
        param1: torch.Tensor, param2: torch.Tensor
    ) -> str:
        """Fastly determine interaction type"""
        shape1, shape2 = param1.shape, param2.shape

        if shape1[1] == shape2[0]:
            return "matmul_12"
        elif shape1[0] == shape2[1]:
            return "matmul_21"
        elif shape1[1] == shape2[1]:
            return "matmul_12t"
        elif shape1[0] == shape2[0]:
            return "matmul_1t2"
        else:
            return "norm_based"

    def _update_importance_scores_batch(self, group_metadata):
        """Batch update importance scores"""
        param_ids = []
        contribution_scores = []

        # Batch collect contribution scores
        for param in group_metadata["param_list"]:
            param_id = id(param)
            compact_state = group_metadata["compact_states"].get(param_id)

            if compact_state is not None:
                contribution = self._compute_parameter_contribution_score_optimized(
                    param, compact_state, param_id
                )
                param_ids.append(param_id)
                contribution_scores.append(contribution)

        # Batch update quantized importance scores
        for param_id, contribution in zip(param_ids, contribution_scores):
            old_quantized = self.quantized_importance_scores.get(
                param_id, quantize_importance_score(1.0)
            )
            old_importance = dequantize_importance_score(old_quantized)

            new_importance = (
                self.importance_decay * old_importance
                + (1 - self.importance_decay) * contribution
            )

            self.quantized_importance_scores[param_id] = quantize_importance_score(
                new_importance
            )

    def _apply_orthogonal_gradient_optimized(
        self, grad: torch.Tensor, param: torch.Tensor
    ) -> torch.Tensor:
        """
        Memory-optimized orthogonal gradient projection
        """
        param_norm = torch.norm(param.data, p=2)
        if param_norm <= 1e-30:
            return grad

        param_flat = param.data.view(-1)
        grad_flat = grad.view(-1)

        if param_flat.shape != grad_flat.shape:
            return grad

        # Use JIT-compiled core function
        orthogonal_grad_flat = orthogonal_gradient_core_optimized(
            grad_flat, param_flat, 1e-30
        )

        return orthogonal_grad_flat.view_as(grad)

    def _compute_adaptive_lr_scale_optimized(
        self, param, group_metadata, state, grad=None, global_step=None
    ):
        """
        Optimized adaptive learning rate scaling calculation
        """
        param_id = id(param)
        compact_state = group_metadata["compact_states"].get(param_id)

        if compact_state is None:
            return 1.0

        # === lr_mask basic adjustment ===
        lr_mask_scale = 1.0
        if self.use_lr_mask and grad is not None and global_step is not None:
            lr_mask_scale = self._update_lr_mask_optimized(
                compact_state, grad, global_step
            )

        # === Adaptive advanced adjustment ===
        adaptive_scale = 1.0
        if self.use_dynamic_adaptation:
            # 1. Adjust based on importance
            quantized_importance = self.quantized_importance_scores.get(
                param_id, quantize_importance_score(1.0)
            )
            importance = dequantize_importance_score(quantized_importance)
            importance_factor = min(
                3.0, max(0.1, importance * self.adaptation_strength)
            )

            # 2. Adjust based on parameter relationships (simplified version)
            relationship_scale = 1.0
            rel_info = self.compressed_relationships.get_relationship(param_id)
            if rel_info is not None:
                compatibility_bonus = rel_info["compatibility"]
                relationship_scale = 1.0 + compatibility_bonus * 0.2

            adaptive_scale = importance_factor * relationship_scale
            adaptive_scale = max(0.01, min(5.0, adaptive_scale))

        # === Combine final scaling factor ===
        # Handle lr_mask_scale being a tensor
        if isinstance(lr_mask_scale, torch.Tensor):
            # If lr_mask_scale is a tensor, multiply directly
            final_scale = lr_mask_scale * adaptive_scale
            # Use torch.clamp instead of max/min for tensors
            final_scale = torch.clamp(final_scale, min=0.001, max=10.0)
        else:
            # If it's a scalar, use the original logic
            final_scale = lr_mask_scale * adaptive_scale
            final_scale = max(0.001, min(10.0, final_scale))

        return final_scale

    def _update_lr_mask_optimized(self, compact_state, grad, global_step):
        """Optimized lr_mask update"""
        if not self.use_lr_mask:
            return 1.0

        # Get or initialize lr_mask
        lr_mask = compact_state.get_tensor("lr_mask", torch.float32)
        if lr_mask is None:
            device = grad.device
            shape = grad.shape
            dtype = torch.bfloat16 if self.reduce_precision else torch.float32
            lr_mask = (
                torch.ones(shape, device=device, dtype=dtype) * self.defaults["lr"]
            )
            compact_state.set_tensor(
                "lr_mask", lr_mask, use_half_precision=self.reduce_precision
            )

        if global_step < self.warmup_steps:
            return self._update_warmup_lr_mask_optimized(
                compact_state, grad, global_step
            )
        else:
            return self._update_post_warmup_lr_mask_optimized(
                compact_state, grad, global_step
            )

    def _update_warmup_lr_mask_optimized(self, compact_state, grad, global_step):
        """Optimized warmup lr_mask update"""
        # Use new get_tensor method to directly get the tensor on the correct device
        last_polarity = compact_state.get_tensor(
            "last_polarity", target_device=grad.device
        )
        current_polarity = grad > 0

        if last_polarity is not None:
            sign_agree = torch.where(last_polarity == current_polarity, 1.0, -1.0)
        else:
            sign_agree = torch.ones_like(
                current_polarity, dtype=torch.float32, device=grad.device
            )

        compact_state.set_tensor("last_polarity", current_polarity)

        # Use new get_tensor method to get lr_mask on the correct device and dtype
        lr_mask = compact_state.get_tensor(
            "lr_mask", target_dtype=torch.float32, target_device=grad.device
        )

        # Use JIT-compiled core update function
        new_lr_mask = compute_lr_mask_update_core(
            lr_mask, sign_agree, self.lr_bump, self.min_lr, self.max_lr
        )

        # Update state
        if self.reduce_precision:
            compact_state.set_tensor(
                "lr_mask", new_lr_mask.to(torch.bfloat16), use_half_precision=True
            )
        else:
            compact_state.set_tensor("lr_mask", new_lr_mask)

        compact_state.set_scalar("avg_lr", torch.mean(new_lr_mask).item())

        # Return relative scaling factor
        base_lr = self.defaults["lr"]
        lr_scale = new_lr_mask / base_lr if base_lr > 0 else new_lr_mask

        return lr_scale

    def _update_post_warmup_lr_mask_optimized(self, compact_state, grad, global_step):
        """Optimized post-warmup lr_mask update"""
        warmup_complete = compact_state.get_scalar("warmup_complete")
        if warmup_complete < 0.5:  # False
            compact_state.set_scalar("warmup_complete", 1.0)  # True

        # Use new get_tensor method to get lr_mask on the correct device and dtype
        lr_mask = compact_state.get_tensor(
            "lr_mask", target_dtype=torch.float32, target_device=grad.device
        )

        # If lr_mask is None, return scalar scaling factor
        if lr_mask is None:
            return 1.0

        # In post-warmup stage, maintain stability
        base_lr = self.defaults["lr"]
        lr_scale = lr_mask / base_lr if base_lr > 0 else lr_mask

        return lr_scale

    def _apply_spd_regularization_optimized(self, param, group, state):
        """Apply SPD regularization (memory optimized version)"""
        if param not in self.initial_params:
            return 0

        initial_param = self.initial_params[param]

        # If initial parameter is on CPU, move it to the same device
        if initial_param.device != param.data.device:
            if self.cpu_offload_states:
                # Temporarily move to GPU for calculation
                initial_param_gpu = initial_param.to(param.data.device)
                param_diff = param.data - initial_param_gpu
                # Immediately clean up temporary tensor
                del initial_param_gpu
            else:
                param_diff = param.data - initial_param
        else:
            param_diff = param.data - initial_param

        # Calculate bias ratio
        param_norm = torch.norm(param.data)
        diff_norm = torch.norm(param_diff)

        if param_norm > 0:
            bias_ratio = diff_norm / param_norm
        else:
            bias_ratio = 0

        # SPD penalty term
        spd_penalty = self.spd_lambda * bias_ratio * param_diff

        return spd_penalty

    @staticmethod
    @torch.jit.script
    def _apply_agr_regularization_optimized(grad: torch.Tensor) -> torch.Tensor:
        """Apply AGR regularization (JIT optimized version)"""
        grad_norm = torch.norm(grad)
        if grad_norm > 1.0:
            clip_factor = 1.0 / grad_norm
            return grad * clip_factor
        return grad

    @staticmethod
    @torch.jit.script
    def _apply_cautious_update_optimized(
        update: torch.Tensor, grad: torch.Tensor
    ) -> torch.Tensor:
        """Apply cautious update strategy (JIT optimized version)"""
        update_flat = update.view(-1)
        grad_flat = grad.view(-1)

        update_norm = torch.norm(update_flat)
        grad_norm = torch.norm(grad_flat)

        if update_norm > 0 and grad_norm > 0:
            alignment = torch.dot(update_flat, grad_flat) / (update_norm * grad_norm)
            if alignment < 0.1:
                return update * 0.5

        return update

    def _apply_tam_damping_optimized(self, momentum, grad, state):
        """Apply TAM damping (memory optimized version)"""
        if "momentum_alignment" not in state:
            state["momentum_alignment"] = 0.0

        try:
            momentum_norm = torch.norm(momentum)
            grad_norm = torch.norm(grad)

            if momentum_norm > 0 and grad_norm > 0:
                momentum_flat = momentum.view(-1)
                grad_flat = grad.view(-1)

                if momentum_flat.size() == grad_flat.size():
                    alignment = torch.dot(momentum_flat, grad_flat) / (
                        momentum_norm * grad_norm
                    )
                    alignment = alignment.item()
                else:
                    alignment = 0.0
            else:
                alignment = 0.0
        except Exception as e:
            logger.debug(f"TAM alignment calculation failed: {e}")
            alignment = 0.0

        # Smooth alignment estimate
        state["momentum_alignment"] = (
            self.tam_beta * state["momentum_alignment"]
            + (1 - self.tam_beta) * alignment
        )

        # Calculate damping factor
        damping_factor = (1 + state["momentum_alignment"]) / 2
        return damping_factor

    def _compute_edge_penalty_optimized(
        self,
        grad: torch.Tensor,
        threshold: float = 0.6,
        cache_key: Optional[str] = None,
    ) -> torch.Tensor:
        """
        Optimized edge penalty calculation, used to control edge overfitting

        Args:
            grad: Gradient tensor
            threshold: Edge detection threshold
            cache_key: Cache key (optional)

        Returns:
            Edge penalty tensor
        """
        if len(grad.shape) < 2:
            return torch.zeros_like(grad)

        # Check cache (if enabled)
        if cache_key and hasattr(self, "edge_cache"):
            cached = self.edge_cache.get(cache_key)
            if cached is not None and cached.shape == grad.shape:
                return cached

        with torch.no_grad():
            # Get tensor using buffer pool
            laplacian = self.buffer_pool.get_buffer_with_priority(
                grad.shape, grad.dtype, grad.device, priority="normal"
            )

            # Simplified edge detection: calculate Laplacian operator
            if len(grad.shape) == 2 and grad.shape[0] > 2 and grad.shape[1] > 2:
                # Horizontal second derivative
                laplacian[1:-1, :] = grad[2:, :] - 2 * grad[1:-1, :] + grad[:-2, :]
                # Vertical second derivative
                laplacian[:, 1:-1] += grad[:, 2:] - 2 * grad[:, 1:-1] + grad[:, :-2]

            # Calculate edge strength
            edge_strength = torch.abs(laplacian)
            edge_mask = (edge_strength > threshold).float()
            result = edge_mask * edge_strength

            # Cache result
            if cache_key and hasattr(self, "edge_cache"):
                self.edge_cache[cache_key] = result.clone()

            # Return buffer
            self.buffer_pool.return_buffer(laplacian)

            return result

    def _compute_frequency_penalty_simplified(self, grad: torch.Tensor) -> torch.Tensor:
        """
        Simplified frequency penalty calculation, used to control high-frequency noise

        Args:
            grad: Gradient tensor

        Returns:
            Frequency penalty tensor
        """
        if len(grad.shape) < 2:
            return torch.zeros_like(grad)

        with torch.no_grad():
            # Use simplified high-frequency detection: calculate adjacent element differences
            if len(grad.shape) == 2:
                h, w = grad.shape
                if h > 1 and w > 1:
                    # Get buffer
                    result = self.buffer_pool.get_buffer_with_priority(
                        grad.shape, grad.dtype, grad.device, priority="normal"
                    )

                    # Calculate horizontal and vertical differences
                    h_diff = torch.abs(grad[:, 1:] - grad[:, :-1])
                    v_diff = torch.abs(grad[1:, :] - grad[:-1, :])

                    # Combine difference information
                    result[:, 1:] = h_diff
                    result[1:, :] += v_diff

                    return result

            return torch.zeros_like(grad)

    def _lora_rank_regularization_fast(
        self, param: torch.Tensor, rank_strength: float = 0.01, use_approx: bool = True
    ) -> torch.Tensor:
        """
        Fast LoRA low-rank regularization, used to control LoRA layer overfitting

        Args:
            param: Parameter tensor
            rank_strength: Rank regularization strength
            use_approx: Whether to use an approximate method

        Returns:
            Low-rank regularization penalty tensor
        """
        if len(param.shape) != 2:
            return torch.zeros_like(param)

        with torch.no_grad():
            if use_approx:
                # Use approximate method: only consider the largest singular values
                # Calculate covariance matrix
                if param.shape[0] <= param.shape[1]:
                    cov = torch.mm(param, param.t())
                else:
                    cov = torch.mm(param.t(), param)

                # Calculate eigenvalues (only take the first few)
                try:
                    eigenvals, _ = torch.linalg.eigh(cov)
                    # Penalize larger eigenvalues (promote low-rank)
                    large_eigenvals = eigenvals[eigenvals.argsort(descending=True)[:10]]
                    rank_penalty_scalar = torch.sum(large_eigenvals) * rank_strength

                    # Create gradient approximation
                    return param * rank_penalty_scalar
                except Exception as e:
                    logger.debug(f"LoRA rank regularization calculation failed: {e}")
                    return torch.zeros_like(param)
            else:
                # Full SVD (if needed)
                try:
                    U, S, Vh = torch.linalg.svd(param, full_matrices=False)
                    # Penalize larger singular values
                    large_s = S[S.argsort(descending=True)[:10]]
                    rank_penalty = torch.sum(large_s) * rank_strength
                    penalty_grad = U @ torch.diag(S * rank_penalty / torch.sum(S)) @ Vh
                    return penalty_grad
                except Exception as e:
                    logger.debug(
                        f"Full SVD rank regularization calculation failed: {e}"
                    )
                    return torch.zeros_like(param)

    def _apply_spatial_awareness_regularization(
        self, grad: torch.Tensor, state: dict
    ) -> torch.Tensor:
        """
        Apply spatial awareness regularization, used to control spatial overfitting

        Args:
            grad: Gradient tensor
            state: Parameter state

        Returns:
            Regularized gradient
        """
        if len(grad.shape) < 2:
            return grad

        with torch.no_grad():
            # Initialize spatial variance tracking
            if "spatial_variance" not in state:
                state["spatial_variance"] = torch.ones_like(grad)

            if "detail_tracker" not in state:
                state["detail_tracker"] = torch.zeros_like(grad)

            # Calculate local variance
            local_variance = torch.zeros_like(grad)
            if len(grad.shape) == 2 and grad.shape[0] > 2 and grad.shape[1] > 2:
                # Calculate variance for 3x3 neighborhood
                for i in range(-1, 2):
                    for j in range(-1, 2):
                        if i == 0 and j == 0:
                            continue

                        # Calculate difference after offset
                        if i == 0:
                            if j == 1:
                                local_variance[:, :-1] += torch.pow(
                                    grad[:, 1:] - grad[:, :-1], 2
                                )
                            elif j == -1:
                                local_variance[:, 1:] += torch.pow(
                                    grad[:, :-1] - grad[:, 1:], 2
                                )
                        elif j == 0:
                            if i == 1:
                                local_variance[:-1, :] += torch.pow(
                                    grad[1:, :] - grad[:-1, :], 2
                                )
                            elif i == -1:
                                local_variance[1:, :] += torch.pow(
                                    grad[:-1, :] - grad[1:, :], 2
                                )

            # Update spatial variance tracking
            state["spatial_variance"] = (
                0.9 * state["spatial_variance"] + 0.1 * local_variance
            )

            # Adjust gradient based on spatial variance
            regularization_factor = 1.0 / (
                1.0 + state["spatial_variance"] * self.detail_preservation
            )

            return grad * regularization_factor

    @torch.no_grad()
    def step(self, closure=None):
        """Execute optimization step - memory optimized version"""
        loss = None
        if closure is not None:
            loss = closure()

        # Check memory and adapt
        self._check_memory_and_adapt()

        # Global step count
        if not hasattr(self, "global_step"):
            self.global_step = 0
        self.global_step += 1

        # Collect completed asynchronous tasks
        completed_relationships = self.async_manager.collect_completed_tasks()
        for relationship_batch in completed_relationships:
            for rel in relationship_batch:
                self.compressed_relationships.add_relationship(
                    rel["param1_id"],
                    rel["param2_id"],
                    rel["compatibility"],
                    rel["interaction_type"],
                )

        for group_idx, group in enumerate(self.param_groups):
            group_metadata = self.param_groups_metadata[group_idx]

            # Periodically update parameter relationships and importance scores
            should_update_relationships = (
                self.global_step - self.last_relationship_update
                >= self.relationship_discovery_interval
            )

            if should_update_relationships:
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"Step {self.global_step}: Updating parameter relationships and importance scores"
                    )

                # Batch update importance scores
                self._update_importance_scores_batch(group_metadata)

                # Asynchronously rediscover parameter relationships
                if self.use_dynamic_adaptation and self.adaptive_features:
                    self._discover_parameter_relationships_async(group_metadata)

                self.last_relationship_update = self.global_step

            # Process each parameter
            for param in group["params"]:
                if param.grad is None:
                    continue

                grad = param.grad.data
                if grad.is_sparse:
                    raise RuntimeError("HinaAdaptive does not support sparse gradients")

                state = self.state[param]
                param_id = id(param)
                compact_state = group_metadata["compact_states"].get(param_id)

                # State initialization
                if len(state) == 0:
                    state["step"] = 0

                    # Select precision based on memory settings
                    if self.reduce_precision:
                        state["exp_avg"] = torch.zeros_like(
                            param.data, dtype=torch.bfloat16
                        )
                        state["exp_avg_sq"] = torch.zeros_like(
                            param.data, dtype=torch.bfloat16
                        )
                        if self.use_adopt_stability:
                            state["exp_avg_sq_prev"] = torch.zeros_like(
                                param.data, dtype=torch.bfloat16
                            )
                    else:
                        state["exp_avg"] = torch.zeros_like(
                            param.data, dtype=torch.float32
                        )
                        state["exp_avg_sq"] = torch.zeros_like(
                            param.data, dtype=torch.float32
                        )
                        if self.use_adopt_stability:
                            state["exp_avg_sq_prev"] = torch.zeros_like(
                                param.data, dtype=torch.float32
                            )

                state["step"] += 1

                beta1, beta2 = group["betas"]
                step_size = group["lr"]

                # AGR regularization
                if self.use_agr:
                    grad = HinaAdaptive._apply_agr_regularization_optimized(grad)

                # === Edge overfitting control ===
                if len(grad.shape) >= 2:
                    # Edge-aware gradient regularization
                    if self.edge_suppression:
                        cache_key = f"edge_p_{param_id}_{state['step']}"
                        edge_penalty = self._compute_edge_penalty_optimized(
                            grad, self.edge_threshold, cache_key
                        )

                        # Apply edge penalty
                        if edge_penalty.numel() > 0:
                            edge_factor = 1.0 + self.edge_penalty * edge_penalty
                            grad = grad * (1.0 / edge_factor)

                            # Update edge history
                            if compact_state is not None:
                                edge_history = compact_state.get_tensor(
                                    "edge_history", target_device=grad.device
                                )
                                if edge_history is not None:
                                    edge_history = (
                                        0.9 * edge_history + 0.1 * edge_penalty
                                    )
                                    compact_state.set_tensor(
                                        "edge_history", edge_history
                                    )
                                    compact_state.set_scalar(
                                        "edge_strength", torch.mean(edge_penalty).item()
                                    )

                    # Frequency-aware gradient adjustment
                    if self.spatial_awareness:
                        freq_penalty = self._compute_frequency_penalty_simplified(grad)
                        if freq_penalty.numel() > 0:
                            grad = grad - self.frequency_penalty * freq_penalty

                            # Update spatial activity
                            if compact_state is not None:
                                spatial_activity = torch.mean(
                                    torch.abs(freq_penalty)
                                ).item()
                                compact_state.set_scalar(
                                    "spatial_activity", spatial_activity
                                )

                    # Apply spatial awareness regularization
                    if self.spatial_awareness:
                        grad = self._apply_spatial_awareness_regularization(grad, state)

                # LoRA low-rank regularization
                if self.lora_rank_penalty and len(param.shape) == 2:
                    rank_penalty = self._lora_rank_regularization_fast(
                        param, self.rank_penalty_strength, use_approx=True
                    )
                    if rank_penalty.numel() > 0:
                        grad = grad + rank_penalty

                        # Update rank tracking
                        if compact_state is not None:
                            rank_penalty_magnitude = torch.mean(
                                torch.abs(rank_penalty)
                            ).item()
                            compact_state.set_scalar(
                                "rank_penalty_history", rank_penalty_magnitude
                            )

                # Orthogonal gradient projection
                if self.use_orthogonal_grad:
                    grad = self._apply_orthogonal_gradient_optimized(grad, param)

                # Bias-corrected learning rate
                bias_correction1 = 1 - beta1 ** state["step"]
                bias_correction2 = 1 - beta2 ** state["step"]

                # Update momentum estimate (considering precision)
                if self.use_adopt_stability and "exp_avg_sq_prev" in state:
                    state["exp_avg_sq_prev"] = state["exp_avg_sq"].clone()

                # Ensure calculations are performed at the correct precision
                if self.reduce_precision:
                    # Calculate in bfloat16 precision
                    grad_bf16 = grad.to(torch.bfloat16)
                    state["exp_avg"].mul_(beta1).add_(grad_bf16, alpha=1 - beta1)
                    state["exp_avg_sq"].mul_(beta2).addcmul_(
                        grad_bf16, grad_bf16, value=1 - beta2
                    )
                else:
                    # Calculate in float32 precision
                    state["exp_avg"].mul_(beta1).add_(grad, alpha=1 - beta1)
                    state["exp_avg_sq"].mul_(beta2).addcmul_(
                        grad, grad, value=1 - beta2
                    )

                # Calculate update
                if self.use_adopt_stability and "exp_avg_sq_prev" in state:
                    exp_avg_sq_hat = torch.maximum(
                        state["exp_avg_sq"], state["exp_avg_sq_prev"]
                    )
                    denom = (exp_avg_sq_hat.sqrt() / math.sqrt(bias_correction2)).add_(
                        group["eps"]
                    )
                else:
                    denom = (
                        state["exp_avg_sq"].sqrt() / math.sqrt(bias_correction2)
                    ).add_(group["eps"])

                # Convert update to float32 to maintain precision
                if self.reduce_precision:
                    exp_avg_corrected = (
                        state["exp_avg"].to(torch.float32) / bias_correction1
                    )
                    denom = denom.to(torch.float32)
                else:
                    exp_avg_corrected = state["exp_avg"] / bias_correction1

                update = exp_avg_corrected / denom

                # TAM damping
                if self.use_tam:
                    damping_factor = self._apply_tam_damping_optimized(
                        state["exp_avg"], grad, state
                    )
                    update = update * damping_factor

                # Cautious update
                if self.use_cautious:
                    update = HinaAdaptive._apply_cautious_update_optimized(update, grad)

                # Dynamic adaptive learning rate adjustment
                current_step_size = step_size
                if (
                    self.use_dynamic_adaptation or self.use_lr_mask
                ) and compact_state is not None:
                    lr_scale = self._compute_adaptive_lr_scale_optimized(
                        param,
                        group_metadata,
                        state,
                        grad=grad,
                        global_step=self.global_step,
                    )

                    # Handle lr_scale being a tensor or scalar
                    if isinstance(lr_scale, torch.Tensor):
                        if lr_scale.numel() == 1:
                            current_step_size *= lr_scale.item()
                        else:
                            # Ensure lr_scale is on the same device as update
                            if lr_scale.device != update.device:
                                lr_scale = lr_scale.to(update.device)
                            # Element-wise learning rate adjustment
                            param.data.add_(update * lr_scale, alpha=-step_size)
                            current_step_size = 0  # Skip subsequent update application
                    else:
                        current_step_size *= lr_scale

                # Apply update (if not already applied)
                if current_step_size != 0:
                    param.data.add_(update, alpha=-current_step_size)

                # Weight decay
                current_weight_decay = group["weight_decay"]

                # Dynamic weight decay
                if self.dynamic_weight_decay:
                    if state["step"] > self.wd_transition_steps:
                        progress = (
                            state["step"] - self.wd_transition_steps
                        ) / self.wd_transition_steps
                        decay_multiplier = max(
                            self.wd_min_ratio,
                            self.wd_decay_factor ** min(progress, 2.0),
                        )
                        current_weight_decay *= decay_multiplier

                if current_weight_decay != 0:
                    param.data.add_(
                        param.data, alpha=-group["lr"] * current_weight_decay
                    )

                # SPD regularization
                if self.use_spd:
                    spd_penalty = self._apply_spd_regularization_optimized(
                        param, group, state
                    )
                    if isinstance(spd_penalty, torch.Tensor):
                        param.data.add_(spd_penalty, alpha=-group["lr"])

        return loss

    def update_device(self, device):
        """When the model is moved to a new device, update the optimizer's internal state"""
        if hasattr(self, "initial_params"):
            for param, initial_param in self.initial_params.items():
                if initial_param.device != device:
                    self.initial_params[param] = initial_param.to(device)

        # Update tensors in all states
        for state in self.state.values():
            for key, value in state.items():
                if isinstance(value, torch.Tensor) and value.device != device:
                    state[key] = value.to(device)

        # Update tensors in compact states
        for group_metadata in self.param_groups_metadata.values():
            for compact_state in group_metadata["compact_states"].values():
                for key, tensor in compact_state.tensor_states.items():
                    if tensor.device != device:
                        compact_state.tensor_states[key] = tensor.to(device)

    def get_optimization_info(self) -> Dict[str, Any]:
        """Get detailed optimization information"""
        info = {
            "optimizer_type": "HinaAdaptive",
            "version": "Memory optimized version v1.0",
            "total_params": sum(len(group["params"]) for group in self.param_groups),
            "features": {
                "spd": self.use_spd,
                "cautious": self.use_cautious,
                "orthogonal_grad": self.use_orthogonal_grad,
                "adopt_stability": self.use_adopt_stability,
                "grams": self.use_grams,
                "agr": self.use_agr,
                "tam": self.use_tam,
                "dynamic_adaptation": self.use_dynamic_adaptation,
                "lr_mask": self.use_lr_mask,
                "dynamic_weight_decay": self.dynamic_weight_decay,
                "edge_suppression": self.edge_suppression,
                "spatial_awareness": self.spatial_awareness,
                "lora_rank_penalty": self.lora_rank_penalty,
            },
            "adaptation_config": {
                "adaptation_strength": self.adaptation_strength,
                "relationship_discovery_interval": self.relationship_discovery_interval,
                "importance_decay": self.importance_decay,
                "compatibility_threshold": self.compatibility_threshold,
            },
            "memory_optimization": {
                "memory_efficient": self.memory_efficient,
                "vram_budget_gb": self.vram_budget_gb,
                "cpu_offload_states": self.cpu_offload_states,
                "reduce_precision": self.reduce_precision,
                "adaptive_features": self.adaptive_features,
                "emergency_simplify": self.emergency_simplify,
            },
            "edge_overfitting_control": {
                "edge_penalty": self.edge_penalty,
                "edge_threshold": self.edge_threshold,
                "frequency_penalty": self.frequency_penalty,
                "detail_preservation": self.detail_preservation,
                "rank_penalty_strength": self.rank_penalty_strength,
                "low_rank_emphasis": self.low_rank_emphasis,
            },
            "current_memory_pressure": self.memory_monitor.check_memory_pressure(),
        }

        # Add dynamic statistics
        if hasattr(self, "global_step"):
            total_relationships = len(self.compressed_relationships.param_pairs)
            total_importance_scores = len(self.quantized_importance_scores)

            avg_importance = 0.0
            if total_importance_scores > 0:
                quantized_scores = list(self.quantized_importance_scores.values())
                avg_importance = (
                    sum(dequantize_importance_score(q) for q in quantized_scores)
                    / total_importance_scores
                )

            info["training_stats"] = {
                "global_step": self.global_step,
                "total_relationships": total_relationships,
                "total_importance_scores": total_importance_scores,
                "avg_importance_score": avg_importance,
                "pending_async_tasks": len(self.async_manager.pending_futures),
            }

        return info

    def get_memory_stats(self) -> Dict[str, Any]:
        """Get detailed memory statistics"""
        stats = {
            "memory_pressure": self.memory_monitor.check_memory_pressure(),
            "buffer_pool_stats": {
                "total_buffer_types": len(self.buffer_pool._buffer_pool),
                "current_memory_mb": self.buffer_pool._current_memory / (1024 * 1024),
                "max_memory_mb": self.buffer_pool._max_total_memory / (1024 * 1024),
            },
            "state_compression": {
                "quantized_importance_scores": len(self.quantized_importance_scores),
                "compressed_relationships": len(
                    self.compressed_relationships.param_pairs
                ),
            },
        }

        if torch.cuda.is_available():
            stats["cuda_memory"] = {
                "allocated_gb": torch.cuda.memory_allocated() / (1024**3),
                "reserved_gb": torch.cuda.memory_reserved() / (1024**3),
                "max_allocated_gb": torch.cuda.max_memory_allocated() / (1024**3),
            }

        return stats

    def optimize_for_vram(self, target_vram_gb: float):
        """Automatically optimize settings based on target VRAM"""
        self.vram_budget_gb = target_vram_gb
        self.memory_monitor.target_vram = target_vram_gb * 1024**3

        current_pressure = self.memory_monitor.check_memory_pressure()

        if current_pressure > 0.9:
            logger.warning(
                f"VRAM usage {current_pressure:.1%} is high, enabling emergency optimization mode"
            )
            # Enable all memory optimizations
            self.reduce_precision = True
            self.cpu_offload_states = True
            self.emergency_simplify = True

            # Reduce asynchronous tasks
            self.relationship_discovery_interval *= 2

            # Force buffer pool cleanup
            self.buffer_pool.smart_cleanup(current_pressure)
            torch.cuda.empty_cache()

        elif current_pressure > 0.7:
            logger.info(
                f"VRAM usage {current_pressure:.1%}, enabling standard optimization mode"
            )
            self.reduce_precision = True
            self.cpu_offload_states = True

        else:
            logger.info(f"VRAM usage {current_pressure:.1%}, memory sufficient")

    def cleanup_resources(self):
        """Clean up resources and release memory"""
        # Clean up buffer pool
        self.buffer_pool._buffer_pool.clear()
        self.buffer_pool._current_memory = 0

        # Shutdown asynchronous manager
        self.async_manager.shutdown()

        # Clean up compressed relationships
        self.compressed_relationships.param_pairs.clear()
        self.compressed_relationships.compatibility_scores = torch.tensor(
            [], dtype=torch.float16
        )
        self.compressed_relationships.interaction_types.clear()

        # Clean up quantized importance scores
        self.quantized_importance_scores.clear()

        # Clean up edge cache
        if hasattr(self, "edge_cache"):
            self.edge_cache.clear()

        # Force garbage collection
        torch.cuda.empty_cache()

        logger.info("All optimizer resources cleaned up")

    def __del__(self):
        """Destructor, ensure resources are cleaned up correctly"""
        try:
            self.cleanup_resources()
        except Exception as e:
            logger.warning(f"Error cleaning up resources: {e}")
</file>

<file path="optimizers/muon.py">
## Based on: https://github.com/KellerJordan/Muon/blob/master/muon.py (MIT)

import torch
import torch.distributed as dist


def zeropower_via_newtonschulz5(G, steps: int):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert (
        G.ndim >= 2
    )  # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = (
            b * A + c * A @ A
        )  # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


def muon_update(grad, momentum, beta=0.95, ns_steps=5, nesterov=True):
    momentum.lerp_(grad, 1 - beta)
    update = grad.lerp_(momentum, beta) if nesterov else momentum
    if update.ndim == 4:  # for the case of conv filters
        update = update.view(len(update), -1)
    update = zeropower_via_newtonschulz5(update, steps=ns_steps)
    update *= max(1, grad.size(-2) / grad.size(-1)) ** 0.5
    return update


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. For efficient orthogonalization we use a Newton-Schulz iteration, which has the
    advantage that it can be stably run in bfloat16 on the GPU.

    Muon should only be used for hidden weight layers. The input embedding, final output layer,
    and any internal gains or biases should be optimized using a standard method such as AdamW.
    Hidden convolutional weights can be trained using Muon by viewing them as 2D and then
    collapsing their last 3 dimensions.

    Arguments:
        lr: The learning rate, in units of spectral norm per update.
        weight_decay: The AdamW-style weight decay.
        momentum: The momentum. A value of 0.95 here is usually fine.
    """

    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        assert (
            isinstance(params, list)
            and len(params) >= 1
            and isinstance(params[0], torch.nn.Parameter)
        )
        params = sorted(params, key=lambda x: x.size(), reverse=True)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * (
                dist.get_world_size() - len(params) % dist.get_world_size()
            )
            for base_i in range(len(params))[:: dist.get_world_size()]:
                if base_i + dist.get_rank() < len(params):
                    p = params[base_i + dist.get_rank()]
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(p)
                    update = muon_update(
                        p.grad, state["momentum_buffer"], beta=group["momentum"]
                    )
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update.reshape(p.shape), alpha=-group["lr"])
                dist.all_gather(
                    params_pad[base_i : base_i + dist.get_world_size()],
                    params_pad[base_i + dist.get_rank()],
                )

        return loss


class SingleDeviceMuon(torch.optim.Optimizer):
    """
    Muon variant for usage in non-distributed settings.
    """

    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    # continue
                    p.grad = torch.zeros_like(p)  # Force synchronization
                state = self.state[p]
                if len(state) == 0:
                    state["momentum_buffer"] = torch.zeros_like(p)
                update = muon_update(
                    p.grad, state["momentum_buffer"], beta=group["momentum"]
                )
                p.mul_(1 - group["lr"] * group["weight_decay"])
                p.add_(update.reshape(p.shape), alpha=-group["lr"])

        return loss


def adam_update(grad, buf1, buf2, step, betas, eps):
    buf1.lerp_(grad, 1 - betas[0])
    buf2.lerp_(grad.square(), 1 - betas[1])
    buf1c = buf1 / (1 - betas[0] ** step)
    buf2c = buf2 / (1 - betas[1] ** step)
    return buf1c / (buf2c.sqrt() + eps)


class MuonWithAuxAdam(torch.optim.Optimizer):
    """
    Distributed Muon variant that can be used for all parameters in the network, since it runs an
    internal AdamW for the parameters that are not compatible with Muon. The user must manually
    specify which parameters shall be optimized with Muon and which with Adam by passing in a
    list of param_groups with the `use_muon` flag set.

    The point of this class is to allow the user to have a single optimizer in their code, rather
    than having both a Muon and an Adam which each need to be stepped.

    You can see an example usage below:

    https://github.com/KellerJordan/modded-nanogpt/blob/master/records/052525_MuonWithAuxAdamExample/b01550f9-03d8-4a9c-86fe-4ab434f1c5e0.txt#L470
    ```
    hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
    embed_params = [p for n, p in model.named_parameters() if "embed" in n]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    from muon import MuonWithAuxAdam
    adam_groups = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    adam_groups = [dict(**g, betas=(0.8, 0.95), eps=1e-10, use_muon=False) for g in adam_groups]
    muon_group = dict(params=hidden_matrix_params, lr=0.05, momentum=0.95, use_muon=True)
    param_groups = [*adam_groups, muon_group]
    optimizer = MuonWithAuxAdam(param_groups)
    ```
    """

    def __init__(self, param_groups):
        for group in param_groups:
            assert "use_muon" in group
            if group["use_muon"]:
                group["params"] = sorted(
                    group["params"], key=lambda x: x.size(), reverse=True
                )
                # defaults
                group["lr"] = group.get("lr", 0.02)
                group["momentum"] = group.get("momentum", 0.95)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(
                    ["params", "lr", "momentum", "weight_decay", "use_muon"]
                )
            else:
                # defaults
                group["lr"] = group.get("lr", 3e-4)
                group["betas"] = group.get("betas", (0.9, 0.95))
                group["eps"] = group.get("eps", 1e-10)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(
                    ["params", "lr", "betas", "eps", "weight_decay", "use_muon"]
                )
        super().__init__(param_groups, dict())

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            if group["use_muon"]:
                params = group["params"]
                params_pad = params + [torch.empty_like(params[-1])] * (
                    dist.get_world_size() - len(params) % dist.get_world_size()
                )
                for base_i in range(len(params))[:: dist.get_world_size()]:
                    if base_i + dist.get_rank() < len(params):
                        p = params[base_i + dist.get_rank()]
                        if p.grad is None:
                            # continue
                            p.grad = torch.zeros_like(p)  # Force synchronization
                        state = self.state[p]
                        if len(state) == 0:
                            state["momentum_buffer"] = torch.zeros_like(p)
                        update = muon_update(
                            p.grad, state["momentum_buffer"], beta=group["momentum"]
                        )
                        p.mul_(1 - group["lr"] * group["weight_decay"])
                        p.add_(update.reshape(p.shape), alpha=-group["lr"])
                    dist.all_gather(
                        params_pad[base_i : base_i + dist.get_world_size()],
                        params_pad[base_i + dist.get_rank()],
                    )
            else:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["exp_avg"] = torch.zeros_like(p)
                        state["exp_avg_sq"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1
                    update = adam_update(
                        p.grad,
                        state["exp_avg"],
                        state["exp_avg_sq"],
                        state["step"],
                        group["betas"],
                        group["eps"],
                    )
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update, alpha=-group["lr"])

        return loss


class SingleDeviceMuonWithAuxAdam(torch.optim.Optimizer):
    """
    Non-distributed variant of MuonWithAuxAdam.
    """

    def __init__(self, param_groups):
        for group in param_groups:
            assert "use_muon" in group
            if group["use_muon"]:
                # defaults
                group["lr"] = group.get("lr", 0.02)
                group["momentum"] = group.get("momentum", 0.95)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(
                    ["params", "lr", "momentum", "weight_decay", "use_muon"]
                )
            else:
                # defaults
                group["lr"] = group.get("lr", 3e-4)
                group["betas"] = group.get("betas", (0.9, 0.95))
                group["eps"] = group.get("eps", 1e-10)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(
                    ["params", "lr", "betas", "eps", "weight_decay", "use_muon"]
                )
        super().__init__(param_groups, dict())

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            if group["use_muon"]:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(p)
                    update = muon_update(
                        p.grad, state["momentum_buffer"], beta=group["momentum"]
                    )
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update.reshape(p.shape), alpha=-group["lr"])
            else:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["exp_avg"] = torch.zeros_like(p)
                        state["exp_avg_sq"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1
                    update = adam_update(
                        p.grad,
                        state["exp_avg"],
                        state["exp_avg_sq"],
                        state["step"],
                        group["betas"],
                        group["eps"],
                    )
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update, alpha=-group["lr"])

        return loss
</file>

<file path="optimizers/optimizer_utils.py">
## Based on: https://github.com/tdrussell/diffusion-pipe/blob/main/optimizers/optimizer_utils.py (MIT)

# Copied from AI Toolkit.

# MIT License

# Copyright (c) 2024 Ostris, LLC

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import torch
from torch import Tensor
from typing import Optional
from optimum.quanto import QBytesTensor


def compute_scale_for_dtype(tensor, dtype):
    """
    Compute appropriate scale for the given tensor and target dtype.

    Args:
        tensor: Input tensor to be quantized
        dtype: Target dtype for quantization
    Returns:
        Appropriate scale factor for the quantization
    """
    if dtype == torch.int8:
        abs_max = torch.max(torch.abs(tensor))
        return abs_max / 127.0 if abs_max > 0 else 1.0
    elif dtype == torch.uint8:
        max_val = torch.max(tensor)
        min_val = torch.min(tensor)
        range_val = max_val - min_val
        return range_val / 255.0 if range_val > 0 else 1.0
    elif dtype in (torch.float8_e4m3fn, torch.float8_e5m2):
        # For float8, we typically want to preserve the magnitude of the values
        # while fitting within the representable range of the format
        abs_max = torch.max(torch.abs(tensor))
        if dtype == torch.float8_e4m3fn:
            # e4m3fn has range [-448, 448] with no infinities
            max_representable = 448.0
        else:  # torch.float8_e5m2
            # e5m2 has range [-57344, 57344] with infinities
            max_representable = 57344.0

        return abs_max / max_representable if abs_max > 0 else 1.0
    else:
        raise ValueError(f"Unsupported dtype for quantization: {dtype}")


def quantize_tensor(tensor, dtype):
    """
    Quantize a floating-point tensor to the target dtype with appropriate scaling.

    Args:
        tensor: Input tensor (float)
        dtype: Target dtype for quantization
    Returns:
        quantized_data: Quantized tensor
        scale: Scale factor used
    """
    scale = compute_scale_for_dtype(tensor, dtype)

    if dtype == torch.int8:
        quantized_data = torch.clamp(torch.round(tensor / scale), -128, 127).to(dtype)
    elif dtype == torch.uint8:
        quantized_data = torch.clamp(torch.round(tensor / scale), 0, 255).to(dtype)
    elif dtype in (torch.float8_e4m3fn, torch.float8_e5m2):
        # For float8, we scale and then cast directly to the target type
        # The casting operation will handle the appropriate rounding
        scaled_tensor = tensor / scale
        quantized_data = scaled_tensor.to(dtype)
    else:
        raise ValueError(f"Unsupported dtype for quantization: {dtype}")

    return quantized_data, scale


def update_parameter(target, result_float):
    """
    Updates a parameter tensor, handling both regular torch.Tensor and QBytesTensor cases
    with proper rescaling for quantized tensors.

    Args:
        target: The parameter to update (either torch.Tensor or QBytesTensor)
        result_float: The new values to assign (torch.Tensor)
    """
    if isinstance(target, QBytesTensor):
        # Get the target dtype from the existing quantized tensor
        target_dtype = target._data.dtype

        # Handle device placement
        device = target._data.device
        result_float = result_float.to(device)

        # Compute new quantized values and scale
        quantized_data, new_scale = quantize_tensor(result_float, target_dtype)

        # Update the internal tensors with newly computed values
        target._data.copy_(quantized_data)
        target._scale.copy_(new_scale)
    else:
        # Regular tensor update
        target.copy_(result_float)


def get_format_params(dtype: torch.dtype) -> tuple[int, int]:
    """
    Returns (mantissa_bits, total_bits) for each format.
    mantissa_bits excludes the implicit leading 1.
    """
    if dtype == torch.float32:
        return 23, 32
    elif dtype == torch.bfloat16:
        return 7, 16
    elif dtype == torch.float16:
        return 10, 16
    elif dtype == torch.float8_e4m3fn:
        return 3, 8
    elif dtype == torch.float8_e5m2:
        return 2, 8
    elif dtype == torch.int8:
        return 0, 8  # Int8 doesn't have mantissa bits
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")


def copy_stochastic_bf16(target: torch.Tensor, source: torch.Tensor):
    # adapted from https://github.com/Nerogar/OneTrainer/blob/411532e85f3cf2b52baa37597f9c145073d54511/modules/util/bf16_stochastic_rounding.py#L5
    # create a random 16 bit integer
    result = torch.randint_like(
        source,
        dtype=torch.int32,
        low=0,
        high=(1 << 16),
    )

    # add the random number to the lower 16 bit of the mantissa
    result.add_(source.view(dtype=torch.int32))

    # mask off the lower 16 bit of the mantissa
    result.bitwise_and_(-65536)  # -65536 = FFFF0000 as a signed int32

    # copy the higher 16 bit into the target tensor
    target.copy_(result.view(dtype=torch.float32))

    del result


def copy_stochastic(
    target: torch.Tensor, source: torch.Tensor, eps: Optional[float] = None
) -> None:
    """
    Performs stochastic rounding from source tensor to target tensor.

    Args:
        target: Destination tensor (determines the target format)
        source: Source tensor (typically float32)
        eps: Optional minimum value for stochastic rounding (for numerical stability)
    """
    with torch.no_grad():
        # If target is float32, just copy directly
        if target.dtype == torch.float32:
            target.copy_(source)
            return

        # Special handling for int8
        if target.dtype == torch.int8:
            # Scale the source values to utilize the full int8 range
            scaled = source * 127.0  # Scale to [-127, 127]

            # Add random noise for stochastic rounding
            noise = torch.rand_like(scaled) - 0.5
            rounded = torch.round(scaled + noise)

            # Clamp to int8 range
            clamped = torch.clamp(rounded, -127, 127)
            target.copy_(clamped.to(torch.int8))
            return

        mantissa_bits, _ = get_format_params(target.dtype)

        # Convert source to int32 view
        source_int = source.view(dtype=torch.int32)

        # Calculate number of bits to round
        bits_to_round = 23 - mantissa_bits  # 23 is float32 mantissa bits

        # Create random integers for stochastic rounding
        rand = torch.randint_like(
            source,
            dtype=torch.int32,
            low=0,
            high=(1 << bits_to_round),
        )

        # Add random values to the bits that will be rounded off
        result = source_int.clone()
        result.add_(rand)

        # Mask to keep only the bits we want
        # Create mask with 1s in positions we want to keep
        mask = (-1) << bits_to_round
        result.bitwise_and_(mask)

        # Handle minimum value threshold if specified
        if eps is not None:
            eps_int = torch.tensor(eps, dtype=torch.float32).view(dtype=torch.int32)
            zero_mask = result.abs() < eps_int
            result[zero_mask] = torch.sign(source_int[zero_mask]) * eps_int

        # Convert back to float32 view
        result_float = result.view(dtype=torch.float32)

        # Special handling for float8 formats
        if target.dtype == torch.float8_e4m3fn:
            result_float.clamp_(-448.0, 448.0)
        elif target.dtype == torch.float8_e5m2:
            result_float.clamp_(-57344.0, 57344.0)

        # Copy the result to the target tensor
        update_parameter(target, result_float)
        # target.copy_(result_float)
        del result, rand, source_int


class Auto8bitTensor:
    def __init__(self, data: Tensor, *args, **kwargs):
        if isinstance(data, dict):  # Add constructor from state dict
            self._load_from_state_dict(data)
        else:
            abs_max = data.abs().max().item()
            scale = abs_max / 127.0 if abs_max > 0 else 1.0

            self.quantized = (data / scale).round().clamp(-127, 127).to(torch.int8)
            self.scale = scale
            self.orig_dtype = data.dtype

    def dequantize(self) -> Tensor:
        return self.quantized.to(dtype=torch.float32) * self.scale

    def to(self, *args, **kwargs):
        # Handle the dtype argument whether it's positional or keyword
        dtype = None
        if args and isinstance(args[0], torch.dtype):
            dtype = args[0]
            args = args[1:]
        elif "dtype" in kwargs:
            dtype = kwargs["dtype"]
            del kwargs["dtype"]

        if dtype is not None:
            # First dequantize then convert to requested dtype
            return self.dequantize().to(dtype=dtype, *args, **kwargs)

        # If no dtype specified, just pass through to parent
        return self.dequantize().to(*args, **kwargs)

    def state_dict(self):
        """Returns a dictionary containing the current state of the tensor."""
        return {
            "quantized": self.quantized,
            "scale": self.scale,
            "orig_dtype": self.orig_dtype,
        }

    def _load_from_state_dict(self, state_dict):
        """Loads the tensor state from a state dictionary."""
        self.quantized = state_dict["quantized"]
        self.scale = state_dict["scale"]
        self.orig_dtype = state_dict["orig_dtype"]

    def __str__(self):
        return f"Auto8bitTensor({self.dequantize()})"


def stochastic_grad_accummulation(param):
    if hasattr(param, "_accum_grad"):
        grad_fp32 = param._accum_grad.clone().to(torch.float32)
        grad_fp32.add_(param.grad.to(torch.float32))
        copy_stochastic(param._accum_grad, grad_fp32)
        del grad_fp32
        del param.grad
    else:
        param._accum_grad = param.grad.clone()
        del param.grad
</file>

<file path="optimizers/prodigy_8bit.py">
## Based on: https://github.com/ostris/ai-toolkit/blob/main/toolkit/optimizers/prodigy_8bit.py (MIT)

import math
import torch
import torch.distributed as dist
from torch.optim import Optimizer

from optimizers.optimizer_utils import (
    Auto8bitTensor,
    copy_stochastic,
    stochastic_grad_accummulation,
)


class Prodigy8bit(Optimizer):
    r"""
    Implements Adam with Prodigy step-sizes.
    Handles stochastic rounding for various precisions as well as stochastic gradient accumulation.
    Stores state in 8bit for memory savings.
    Leave LR set to 1 unless you encounter instability.

    Arguments:
        params (iterable):
            Iterable of parameters to optimize or dicts defining parameter groups.
        lr (float):
            Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate.
        betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        beta3 (float):
            coefficients for computing the Prodidy stepsize using running averages.
            If set to None, uses the value of square root of beta2 (default: None).
        eps (float):
            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).
        weight_decay (float):
            Weight decay, i.e. a L2 penalty (default: 0).
        decouple (boolean):
            Use AdamW style decoupled weight decay
        use_bias_correction (boolean):
            Turn on Adam's bias correction. Off by default.
        safeguard_warmup (boolean):
            Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default.
        d0 (float):
            Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.
        d_coef (float):
            Coefficient in the expression for the estimate of d (default 1.0).
            Values such as 0.5 and 2.0 typically work as well.
            Changing this parameter is the preferred way to tune the method.
        growth_rate (float):
            prevent the D estimate from growing faster than this multiplicative rate.
            Default is inf, for unrestricted. Values like 1.02 give a kind of learning
            rate warmup effect.
        fsdp_in_use (bool):
            If you're using sharded parameters, this should be set to True. The optimizer
            will attempt to auto-detect this, but if you're using an implementation other
            than PyTorch's builtin version, the auto-detection won't work.
    """

    def __init__(
        self,
        params,
        lr=1.0,
        betas=(0.9, 0.999),
        beta3=None,
        eps=1e-8,
        weight_decay=0,
        decouple=True,
        use_bias_correction=False,
        safeguard_warmup=False,
        d0=1e-6,
        d_coef=1.0,
        growth_rate=float("inf"),
        fsdp_in_use=False,
    ):
        if not 0.0 < d0:
            raise ValueError("Invalid d0 value: {}".format(d0))
        if not 0.0 < lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 < eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

        if decouple and weight_decay > 0:
            print(f"Using decoupled weight decay")

        defaults = dict(
            lr=lr,
            betas=betas,
            beta3=beta3,
            eps=eps,
            weight_decay=weight_decay,
            d=d0,
            d0=d0,
            d_max=d0,
            d_numerator=0.0,
            d_coef=d_coef,
            k=0,
            growth_rate=growth_rate,
            use_bias_correction=use_bias_correction,
            decouple=decouple,
            safeguard_warmup=safeguard_warmup,
            fsdp_in_use=fsdp_in_use,
        )
        self.d0 = d0
        super(Prodigy8bit, self).__init__(params, defaults)

        self.is_stochastic_rounding_accumulation = False

        # setup stochastic grad accum hooks
        for group in self.param_groups:
            for param in group["params"]:
                if param.requires_grad and param.dtype != torch.float32:
                    self.is_stochastic_rounding_accumulation = True
                    param.register_post_accumulate_grad_hook(
                        stochastic_grad_accummulation
                    )

    @property
    def supports_memory_efficient_fp16(self):
        return False

    @property
    def supports_flat_params(self):
        return True

    def step_hook(self):
        if not self.is_stochastic_rounding_accumulation:
            return
        # copy over stochastically rounded grads
        for group in self.param_groups:
            for param in group["params"]:
                if param.requires_grad and hasattr(param, "_accum_grad"):
                    param.grad = param._accum_grad
                    del param._accum_grad

    @torch.no_grad()
    def step(self, closure=None):
        """Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        # call pre step
        self.step_hook()
        loss = None
        if closure is not None:
            loss = closure()

        d_denom = 0.0

        group = self.param_groups[0]
        use_bias_correction = group["use_bias_correction"]
        beta1, beta2 = group["betas"]
        beta3 = group["beta3"]
        if beta3 is None:
            beta3 = math.sqrt(beta2)
        k = group["k"]

        d = group["d"]
        d_max = group["d_max"]
        d_coef = group["d_coef"]
        lr = max(group["lr"] for group in self.param_groups)

        if use_bias_correction:
            bias_correction = ((1 - beta2 ** (k + 1)) ** 0.5) / (1 - beta1 ** (k + 1))
        else:
            bias_correction = 1

        dlr = d * lr * bias_correction

        growth_rate = group["growth_rate"]
        decouple = group["decouple"]
        fsdp_in_use = group["fsdp_in_use"]

        d_numerator = group["d_numerator"]
        d_numerator *= beta3

        for group in self.param_groups:
            decay = group["weight_decay"]
            k = group["k"]
            eps = group["eps"]
            group_lr = group["lr"]
            d0 = group["d0"]
            safeguard_warmup = group["safeguard_warmup"]

            if group_lr not in [lr, 0.0]:
                raise RuntimeError(
                    f"Setting different lr values in different parameter groups is only supported for values of 0"
                )

            for p in group["params"]:
                if p.grad is None:
                    continue
                if hasattr(p, "_fsdp_flattened"):
                    fsdp_in_use = True

                grad = p.grad.data.to(torch.float32)
                p_fp32 = p.clone().to(torch.float32)

                # Apply weight decay (coupled variant)
                if decay != 0 and not decouple:
                    grad.add_(p_fp32.data, alpha=decay)

                state = self.state[p]

                # State initialization
                if "step" not in state:
                    state["step"] = 0
                    state["s"] = Auto8bitTensor(torch.zeros_like(p_fp32.data).detach())
                    state["p0"] = Auto8bitTensor(p_fp32.detach().clone())
                    # Exponential moving average of gradient values
                    state["exp_avg"] = Auto8bitTensor(
                        torch.zeros_like(p_fp32.data).detach()
                    )
                    # Exponential moving average of squared gradient values
                    state["exp_avg_sq"] = Auto8bitTensor(
                        torch.zeros_like(p_fp32.data).detach()
                    )

                exp_avg = state["exp_avg"].to(torch.float32)
                exp_avg_sq = state["exp_avg_sq"].to(torch.float32)

                s = state["s"].to(torch.float32)
                p0 = state["p0"].to(torch.float32)

                if group_lr > 0.0:
                    # we use d / d0 instead of just d to avoid getting values that are too small
                    d_numerator += (
                        (d / d0)
                        * dlr
                        * torch.dot(
                            grad.flatten(), (p0.data - p_fp32.data).flatten()
                        ).item()
                    )

                    # Adam EMA updates
                    exp_avg.mul_(beta1).add_(grad, alpha=d * (1 - beta1))
                    exp_avg_sq.mul_(beta2).addcmul_(
                        grad, grad, value=d * d * (1 - beta2)
                    )

                    if safeguard_warmup:
                        s.mul_(beta3).add_(grad, alpha=((d / d0) * d))
                    else:
                        s.mul_(beta3).add_(grad, alpha=((d / d0) * dlr))
                    d_denom += s.abs().sum().item()

                # update state with stochastic rounding
                state["exp_avg"] = Auto8bitTensor(exp_avg)
                state["exp_avg_sq"] = Auto8bitTensor(exp_avg_sq)
                state["s"] = Auto8bitTensor(s)
                state["p0"] = Auto8bitTensor(p0)

        d_hat = d

        # if we have not done any progres, return
        # if we have any gradients available, will have d_denom > 0 (unless \|g\|=0)
        if d_denom == 0:
            return loss

        if lr > 0.0:
            if fsdp_in_use:
                dist_tensor = torch.zeros(2).cuda()
                dist_tensor[0] = d_numerator
                dist_tensor[1] = d_denom
                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)
                global_d_numerator = dist_tensor[0]
                global_d_denom = dist_tensor[1]
            else:
                global_d_numerator = d_numerator
                global_d_denom = d_denom

            d_hat = d_coef * global_d_numerator / global_d_denom
            if d == group["d0"]:
                d = max(d, d_hat)
            d_max = max(d_max, d_hat)
            d = min(d_max, d * growth_rate)

        for group in self.param_groups:
            group["d_numerator"] = global_d_numerator
            group["d_denom"] = global_d_denom
            group["d"] = d
            group["d_max"] = d_max
            group["d_hat"] = d_hat

            decay = group["weight_decay"]
            k = group["k"]
            eps = group["eps"]

            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data.to(torch.float32)
                p_fp32 = p.clone().to(torch.float32)

                state = self.state[p]

                exp_avg = state["exp_avg"].to(torch.float32)
                exp_avg_sq = state["exp_avg_sq"].to(torch.float32)

                state["step"] += 1

                denom = exp_avg_sq.sqrt().add_(d * eps)

                # Apply weight decay (decoupled variant)
                if decay != 0 and decouple:
                    p_fp32.data.add_(p_fp32.data, alpha=-decay * dlr)

                # Take step
                p_fp32.data.addcdiv_(exp_avg, denom, value=-dlr)
                # apply stochastic rounding
                copy_stochastic(p.data, p_fp32.data)

            group["k"] = k + 1

        return loss
</file>

<file path="optimizers/safe_globals_manager.py">
"""Safe globals manager for custom optimizer classes.

This module provides a centralized way to add custom optimizer classes to PyTorch's
safe globals list for state loading compatibility with PyTorch 2.6+.
"""

import logging
import torch

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class SafeGlobalsManager:
    """Manages PyTorch safe globals for custom optimizer classes."""

    @staticmethod
    def add_custom_optimizer_safe_globals():
        """Add custom optimizer classes to PyTorch's safe globals list for state loading."""
        try:
            # Import Fira classes that need to be in the safe globals list
            from fira.gradient_projection import GradientProjector
            from fira import FiraAdamW

            # Import custom classes from other optimizers
            from optimizers.optimizer_utils import Auto8bitTensor
            from optimizers.hina_adaptive import (
                EnhancedBufferPool,
                CompactStateDict,
                CompressedRelationships,
                AsyncComputeManager,
            )

            # Add to PyTorch's safe globals
            torch.serialization.add_safe_globals(
                [
                    # Fira optimizer classes
                    GradientProjector,
                    FiraAdamW,
                    # Automagic optimizer classes
                    Auto8bitTensor,
                    # HinaAdaptive optimizer classes
                    EnhancedBufferPool,
                    CompactStateDict,
                    CompressedRelationships,
                    AsyncComputeManager,
                ]
            )
            logger.info("‚úÖ Added custom optimizer classes to PyTorch safe globals")
        except ImportError:
            logger.warning(
                "‚ö†Ô∏è  Some optimizer packages not available, skipping safe globals addition"
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to add custom optimizer safe globals: {e}")
</file>

<file path="optimizers/sana_optimizer.py">
## Based on: https://github.com/NVlabs/Sana/blob/main/diffusion/utils/optimizer.py (Apache)

# Copyright 2024 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import logging
import math
from typing import Callable, Optional, Tuple

import numpy as np
import torch
import torch.optim
from bitsandbytes.optim import AdamW8bit
from mmcv import Config
from mmcv.runner import OPTIMIZER_BUILDERS, OPTIMIZERS, DefaultOptimizerConstructor
from mmcv.runner import build_optimizer as mm_build_optimizer
from mmcv.utils import _BatchNorm, _InstanceNorm
from termcolor import colored
from torch.nn import GroupNorm, LayerNorm
from torch.optim.optimizer import Optimizer

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def auto_scale_lr(effective_bs, optimizer_cfg, rule="linear", base_batch_size=256):
    assert rule in ["linear", "sqrt"]

    # scale by world size
    if rule == "sqrt":
        scale_ratio = math.sqrt(effective_bs / base_batch_size)
    elif rule == "linear":
        scale_ratio = effective_bs / base_batch_size
    optimizer_cfg["lr"] *= scale_ratio
    logger.info(
        f'Automatically adapt lr to {optimizer_cfg["lr"]:.5f} (using {rule} scaling rule).'
    )
    return scale_ratio


@OPTIMIZER_BUILDERS.register_module()
class MyOptimizerConstructor(DefaultOptimizerConstructor):
    def add_params(self, params, module, prefix="", is_dcn_module=None):
        """Add all parameters of module to the params list.

        The parameters of the given module will be added to the list of param
        groups, with specific rules defined by paramwise_cfg.

        Args:
            params (list[dict]): A list of param groups, it will be modified
                in place.
            module (nn.Module): The module to be added.
            prefix (str): The prefix of the module

        """
        # get param-wise options
        custom_keys = self.paramwise_cfg.get("custom_keys", {})
        # first sort with alphabet order and then sort with reversed len of str
        # sorted_keys = sorted(sorted(custom_keys.keys()), key=len, reverse=True)

        bias_lr_mult = self.paramwise_cfg.get("bias_lr_mult", 1.0)
        bias_decay_mult = self.paramwise_cfg.get("bias_decay_mult", 1.0)
        norm_decay_mult = self.paramwise_cfg.get("norm_decay_mult", 1.0)
        bypass_duplicate = self.paramwise_cfg.get("bypass_duplicate", False)

        # special rules for norm layers and depth-wise conv layers
        is_norm = isinstance(module, (_BatchNorm, _InstanceNorm, GroupNorm, LayerNorm))

        for name, param in module.named_parameters(recurse=False):
            base_lr = self.base_lr
            if name == "bias" and not (is_norm or is_dcn_module):
                base_lr *= bias_lr_mult

            # apply weight decay policies
            base_wd = self.base_wd
            if self.base_wd is not None:
                # norm decay
                if is_norm:
                    base_wd *= norm_decay_mult
                # bias lr and decay
                elif name == "bias" and not is_dcn_module:
                    # TODO: current bias_decay_mult will have affect on DCN
                    base_wd *= bias_decay_mult

            param_group = {"params": [param]}
            if not param.requires_grad:
                param_group["requires_grad"] = False  # type: ignore
                params.append(param_group)
                continue
            if bypass_duplicate and self._is_in(param_group, params):
                logger = get_root_logger()  # type: ignore
                logger.warn(
                    f"{prefix} is duplicate. It is skipped since "
                    f"bypass_duplicate={bypass_duplicate}"
                )
                continue
            # if the parameter match one of the custom keys, ignore other rules
            is_custom = False
            for key in custom_keys:
                if isinstance(key, tuple):
                    scope, key_name = key
                else:
                    scope, key_name = None, key
                if scope is not None and scope not in f"{prefix}":
                    continue
                if key_name in f"{prefix}.{name}":
                    is_custom = True
                    if "lr_mult" in custom_keys[key]:
                        # if 'base_classes' in f'{prefix}.{name}' or 'attn_base' in f'{prefix}.{name}':
                        #     param_group['lr'] = self.base_lr
                        # else:
                        param_group["lr"] = self.base_lr * custom_keys[key]["lr_mult"]
                    elif "lr" not in param_group:
                        param_group["lr"] = base_lr
                    if self.base_wd is not None:
                        if "decay_mult" in custom_keys[key]:
                            param_group["weight_decay"] = (
                                self.base_wd * custom_keys[key]["decay_mult"]
                            )
                        elif "weight_decay" not in param_group:
                            param_group["weight_decay"] = base_wd

            if not is_custom:
                # bias_lr_mult affects all bias parameters
                # except for norm.bias dcn.conv_offset.bias
                if base_lr != self.base_lr:
                    param_group["lr"] = base_lr
                if base_wd != self.base_wd:
                    param_group["weight_decay"] = base_wd
            params.append(param_group)

        for child_name, child_mod in module.named_children():
            child_prefix = f"{prefix}.{child_name}" if prefix else child_name
            self.add_params(
                params, child_mod, prefix=child_prefix, is_dcn_module=is_dcn_module
            )


def build_optimizer(model, optimizer_cfg):
    # default parameter-wise config

    if hasattr(model, "module"):
        model = model.module
    # set optimizer constructor
    optimizer_cfg.setdefault("constructor", "MyOptimizerConstructor")
    # parameter-wise setting: cancel weight decay for some specific modules
    custom_keys = dict()
    for name, module in model.named_modules():
        if hasattr(module, "zero_weight_decay"):
            custom_keys.update(
                {(name, key): dict(decay_mult=0) for key in module.zero_weight_decay}
            )

    paramwise_cfg = Config(dict(cfg=dict(custom_keys=custom_keys)))
    given_cfg = optimizer_cfg.get("paramwise_cfg")
    if given_cfg:
        paramwise_cfg.merge_from_dict(dict(cfg=given_cfg))
    optimizer_cfg["paramwise_cfg"] = paramwise_cfg.cfg
    # build optimizer
    optimizer = mm_build_optimizer(model, optimizer_cfg)

    weight_decay_groups = dict()
    lr_groups = dict()
    for group in optimizer.param_groups:
        if not group.get("requires_grad", True):
            continue
        lr_groups.setdefault(group["lr"], []).append(group)
        weight_decay_groups.setdefault(group["weight_decay"], []).append(group)

    learnable_count, fix_count = 0, 0
    for p in model.parameters():
        if p.requires_grad:
            learnable_count += 1
        else:
            fix_count += 1
    fix_info = colored(f"{learnable_count} are learnable, {fix_count} are fix", "green")
    lr_info = "Lr group: " + ", ".join(
        [f"{len(group)} params with lr {lr:.5f}" for lr, group in lr_groups.items()]
    )
    wd_info = "Weight decay group: " + ", ".join(
        [
            f"{len(group)} params with weight decay {wd}"
            for wd, group in weight_decay_groups.items()
        ]
    )
    opt_info = f"{optimizer.__class__.__name__} Optimizer: total {len(optimizer.param_groups)} param groups, {fix_info}. {lr_info}; {wd_info}."
    logger.info(opt_info)

    return optimizer


@OPTIMIZERS.register_module()
class Lion(Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-4,
        betas: Tuple[float, float] = (0.9, 0.99),
        weight_decay: float = 0.0,
    ):
        assert lr > 0.0
        assert all([0.0 <= beta <= 1.0 for beta in betas])

        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)

        super().__init__(params, defaults)

    @staticmethod
    def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):
        # stepweight decay
        p.data.mul_(1 - lr * wd)

        # weight update
        update = exp_avg.clone().lerp_(grad, 1 - beta1).sign_()
        p.add_(update, alpha=-lr)

        # decay the momentum running average coefficient
        exp_avg.lerp_(grad, 1 - beta2)

    @staticmethod
    def exists(val):
        return val is not None

    @torch.no_grad()
    def step(self, closure: Optional[Callable] = None):

        loss = None
        if self.exists(closure):
            with torch.enable_grad():
                loss = closure()  # type: ignore

        for group in self.param_groups:
            for p in filter(lambda p: self.exists(p.grad), group["params"]):

                grad, lr, wd, beta1, beta2, state = (
                    p.grad,
                    group["lr"],
                    group["weight_decay"],
                    *group["betas"],
                    self.state[p],
                )

                # init state - exponential moving average of gradient values
                if len(state) == 0:
                    state["exp_avg"] = torch.zeros_like(p)

                exp_avg = state["exp_avg"]

                self.update_fn(p, grad, exp_avg, lr, wd, beta1, beta2)

        return loss


@OPTIMIZERS.register_module()
class AdamW8bitWrapper(AdamW8bit):
    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)


@OPTIMIZERS.register_module()
class CAMEWrapper(torch.optim.Optimizer):
    """Implements CAME algorithm.
    This implementation is based on:
    `CAME: Confidence-guided Adaptive Memory Efficient Optimization`
    Args:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float, optional): external learning rate (default: None)
        eps (tuple[float, float]): regularization constants for square gradient
            and instability respectively (default: (1e-30, 1e-16))
        clip_threshold (float): threshold of root-mean-square of
            final gradient update (default: 1.0)
        betas (tuple[float, float, float]): coefficient used for computing running averages of
        update, square gradient and instability (default: (0.9, 0.999, 0.9999)))
        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    """

    def __init__(
        self,
        params,
        lr=None,
        eps=(1e-30, 1e-16),
        clip_threshold=1.0,
        betas=(0.9, 0.999, 0.9999),
        weight_decay=0.0,
    ):
        assert lr > 0.0  # type: ignore
        assert all([0.0 <= beta <= 1.0 for beta in betas])

        defaults = dict(
            lr=lr,
            eps=eps,
            clip_threshold=clip_threshold,
            betas=betas,
            weight_decay=weight_decay,
        )
        super().__init__(params, defaults)

    @property
    def supports_memory_efficient_fp16(self):
        return True

    @property
    def supports_flat_params(self):
        return False

    def _get_options(self, param_shape):
        if len(param_shape) == 4:  # Convolutional layer
            if param_shape[2] == 1 and param_shape[3] == 1:  # 1x1 conv
                return True, "1x1_conv"
            else:  # 3x3 conv or others
                return False, "conv"
        elif len(param_shape) == 2:  # Linear layer, exactly 2D
            return True, "linear"
        return False, "other"

    def _rms(self, tensor):
        return tensor.norm(2) / (tensor.numel() ** 0.5)

    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):
        r_factor = (
            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True))
            .rsqrt_()
            .unsqueeze(-1)
        )
        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()
        return torch.mul(r_factor, c_factor)

    def step(self, closure=None):
        """Performs a single optimization step.
        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.dtype in {torch.float16, torch.bfloat16}:
                    grad = grad.float()
                if grad.is_sparse:
                    raise RuntimeError("CAME does not support sparse gradients.")

                state = self.state[p]
                grad_shape = grad.shape

                # factored = self._get_options(grad_shape)
                factored, layer_type = self._get_options(grad_shape)
                # State Initialization
                if len(state) == 0:
                    state["step"] = 0

                    state["exp_avg"] = torch.zeros_like(grad)
                    if factored:
                        if layer_type == "1x1_conv" or layer_type == "linear":
                            # 1x1 conv and linear layers can be handled in the same way
                            state["exp_avg_sq_row"] = torch.zeros(
                                grad_shape[0]
                            ).type_as(grad)
                            state["exp_avg_sq_col"] = torch.zeros(
                                grad_shape[1]
                            ).type_as(grad)
                            state["exp_avg_res_row"] = torch.zeros(
                                grad_shape[0]
                            ).type_as(grad)
                            state["exp_avg_res_col"] = torch.zeros(
                                grad_shape[1]
                            ).type_as(grad)
                        else:
                            state["exp_avg_sq"] = torch.zeros_like(grad)

                    else:
                        state["exp_avg_sq"] = torch.zeros_like(grad)

                    state["RMS"] = 0

                state["step"] += 1
                state["RMS"] = self._rms(p.data)

                update = (grad**2) + group["eps"][0]
                if factored:
                    exp_avg_sq_row = state["exp_avg_sq_row"]
                    exp_avg_sq_col = state["exp_avg_sq_col"]

                    if layer_type == "1x1_conv" or layer_type == "linear":
                        # Handle dimensions
                        if len(grad_shape) == 4:  # 1x1 conv
                            update_reshaped = update.squeeze(-1).squeeze(
                                -1
                            )  # Remove the last two dimensions
                        else:
                            update_reshaped = update

                        exp_avg_sq_row.mul_(group["betas"][1]).add_(
                            update_reshaped.mean(dim=1), alpha=1.0 - group["betas"][1]
                        )
                        exp_avg_sq_col.mul_(group["betas"][1]).add_(
                            update_reshaped.mean(dim=0), alpha=1.0 - group["betas"][1]
                        )

                    # Approximate calculation
                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)
                    if layer_type == "1x1_conv":
                        # Need to reshape back to 4D
                        update = update.view(grad_shape[0], grad_shape[1], 1, 1)
                    update.mul_(grad)
                else:
                    # 3x3 conv or other cases: use standard AdamW method
                    exp_avg_sq = state["exp_avg_sq"]
                    exp_avg_sq.mul_(group["betas"][1]).add_(
                        update, alpha=1.0 - group["betas"][1]
                    )
                    update = exp_avg_sq.rsqrt().mul_(grad)

                update.div_(
                    (self._rms(update) / group["clip_threshold"]).clamp_(min=1.0)
                )

                exp_avg = state["exp_avg"]
                exp_avg.mul_(group["betas"][0]).add_(
                    update, alpha=1 - group["betas"][0]
                )

                # Confidence-guided strategy
                # Calculation of instability
                res = (update - exp_avg) ** 2 + group["eps"][1]

                if factored:
                    exp_avg_res_row = state["exp_avg_res_row"]
                    exp_avg_res_col = state["exp_avg_res_col"]

                    if layer_type == "1x1_conv" or layer_type == "linear":
                        # Handle dimensions
                        if len(grad_shape) == 4:  # 1x1 conv
                            res_reshaped = res.squeeze(-1).squeeze(
                                -1
                            )  # Remove last two dimensions
                        else:
                            res_reshaped = res

                        # Update residual statistics
                        exp_avg_res_row.mul_(group["betas"][2]).add_(
                            res_reshaped.mean(dim=1), alpha=1.0 - group["betas"][2]
                        )
                        exp_avg_res_col.mul_(group["betas"][2]).add_(
                            res_reshaped.mean(dim=0), alpha=1.0 - group["betas"][2]
                        )

                    # Approximate calculation
                    res_approx = self._approx_sq_grad(exp_avg_res_row, exp_avg_res_col)
                    if layer_type == "1x1_conv":
                        # Need to reshape back to 4D
                        res_approx = res_approx.view(grad_shape[0], grad_shape[1], 1, 1)
                    update = res_approx.mul_(exp_avg)
                else:
                    update = exp_avg.clone()

                if group["weight_decay"] != 0:
                    p.data.add_(p.data, alpha=-group["weight_decay"] * group["lr"])

                update.mul_(group["lr"])
                p.data.add_(-update)

        return loss


@OPTIMIZERS.register_module()
class CAME8BitWrapper(torch.optim.Optimizer):
    """8-bit implementation of the CAME optimizer

    Args:
        params (iterable): Parameters to optimize
        lr (float): Learning rate
        eps (tuple[float, float]): Numerical stability constants
        clip_threshold (float): Gradient clipping threshold
        betas (tuple[float, float, float]): Momentum coefficients
        weight_decay (float): Weight decay
        block_size (int): Quantization block size, larger blocks are more memory efficient but less precise
        min_8bit_size (int): Minimum parameter size to use 8-bit, only layers larger than this will be quantized

    Note:
        1. Only large Linear and 1x1 Conv layers are quantized to 8-bit
        2. All statistics (e.g., exp_avg_sq_row) remain in 32-bit for stability
        3. Uses a simple min-max quantization strategy, each block is quantized separately
    """

    def __init__(
        self,
        params,
        lr=None,
        eps=(1e-30, 1e-16),
        clip_threshold=1.0,
        betas=(0.9, 0.999, 0.9999),
        weight_decay=0.0,
        block_size=2048,  # Quantization block size
        min_8bit_size=16384,  # Minimum parameter size to use 8-bit
    ):
        assert lr > 0.0  # type: ignore
        assert all([0.0 <= beta <= 1.0 for beta in betas])

        logger.info(
            f"Initializing CAME8bit with block_size={block_size}, min_8bit_size={min_8bit_size}"
        )

        defaults = dict(
            lr=lr,
            eps=eps,
            clip_threshold=clip_threshold,
            betas=betas,
            weight_decay=weight_decay,
            block_size=block_size,
            min_8bit_size=min_8bit_size,
        )
        super().__init__(params, defaults)

    def print_layer_info(self, param_shape, use_8bit):
        """Prints layer information, including parameter count and whether 8-bit quantization is used

        Args:
            param_shape (tuple): Shape of the parameters
            use_8bit (bool): Whether 8-bit quantization is used
        """
        size = np.prod(param_shape)  # Calculate parameter count
        layer_type = "unknown"
        if len(param_shape) == 1:
            layer_type = "1D Layer"
        elif len(param_shape) == 2:
            layer_type = "Linear"
        elif len(param_shape) == 4:
            if param_shape[2] == 1 and param_shape[3] == 1:
                layer_type = "1x1 Conv"
            else:
                layer_type = "Conv"

        status = "8bit" if use_8bit else "32bit"
        print(
            f"{layer_type} layer with shape {param_shape}: {size:,} params -> using {status}"
        )

    def _should_use_8bit(self, param_shape):
        """Determines whether parameters should be quantized to 8-bit

        Rules:
        1. Linear layers: parameter count > min_8bit_size
        2. 1x1 conv layers: parameter count > min_8bit_size
        3. Other cases: use 32-bit
        """
        if len(param_shape) == 2:  # Linear layers
            return param_shape[0] * param_shape[1] > self.defaults["min_8bit_size"]
        elif (
            len(param_shape) == 4 and param_shape[2] == 1 and param_shape[3] == 1
        ):  # Only quantize 1x1 conv
            return param_shape[0] * param_shape[1] > self.defaults["min_8bit_size"]
        return False  # Other layers are not quantized

    def _quantize_state(self, state_tensor, block_size=2048):
        """Quantizes the state tensor to 8-bit

        Args:
            state_tensor: Tensor to be quantized
            block_size: Block size for quantization

        Returns:
            List of quantized data blocks, each block contains:
            - data: uint8 data
            - scale: Quantization scale
            - min: Minimum value
        """
        if state_tensor.numel() <= 1:
            return state_tensor

        quantized_chunks = []
        for chunk in state_tensor.split(block_size):
            # Calculate quantization parameters
            chunk_min = chunk.min()
            chunk_max = chunk.max()
            scale = (chunk_max - chunk_min) / 255

            # Quantize to 0-255 range
            quantized_chunk = ((chunk - chunk_min) / scale).round().byte()
            quantized_chunks.append(
                {"data": quantized_chunk, "scale": scale, "min": chunk_min}
            )
        return quantized_chunks

    def _dequantize_state(self, quantized_chunks):
        """Dequantizes 8-bit quantized data to 32-bit floats

        Args:
            quantized_chunks: List of quantized data blocks

        Returns:
            Dequantized 32-bit float tensor
        """
        if not isinstance(quantized_chunks, list):
            return quantized_chunks

        chunks = []
        for chunk_dict in quantized_chunks:
            # Dequantize: value = data * scale + min
            chunk = chunk_dict["data"].float() * chunk_dict["scale"] + chunk_dict["min"]
            chunks.append(chunk)
        return torch.cat(chunks)

    def _dequantize_state_first_step(self, quantized_chunks):
        """Efficient dequantization specifically for the first step"""
        if not isinstance(quantized_chunks, list):
            return quantized_chunks

        # 1. Dequantize all chunks to CPU first
        dequantized_chunks = []
        for chunk_dict in quantized_chunks:
            chunk = chunk_dict["data"].float() * chunk_dict["scale"] + chunk_dict["min"]
            dequantized_chunks.append(chunk)
            # Clear original data
            del chunk_dict["data"]
            torch.cuda.empty_cache()

        # 2. Concatenate all chunks
        result = torch.cat(dequantized_chunks)

        # 3. Clear intermediate results
        del dequantized_chunks
        torch.cuda.empty_cache()

        return result

    def _get_options(self, param_shape):
        if len(param_shape) == 4:  # Convolutional layer
            if param_shape[2] == 1 and param_shape[3] == 1:  # 1x1 conv
                return True, "1x1_conv"
            else:  # 3x3 conv or others
                return False, "conv"
        elif len(param_shape) == 2:  # Linear layer
            return True, "linear"
        return False, "other"

    def _rms(self, tensor):
        return tensor.norm(2) / (tensor.numel() ** 0.5)

    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):
        r_factor = (
            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True))
            .rsqrt_()
            .unsqueeze(-1)
        )
        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()
        return torch.mul(r_factor, c_factor)

    def step(self, closure=None):
        """Performs a single optimization step

        Main steps:
        1. Determine whether 8-bit quantization is needed
        2. Update first and second moment estimates
        3. Calculate update step size
        4. Apply confidence-guided strategy
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if grad.dtype in {torch.float16, torch.bfloat16}:
                    grad = grad.float()
                if grad.is_sparse:
                    raise RuntimeError("CAME8bit does not support sparse gradients.")

                state = self.state[p]
                grad_shape = grad.shape
                factored, layer_type = self._get_options(grad_shape)

                # Determine whether to use 8-bit quantization
                use_8bit = self._should_use_8bit(grad_shape)

                # State Initialization
                if len(state) == 0:
                    self.print_layer_info(grad_shape, use_8bit)

                    state["step"] = 0
                    # Only use 8-bit quantization for large matrices
                    if use_8bit:
                        state["exp_avg"] = self._quantize_state(
                            torch.zeros_like(grad), group["block_size"]
                        )
                    else:
                        state["exp_avg"] = torch.zeros_like(grad)

                    if factored:
                        if layer_type == "1x1_conv" or layer_type == "linear":
                            # Row and column statistics remain in 32-bit
                            state["exp_avg_sq_row"] = torch.zeros(
                                grad_shape[0]
                            ).type_as(grad)
                            state["exp_avg_sq_col"] = torch.zeros(
                                grad_shape[1]
                            ).type_as(grad)
                            state["exp_avg_res_row"] = torch.zeros(
                                grad_shape[0]
                            ).type_as(grad)
                            state["exp_avg_res_col"] = torch.zeros(
                                grad_shape[1]
                            ).type_as(grad)
                        else:
                            if use_8bit:
                                state["exp_avg_sq"] = self._quantize_state(
                                    torch.zeros_like(grad), group["block_size"]
                                )
                            else:
                                state["exp_avg_sq"] = torch.zeros_like(grad)
                    else:
                        if use_8bit:
                            state["exp_avg_sq"] = self._quantize_state(
                                torch.zeros_like(grad), group["block_size"]
                            )
                        else:
                            state["exp_avg_sq"] = torch.zeros_like(grad)
                    state["RMS"] = 0

                state["step"] += 1
                state["RMS"] = self._rms(p.data)

                exp_avg = (
                    self._dequantize_state(state["exp_avg"])
                    if use_8bit
                    else state["exp_avg"]
                )

                update = (grad**2) + group["eps"][0]
                if factored:
                    # Row and column decomposition case
                    exp_avg_sq_row = state["exp_avg_sq_row"]  # 32-bit
                    exp_avg_sq_col = state["exp_avg_sq_col"]  # 32-bit

                    if layer_type == "1x1_conv" or layer_type == "linear":
                        if len(grad_shape) == 4:
                            update_reshaped = update.squeeze(-1).squeeze(-1)
                        else:
                            update_reshaped = update

                        # Update row and column statistics
                        exp_avg_sq_row.mul_(group["betas"][1]).add_(
                            update_reshaped.mean(dim=1), alpha=1.0 - group["betas"][1]
                        )
                        exp_avg_sq_col.mul_(group["betas"][1]).add_(
                            update_reshaped.mean(dim=0), alpha=1.0 - group["betas"][1]
                        )

                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)
                    if layer_type == "1x1_conv":
                        update = update.view(grad_shape[0], grad_shape[1], 1, 1)
                    update.mul_(grad)
                else:
                    # Non-decomposition case
                    exp_avg_sq = (
                        self._dequantize_state(state["exp_avg_sq"])
                        if use_8bit
                        else state["exp_avg_sq"]
                    )
                    exp_avg_sq.mul_(group["betas"][1]).add_(
                        update, alpha=1.0 - group["betas"][1]
                    )
                    if use_8bit:
                        state["exp_avg_sq"] = self._quantize_state(
                            exp_avg_sq, group["block_size"]
                        )
                    else:
                        state["exp_avg_sq"] = exp_avg_sq
                    update = exp_avg_sq.rsqrt().mul_(grad)

                # Gradient clipping
                update.div_(
                    (self._rms(update) / group["clip_threshold"]).clamp_(min=1.0)
                )

                # Update first moment
                exp_avg.mul_(group["betas"][0]).add_(
                    update, alpha=1 - group["betas"][0]
                )

                # Re-quantize (if needed)
                if use_8bit:
                    state["exp_avg"] = self._quantize_state(
                        exp_avg, group["block_size"]
                    )
                else:
                    state["exp_avg"] = exp_avg

                # Confidence-guided strategy
                res = (update - exp_avg) ** 2 + group["eps"][1]

                if factored:
                    exp_avg_res_row = state["exp_avg_res_row"]  # 32-bit
                    exp_avg_res_col = state["exp_avg_res_col"]  # 32-bit

                    if layer_type == "1x1_conv" or layer_type == "linear":
                        if len(grad_shape) == 4:
                            res_reshaped = res.squeeze(-1).squeeze(-1)
                        else:
                            res_reshaped = res

                        # Update residual statistics
                        exp_avg_res_row.mul_(group["betas"][2]).add_(
                            res_reshaped.mean(dim=1), alpha=1.0 - group["betas"][2]
                        )
                        exp_avg_res_col.mul_(group["betas"][2]).add_(
                            res_reshaped.mean(dim=0), alpha=1.0 - group["betas"][2]
                        )

                    res_approx = self._approx_sq_grad(exp_avg_res_row, exp_avg_res_col)
                    if layer_type == "1x1_conv":
                        res_approx = res_approx.view(grad_shape[0], grad_shape[1], 1, 1)
                    update = res_approx.mul_(exp_avg)
                else:
                    update = exp_avg.clone()

                # Weight decay
                if group["weight_decay"] != 0:
                    p.data.add_(p.data, alpha=-group["weight_decay"] * group["lr"])

                # Apply update
                update.mul_(group["lr"])
                p.data.add_(-update)

        return loss

    def load_state_dict(self, state_dict):
        """Loads the state dictionary and converts the corresponding states to 8-bit"""
        super().load_state_dict(state_dict)  # Call the parent class method

        for state in self.state.values():
            for key in [
                "exp_avg",
                "exp_avg_sq",
                "exp_avg_sq_row",
                "exp_avg_sq_col",
                "exp_avg_res_row",
                "exp_avg_res_col",
            ]:
                if key in state:
                    if isinstance(state[key], list):
                        state[key] = [
                            {
                                "data": exp[
                                    "data"
                                ].byte(),  # Directly convert data to 8-bit
                                "scale": exp["scale"],  # Keep scale unchanged
                                "min": exp["min"],  # Keep min unchanged
                            }
                            for exp in state[key]
                        ]
                    elif isinstance(state[key], torch.Tensor):
                        # If it's a tensor, keep it as 32-bit
                        state[key] = state[key].float()  # Ensure it's 32-bit

        del state_dict
        torch.cuda.empty_cache()
</file>

<file path="optimizers/soap.py">
## Based on: https://github.com/bghira/SimpleTuner/blob/main/helpers/training/optimizers/soap/__init__.py (AGPLv3)

import torch
import torch.nn as nn
import torch.optim as optim

from itertools import chain

# Parts of the code are modifications of Pytorch's AdamW optimizer
# Parts of the code are modifications of code from https://github.com/jiaweizzhao/GaLore/blob/master/galore_torch/galore_projector.py


class SOAP(optim.Optimizer):
    """
    Implements SOAP algorithm (https://arxiv.org/abs/2409.11321).

    Parameters:
        params (`Iterable[nn.parameter.Parameter]`):
            Iterable of parameters to optimize or dictionaries defining parameter groups.
        lr (`float`, *optional*, defaults to 0.003):
            The learning rate to use.
        betas (`Tuple[float,float]`, *optional*, defaults to `(0.95, 0.95)`):
            Adam's betas parameters (b1, b2).
        shampoo_beta (`float`, *optional*, defaults to -1):
            If >= 0, use this beta for the preconditioner (L and R in paper, state['GG'] below) moving average instead of betas[1].
        eps (`float`, *optional*, defaults to 1e-08):
            Adam's epsilon for numerical stability.
        weight_decay (`float`, *optional*, defaults to 0.01): weight decay coefficient.
        precondition_frequency (`int`, *optional*, defaults to 10):
            How often to update the preconditioner.
        max_precond_dim (`int`, *optional*, defaults to 10000):
            Maximum dimension of the preconditioner.
            Set to 10000, so that we exclude most common vocab sizes while including layers.
        merge_dims (`bool`, *optional*, defaults to `False`):
            Whether or not to merge dimensions of the preconditioner.
        precondition_1d (`bool`, *optional*, defaults to `False`):
            Whether or not to precondition 1D gradients.
        normalize_grads (`bool`, *optional*, defaults to `False`):
            Whether or not to normalize gradients per layer.
            Helps at large precondition_frequency (~100 in our experiments),
            but hurts performance at small precondition_frequency (~10 in our experiments).
        data_format (`str`, *optional*, defaults to `channels_first`):
            Data format of the input for convolutional layers.
            Should be "channels_last" for data_format of NHWC and "channels_first" for NCHW.
        correct_bias (`bool`, *optional*, defaults to `True`):
            Whether or not to use bias correction in Adam.
    """

    def __init__(
        self,
        params,
        lr: float = 3e-3,
        betas=(0.95, 0.95),
        shampoo_beta: float = -1,
        eps: float = 1e-8,
        weight_decay: float = 0.01,
        precondition_frequency: int = 10,
        max_precond_dim: int = 10000,  #
        merge_dims: bool = False,  # Merge dimensions till the product of the dimensions is less than or equal to max_precond_dim.
        precondition_1d: bool = False,
        normalize_grads: bool = False,
        data_format: str = "channels_first",
        correct_bias: bool = True,
    ):
        defaults = {
            "lr": lr,
            "betas": betas,
            "shampoo_beta": shampoo_beta,
            "eps": eps,
            "weight_decay": weight_decay,
            "precondition_frequency": precondition_frequency,
            "max_precond_dim": max_precond_dim,
            "merge_dims": merge_dims,
            "precondition_1d": precondition_1d,
            "normalize_grads": normalize_grads,
            "correct_bias": correct_bias,
        }
        super().__init__(params, defaults)
        self._data_format = data_format

    def merge_dims(self, grad, max_precond_dim):
        """
        Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.
        """
        assert self._data_format in ["channels_first", "channels_last"]
        if self._data_format == "channels_last" and grad.dim() == 4:
            grad = grad.permute(0, 3, 1, 2)
        shape = grad.shape
        new_shape = []

        curr_shape = 1
        for sh in shape:
            temp_shape = curr_shape * sh
            if temp_shape > max_precond_dim:
                if curr_shape > 1:
                    new_shape.append(curr_shape)
                    curr_shape = sh
                else:
                    new_shape.append(sh)
                    curr_shape = 1
            else:
                curr_shape = temp_shape

        if curr_shape > 1 or len(new_shape) == 0:
            new_shape.append(curr_shape)

        new_grad = grad.reshape(new_shape)
        return new_grad

    @torch.no_grad()
    def step(self, closure=None):
        """
        Performs a single optimization step.

        Arguments:
            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad

                state = self.state[p]

                if "step" not in state:
                    state["step"] = 0

                # State initialization
                if "exp_avg" not in state:
                    # Exponential moving average of gradient values
                    state["exp_avg"] = torch.zeros_like(grad)
                    # Exponential moving average of squared gradient values
                    state["exp_avg_sq"] = torch.zeros_like(grad)

                if "Q" not in state:
                    self.init_preconditioner(
                        grad,
                        state,
                        precondition_frequency=group["precondition_frequency"],
                        precondition_1d=group["precondition_1d"],
                        shampoo_beta=(
                            group["shampoo_beta"]
                            if group["shampoo_beta"] >= 0
                            else group["betas"][1]
                        ),
                        max_precond_dim=group["max_precond_dim"],
                        merge_dims=group["merge_dims"],
                    )
                    self.update_preconditioner(
                        grad,
                        state,
                        max_precond_dim=group["max_precond_dim"],
                        merge_dims=group["merge_dims"],
                        precondition_1d=group["precondition_1d"],
                    )
                    continue  # first step is skipped so that we never use the current gradients in the projection.

                # Projecting gradients to the eigenbases of Shampoo's preconditioner
                # i.e. projecting to the eigenbases of matrices in state['GG']
                grad_projected = self.project(
                    grad,
                    state,
                    merge_dims=group["merge_dims"],
                    max_precond_dim=group["max_precond_dim"],
                )

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                beta1, beta2 = group["betas"]

                state["step"] += 1

                # Decay the first and second moment running average coefficient
                # In-place operations to update the averages at the same time
                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))
                exp_avg_sq.mul_(beta2).add_(
                    grad_projected.square(), alpha=(1.0 - beta2)
                )

                denom = exp_avg_sq.sqrt().add_(group["eps"])

                # Projecting the exponential moving average of gradients to the eigenbases of Shampoo's preconditioner
                # i.e. projecting to the eigenbases of matrices in state['GG']
                exp_avg_projected = self.project(
                    exp_avg,
                    state,
                    merge_dims=group["merge_dims"],
                    max_precond_dim=group["max_precond_dim"],
                )

                step_size = group["lr"]
                if group["correct_bias"]:
                    bias_correction1 = 1.0 - beta1 ** (state["step"])
                    bias_correction2 = 1.0 - beta2 ** (state["step"])
                    step_size = step_size * (bias_correction2**0.5) / bias_correction1

                # Projecting back the preconditioned (by Adam) exponential moving average of gradients
                # to the original space
                norm_grad = self.project_back(
                    exp_avg_projected / denom,
                    state,
                    merge_dims=group["merge_dims"],
                    max_precond_dim=group["max_precond_dim"],
                )

                if group["normalize_grads"]:
                    norm_grad = norm_grad / (1e-30 + torch.mean(norm_grad**2) ** 0.5)

                p.add_(norm_grad, alpha=-step_size)

                # From AdamW code: Just adding the square of the weights to the loss function is *not*
                # the correct way of using L2 regularization/weight decay with Adam,
                # since that will interact with the m and v parameters in strange ways.
                #
                # Instead we want to decay the weights in a manner that doesn't interact
                # with the m/v parameters. This is equivalent to adding the square
                # of the weights to the loss with plain (non-momentum) SGD.
                # Add weight decay at the end (fixed version)
                if group["weight_decay"] > 0.0:
                    p.add_(p, alpha=(-group["lr"] * group["weight_decay"]))

                # Update is done after the gradient step to avoid using current gradients in the projection.
                self.update_preconditioner(
                    grad,
                    state,
                    max_precond_dim=group["max_precond_dim"],
                    merge_dims=group["merge_dims"],
                    precondition_1d=group["precondition_1d"],
                )

        return loss

    def init_preconditioner(
        self,
        grad,
        state,
        precondition_frequency=10,
        shampoo_beta=0.95,
        max_precond_dim=10000,
        precondition_1d=False,
        merge_dims=False,
    ):
        """
        Initializes the preconditioner matrices (L and R in the paper).
        """
        state["GG"] = (
            []
        )  # Will hold all the preconditioner matrices (L and R in the paper).
        if grad.dim() == 1:
            if not precondition_1d or grad.shape[0] > max_precond_dim:
                state["GG"].append([])
            else:
                state["GG"].append(
                    torch.zeros(grad.shape[0], grad.shape[0], device=grad.device)
                )
        else:
            if merge_dims:
                grad = self.merge_dims(grad, max_precond_dim)

            for sh in grad.shape:
                if sh > max_precond_dim:
                    state["GG"].append([])
                else:
                    state["GG"].append(torch.zeros(sh, sh, device=grad.device))

        state["Q"] = None  # Will hold all the eigenbases of the preconditioner.
        state["precondition_frequency"] = precondition_frequency
        state["shampoo_beta"] = shampoo_beta

    def project(self, grad, state, merge_dims=False, max_precond_dim=10000):
        """
        Projects the gradient to the eigenbases of the preconditioner.
        """
        original_shape = grad.shape
        if merge_dims:
            if grad.dim() == 4 and self._data_format == "channels_last":
                permuted_shape = grad.permute(0, 3, 1, 2).shape
            grad = self.merge_dims(grad, max_precond_dim)

        for mat in state["Q"]:
            if len(mat) > 0:
                grad = torch.tensordot(
                    grad,
                    mat.to(grad.dtype),
                    dims=[[0], [0]],  # type: ignore
                )
            else:
                permute_order = list(range(1, len(grad.shape))) + [0]
                grad = grad.permute(permute_order)

        if merge_dims:
            if self._data_format == "channels_last" and len(original_shape) == 4:
                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                grad = grad.reshape(original_shape)
        return grad

    def update_preconditioner(
        self,
        grad,
        state,
        max_precond_dim=10000,
        merge_dims=False,
        precondition_1d=False,
    ):
        """
        Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).
        """
        if grad.dim() == 1:
            if precondition_1d and grad.shape[0] <= max_precond_dim:
                state["GG"][0].lerp_(
                    grad.unsqueeze(1) @ grad.unsqueeze(0), 1 - state["shampoo_beta"]
                )
        else:
            if merge_dims:
                new_grad = self.merge_dims(grad, max_precond_dim)
                for idx, sh in enumerate(new_grad.shape):
                    if sh <= max_precond_dim:
                        outer_product = torch.tensordot(
                            new_grad,
                            new_grad,
                            dims=[
                                [
                                    *chain(
                                        range(idx), range(idx + 1, len(new_grad.shape))
                                    )
                                ]
                            ]
                            * 2,  # type: ignore
                        )
                        state["GG"][idx].lerp_(outer_product, 1 - state["shampoo_beta"])
            else:
                for idx, sh in enumerate(grad.shape):
                    if sh <= max_precond_dim:
                        outer_product = torch.tensordot(
                            grad,
                            grad,
                            # Contracts across all dimensions except for k.
                            dims=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]]
                            * 2,  # type: ignore
                        )
                        state["GG"][idx].lerp_(
                            outer_product.to(state["GG"][idx].dtype),
                            1 - state["shampoo_beta"],
                        )

        if state["Q"] is None:
            state["Q"] = self.get_orthogonal_matrix(state["GG"])
        if state["step"] > 0 and state["step"] % state["precondition_frequency"] == 0:
            state["Q"] = self.get_orthogonal_matrix_QR(
                state, max_precond_dim, merge_dims
            )

    def project_back(self, grad, state, merge_dims=False, max_precond_dim=10000):
        """
        Projects the gradient back to the original space.
        """
        original_shape = grad.shape
        if merge_dims:
            if self._data_format == "channels_last" and grad.dim() == 4:
                permuted_shape = grad.permute(0, 3, 1, 2).shape
            grad = self.merge_dims(grad, max_precond_dim)
        for mat in state["Q"]:
            if len(mat) > 0:
                grad = torch.tensordot(
                    grad.to(mat.dtype),
                    mat,
                    dims=[[0], [1]],  # type: ignore
                )
            else:
                permute_order = list(range(1, len(grad.shape))) + [0]
                grad = grad.permute(permute_order)

        if merge_dims:
            if self._data_format == "channels_last" and len(original_shape) == 4:
                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                grad = grad.reshape(original_shape)
        return grad

    def get_orthogonal_matrix(self, mat):
        """
        Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.
        """
        matrix = []
        for m in mat:
            if len(m) == 0:
                matrix.append([])
                continue
            if m.data.dtype != torch.float:
                float_data = False
                original_type = m.data.dtype
                original_device = m.data.device
                matrix.append(m.data.float())
            else:
                float_data = True
                matrix.append(m.data)

        final = []
        for m in matrix:
            if len(m) == 0:
                final.append([])
                continue
            try:
                _, Q = torch.linalg.eigh(
                    m + 1e-30 * torch.eye(m.shape[0], device=m.device)
                )
            except:
                _, Q = torch.linalg.eigh(
                    m.to(torch.float64) + 1e-30 * torch.eye(m.shape[0], device=m.device)
                )
                Q = Q.to(m.dtype)
            Q = torch.flip(Q, [1])

            if not float_data:
                Q = Q.to(original_device).type(original_type)
            final.append(Q)
        return final

    def get_orthogonal_matrix_QR(self, state, max_precond_dim=10000, merge_dims=False):
        """
        Computes the eigenbases of the preconditioner using one round of power iteration
        followed by torch.linalg.qr decomposition.
        """
        precond_list = state["GG"]
        orth_list = state["Q"]

        matrix = []
        orth_matrix = []
        for m, o in zip(precond_list, orth_list):
            if len(m) == 0:
                matrix.append([])
                orth_matrix.append([])
                continue
            if m.data.dtype != torch.float:
                float_data = False
                original_type = m.data.dtype
                original_device = m.data.device
                matrix.append(m.data.float())
                orth_matrix.append(o.data.float())
            else:
                float_data = True
                matrix.append(m.data.float())
                orth_matrix.append(o.data.float())

        orig_shape = state["exp_avg_sq"].shape
        if self._data_format == "channels_last" and len(orig_shape) == 4:
            permuted_shape = state["exp_avg_sq"].permute(0, 3, 1, 2).shape
        if merge_dims:
            exp_avg_sq = self.merge_dims(state["exp_avg_sq"], max_precond_dim)
        else:
            exp_avg_sq = state["exp_avg_sq"]

        final = []
        for ind, (m, o) in enumerate(zip(matrix, orth_matrix)):
            if len(m) == 0:
                final.append([])
                continue
            est_eig = torch.diag(o.T @ m @ o)
            sort_idx = torch.argsort(est_eig, descending=True)
            exp_avg_sq = exp_avg_sq.index_select(ind, sort_idx)
            o = o[:, sort_idx]
            power_iter = m @ o
            Q, _ = torch.linalg.qr(power_iter)

            if not float_data:
                Q = Q.to(original_device).type(original_type)
            final.append(Q)

        if merge_dims:
            if self._data_format == "channels_last" and len(orig_shape) == 4:
                exp_avg_sq = exp_avg_sq.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                exp_avg_sq = exp_avg_sq.reshape(orig_shape)

        state["exp_avg_sq"] = exp_avg_sq
        return final
</file>

<file path="optimizers/sophia.py">
## Based on: https://github.com/Liuhong99/Sophia/blob/main/sophia.py (MIT)

import math
import torch
from torch import Tensor
from torch.optim.optimizer import Optimizer
from typing import List, Optional


class SophiaG(Optimizer):
    def __init__(
        self,
        params,
        lr=1e-4,
        betas=(0.965, 0.99),
        rho=0.04,
        weight_decay=1e-1,
        *,
        maximize: bool = False,
        capturable: bool = False,
    ):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
        if not 0.0 <= rho:
            raise ValueError("Invalid rho parameter at index 1: {}".format(rho))
        if not 0.0 <= weight_decay:
            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
        defaults = dict(
            lr=lr,
            betas=betas,
            rho=rho,
            weight_decay=weight_decay,
            maximize=maximize,
            capturable=capturable,
        )
        super(SophiaG, self).__init__(params, defaults)

    def __setstate__(self, state):
        super().__setstate__(state)
        for group in self.param_groups:
            group.setdefault("maximize", False)
            group.setdefault("capturable", False)
        state_values = list(self.state.values())
        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(
            state_values[0]["step"]
        )
        if not step_is_tensor:
            for s in state_values:
                s["step"] = torch.tensor(float(s["step"]))

    @torch.no_grad()
    def update_hessian(self):
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            for p in group["params"]:
                if p.grad is None:
                    continue
                state = self.state[p]

                if len(state) == 0:
                    state["step"] = (
                        torch.zeros((1,), dtype=torch.float, device=p.device)
                        if self.defaults["capturable"]
                        else torch.tensor(0.0)
                    )
                    state["exp_avg"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )
                    state["hessian"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )

                if "hessian" not in state.keys():
                    state["hessian"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )

                state["hessian"].mul_(beta2).addcmul_(p.grad, p.grad, value=1 - beta2)

    @torch.no_grad()
    def step(self, closure=None, bs=5120):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params_with_grad = []
            grads = []
            exp_avgs = []
            state_steps = []
            hessian = []
            beta1, beta2 = group["betas"]

            for p in group["params"]:
                if p.grad is None:
                    continue
                params_with_grad.append(p)

                if p.grad.is_sparse:
                    raise RuntimeError("Hero does not support sparse gradients")
                grads.append(p.grad)
                state = self.state[p]
                # State initialization
                if len(state) == 0:
                    state["step"] = (
                        torch.zeros((1,), dtype=torch.float, device=p.device)
                        if self.defaults["capturable"]
                        else torch.tensor(0.0)
                    )
                    state["exp_avg"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )
                    state["hessian"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )

                if "hessian" not in state.keys():
                    state["hessian"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )

                exp_avgs.append(state["exp_avg"])
                state_steps.append(state["step"])
                hessian.append(state["hessian"])

                if self.defaults["capturable"]:
                    bs = torch.ones((1,), dtype=torch.float, device=p.device) * bs

            sophiag(
                params_with_grad,
                grads,
                exp_avgs,
                hessian,
                state_steps,
                bs=bs,  # type: ignore
                beta1=beta1,
                beta2=beta2,
                rho=group["rho"],
                lr=group["lr"],
                weight_decay=group["weight_decay"],
                maximize=group["maximize"],
                capturable=group["capturable"],
            )

        return loss


def sophiag(
    params: List[Tensor],
    grads: List[Tensor],
    exp_avgs: List[Tensor],
    hessian: List[Tensor],
    state_steps: List[Tensor],
    capturable: bool = False,
    *,
    bs: int,
    beta1: float,
    beta2: float,
    rho: float,
    lr: float,
    weight_decay: float,
    maximize: bool,
):

    if not all(isinstance(t, torch.Tensor) for t in state_steps):
        raise RuntimeError(
            "API has changed, `state_steps` argument must contain a list of singleton tensors"
        )

    func = _single_tensor_sophiag

    func(
        params,
        grads,
        exp_avgs,
        hessian,
        state_steps,
        bs=bs,
        beta1=beta1,
        beta2=beta2,
        rho=rho,
        lr=lr,
        weight_decay=weight_decay,
        maximize=maximize,
        capturable=capturable,
    )


def _single_tensor_sophiag(
    params: List[Tensor],
    grads: List[Tensor],
    exp_avgs: List[Tensor],
    hessian: List[Tensor],
    state_steps: List[Tensor],
    *,
    bs: int,
    beta1: float,
    beta2: float,
    rho: float,
    lr: float,
    weight_decay: float,
    maximize: bool,
    capturable: bool,
):

    for i, param in enumerate(params):
        grad = grads[i] if not maximize else -grads[i]
        exp_avg = exp_avgs[i]
        hess = hessian[i]
        step_t = state_steps[i]

        if capturable:
            assert param.is_cuda and step_t.is_cuda and bs.is_cuda  # type: ignore

        if torch.is_complex(param):
            grad = torch.view_as_real(grad)
            exp_avg = torch.view_as_real(exp_avg)
            hess = torch.view_as_real(hess)
            param = torch.view_as_real(param)

        # update step
        step_t += 1

        # Perform stepweight decay
        param.mul_(1 - lr * weight_decay)

        # Decay the first and second moment running average coefficient
        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

        if capturable:
            step_size = lr
            step_size_neg = step_size.neg()  # type: ignore

            ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None, 1)
            param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)
        else:
            step_size_neg = -lr

            ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None, 1)
            param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)
</file>

<file path="reward/clip_model.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/videox_fun/reward/MPS/trainer/models/clip_model.py (Apache)

from dataclasses import dataclass
from transformers import CLIPModel as HFCLIPModel
from transformers import AutoTokenizer

from torch import nn, einsum

from transformers import CLIPConfig
from typing import Any, Optional, Tuple, Union
import torch

from reward.cross_modeling import Cross_model


class XCLIPModel(HFCLIPModel):
    def __init__(self, config: CLIPConfig):
        super().__init__(config)

    def get_text_features(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> torch.FloatTensor:

        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        text_outputs = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # pooled_output = text_outputs[1]
        # text_features = self.text_projection(pooled_output)
        last_hidden_state = text_outputs[0]
        text_features = self.text_projection(last_hidden_state)

        pooled_output = text_outputs[1]
        text_features_EOS = self.text_projection(pooled_output)

        # del last_hidden_state, text_outputs
        # gc.collect()

        return text_features, text_features_EOS  # type: ignore

    def get_image_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> torch.FloatTensor:

        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # pooled_output = vision_outputs[1]  # pooled_output
        # image_features = self.visual_projection(pooled_output)
        last_hidden_state = vision_outputs[0]
        image_features = self.visual_projection(last_hidden_state)

        return image_features


@dataclass
class ClipModelConfig:
    _target_: str = "trainer.models.clip_model.CLIPModel"
    pretrained_model_name_or_path: str = "openai/clip-vit-base-patch32"


class CLIPModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Modified: We convert the original ckpt (contains the entire model) to a `state_dict`.
        # self.model = XCLIPModel.from_pretrained(ckpt)
        self.model = XCLIPModel(config)
        self.cross_model = Cross_model(dim=1024, layer_num=4, heads=16)

    def get_text_features(self, *args, **kwargs):
        return self.model.get_text_features(*args, **kwargs)

    def get_image_features(self, *args, **kwargs):
        return self.model.get_image_features(*args, **kwargs)

    def forward(self, text_inputs=None, image_inputs=None, condition_inputs=None):
        outputs = ()

        text_f, text_EOS = self.model.get_text_features(text_inputs)  # B*77*1024
        outputs += (text_EOS,)

        image_f = self.model.get_image_features(image_inputs.half())  # type: ignore # 2B*257*1024
        # [B, 77, 1024]
        condition_f, _ = self.model.get_text_features(condition_inputs)  # B*5*1024

        sim_text_condition = einsum("b i d, b j d -> b j i", text_f, condition_f)
        sim_text_condition = torch.max(sim_text_condition, dim=1, keepdim=True)[0]
        sim_text_condition = sim_text_condition / sim_text_condition.max()
        mask = torch.where(sim_text_condition > 0.01, 0, float("-inf"))  # B*1*77

        # Modified: Support both torch.float16 and torch.bfloat16
        # mask = mask.repeat(1,image_f.shape[1],1) # B*257*77
        model_dtype = next(self.cross_model.parameters()).dtype
        mask = mask.repeat(1, image_f.shape[1], 1).to(model_dtype)  # B*257*77
        # bc = int(image_f.shape[0]/2)

        # Modified: The original input consists of a (batch of) text and two (batches of) images,
        # primarily used to compute which (batch of) image is more consistent with the text.
        # The modified input consists of a (batch of) text and a (batch of) images.
        # sim0 = self.cross_model(image_f[:bc,:,:], text_f,mask.half())
        # sim1 = self.cross_model(image_f[bc:,:,:], text_f,mask.half())
        # outputs += sim0[:,0,:],
        # outputs += sim1[:,0,:],
        sim = self.cross_model(image_f, text_f, mask)
        outputs += (sim[:, 0, :],)

        return outputs

    @property
    def logit_scale(self):
        return self.model.logit_scale

    def save(self, path):
        self.model.save_pretrained(path)
</file>

<file path="reward/cross_modeling.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/videox_fun/reward/MPS/trainer/models/cross_modeling.py (Apache)

import torch
from torch import einsum, nn
import torch.nn.functional as F
from einops import rearrange, repeat

# helper functions


def exists(val):
    return val is not None


def default(val, d):
    return val if exists(val) else d


# normalization
# they use layernorm without bias, something that pytorch does not offer


class LayerNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.register_buffer("bias", torch.zeros(dim))

    def forward(self, x):
        return F.layer_norm(x, x.shape[-1:], self.weight, self.bias)  # type: ignore


# residual


class Residual(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    def forward(self, x, *args, **kwargs):
        return self.fn(x, *args, **kwargs) + x


# rotary positional embedding
# https://arxiv.org/abs/2104.09864


class RotaryEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, max_seq_len, *, device):
        seq = torch.arange(max_seq_len, device=device, dtype=self.inv_freq.dtype)  # type: ignore
        freqs = einsum("i , j -> i j", seq, self.inv_freq)
        return torch.cat((freqs, freqs), dim=-1)


def rotate_half(x):
    x = rearrange(x, "... (j d) -> ... j d", j=2)
    x1, x2 = x.unbind(dim=-2)
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(pos, t):
    return (t * pos.cos()) + (rotate_half(t) * pos.sin())


# classic Noam Shazeer paper, except here they use SwiGLU instead of the more popular GEGLU for gating the feedforward
# https://arxiv.org/abs/2002.05202


class SwiGLU(nn.Module):
    def forward(self, x):
        x, gate = x.chunk(2, dim=-1)
        return F.silu(gate) * x


# parallel attention and feedforward with residual
# discovered by Wang et al + EleutherAI from GPT-J fame


class ParallelTransformerBlock(nn.Module):
    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):
        super().__init__()
        self.norm = LayerNorm(dim)

        attn_inner_dim = dim_head * heads
        ff_inner_dim = dim * ff_mult
        self.fused_dims = (attn_inner_dim, dim_head, dim_head, (ff_inner_dim * 2))

        self.heads = heads
        self.scale = dim_head**-0.5
        self.rotary_emb = RotaryEmbedding(dim_head)

        self.fused_attn_ff_proj = nn.Linear(dim, sum(self.fused_dims), bias=False)
        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)

        self.ff_out = nn.Sequential(SwiGLU(), nn.Linear(ff_inner_dim, dim, bias=False))

        self.register_buffer("pos_emb", None, persistent=False)

    def get_rotary_embedding(self, n, device):
        if self.pos_emb is not None and self.pos_emb.shape[-2] >= n:  # type: ignore
            return self.pos_emb[:n]  # type: ignore

        pos_emb = self.rotary_emb(n, device=device)
        self.register_buffer("pos_emb", pos_emb, persistent=False)
        return pos_emb

    def forward(self, x, attn_mask=None):
        """
        einstein notation
        b - batch
        h - heads
        n, i, j - sequence length (base sequence length, source, target)
        d - feature dimension
        """

        n, device, h = x.shape[1], x.device, self.heads

        # pre layernorm

        x = self.norm(x)

        # attention queries, keys, values, and feedforward inner

        q, k, v, ff = self.fused_attn_ff_proj(x).split(self.fused_dims, dim=-1)

        # split heads
        # they use multi-query single-key-value attention, yet another Noam Shazeer paper
        # they found no performance loss past a certain scale, and more efficient decoding obviously
        # https://arxiv.org/abs/1911.02150

        q = rearrange(q, "b n (h d) -> b h n d", h=h)

        # rotary embeddings

        positions = self.get_rotary_embedding(n, device)
        q, k = map(lambda t: apply_rotary_pos_emb(positions, t), (q, k))

        # scale

        q = q * self.scale

        # similarity

        sim = einsum("b h i d, b j d -> b h i j", q, k)

        # extra attention mask - for masking out attention from text CLS token to padding

        if exists(attn_mask):
            attn_mask = rearrange(attn_mask, "b i j -> b 1 i j")
            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)  # type: ignore

        # attention

        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        # aggregate values

        out = einsum("b h i j, b j d -> b h i d", attn, v)

        # merge heads

        out = rearrange(out, "b h n d -> b n (h d)")
        return self.attn_out(out) + self.ff_out(ff)


# cross attention - using multi-query + one-headed key / values as in PaLM w/ optional parallel feedforward


class CrossAttention(nn.Module):
    def __init__(
        self,
        dim,
        *,
        context_dim=None,
        dim_head=64,
        heads=12,
        parallel_ff=False,
        ff_mult=4,
        norm_context=False,
    ):
        super().__init__()
        self.heads = heads
        self.scale = dim_head**-0.5
        inner_dim = heads * dim_head
        context_dim = default(context_dim, dim)

        self.norm = LayerNorm(dim)
        self.context_norm = LayerNorm(context_dim) if norm_context else nn.Identity()

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(context_dim, dim_head * 2, bias=False)  # type: ignore
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

        # whether to have parallel feedforward

        ff_inner_dim = ff_mult * dim

        self.ff = (
            nn.Sequential(
                nn.Linear(dim, ff_inner_dim * 2, bias=False),
                SwiGLU(),
                nn.Linear(ff_inner_dim, dim, bias=False),
            )
            if parallel_ff
            else None
        )

    def forward(self, x, context, mask):
        """
        einstein notation
        b - batch
        h - heads
        n, i, j - sequence length (base sequence length, source, target)
        d - feature dimension
        """

        # pre-layernorm, for queries and context

        x = self.norm(x)
        context = self.context_norm(context)

        # get queries

        q = self.to_q(x)
        q = rearrange(q, "b n (h d) -> b h n d", h=self.heads)

        # scale

        q = q * self.scale

        # get key / values

        k, v = self.to_kv(context).chunk(2, dim=-1)

        # query / key similarity

        sim = einsum("b h i d, b j d -> b h i j", q, k)

        # attention
        mask = mask.unsqueeze(1).repeat(1, self.heads, 1, 1)
        sim = sim + mask  # context mask
        sim = sim - sim.amax(dim=-1, keepdim=True)
        attn = sim.softmax(dim=-1)

        # aggregate

        out = einsum("b h i j, b j d -> b h i d", attn, v)

        # merge and combine heads

        out = rearrange(out, "b h n d -> b n (h d)")
        out = self.to_out(out)

        # add parallel feedforward (for multimodal layers)

        if exists(self.ff):
            out = out + self.ff(x)  # type: ignore

        return out


class Cross_model(nn.Module):
    def __init__(self, dim=512, layer_num=4, dim_head=64, heads=8, ff_mult=4):
        super().__init__()

        self.layers = nn.ModuleList([])

        for ind in range(layer_num):
            self.layers.append(
                nn.ModuleList(
                    [
                        Residual(
                            CrossAttention(
                                dim=dim,
                                dim_head=dim_head,
                                heads=heads,
                                parallel_ff=True,
                                ff_mult=ff_mult,
                            )
                        ),
                        Residual(
                            ParallelTransformerBlock(
                                dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult
                            )
                        ),
                    ]
                )
            )

    def forward(self, query_tokens, context_tokens, mask):
        for cross_attn, self_attn_ff in self.layers:  # type: ignore
            query_tokens = cross_attn(query_tokens, context_tokens, mask)
            query_tokens = self_attn_ff(query_tokens)

        return query_tokens
</file>

<file path="reward/improved_aesthetic_predictor.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/videox_fun/reward/improved_aesthetic_predictor.py (Apache)

import os

import torch
import torch.nn as nn
from transformers import CLIPModel
from torchvision.datasets.utils import download_url

URL = "https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/Third_Party/sac%2Blogos%2Bava1-l14-linearMSE.pth"
FILENAME = "sac+logos+ava1-l14-linearMSE.pth"
MD5 = "b1047fd767a00134b8fd6529bf19521a"


class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(768, 1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 128),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Linear(16, 1),
        )

    def forward(self, embed):
        return self.layers(embed)


class ImprovedAestheticPredictor(nn.Module):
    def __init__(
        self, encoder_path="openai/clip-vit-large-patch14", predictor_path=None
    ):
        super().__init__()
        self.encoder = CLIPModel.from_pretrained(encoder_path)
        self.predictor = MLP()
        if predictor_path is None or not os.path.exists(predictor_path):
            download_url(URL, torch.hub.get_dir(), FILENAME, md5=MD5)
            predictor_path = os.path.join(torch.hub.get_dir(), FILENAME)
        state_dict = torch.load(predictor_path, map_location="cpu")
        self.predictor.load_state_dict(state_dict)
        self.eval()

    def forward(self, pixel_values):
        embed = self.encoder.get_image_features(pixel_values=pixel_values)  # type: ignore
        embed = embed / torch.linalg.vector_norm(embed, dim=-1, keepdim=True)

        return self.predictor(embed).squeeze(1)
</file>

<file path="reward/reward_fn.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/videox_fun/reward/reward_fn.py (Apache)

import os
from abc import ABC, abstractmethod

import torch
import torchvision.transforms as transforms
from einops import rearrange
from torchvision.datasets.utils import download_url
from typing import Optional, Tuple


class BaseReward(ABC):
    """An base class for reward models. A custom Reward class must implement two functions below."""

    def __init__(self):
        """Define your reward model and image transformations (optional) here."""
        pass

    @abstractmethod
    def __call__(
        self, batch_frames: torch.Tensor, batch_prompt: Optional[list[str]] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Given batch frames with shape `[B, C, T, H, W]` extracted from a list of videos and a list of prompts
        (optional) correspondingly, return the loss and reward computed by your reward model (reduction by mean).
        """
        pass


class AestheticReward(BaseReward):
    """Aesthetic Predictor [V2](https://github.com/christophschuhmann/improved-aesthetic-predictor)
    and [V2.5](https://github.com/discus0434/aesthetic-predictor-v2-5) reward model.
    """

    def __init__(
        self,
        encoder_path="openai/clip-vit-large-patch14",
        predictor_path=None,
        version="v2",
        device="cpu",
        dtype=torch.float16,
        max_reward=10,
        loss_scale=0.1,
    ):
        from reward.improved_aesthetic_predictor import ImprovedAestheticPredictor
        from reward.siglip_v2_5 import convert_v2_5_from_siglip

        self.encoder_path = encoder_path
        self.predictor_path = predictor_path
        self.version = version
        self.device = device
        self.dtype = dtype
        self.max_reward = max_reward
        self.loss_scale = loss_scale

        if self.version != "v2" and self.version != "v2.5":
            raise ValueError("Only v2 and v2.5 are supported.")
        if self.version == "v2":
            assert "clip-vit-large-patch14" in encoder_path.lower()
            self.model = ImprovedAestheticPredictor(
                encoder_path=self.encoder_path, predictor_path=self.predictor_path
            )
            # https://huggingface.co/openai/clip-vit-large-patch14/blob/main/preprocessor_config.json
            # TODO: [transforms.Resize(224), transforms.CenterCrop(224)] for any aspect ratio.
            self.transform = transforms.Compose(
                [
                    transforms.Resize(
                        (224, 224), interpolation=transforms.InterpolationMode.BICUBIC
                    ),
                    transforms.Normalize(
                        mean=[0.48145466, 0.4578275, 0.40821073],
                        std=[0.26862954, 0.26130258, 0.27577711],
                    ),
                ]
            )
        elif self.version == "v2.5":
            assert "siglip-so400m-patch14-384" in encoder_path.lower()
            self.model, _ = convert_v2_5_from_siglip(
                encoder_model_name=self.encoder_path
            )
            # https://huggingface.co/google/siglip-so400m-patch14-384/blob/main/preprocessor_config.json
            self.transform = transforms.Compose(
                [
                    transforms.Resize(
                        (384, 384), interpolation=transforms.InterpolationMode.BICUBIC
                    ),
                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
                ]
            )

        self.model.to(device=self.device, dtype=self.dtype)  # type: ignore
        self.model.requires_grad_(False)

    def __call__(
        self, batch_frames: torch.Tensor, batch_prompt: Optional[list[str]] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_frames = rearrange(batch_frames, "b c t h w -> t b c h w")
        batch_loss, batch_reward = 0, 0
        for frames in batch_frames:
            pixel_values = torch.stack([self.transform(frame) for frame in frames])
            pixel_values = pixel_values.to(self.device, dtype=self.dtype)
            if self.version == "v2":
                reward = self.model(pixel_values)
            elif self.version == "v2.5":
                reward = self.model(pixel_values).logits.squeeze()
            # Convert reward to loss in [0, 1].
            if self.max_reward is None:
                loss = (-1 * reward) * self.loss_scale
            else:
                loss = abs(reward - self.max_reward) * self.loss_scale
            batch_loss, batch_reward = (
                batch_loss + loss.mean(),  # type: ignore
                batch_reward + reward.mean(),
            )

        return batch_loss / batch_frames.shape[0], batch_reward / batch_frames.shape[0]  # type: ignore


class HPSReward(BaseReward):
    """[HPS](https://github.com/tgxs002/HPSv2) v2 and v2.1 reward model."""

    def __init__(
        self,
        model_path=None,
        version="v2.0",
        device="cpu",
        dtype=torch.float16,
        max_reward=1,
        loss_scale=1,
    ):
        from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer

        self.model_path = model_path
        self.version = version
        self.device = device
        self.dtype = dtype
        self.max_reward = max_reward
        self.loss_scale = loss_scale

        self.model, _, _ = create_model_and_transforms(
            "ViT-H-14",
            "laion2B-s32B-b79K",
            precision=self.dtype,  # type: ignore
            device=self.device,
            jit=False,
            force_quick_gelu=False,
            force_custom_text=False,
            force_patch_dropout=False,
            force_image_size=None,
            pretrained_image=False,
            image_mean=None,
            image_std=None,
            light_augmentation=True,
            aug_cfg={},
            output_dict=True,
            with_score_predictor=False,
            with_region_predictor=False,
        )
        self.tokenizer = get_tokenizer("ViT-H-14")

        # https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/blob/main/preprocessor_config.json
        # TODO: [transforms.Resize(224), transforms.CenterCrop(224)] for any aspect ratio.
        self.transform = transforms.Compose(
            [
                transforms.Resize(
                    (224, 224), interpolation=transforms.InterpolationMode.BICUBIC
                ),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711],
                ),
            ]
        )

        if version == "v2.0":
            url = "https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/Third_Party/HPS_v2_compressed.pt"
            filename = "HPS_v2_compressed.pt"
            md5 = "fd9180de357abf01fdb4eaad64631db4"
        elif version == "v2.1":
            url = "https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/Third_Party/HPS_v2.1_compressed.pt"
            filename = "HPS_v2.1_compressed.pt"
            md5 = "4067542e34ba2553a738c5ac6c1d75c0"
        else:
            raise ValueError("Only v2.0 and v2.1 are supported.")
        if self.model_path is None or not os.path.exists(self.model_path):
            download_url(url, torch.hub.get_dir(), md5=md5)
            model_path = os.path.join(torch.hub.get_dir(), filename)

        state_dict = torch.load(model_path, map_location="cpu")["state_dict"]  # type: ignore
        self.model.load_state_dict(state_dict)
        self.model.to(device=self.device, dtype=self.dtype)
        self.model.requires_grad_(False)
        self.model.eval()

    def __call__(
        self, batch_frames: torch.Tensor, batch_prompt: list[str]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        assert batch_frames.shape[0] == len(batch_prompt)
        # Compute batch reward and loss in frame-wise.
        batch_frames = rearrange(batch_frames, "b c t h w -> t b c h w")
        batch_loss, batch_reward = 0, 0
        for frames in batch_frames:
            image_inputs = torch.stack([self.transform(frame) for frame in frames])
            image_inputs = image_inputs.to(device=self.device, dtype=self.dtype)
            text_inputs = self.tokenizer(batch_prompt).to(device=self.device)
            outputs = self.model(image_inputs, text_inputs)

            image_features, text_features = (
                outputs["image_features"],
                outputs["text_features"],
            )
            logits = image_features @ text_features.T
            reward = torch.diagonal(logits)
            # Convert reward to loss in [0, 1].
            if self.max_reward is None:
                loss = (-1 * reward) * self.loss_scale
            else:
                loss = abs(reward - self.max_reward) * self.loss_scale

            batch_loss, batch_reward = (
                batch_loss + loss.mean(),
                batch_reward + reward.mean(),
            )

        return batch_loss / batch_frames.shape[0], batch_reward / batch_frames.shape[0]  # type: ignore


class PickScoreReward(BaseReward):
    """[PickScore](https://github.com/yuvalkirstain/PickScore) reward model."""

    def __init__(
        self,
        model_path="yuvalkirstain/PickScore_v1",
        device="cpu",
        dtype=torch.float16,
        max_reward=1,
        loss_scale=1,
    ):
        from transformers import AutoProcessor, AutoModel

        self.model_path = model_path
        self.device = device
        self.dtype = dtype
        self.max_reward = max_reward
        self.loss_scale = loss_scale

        # https://huggingface.co/yuvalkirstain/PickScore_v1/blob/main/preprocessor_config.json
        self.transform = transforms.Compose(
            [
                transforms.Resize(
                    224, interpolation=transforms.InterpolationMode.BICUBIC
                ),
                transforms.CenterCrop(224),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711],
                ),
            ]
        )
        self.processor = AutoProcessor.from_pretrained(
            "laion/CLIP-ViT-H-14-laion2B-s32B-b79K", torch_dtype=self.dtype
        )
        self.model = (
            AutoModel.from_pretrained(model_path, torch_dtype=self.dtype)
            .eval()
            .to(device)
        )
        self.model.requires_grad_(False)
        self.model.eval()

    def __call__(
        self, batch_frames: torch.Tensor, batch_prompt: list[str]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        assert batch_frames.shape[0] == len(batch_prompt)
        # Compute batch reward and loss in frame-wise.
        batch_frames = rearrange(batch_frames, "b c t h w -> t b c h w")
        batch_loss, batch_reward = 0, 0
        for frames in batch_frames:
            image_inputs = torch.stack([self.transform(frame) for frame in frames])
            image_inputs = image_inputs.to(device=self.device, dtype=self.dtype)
            text_inputs = self.processor(
                text=batch_prompt,
                padding=True,
                truncation=True,
                max_length=77,
                return_tensors="pt",
            ).to(self.device)
            image_features = self.model.get_image_features(pixel_values=image_inputs)
            text_features = self.model.get_text_features(**text_inputs)
            image_features = image_features / torch.norm(
                image_features, dim=-1, keepdim=True
            )
            text_features = text_features / torch.norm(
                text_features, dim=-1, keepdim=True
            )

            logits = image_features @ text_features.T
            reward = torch.diagonal(logits)
            # Convert reward to loss in [0, 1].
            if self.max_reward is None:
                loss = (-1 * reward) * self.loss_scale
            else:
                loss = abs(reward - self.max_reward) * self.loss_scale

            batch_loss, batch_reward = (
                batch_loss + loss.mean(),
                batch_reward + reward.mean(),
            )

        return batch_loss / batch_frames.shape[0], batch_reward / batch_frames.shape[0]  # type: ignore


class MPSReward(BaseReward):
    """[MPS](https://github.com/Kwai-Kolors/MPS) reward model."""

    def __init__(
        self,
        model_path=None,
        device="cpu",
        dtype=torch.float16,
        max_reward=1,
        loss_scale=1,
    ):
        from transformers import AutoTokenizer, AutoConfig
        from reward.clip_model import CLIPModel

        self.model_path = model_path
        self.device = device
        self.dtype = dtype
        self.condition = "light, color, clarity, tone, style, ambiance, artistry, shape, face, hair, hands, limbs, structure, instance, texture, quantity, attributes, position, number, location, word, things."
        self.max_reward = max_reward
        self.loss_scale = loss_scale

        processor_name_or_path = "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
        # https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K/blob/main/preprocessor_config.json
        # TODO: [transforms.Resize(224), transforms.CenterCrop(224)] for any aspect ratio.
        self.transform = transforms.Compose(
            [
                transforms.Resize(
                    (224, 224), interpolation=transforms.InterpolationMode.BICUBIC
                ),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711],
                ),
            ]
        )

        # We convert the original [ckpt](http://drive.google.com/file/d/17qrK_aJkVNM75ZEvMEePpLj6L867MLkN/view?usp=sharing)
        # (contains the entire model) to a `state_dict`.
        url = "https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/Third_Party/MPS_overall.pth"
        filename = "MPS_overall.pth"
        md5 = "1491cbbbd20565747fe07e7572e2ac56"
        if self.model_path is None or not os.path.exists(self.model_path):
            download_url(url, torch.hub.get_dir(), md5=md5)
            model_path = os.path.join(torch.hub.get_dir(), filename)

        self.tokenizer = AutoTokenizer.from_pretrained(
            processor_name_or_path, trust_remote_code=True
        )
        config = AutoConfig.from_pretrained(processor_name_or_path)
        self.model = CLIPModel(config)
        state_dict = torch.load(model_path, map_location="cpu")  # type: ignore
        self.model.load_state_dict(state_dict, strict=False)
        self.model.to(device=self.device, dtype=self.dtype)
        self.model.requires_grad_(False)
        self.model.eval()

    def _tokenize(self, caption):
        input_ids = self.tokenizer(
            caption,
            max_length=self.tokenizer.model_max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        ).input_ids

        return input_ids

    def __call__(
        self,
        batch_frames: torch.Tensor,
        batch_prompt: list[str],
        batch_condition: Optional[list[str]] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if batch_condition is None:
            batch_condition = [self.condition] * len(batch_prompt)
        batch_frames = rearrange(batch_frames, "b c t h w -> t b c h w")
        batch_loss, batch_reward = 0, 0
        for frames in batch_frames:
            image_inputs = torch.stack([self.transform(frame) for frame in frames])
            image_inputs = image_inputs.to(device=self.device, dtype=self.dtype)
            text_inputs = self._tokenize(batch_prompt).to(self.device)
            condition_inputs = self._tokenize(batch_condition).to(device=self.device)
            text_features, image_features = self.model(
                text_inputs, image_inputs, condition_inputs
            )

            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            # reward = self.model.logit_scale.exp() * torch.diag(torch.einsum('bd,cd->bc', text_features, image_features))
            logits = image_features @ text_features.T
            reward = torch.diagonal(logits)
            # Convert reward to loss in [0, 1].
            if self.max_reward is None:
                loss = (-1 * reward) * self.loss_scale
            else:
                loss = abs(reward - self.max_reward) * self.loss_scale

            batch_loss, batch_reward = (
                batch_loss + loss.mean(),
                batch_reward + reward.mean(),
            )

        return batch_loss / batch_frames.shape[0], batch_reward / batch_frames.shape[0]  # type: ignore


if __name__ == "__main__":
    import numpy as np
    from decord import VideoReader

    video_path_list = ["your_video_path_1.mp4", "your_video_path_2.mp4"]
    prompt_list = ["your_prompt_1", "your_prompt_2"]
    num_sampled_frames = 8

    to_tensor = transforms.ToTensor()

    sampled_frames_list = []
    for video_path in video_path_list:
        vr = VideoReader(video_path)
        sampled_frame_indices = np.linspace(
            0, len(vr), num_sampled_frames, endpoint=False, dtype=int
        )
        sampled_frames = vr.get_batch(sampled_frame_indices).asnumpy()
        sampled_frames = torch.stack([to_tensor(frame) for frame in sampled_frames])
        sampled_frames_list.append(sampled_frames)
    sampled_frames = torch.stack(sampled_frames_list)
    sampled_frames = rearrange(sampled_frames, "b t c h w -> b c t h w")

    aesthetic_reward_v2 = AestheticReward(device="cuda", dtype=torch.bfloat16)
    print(f"aesthetic_reward_v2: {aesthetic_reward_v2(sampled_frames)}")

    aesthetic_reward_v2_5 = AestheticReward(
        encoder_path="google/siglip-so400m-patch14-384",
        version="v2.5",
        device="cuda",
        dtype=torch.bfloat16,
    )
    print(f"aesthetic_reward_v2_5: {aesthetic_reward_v2_5(sampled_frames)}")

    hps_reward_v2 = HPSReward(device="cuda", dtype=torch.bfloat16)
    print(f"hps_reward_v2: {hps_reward_v2(sampled_frames, prompt_list)}")

    hps_reward_v2_1 = HPSReward(version="v2.1", device="cuda", dtype=torch.bfloat16)
    print(f"hps_reward_v2_1: {hps_reward_v2_1(sampled_frames, prompt_list)}")

    pick_score = PickScoreReward(device="cuda", dtype=torch.bfloat16)
    print(f"pick_score_reward: {pick_score(sampled_frames, prompt_list)}")

    mps_score = MPSReward(device="cuda", dtype=torch.bfloat16)
    print(f"mps_reward: {mps_score(sampled_frames, prompt_list)}")
</file>

<file path="reward/reward_training_core.py">
"""Reward-based training core for WAN Reward LoRA.

Implements a generate-then-reward backprop training loop similar to the
reference approach, reusing local reward functions in `criteria.reward_fn`.
"""

from __future__ import annotations

import argparse
import json
import math
import os
import random
from typing import Any, Dict, List, Optional, Tuple

import torch
from accelerate import Accelerator
from einops import rearrange
import logging
from common.logger import get_logger
from generation.sampling import save_videos_grid
from utils.train_utils import clean_memory_on_device
from wan.modules.model import WanModel
from wan.modules.t5 import T5EncoderModel
from wan.modules.vae import WanVAE
from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler


logger = get_logger(__name__)


def _load_prompts(args: argparse.Namespace) -> List[str]:
    """Load training prompts strictly from TOML-derived args.

    Only uses args.reward_prompts (list[str]) which must be populated from the
    TOML (either as a list or via enumerated reward_promptN keys). No files.
    """
    prompts: List[str] = list(getattr(args, "reward_prompts", []) or [])
    if len(prompts) == 0:
        raise ValueError(
            "No reward prompts found in config (reward_prompts or reward_promptN)"
        )
    return prompts


def _parse_reward_fn(
    device: torch.device,
    dtype: torch.dtype,
    fn_name: str,
    fn_kwargs_json: Optional[str],
):
    from reward import reward_fn as reward_lib

    if not hasattr(reward_lib, fn_name):
        raise ValueError(f"Unknown reward function: {fn_name}")
    kwargs: Dict[str, Any] = {}
    if fn_kwargs_json:
        try:
            kwargs = json.loads(fn_kwargs_json)
        except Exception as e:
            raise ValueError(f"Invalid reward_fn_kwargs JSON: {e}")
    # normalize dtype for external libs
    return getattr(reward_lib, fn_name)(device=str(device), dtype=dtype, **kwargs)


def _compute_seq_len(latents: torch.Tensor, patch_size: Tuple[int, int, int]) -> int:
    _, _, lat_f, lat_h, lat_w = latents.shape
    pt, ph, pw = patch_size
    return max(1, (lat_f * lat_h * lat_w) // (pt * ph * pw))


class RewardTrainingCore:
    """Reward-based trainer for WAN Reward LoRA."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config

    def _ensure_vae(self, vae: Optional[WanVAE]) -> WanVAE:
        if vae is None:
            raise ValueError("VAE is required for reward training but not loaded")
        return vae

    def _build_t5(
        self, accelerator: Accelerator, args: argparse.Namespace
    ) -> T5EncoderModel:
        # Select config by task
        text_len = self.config["text_len"]
        t5_dtype = self.config["t5_dtype"]
        t5_path = args.t5
        if t5_path is None or not str(t5_path).strip():
            raise ValueError("T5 checkpoint path (t5) is required for reward training")
        logger.info(f"Loading T5 encoder for reward training: {t5_path}")
        return T5EncoderModel(
            text_len=text_len,
            dtype=t5_dtype,
            device=accelerator.device,
            weight_path=t5_path,
            fp8=getattr(args, "fp8_t5", False),
        )

    def _encode_prompts(
        self, t5: T5EncoderModel, prompts: List[str], device: torch.device
    ) -> List[torch.Tensor]:
        embeds_list: List[torch.Tensor] = []
        with torch.no_grad():
            for p in prompts:
                out = t5([p], device)
                embeds_list.append(out[0])  # (L, D)
        return embeds_list

    def run_reward_training_loop(
        self,
        args: argparse.Namespace,
        accelerator: Accelerator,
        transformer: WanModel,
        network: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        lr_scheduler: Any,
        trainable_params: List[Dict[str, Any]],
        save_model: Any,
        remove_model: Any,
        vae: Optional[WanVAE] = None,
        is_main_process: bool = False,
        global_step: int = 0,
    ) -> Tuple[int, torch.nn.Module]:
        """Main reward-based training loop.

        Notes:
        - Generates latents per step, performs per-timestep denoising with optional CFG,
          decodes a subset of frames, computes reward loss, and backprops on selected steps.
        - Saves checkpoints periodically according to standard args.save_every_* settings.
        """

        device = accelerator.device
        network_dtype = (
            torch.bfloat16
            if args.mixed_precision == "bf16"
            else (torch.float16 if args.mixed_precision == "fp16" else torch.float32)
        )

        vae = self._ensure_vae(vae)
        reward_batch_size = int(getattr(args, "reward_train_batch_size", 1))
        height = int(getattr(args, "reward_train_sample_height", 256))
        width = int(getattr(args, "reward_train_sample_width", 256))
        video_len = int(getattr(args, "reward_video_length", 49))
        guidance_scale = float(getattr(args, "reward_guidance_scale", 6.0))
        num_inference_steps = int(getattr(args, "reward_num_inference_steps", 50))
        num_decoded_latents = int(getattr(args, "reward_num_decoded_latents", 1))
        validation_steps = int(getattr(args, "reward_validation_steps", 10000))

        # Prepare scheduler (Flow UniPC multi-step)
        scheduler = FlowUniPCMultistepScheduler(
            shift=int(getattr(args, "discrete_flow_shift", 1))
        )

        # Build T5 encoder and load prompts
        t5 = self._build_t5(accelerator, args)
        prompt_pool = _load_prompts(args)

        # Reward function
        weight_dtype = network_dtype
        reward_fn_name = getattr(args, "reward_fn", "HPSReward")
        reward_fn_kwargs = getattr(args, "reward_fn_kwargs", None)
        reward_fn = _parse_reward_fn(
            device, weight_dtype, reward_fn_name, reward_fn_kwargs
        )

        # Trainer loop size
        max_steps = int(getattr(args, "max_train_steps", 10000))
        checkpoint_every = int(getattr(args, "save_every_n_steps", 1000) or 1000)

        # Latent/patch sizing
        vae_stride_t, vae_stride_hw = (
            self.config["vae_stride"][0],
            self.config["vae_stride"][1],
        )
        lat_f = 1 if video_len == 1 else (video_len - 1) // vae_stride_t + 1
        lat_h, lat_w = height // vae_stride_hw, width // vae_stride_hw
        in_channels = getattr(transformer, "in_dim", 16)

        # Determine backprop steps strategy
        backprop_enabled = bool(getattr(args, "reward_backprop", True))
        strategy = str(getattr(args, "reward_backprop_strategy", "last"))
        tail_k = int(getattr(args, "reward_backprop_num_steps", 5))
        manual_steps: Optional[List[int]] = getattr(
            args, "reward_backprop_step_list", None
        )
        random_range = (
            int(getattr(args, "reward_backprop_random_start_step", 0)),
            int(
                getattr(
                    args, "reward_backprop_random_end_step", num_inference_steps - 1
                )
            ),
        )
        stop_grad_latent_input = bool(
            getattr(args, "reward_stop_latent_model_input_gradient", False)
        )

        # Training
        transformer = accelerator.unwrap_model(transformer)
        transformer.switch_block_swap_for_training()

        progress = range(global_step, max_steps)
        for step_idx in progress:
            # Batch selection
            batch_prompts = random.choices(prompt_pool, k=reward_batch_size)

            # Encode prompts and negative prompts
            prompt_embeds = self._encode_prompts(t5, batch_prompts, device)
            negative_embeds = self._encode_prompts(t5, [""] * reward_batch_size, device)

            # Prepare latents
            latents = torch.randn(
                reward_batch_size,
                in_channels,
                lat_f,
                lat_h,
                lat_w,
                device=device,
                dtype=network_dtype,
            )

            # Prepare scheduler
            scheduler.set_timesteps(
                num_inference_steps,
                device=device,
                shift=int(getattr(args, "discrete_flow_shift", 1)),
            )
            timesteps = scheduler.timesteps

            # Per-step denoising
            for i, t in enumerate(timesteps):
                # Prepare model input
                latent_input = latents
                if stop_grad_latent_input:
                    latent_input = latent_input.detach()

                # seq len for DiT
                seq_len = _compute_seq_len(
                    latent_input, tuple(self.config["patch_size"])
                )

                # Two passes for CFG
                with accelerator.autocast():
                    # cond
                    cond_pred_list = transformer(
                        latent_input,
                        t=t.unsqueeze(0).repeat(reward_batch_size),
                        context=[
                            x.to(device=device, dtype=network_dtype)
                            for x in prompt_embeds
                        ],
                        seq_len=seq_len,
                        y=None,
                        clip_fea=None,
                    )
                    cond_pred = torch.stack(cond_pred_list, dim=0)  # (B, C, F, H, W)

                    if guidance_scale > 1.0:
                        uncond_pred_list = transformer(
                            latent_input,
                            t=t.unsqueeze(0).repeat(reward_batch_size),
                            context=[
                                x.to(device=device, dtype=network_dtype)
                                for x in negative_embeds
                            ],
                            seq_len=seq_len,
                            y=None,
                            clip_fea=None,
                        )
                        uncond_pred = torch.stack(uncond_pred_list, dim=0)
                        noise_pred = uncond_pred + guidance_scale * (
                            cond_pred - uncond_pred
                        )
                    else:
                        noise_pred = cond_pred

                # Determine if backprop applies for this step
                do_backprop = False
                if backprop_enabled:
                    if manual_steps is not None:
                        do_backprop = i in manual_steps
                    elif strategy == "last":
                        do_backprop = i == (num_inference_steps - 1)
                    elif strategy == "tail":
                        do_backprop = i >= max(0, num_inference_steps - tail_k)
                    elif strategy == "uniform":
                        interval = max(1, num_inference_steps // max(1, tail_k))
                        do_backprop = (i % interval) == 0
                    elif strategy == "random":
                        rs, re = random_range
                        selected = set(
                            random.sample(
                                list(range(rs, re + 1)),
                                k=min(tail_k, max(1, re - rs + 1)),
                            )
                        )
                        do_backprop = i in selected

                if not do_backprop:
                    noise_pred = noise_pred.detach()

                # Scheduler step
                latents = scheduler.step(
                    noise_pred,
                    t,
                    latents,
                    return_dict=False,
                    generator=None,
                )[0]

            # Decode subset of frames for reward
            with torch.autocast(device.type, dtype=vae.dtype):
                # Select first N temporal slices along latent T
                indices = list(range(min(num_decoded_latents, latents.shape[2])))
                sub_latents = latents[:, :, indices, :, :]
                decoded_out = vae.decode(sub_latents)
                # decoded_out can be a list of length B with tensors of shape (T, C, H, W)
                # or a tensor (B, T, C, H, W). Normalize to (B, T, C, H, W).
                if isinstance(decoded_out, list):
                    decoded_btchw = torch.stack(decoded_out, dim=0)
                else:
                    decoded_btchw = decoded_out
            # Scale to [0,1]
            decoded_btchw = (decoded_btchw / 2 + 0.5).clamp(0, 1)
            # For reward: (B, C, T, H, W)
            decoded = decoded_btchw.permute(0, 2, 1, 3, 4).contiguous()

            # Compute reward loss
            # Reward expects (B, C, T, H, W)
            loss, reward_val = reward_fn(
                decoded.to(device=device, dtype=weight_dtype), batch_prompts
            )

            # Log and backward
            accelerator.backward(loss)
            if args.max_grad_norm and float(args.max_grad_norm) > 0:
                accelerator.clip_grad_norm_(
                    [p for p in network.parameters() if p.requires_grad],
                    float(args.max_grad_norm),
                )
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad(set_to_none=True)

            # Periodic checkpointing
            global_step += 1
            if (
                is_main_process
                and checkpoint_every
                and (global_step % checkpoint_every == 0)
            ):
                from utils import train_utils as _tu

                ckpt_name = _tu.get_step_ckpt_name(args.output_name, global_step)
                save_model(
                    ckpt_name, accelerator.unwrap_model(network), global_step, None
                )

            # Periodic validation sample (save tiny mp4 of decoded subset)
            if validation_steps and (global_step % validation_steps == 0):
                try:
                    out_dir = os.path.join(args.output_dir, "train_sample")
                    os.makedirs(out_dir, exist_ok=True)
                    # decoded_btchw holds (B, T, C, H, W)
                    save_videos_grid(
                        decoded_btchw.to(torch.float32).detach().cpu(),
                        os.path.join(out_dir, f"sample-{global_step}.mp4"),
                        fps=16,
                    )
                except Exception as e:
                    logger.warning(f"Failed to save reward training sample video: {e}")

            # Tracker logs
            if accelerator.is_main_process and len(accelerator.trackers) > 0:
                accelerator.log(
                    {
                        "train_loss": float(loss.detach().item()),
                        "train_reward": float(reward_val.detach().item()),
                    },
                    step=global_step,
                )

            # Early exit
            if global_step >= max_steps:
                break

            # Clean memory
            clean_memory_on_device(device)

        transformer.switch_block_swap_for_training()
        return global_step, network
</file>

<file path="reward/siglip_v2_5.py">
## Based on https://github.com/aigc-apps/VideoX-Fun/blob/main/videox_fun/video_caption/utils/siglip_v2_5.py (Apache)

# Borrowed from https://github.com/discus0434/aesthetic-predictor-v2-5/blob/3125a9e/src/aesthetic_predictor_v2_5/siglip_v2_5.py.
import os
from collections import OrderedDict
from os import PathLike
from typing import Final

import torch
import torch.nn as nn
from transformers import (
    SiglipImageProcessor,
    SiglipVisionConfig,
    SiglipVisionModel,
    logging,
)
from transformers.image_processing_utils import BatchFeature
from transformers.modeling_outputs import ImageClassifierOutputWithNoAttention

logging.set_verbosity_error()

URL: Final[str] = (
    "https://github.com/discus0434/aesthetic-predictor-v2-5/raw/main/models/aesthetic_predictor_v2_5.pth"
)


class AestheticPredictorV2_5Head(nn.Module):
    def __init__(self, config: SiglipVisionConfig) -> None:
        super().__init__()
        self.scoring_head = nn.Sequential(
            nn.Linear(config.hidden_size, 1024),
            nn.Dropout(0.5),
            nn.Linear(1024, 128),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 16),
            nn.Dropout(0.2),
            nn.Linear(16, 1),
        )

    def forward(self, image_embeds: torch.Tensor) -> torch.Tensor:
        return self.scoring_head(image_embeds)


class AestheticPredictorV2_5Model(SiglipVisionModel):
    PATCH_SIZE = 14

    def __init__(self, config: SiglipVisionConfig, *args, **kwargs) -> None:
        super().__init__(config, *args, **kwargs)
        self.layers = AestheticPredictorV2_5Head(config)
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor | None = None,
        labels: torch.Tensor | None = None,
        return_dict: bool | None = None,
    ) -> tuple | ImageClassifierOutputWithNoAttention:
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        outputs = super().forward(
            pixel_values=pixel_values,
            return_dict=return_dict,
        )
        image_embeds = outputs.pooler_output  # type: ignore
        image_embeds_norm = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
        prediction = self.layers(image_embeds_norm)

        loss = None
        if labels is not None:
            loss_fct = nn.MSELoss()
            loss = loss_fct()

        if not return_dict:
            return (loss, prediction, image_embeds)

        return ImageClassifierOutputWithNoAttention(
            loss=loss,
            logits=prediction,
            hidden_states=image_embeds,  # type: ignore
        )


class AestheticPredictorV2_5Processor(SiglipImageProcessor):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    def __call__(self, *args, **kwargs) -> BatchFeature:
        return super().__call__(*args, **kwargs)

    @classmethod
    def from_pretrained(
        self,
        pretrained_model_name_or_path: (
            str | PathLike
        ) = "google/siglip-so400m-patch14-384",
        *args,
        **kwargs,
    ) -> "AestheticPredictorV2_5Processor":
        return super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)  # type: ignore


def convert_v2_5_from_siglip(
    predictor_name_or_path: str | PathLike | None = None,
    encoder_model_name: str = "google/siglip-so400m-patch14-384",
    *args,
    **kwargs,
) -> tuple[AestheticPredictorV2_5Model, AestheticPredictorV2_5Processor]:
    model = AestheticPredictorV2_5Model.from_pretrained(
        encoder_model_name, *args, **kwargs
    )

    processor = AestheticPredictorV2_5Processor.from_pretrained(
        encoder_model_name, *args, **kwargs
    )

    if predictor_name_or_path is None or not os.path.exists(predictor_name_or_path):
        state_dict = torch.hub.load_state_dict_from_url(URL, map_location="cpu")
    else:
        state_dict = torch.load(predictor_name_or_path, map_location="cpu")

    assert isinstance(state_dict, OrderedDict)

    model.layers.load_state_dict(state_dict)  # type: ignore
    model.eval()

    return model, processor  # type: ignore
</file>

<file path="scheduling/fopp.py">
## Based on: https://arxiv.org/abs/2503.07418

"""
FoPP (Frame-oriented Probability Propagation) scheduler and asynchronous noise application for AR-Diffusion training.

Implements the algorithm from "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion".

Author: [Your Name]
"""

import numpy as np
import torch
from typing import Tuple, Optional, Literal


class FoPPScheduler:
    """
    Implements the Frame-oriented Probability Propagation (FoPP) timestep scheduler
    for AR-Diffusion, supporting batch sampling and precomputed DP matrices.

    Args:
        num_frames (int): Number of frames per video (F)
        num_timesteps (int): Number of diffusion timesteps (T)
        device (torch.device, optional): Device for output tensors
    """

    def __init__(
        self,
        num_frames: int,
        num_timesteps: int,
        device: Optional[torch.device] = None,
        seed: Optional[int] = None,
    ):
        if not (num_frames > 0 and num_timesteps > 0):
            raise ValueError("Number of frames and timesteps must be positive.")
        self.num_frames = num_frames
        self.num_timesteps = num_timesteps
        self.device = device
        # Create a deterministic RNG if seed provided; otherwise use a fresh generator
        self._rng = (
            np.random.default_rng(seed) if seed is not None else np.random.default_rng()
        )
        self.ds, self.de = self._precompute_matrices()

    def _precompute_matrices(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Precomputes the d^s and d^e matrices using dynamic programming.
        """
        F, T = self.num_frames, self.num_timesteps
        ds = np.zeros((F, T), dtype=np.float64)
        de = np.zeros((F, T), dtype=np.float64)
        ds[F - 1, :] = 1
        ds[:, T - 1] = 1
        for i in range(F - 2, -1, -1):
            for j in range(T - 2, -1, -1):
                ds[i, j] = ds[i, j + 1] + ds[i + 1, j]
        de[0, :] = 1
        de[:, 0] = 1
        for i in range(1, F):
            for j in range(1, T):
                de[i, j] = de[i, j - 1] + de[i - 1, j]
        return ds, de

    def sample(self) -> np.ndarray:
        """
        Samples a single non-decreasing timestep composition (t_1, ..., t_F).
        Returns:
            np.ndarray: (F,) array of sampled timesteps (0-based)
        """
        F, T = self.num_frames, self.num_timesteps
        timesteps = np.zeros(F, dtype=int)
        f = self._rng.integers(0, F)
        tf = self._rng.integers(0, T)
        timesteps[f] = tf
        for i in range(f - 1, -1, -1):
            t_next = timesteps[i + 1]
            possible_range_end = t_next + 1
            weights = self.de[i, :possible_range_end]
            probs = weights / (np.sum(weights) + 1e-8)
            sampled_t = self._rng.choice(possible_range_end, p=probs)
            timesteps[i] = sampled_t
        for i in range(f + 1, F):
            t_prev = timesteps[i - 1]
            possible_timesteps = np.arange(t_prev, T)
            if len(possible_timesteps) == 0:
                timesteps[i] = t_prev
                continue
            weights = self.ds[i, t_prev:]
            probs = weights / (np.sum(weights) + 1e-8)
            sampled_t = self._rng.choice(possible_timesteps, p=probs)
            timesteps[i] = sampled_t
        return timesteps

    def sample_batch(self, batch_size: int) -> np.ndarray:
        """
        Samples a batch of non-decreasing timestep compositions.
        Args:
            batch_size (int): Number of samples to generate
        Returns:
            np.ndarray: (batch_size, F) array of sampled timesteps
        """
        return np.stack([self.sample() for _ in range(batch_size)], axis=0)


def get_linear_beta_schedule(
    num_timesteps: int, beta_start: float = 0.0001, beta_end: float = 0.002
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generates a linear beta schedule and computes alphas and alpha_bar (cumulative product).
    Args:
        num_timesteps (int): Number of diffusion steps (T)
        beta_start (float): Linear start value for beta
        beta_end (float): Linear end value for beta
    Returns:
        betas (np.ndarray): (T,) array
        alphas (np.ndarray): (T,) array
        alpha_bar (np.ndarray): (T,) array
    """
    betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float64)
    alphas = 1.0 - betas
    alpha_bar = np.cumprod(alphas)
    return betas, alphas, alpha_bar


def get_cosine_beta_schedule(
    num_timesteps: int, s: float = 0.008
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generates a cosine beta schedule (for experimentation).
    Args:
        num_timesteps (int): Number of diffusion steps (T)
        s (float): Small offset for stability
    Returns:
        betas (np.ndarray): (T,) array
        alphas (np.ndarray): (T,) array
        alpha_bar (np.ndarray): (T,) array
    """
    steps = np.arange(num_timesteps + 1, dtype=np.float64)
    t = steps / num_timesteps
    alphas_cumprod = np.cos(((t + s) / (1 + s)) * np.pi / 2) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    alphas = 1.0 - betas
    alpha_bar = np.cumprod(alphas)
    return betas, alphas, alpha_bar


def get_alpha_bar_schedule(
    num_timesteps: int,
    schedule_type: Literal["linear", "cosine"] = "linear",
    beta_start: float = 0.0001,
    beta_end: float = 0.002,
) -> np.ndarray:
    """
    Returns the alpha_bar schedule for the given type.
    Args:
        num_timesteps (int): Number of diffusion steps (T)
        schedule_type (str): 'linear' or 'cosine'
        beta_start (float): For linear schedule
        beta_end (float): For linear schedule
    Returns:
        alpha_bar (np.ndarray): (T,) array
    """
    if schedule_type == "linear":
        _, _, alpha_bar = get_linear_beta_schedule(num_timesteps, beta_start, beta_end)
    elif schedule_type == "cosine":
        _, _, alpha_bar = get_cosine_beta_schedule(num_timesteps)
    else:
        raise ValueError(f"Unknown schedule_type: {schedule_type}")
    return alpha_bar


def apply_asynchronous_noise(
    z0: torch.Tensor,
    timesteps_composition: torch.Tensor,
    noise: torch.Tensor,
    alpha_bar: np.ndarray,
) -> torch.Tensor:
    """
    Applies noise to each frame in the latent batch according to its own timestep.
    Args:
        z0 (torch.Tensor): Clean latents, shape (B, F, ...)
        timesteps_composition (torch.Tensor): Timesteps per frame, shape (B, F), int64
        noise (torch.Tensor): Noise tensor, same shape as z0
        alpha_bar (np.ndarray): Precomputed alpha_bar schedule, shape (T,)
    Returns:
        torch.Tensor: Noisy latents, same shape as z0
    """
    # Gather alpha_bar for each frame
    # timesteps_composition is 0-based, alpha_bar[0] is valid
    device = z0.device
    B, F = timesteps_composition.shape
    alpha_bar_t = torch.from_numpy(alpha_bar).to(device=device, dtype=z0.dtype)[
        timesteps_composition
    ]
    # Reshape for broadcasting
    while alpha_bar_t.dim() < z0.dim():
        alpha_bar_t = alpha_bar_t.unsqueeze(-1)
    noisy_z = torch.sqrt(alpha_bar_t) * z0 + torch.sqrt(1 - alpha_bar_t) * noise
    return noisy_z
</file>

<file path="scheduling/fvdm.py">
## Based on: https://arxiv.org/abs/2410.03160

import argparse
import torch
from typing import Tuple, Any


def get_noisy_model_input_and_timesteps_fvdm(
    args: argparse.Namespace,
    noise: torch.Tensor,
    latents: torch.Tensor,
    noise_scheduler: Any,
    device: torch.device,
    dtype: torch.dtype,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Generate FVDM/PUSA vectorized noisy inputs and timesteps for a Flow Matching trainer.

    This function implements the Rectified Flow noising process:
    x_t = (1-t) * x_0 + t * x_1
    where x_0 is the clean latent and x_1 is the noise.
    The model's objective will be to predict the velocity v = x_1 - x_0.
    """
    if latents.ndim != 5:
        raise ValueError(
            f"FVDM requires 5D latents (B, C, F, H, W), but got {latents.ndim}D"
        )

    B, C, F, H, W = latents.shape
    T = noise_scheduler.num_train_timesteps

    # 1. Probabilistic Timestep Sampling Strategy (PTSS)
    if torch.rand((), device=device) < args.fvdm_ptss_p:
        # Asynchronous: Sample a different continuous time t for each frame
        t_cont = torch.rand((B, F), device=device, dtype=dtype)
    else:
        # Synchronous: Sample one continuous time t for the whole clip
        t_cont = torch.rand((B, 1), device=device, dtype=dtype).expand(B, F)

    # 2. Apply min/max timestep constraints
    # These are normalized to the [0, 1] range of t_cont
    t_min = getattr(args, "min_timestep", 0.0) / T
    t_max = getattr(args, "max_timestep", T) / T
    t_cont = t_cont * (t_max - t_min) + t_min

    # 3. Add noise using the Flow Matching equation
    # For Rectified Flow, the noise level `sigma` is equivalent to the time `t`.
    sigma_cont = t_cont

    # Reshape sigma for broadcasting: (B, F) -> (B, 1, F, 1, 1)
    sigma_broadcast = sigma_cont.view(B, 1, F, 1, 1)

    noisy_model_input = (1.0 - sigma_broadcast) * latents + sigma_broadcast * noise

    # 4. Prepare discrete timesteps for the model's embedding layer
    # WanModel expects timesteps in the integer range [0, 999]
    timesteps_discrete = (sigma_cont * (T - 1)).round().long().clamp(0, T - 1)

    return noisy_model_input, timesteps_discrete, sigma_cont
</file>

<file path="scheduling/timestep_distribution.py">
## Based on: https://github.com/tdrussell/diffusion-pipe/commit/53f4fe7569eedce4ae3a877bbfcff7e784de1d53 (MIT)

"""Timestep distribution utilities for diffusion model training.

This module provides pre-computed timestep distribution functionality for more consistent
and reproducible training compared to on-the-fly random sampling.
"""

import argparse
import logging
from typing import Optional

import torch

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


class TimestepDistribution:
    """Handles pre-computed timestep distribution for consistent training.

    This class provides better timestep coverage and reproducibility compared to
    random on-the-fly sampling by pre-computing a quantized distribution.

    Benefits:
    - Guaranteed uniform timestep coverage (no random gaps)
    - Reproducible training (same distribution every run)
    - Better convergence due to consistent denoising coverage
    - Performance optimization (no repeated distribution calculations)
    - Support for curriculum learning via timestep range slicing
    """

    def __init__(self):
        self.distribution: Optional[torch.Tensor] = None
        self.is_initialized: bool = False
        # One-time usage log guard so we don't spam every step
        self.usage_logged: bool = False

    def initialize(self, args: argparse.Namespace) -> None:
        """Initialize the pre-computed timestep distribution.

        Args:
            args: Training arguments containing timestep sampling configuration
        """
        if self.is_initialized:
            return

        logger.info("Initializing pre-computed timestep distribution...")

        # Log configuration details
        self._log_configuration(args)

        # Create the base distribution
        self.distribution = self._create_distribution(args)
        initial_buckets = len(self.distribution)

        # Apply min/max timestep constraints if specified
        self._apply_timestep_constraints(args)

        self.is_initialized = True

        # Log final results with statistics
        self._log_initialization_results(args, initial_buckets)

    def _create_distribution(self, args: argparse.Namespace) -> torch.Tensor:
        """Create the base timestep distribution based on sampling method.

        Args:
            args: Training arguments containing timestep sampling configuration

        Returns:
            Pre-computed timestep distribution tensor
        """
        n_buckets = getattr(args, "precomputed_timestep_buckets", 10000)
        delta = 1 / n_buckets
        min_quantile = delta
        max_quantile = 1 - delta
        quantiles = torch.linspace(min_quantile, max_quantile, n_buckets)

        # Apply distribution transformation based on sampling method
        if args.timestep_sampling == "sigmoid":
            dist = torch.distributions.normal.Normal(0, 1)
            t = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t = torch.sigmoid(t * sigmoid_scale)
        elif args.timestep_sampling == "shift":
            dist = torch.distributions.normal.Normal(0, 1)
            t = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t = torch.sigmoid(t * sigmoid_scale)
            shift = getattr(args, "discrete_flow_shift", 1.0)
            t = (t * shift) / (1 + (shift - 1) * t)
        elif args.timestep_sampling == "logit_normal":
            dist = torch.distributions.normal.Normal(0, 1)
            t = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t = t * sigmoid_scale
            t = torch.sigmoid(t)
        elif args.timestep_sampling == "flux_shift":
            # Precompute using a fixed base area; can be overridden via args.precomputed_midshift_area
            dist = torch.distributions.normal.Normal(0, 1)
            z = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t = torch.sigmoid(z * sigmoid_scale)
            base_area = getattr(args, "precomputed_midshift_area", None)
            if base_area is None:
                # Try to infer from height/width if available
                h = getattr(args, "latent_height", None)
                w = getattr(args, "latent_width", None)
                if h is None or w is None:
                    h = getattr(args, "height", None)
                    w = getattr(args, "width", None)
                    if h is not None and w is not None:
                        # Heuristic: latents are often 1/8 or 1/16 of pixel dims; use /16 then /2 in code path => /32
                        h = max(int(h) // 32, 1)
                        w = max(int(w) // 32, 1)
                if h is None or w is None:
                    base_area = 1024.0
                else:
                    base_area = float((h // 2) * (w // 2))
            # Flux mapping
            # y1=0.5 at x=256, y2=1.15 at x=4096
            m = (1.15 - 0.5) / (4096 - 256)
            b = 0.5 - m * 256
            mu = m * float(base_area) + b
            shift = torch.exp(torch.tensor(mu))
            t = (t * shift) / (1 + (shift - 1) * t)
        elif args.timestep_sampling == "qwen_shift":
            # Qwen variant with different mu mapping
            dist = torch.distributions.normal.Normal(0, 1)
            z = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t = torch.sigmoid(z * sigmoid_scale)
            base_area = getattr(args, "precomputed_midshift_area", None)
            if base_area is None:
                h = getattr(args, "latent_height", None)
                w = getattr(args, "latent_width", None)
                if h is None or w is None:
                    h = getattr(args, "height", None)
                    w = getattr(args, "width", None)
                    if h is not None and w is not None:
                        h = max(int(h) // 32, 1)
                        w = max(int(w) // 32, 1)
                if h is None or w is None:
                    base_area = 1024.0
                else:
                    base_area = float((h // 2) * (w // 2))
            # Qwen mapping: (256, 0.5) -> (8192, 0.9)
            m = (0.9 - 0.5) / (8192 - 256)
            b = 0.5 - m * 256
            mu = m * float(base_area) + b
            shift = torch.exp(torch.tensor(mu))
            t = (t * shift) / (1 + (shift - 1) * t)
        elif args.timestep_sampling == "logsnr":
            # Use Normal icdf with provided mean/std, then transform
            mean = getattr(args, "logit_mean", 0.0)
            std = getattr(args, "logit_std", 1.0)
            dist = torch.distributions.normal.Normal(mean, std)
            logsnr = dist.icdf(quantiles)
            t = torch.sigmoid(-logsnr / 2)
        elif args.timestep_sampling in ("qinglong_flux", "qinglong_qwen"):
            # Mixture: 80% mid_shift, 7.5% logsnr, 12.5% logsnr2
            n_buckets = getattr(args, "precomputed_timestep_buckets", 10000)
            mid_count = int(n_buckets * 0.80)
            l1_count = int(n_buckets * 0.075)
            l2_count = n_buckets - mid_count - l1_count

            # Build quantiles per component
            def linspace01(n: int) -> torch.Tensor:
                if n <= 0:
                    return torch.empty(0)
                delta = 1 / max(n, 1)
                return torch.linspace(delta, 1 - delta, n)

            # mid_shift component
            dist0 = torch.distributions.normal.Normal(0, 1)
            z0 = dist0.icdf(linspace01(mid_count))
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            t0 = torch.sigmoid(z0 * sigmoid_scale)
            base_area = getattr(args, "precomputed_midshift_area", None)
            if base_area is None:
                h = getattr(args, "latent_height", None)
                w = getattr(args, "latent_width", None)
                if h is None or w is None:
                    h = getattr(args, "height", None)
                    w = getattr(args, "width", None)
                    if h is not None and w is not None:
                        h = max(int(h) // 32, 1)
                        w = max(int(w) // 32, 1)
                if h is None or w is None:
                    base_area = 1024.0
                else:
                    base_area = float((h // 2) * (w // 2))

            if args.timestep_sampling == "qinglong_qwen":
                m = (0.9 - 0.5) / (8192 - 256)
                b = 0.5 - m * 256
            else:
                m = (1.15 - 0.5) / (4096 - 256)
                b = 0.5 - m * 256
            mu = m * float(base_area) + b
            shift = torch.exp(torch.tensor(mu))
            t0 = (t0 * shift) / (1 + (shift - 1) * t0)

            # logsnr component
            mean1 = getattr(args, "logit_mean", 0.0)
            std1 = getattr(args, "logit_std", 1.0)
            dist1 = torch.distributions.normal.Normal(mean1, std1)
            z1 = dist1.icdf(linspace01(l1_count))
            t1 = torch.sigmoid(-z1 / 2)

            # logsnr2 component (fixed mean=5.36, std=1.0)
            dist2 = torch.distributions.normal.Normal(5.36, 1.0)
            z2 = dist2.icdf(linspace01(l2_count))
            t2 = torch.sigmoid(-z2 / 2)

            # Combine and sort
            t = torch.cat([t0, t1, t2])
            t, _ = torch.sort(t)
        elif args.timestep_sampling == "bell_shaped":
            # Bell-Shaped: build density on x‚àà[0,1], compute CDF, then invert to get quantiles
            x = torch.linspace(0.0, 1.0, n_buckets)
            bell_std = float(getattr(args, "bell_std", 0.2))
            bell_center = float(getattr(args, "bell_center", 0.5))
            # Unnormalized density
            y = torch.exp(-0.5 * ((x - bell_center) / max(bell_std, 1e-6)) ** 2)
            # Avoid degenerate cases
            y = torch.clamp(y, min=1e-12)
            # CDF in [0,1]
            cdf = torch.cumsum(y, dim=0)
            cdf = cdf / cdf[-1]
            # Invert CDF at uniform quantiles (reuse 'quantiles' spacing from above scope)
            q = torch.linspace(1 / n_buckets, 1 - 1 / n_buckets, n_buckets)
            idx = torch.searchsorted(cdf, q, right=False).clamp(
                min=1, max=n_buckets - 1
            )
            cdf_lo = cdf[idx - 1]
            cdf_hi = cdf[idx]
            x_lo = x[idx - 1]
            x_hi = x[idx]
            # Linear interpolation between (cdf_lo,x_lo) and (cdf_hi,x_hi)
            denom = torch.clamp(cdf_hi - cdf_lo, min=1e-12)
            w = (q - cdf_lo) / denom
            t = x_lo + w * (x_hi - x_lo)
        elif args.timestep_sampling == "half_bell":
            # Half Bell-Shaped: density is bell on first half, flat on second; invert CDF
            x = torch.linspace(0.0, 1.0, n_buckets)
            bell_std = float(getattr(args, "bell_std", 0.2))
            y = torch.exp(-0.5 * ((x - 0.5) / max(bell_std, 1e-6)) ** 2)
            # Flatten second half to its max value
            mid_point = n_buckets // 2
            if mid_point < n_buckets:
                y[mid_point:] = y[:mid_point].max()
            # Normalize and build CDF
            y = torch.clamp(y, min=1e-12)
            cdf = torch.cumsum(y, dim=0)
            cdf = cdf / cdf[-1]
            # Invert at uniform quantiles
            q = torch.linspace(1 / n_buckets, 1 - 1 / n_buckets, n_buckets)
            idx = torch.searchsorted(cdf, q, right=False).clamp(
                min=1, max=n_buckets - 1
            )
            cdf_lo = cdf[idx - 1]
            cdf_hi = cdf[idx]
            x_lo = x[idx - 1]
            x_hi = x[idx]
            denom = torch.clamp(cdf_hi - cdf_lo, min=1e-12)
            w = (q - cdf_lo) / denom
            t = x_lo + w * (x_hi - x_lo)
        elif args.timestep_sampling == "lognorm_blend":
            # LogNormal Blend - combines lognormal distribution with linear sampling
            alpha = getattr(args, "lognorm_blend_alpha", 0.75)

            # LogNormal distribution for first portion
            lognormal = torch.distributions.LogNormal(loc=0, scale=0.333)
            t1_size = int(n_buckets * alpha)
            t1 = lognormal.icdf(quantiles[:t1_size])
            t1_max = t1.max()  # type: ignore
            if t1_max > 0:
                t1 = 1 - t1 / t1_max  # type: ignore # Scale to [0, 1]
            else:
                t1 = torch.zeros_like(t1)  # type: ignore

            # Linear distribution for remaining portion
            t2_size = n_buckets - t1_size
            t2 = torch.linspace(0, 1, t2_size)

            # Combine and sort
            t = torch.cat([t1, t2])
            t, _ = torch.sort(t)
        elif args.timestep_sampling == "enhanced_sigmoid":
            # Enhanced sigmoid with additional parameters
            dist = torch.distributions.normal.Normal(0, 1)
            t = dist.icdf(quantiles)
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            sigmoid_bias = getattr(args, "sigmoid_bias", 0.0)
            t = torch.sigmoid((t + sigmoid_bias) * sigmoid_scale)
        else:
            # Uniform distribution for "uniform" and other methods
            t = quantiles

        return t

    def _log_configuration(self, args: argparse.Namespace) -> None:
        """Log detailed configuration information."""
        logger.info("Timestep Distribution Configuration:")
        logger.info(f"   Sampling Method: '{args.timestep_sampling}'")
        logger.info(
            f"   Bucket Count: {getattr(args, 'precomputed_timestep_buckets', 10000):,}"
        )

        # Log method-specific parameters
        if args.timestep_sampling == "sigmoid":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
        elif args.timestep_sampling == "shift":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            shift = getattr(args, "discrete_flow_shift", 1.0)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(f"   Flow Shift: {shift}")
        elif args.timestep_sampling == "logit_normal":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(f"   Distribution: Logit-Normal (Normal(0,1) -> sigmoid)")
        elif args.timestep_sampling == "bell_shaped":
            bell_std = getattr(args, "bell_std", 0.2)
            bell_center = getattr(args, "bell_center", 0.5)
            logger.info(f"   Bell Standard Deviation: {bell_std}")
            logger.info(f"   Bell Center: {bell_center}")
            logger.info(
                f"   Distribution: Bell-Shaped Mean-Normalized (centered at {bell_center})"
            )
        elif args.timestep_sampling == "half_bell":
            bell_std = getattr(args, "bell_std", 0.2)
            logger.info(f"   Bell Standard Deviation: {bell_std}")
            logger.info(f"   Distribution: Half Bell-Shaped (bell curve + flat tail)")
        elif args.timestep_sampling == "lognorm_blend":
            alpha = getattr(args, "lognorm_blend_alpha", 0.75)
            logger.info(f"   LogNormal Blend Alpha: {alpha}")
            logger.info(f"   Distribution: LogNormal({alpha}) + Linear({1-alpha})")
        elif args.timestep_sampling == "enhanced_sigmoid":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            sigmoid_bias = getattr(args, "sigmoid_bias", 0.0)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(f"   Sigmoid Bias: {sigmoid_bias}")
            logger.info(f"   Distribution: Enhanced Sigmoid (with bias)")
        elif args.timestep_sampling == "flux_shift":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            base_area = getattr(args, "precomputed_midshift_area", None)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(
                f"   MidShift Base Area: {base_area if base_area is not None else 'auto'}"
            )
            logger.info("   Distribution: Flux Mid-Shift (precomputed)")
        elif args.timestep_sampling == "qwen_shift":
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            base_area = getattr(args, "precomputed_midshift_area", None)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(
                f"   MidShift Base Area: {base_area if base_area is not None else 'auto'}"
            )
            logger.info("   Distribution: Qwen Mid-Shift (precomputed)")
        elif args.timestep_sampling == "logsnr":
            mean = getattr(args, "logit_mean", 0.0)
            std = getattr(args, "logit_std", 1.0)
            logger.info(f"   LogSNR Mean: {mean}")
            logger.info(f"   LogSNR Std: {std}")
            logger.info("   Distribution: LogSNR")
        elif args.timestep_sampling in ("qinglong_flux", "qinglong_qwen"):
            sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
            base_area = getattr(args, "precomputed_midshift_area", None)
            mean = getattr(args, "logit_mean", 0.0)
            std = getattr(args, "logit_std", 1.0)
            logger.info(f"   Sigmoid Scale: {sigmoid_scale}")
            logger.info(
                f"   MidShift Base Area: {base_area if base_area is not None else 'auto'}"
            )
            logger.info(f"   LogSNR Mean: {mean}")
            logger.info(f"   LogSNR Std: {std}")
            variant = "Qwen" if args.timestep_sampling == "qinglong_qwen" else "Flux"
            logger.info(
                f"   Distribution: Qinglong ({variant}) Mixture 0.80/0.075/0.125"
            )

        # Log timestep constraints
        min_timestep = getattr(args, "min_timestep", None)
        max_timestep = getattr(args, "max_timestep", None)
        if min_timestep is not None or max_timestep is not None:
            min_val = min_timestep if min_timestep is not None else 0
            max_val = max_timestep if max_timestep is not None else 1000
            logger.info(f"   Timestep Range: [{min_val}, {max_val}] (out of [0, 1000])")
            logger.info(
                f"   Normalized Range: [{min_val/1000:.3f}, {max_val/1000:.3f}]"
            )
        else:
            logger.info(f"   Timestep Range: [0, 1000] (full range)")

    def _log_initialization_results(
        self, args: argparse.Namespace, initial_buckets: int
    ) -> None:
        """Log initialization results with statistics."""
        final_buckets = len(self.distribution)  # type: ignore
        stats = self.get_stats()

        logger.info("Pre-computed timestep distribution ready.")
        logger.info(
            f"   Final Buckets: {final_buckets:,} (from {initial_buckets:,} initial)"
        )

        if final_buckets < initial_buckets:
            reduction_pct = (1 - final_buckets / initial_buckets) * 100
            logger.info(
                f"   Constraint Reduction: {reduction_pct:.1f}% of buckets removed"
            )

        logger.info(
            f"   Value Range: [{stats['min_value']:.6f}, {stats['max_value']:.6f}]"
        )
        logger.info(
            f"   Distribution stats: mean={stats['mean_value']:.6f}, std={stats['std_value']:.6f}"
        )

        # Memory usage estimation
        memory_mb = (final_buckets * 4) / (1024 * 1024)  # 4 bytes per float32
        logger.info(f"   Memory Usage: ~{memory_mb:.2f} MB")

    def _apply_timestep_constraints(self, args: argparse.Namespace) -> None:
        """Apply min/max timestep constraints to the distribution.

        Args:
            args: Training arguments containing min/max timestep settings
        """
        t_min = (
            getattr(args, "min_timestep", 0) / 1000.0
            if hasattr(args, "min_timestep") and args.min_timestep is not None
            else 0.0
        )
        t_max = (
            getattr(args, "max_timestep", 1000) / 1000.0
            if hasattr(args, "max_timestep") and args.max_timestep is not None
            else 1.0
        )

        if (t_min > 0.0 or t_max < 1.0) and self.distribution is not None:
            original_size = len(self.distribution)
            self.distribution = self._slice_distribution(
                self.distribution, t_min, t_max
            )
            logger.info(
                f"   Applied constraints: {original_size:,} -> {len(self.distribution):,} buckets"
            )

    def _slice_distribution(
        self, distribution: torch.Tensor, min_t: float = 0.0, max_t: float = 1.0
    ) -> torch.Tensor:
        """Slice timestep distribution to specific range for curriculum learning.

        Args:
            distribution: Full timestep distribution
            min_t: Minimum timestep value (0.0 to 1.0)
            max_t: Maximum timestep value (0.0 to 1.0)

        Returns:
            Sliced distribution within the specified range
        """
        start = torch.searchsorted(distribution, min_t).item()
        end = torch.searchsorted(distribution, max_t).item()
        return distribution[start:end]

    def sample(
        self,
        batch_size: int,
        device: torch.device,
        quantile: Optional[float] = None,
    ) -> torch.Tensor:
        """Sample timesteps from the pre-computed distribution.

        Args:
            batch_size: Number of timesteps to sample
            device: Target device for the sampled timesteps
            quantile: Optional specific quantile for deterministic sampling

        Returns:
            Sampled timesteps tensor of shape [batch_size]
        """
        if not self.is_initialized or self.distribution is None:
            raise RuntimeError(
                "TimestepDistribution not initialized. Call initialize() first."
            )

        if quantile is not None:
            # Deterministic sampling at specific quantile
            q = float(max(0.0, min(quantile, 1.0 - 1e-9)))
            i = (torch.full((batch_size,), q) * len(self.distribution)).to(torch.int64)
            i = torch.clamp(i, 0, len(self.distribution) - 1)
        else:
            # Random sampling from distribution
            i = torch.randint(
                0, len(self.distribution), size=(batch_size,), dtype=torch.int64
            )

        return self.distribution[i].to(device)

    def get_stats(self) -> dict:
        """Get statistics about the current distribution.

        Returns:
            Dictionary containing distribution statistics
        """
        if not self.is_initialized or self.distribution is None:
            return {"initialized": False}

        return {
            "initialized": True,
            "num_buckets": len(self.distribution),
            "min_value": self.distribution.min().item(),
            "max_value": self.distribution.max().item(),
            "mean_value": self.distribution.mean().item(),
            "std_value": self.distribution.std().item(),
        }


def create_timestep_distribution() -> TimestepDistribution:
    """Factory function to create a new TimestepDistribution instance.

    Returns:
        New TimestepDistribution instance
    """
    return TimestepDistribution()


def should_use_precomputed_timesteps(args: argparse.Namespace) -> bool:
    """Check if pre-computed timestep distribution should be used.

    Args:
        args: Training arguments

    Returns:
        True if pre-computed timesteps should be used, False otherwise
    """
    return getattr(args, "use_precomputed_timesteps", False)
</file>

<file path="scheduling/timestep_logging.py">
"""Utilities for logging timestep distributions to TensorBoard.

This module centralizes all visualization and logging of timestep distributions
to keep training core clean and maintainable.
"""

from __future__ import annotations

from typing import Any, Optional, Protocol
import os
from collections import deque
from typing import Deque, List


class _TBWriter(Protocol):
    def add_histogram(
        self, tag: str, values: Any, global_step: int, bins: Optional[int] = None
    ) -> None: ...
    def add_figure(self, tag: str, figure: Any, global_step: int) -> None: ...
    def add_scalar(self, tag: str, scalar_value: Any, global_step: int) -> None: ...


def _get_tensorboard_writer(accelerator) -> Optional[_TBWriter]:
    try:
        for tracker in accelerator.trackers:
            if getattr(tracker, "name", None) == "tensorboard" and hasattr(
                tracker, "writer"
            ):
                return tracker.writer
    except Exception:
        return None
    return None


def log_initial_timestep_distribution(accelerator, args, timestep_distribution) -> None:
    """Log the initial/expected timestep distribution.

    Preference order:
    1) Precomputed buckets if enabled and initialized
    2) Simulate via configured sampler and apply constraints
    """
    writer = _get_tensorboard_writer(accelerator)
    if writer is None:
        return

    import torch

    log_mode = str(getattr(args, "log_timestep_distribution", "off")).lower()
    if log_mode == "off":
        return
    num_bins = max(10, int(getattr(args, "log_timestep_distribution_bins", 100)))

    # Optional: only log once per logging directory using a marker file
    try:
        init_once = bool(getattr(args, "log_timestep_distribution_init_once", True))
        if init_once:
            logdir = getattr(args, "logging_dir", None)
            if logdir:
                os.makedirs(logdir, exist_ok=True)
                marker_path = os.path.join(
                    logdir, "timestep_initial_expected_logged.marker"
                )
                if os.path.exists(marker_path):
                    return
    except Exception:
        # Fallback to normal behavior if anything goes wrong
        pass

    # Prefer precomputed buckets if available
    dist = None
    try:
        from scheduling.timestep_distribution import should_use_precomputed_timesteps

        if (
            should_use_precomputed_timesteps(args)
            and getattr(timestep_distribution, "is_initialized", False)
            and getattr(timestep_distribution, "distribution", None) is not None
        ):
            dist = timestep_distribution.distribution
    except Exception:
        dist = None

    if dist is None:
        # Simulate by sampling many timesteps via the configured sampler
        try:
            from scheduling.timestep_utils import (
                _generate_timesteps_from_distribution,
                _apply_timestep_constraints,
            )

            samples = int(getattr(args, "log_timestep_distribution_samples", 20000))
            device = accelerator.device
            t = _generate_timesteps_from_distribution(args, samples, device)
            t = _apply_timestep_constraints(t, args, samples, device)
            dist = t
        except Exception:
            return

    # Histogram
    try:
        writer.add_histogram(
            "timestep/initial_expected",
            dist.detach().cpu(),
            global_step=0,
            bins=num_bins,
        )
    except TypeError:
        writer.add_histogram(
            "timestep/initial_expected", dist.detach().cpu(), global_step=0
        )

    # Chart image (optional)
    if log_mode in ["chart", "both"]:
        try:
            import matplotlib

            matplotlib.use("Agg")
            import matplotlib.pyplot as plt

            values = dist.detach().cpu().numpy()
            fig, ax = plt.subplots(figsize=(4.5, 3.0), dpi=120)
            ax.hist(values, bins=num_bins, range=(0.0, 1.0), density=True, alpha=0.8)
            ax.set_title("Initial timestep distribution (expected)")
            ax.set_xlabel("t in [0,1]")
            ax.set_ylabel("density")
            fig.tight_layout()
            writer.add_figure("timestep/initial_expected_chart", fig, global_step=0)
            plt.close(fig)
        except Exception:
            pass

    # Optional probability mass function (PMF) bar chart over [1..1000]
    try:
        if bool(getattr(args, "log_timestep_distribution_pmf", False)):
            import matplotlib

            matplotlib.use("Agg")
            import matplotlib.pyplot as plt
            import torch as _torch
            import numpy as _np

            idx_exp = (_torch.round(dist * 1000.0 + 1.0)).clamp_(1, 1000).long()
            counts = _torch.zeros(1000, dtype=_torch.float32)
            counts.index_add_(
                0, idx_exp - 1, _torch.ones_like(idx_exp, dtype=_torch.float32)
            )
            pmf = counts / counts.sum().clamp_min(1.0)

            x = _np.arange(1, 1001, dtype=_np.int32)
            y = pmf.numpy()

            tmin = int(getattr(args, "min_timestep", 0))
            tmax = int(getattr(args, "max_timestep", 1000))
            lo = max(1, tmin)
            hi = max(lo + 1, min(1000, tmax))

            fig, ax = plt.subplots(figsize=(6.5, 3.0), dpi=120)
            ax.bar(x[lo - 1 : hi], y[lo - 1 : hi], width=1.0)
            ax.set_title("Initial expected PMF over indices")
            ax.set_xlabel("timestep index [1..1000]")
            ax.set_ylabel("probability")
            fig.tight_layout()
            writer.add_figure("timestep/initial_expected_pmf", fig, global_step=0)
            plt.close(fig)
    except Exception:
        pass

    # Write marker so restarts do not duplicate this log
    try:
        if bool(getattr(args, "log_timestep_distribution_init_once", True)):
            logdir = getattr(args, "logging_dir", None)
            if logdir:
                marker_path = os.path.join(
                    logdir, "timestep_initial_expected_logged.marker"
                )
                with open(marker_path, "w", encoding="utf-8") as f:
                    f.write("logged\n")
    except Exception:
        pass


def log_live_timestep_distribution(
    accelerator, args, timesteps, global_step: int
) -> None:
    """Log the live (used) timestep indices during training.

    Logs histogram of indices in 1..1000, and an optional chart of normalized
    values in [0,1].
    """
    writer = _get_tensorboard_writer(accelerator)
    if writer is None:
        return

    import torch

    log_mode = str(getattr(args, "log_timestep_distribution", "off")).lower()
    if log_mode == "off":
        return
    interval = int(getattr(args, "log_timestep_distribution_interval", 1000))
    if interval <= 0 or (global_step % interval != 0):
        return

    num_bins = max(10, int(getattr(args, "log_timestep_distribution_bins", 100)))

    # Histogram over integer indices 1..1000
    ts = timesteps.detach().float().cpu()
    try:
        writer.add_histogram(
            "timestep/used_indices", ts, global_step=global_step, bins=num_bins
        )
    except TypeError:
        writer.add_histogram("timestep/used_indices", ts, global_step=global_step)

    # Rolling-window histogram over recent indices
    try:
        global _USED_INDICES_BUFFER
        window_size = int(getattr(args, "log_timestep_distribution_window", 10000))
        if window_size > 0:
            # Initialize or resize the global buffer lazily
            try:
                _ = _USED_INDICES_BUFFER  # type: ignore
            except NameError:
                _USED_INDICES_BUFFER = None  # type: ignore

            if (
                _USED_INDICES_BUFFER is None  # type: ignore
                or getattr(_USED_INDICES_BUFFER, "maxlen", None) != window_size  # type: ignore
            ):
                _USED_INDICES_BUFFER = deque(maxlen=window_size)  # type: ignore

            # Extend buffer with current step values
            values: List[float] = ts.tolist()  # type: ignore
            _USED_INDICES_BUFFER.extend(values)  # type: ignore

            # Log histogram of the aggregated window
            import torch as _torch  # local import to avoid polluting module scope

            if len(_USED_INDICES_BUFFER) > 0:  # type: ignore
                agg = _torch.tensor(list(_USED_INDICES_BUFFER), dtype=_torch.float32)  # type: ignore
                try:
                    writer.add_histogram(
                        "timestep/used_indices_window",
                        agg,
                        global_step=global_step,
                        bins=num_bins,
                    )
                except TypeError:
                    writer.add_histogram(
                        "timestep/used_indices_window", agg, global_step=global_step
                    )

                # Scalars: mean/std over window (indices and normalized t)
                try:
                    mean_idx = float(agg.mean().item())
                    std_idx = float(agg.std(unbiased=False).item())
                    writer.add_scalar(
                        "timestep/window/mean_index", mean_idx, global_step
                    )
                    writer.add_scalar("timestep/window/std_index", std_idx, global_step)
                    mean_t = float((agg / 1000.0).mean().item())
                    writer.add_scalar("timestep/window/mean_t", mean_t, global_step)
                except Exception:
                    pass

                # Band ratios using configured edges, e.g., "850,900,950"
                try:
                    bands_raw = str(
                        getattr(args, "log_timestep_distribution_bands", "")
                    ).strip()
                    if bands_raw:
                        edges = [
                            int(x) for x in bands_raw.split(",") if str(x).strip() != ""
                        ]
                        # Allow 0 as a lower-bound sentinel; indices are clamped to [1,1000]
                        edges = sorted([e for e in edges if 0 <= e <= 1000])
                        if len(edges) >= 2:
                            prev = edges[0]
                            for e in edges[1:]:
                                mask = (agg >= prev) & (agg < e)
                                ratio = float(mask.float().mean().item())
                                writer.add_scalar(
                                    f"timestep/window/ratio_{prev}_{e}",
                                    ratio,
                                    global_step,
                                )
                                prev = e
                            # Tail bucket [prev, 1000]
                            mask = agg >= prev
                            ratio = float(mask.float().mean().item())
                            writer.add_scalar(
                                f"timestep/window/ratio_{prev}_1000", ratio, global_step
                            )
                except Exception:
                    pass

                # KL divergence to expected distribution simulated under current config
                try:
                    counts_w = _torch.zeros(1000, dtype=_torch.float32)
                    idx = agg.long().clamp_(1, 1000) - 1
                    counts_w.index_add_(
                        0, idx, _torch.ones_like(agg, dtype=_torch.float32)
                    )
                    p = counts_w / counts_w.sum().clamp_min(1.0)

                    samples = int(
                        getattr(args, "log_timestep_distribution_samples", 20000)
                    )
                    from scheduling.timestep_utils import (
                        _generate_timesteps_from_distribution as _gen,
                        _apply_timestep_constraints as _apply,
                    )

                    t_exp = _gen(args, samples, accelerator.device)
                    t_exp = _apply(t_exp, args, samples, accelerator.device)
                    idx_exp = (
                        (_torch.round(t_exp * 1000.0 + 1.0)).clamp_(1, 1000).long()
                    )
                    counts_e = _torch.zeros(1000, dtype=_torch.float32)
                    counts_e.index_add_(
                        0, idx_exp - 1, _torch.ones_like(idx_exp, dtype=_torch.float32)
                    )
                    q = counts_e / counts_e.sum().clamp_min(1.0)

                    eps = 1e-8
                    kl = float((p * (p.add(eps).log() - q.add(eps).log())).sum().item())
                    writer.add_scalar("timestep/window/kl_to_expected", kl, global_step)
                except Exception:
                    pass

                # Optional PMF bar chart for rolling window
                try:
                    if bool(getattr(args, "log_timestep_distribution_pmf", False)):
                        import matplotlib.pyplot as plt
                        import numpy as _np

                        pmf = p
                        x = _np.arange(1, 1001, dtype=_np.int32)
                        y = pmf.numpy()
                        tmin = int(getattr(args, "min_timestep", 0))
                        tmax = int(getattr(args, "max_timestep", 1000))
                        lo = max(1, tmin)
                        hi = max(lo + 1, min(1000, tmax))

                        fig, ax = plt.subplots(figsize=(6.5, 3.0), dpi=120)
                        ax.bar(x[lo - 1 : hi], y[lo - 1 : hi], width=1.0)
                        ax.set_title("Rolling window PMF over indices")
                        ax.set_xlabel("timestep index [1..1000]")
                        ax.set_ylabel("probability")
                        fig.tight_layout()
                        writer.add_figure(
                            "timestep/used_pmf_window", fig, global_step=global_step
                        )
                        plt.close(fig)
                except Exception:
                    pass
    except Exception:
        # Never block training on logging failures
        pass

    # Chart image (normalized 0..1)
    if log_mode in ["chart", "both"]:
        try:
            import matplotlib.pyplot as plt

            ts_norm = (ts.numpy() / 1000.0).clip(0.0, 1.0)
            fig, ax = plt.subplots(figsize=(4.5, 3.0), dpi=120)
            ax.hist(ts_norm, bins=num_bins, range=(0.0, 1.0), density=True, alpha=0.8)
            ax.set_title("Live timestep distribution (used)")
            ax.set_xlabel("t in [0,1]")
            ax.set_ylabel("density")
            fig.tight_layout()
            writer.add_figure("timestep/used_chart", fig, global_step=global_step)
            plt.close(fig)
        except Exception:
            pass

        # Rolling-window chart
        try:
            import matplotlib.pyplot as plt
            import numpy as _np

            window_size = int(getattr(args, "log_timestep_distribution_window", 10000))
            if window_size > 0:
                if (
                    "_USED_INDICES_BUFFER" in globals()
                    and _USED_INDICES_BUFFER is not None
                ):
                    arr = _np.asarray(list(_USED_INDICES_BUFFER), dtype=_np.float32)
                    if arr.size > 0:
                        ts_norm_w = (arr / 1000.0).clip(0.0, 1.0)
                        fig, ax = plt.subplots(figsize=(4.5, 3.0), dpi=120)
                        ax.hist(
                            ts_norm_w,
                            bins=num_bins,
                            range=(0.0, 1.0),
                            density=True,
                            alpha=0.8,
                        )
                        ax.set_title("Live timestep distribution (window)")
                        ax.set_xlabel("t in [0,1]")
                        ax.set_ylabel("density")
                        fig.tight_layout()
                        writer.add_figure(
                            "timestep/used_chart_window", fig, global_step=global_step
                        )
                        plt.close(fig)
        except Exception:
            pass


def log_loss_scatterplot(
    accelerator,
    args,
    timesteps,
    model_pred,
    target,
    global_step: int,
) -> None:
    """Log a scatter plot of per-sample loss vs timestep to TensorBoard.

    Safely no-ops if TensorBoard is not enabled, matplotlib is missing,
    or the feature is disabled in args.
    """
    writer = _get_tensorboard_writer(accelerator)
    if writer is None:
        return

    # Gate via config flag and interval
    enabled = bool(getattr(args, "log_loss_scatterplot", False))
    if not enabled:
        return
    interval = int(getattr(args, "log_loss_scatterplot_interval", 500))
    if interval <= 0 or (global_step % interval != 0):
        return

    try:
        import torch as _torch
        import matplotlib.pyplot as _plt

        # Compute per-sample velocity MSE matching extra-train-metrics convention
        with _torch.no_grad():
            mp = model_pred.float()
            tgt = target.float()
            # Expect shapes (B, C, F, H, W). Reduce all but batch
            per_sample = _torch.nn.functional.mse_loss(mp, tgt, reduction="none").mean(
                dim=[1, 2, 3, 4]
            )

            # Ensure timesteps alignment (1D of length B)
            ts = timesteps.detach().float().view(-1)
            if per_sample.shape[0] != ts.shape[0]:
                # Best-effort alignment: take min length
                n = int(min(per_sample.shape[0], ts.shape[0]))
                per_sample = per_sample[:n]
                ts = ts[:n]

            # Move to CPU
            ts_np = ts.cpu().numpy()
            loss_np = per_sample.cpu().numpy()

        # Plot
        fig, ax = _plt.subplots(figsize=(6.5, 4.5), dpi=120)
        ax.scatter(ts_np, loss_np, alpha=0.5, s=6)
        ax.set_yscale("log")
        ax.set_xlabel("Timestep (index)")
        ax.set_ylabel("Per-sample velocity loss (log)")
        ax.set_title(f"Loss vs. Timestep (Step: {global_step})")
        ax.grid(True, which="both", ls="--", alpha=0.6)
        fig.tight_layout()

        writer.add_figure("timestep/loss_scatter", fig, global_step=global_step)
        _plt.close(fig)
    except Exception:
        # Never block training on logging failures
        return


def log_show_timesteps_figure_unconditional(
    accelerator: Any,
    args: Any,
    timestep_distribution: Any,
    noise_scheduler: Any,
) -> None:
    """Always log a two-subplot figure (sampled timesteps and loss weighting) to TensorBoard.

    Runs once per logging directory (guarded by a marker file) and does not depend on
    show_timesteps or log_timestep_distribution settings.
    """
    logdir = getattr(args, "logging_dir", None)
    if not logdir:
        return
    try:
        os.makedirs(logdir, exist_ok=True)
        marker_path = os.path.join(logdir, "timestep_show_timesteps_logged.marker")
        if os.path.exists(marker_path):
            return
    except Exception:
        marker_path = None

    writer = _get_tensorboard_writer(accelerator)
    created_writer = False
    if writer is None:
        try:
            try:
                from tensorboardX import SummaryWriter as _SummaryWriter  # type: ignore
            except ImportError:
                from torch.utils.tensorboard.writer import SummaryWriter as _SummaryWriter  # type: ignore
            writer = _SummaryWriter(log_dir=logdir)
            created_writer = True
        except Exception:
            writer = None
    if writer is None:
        return

    try:
        import torch
        import numpy as _np
        import matplotlib.pyplot as plt

        try:
            from scheduling.timestep_utils import (
                _generate_timesteps_from_distribution,
                _apply_timestep_constraints,
            )

            samples = int(getattr(args, "log_timestep_distribution_samples", 20000))
            _device_any = getattr(accelerator, "device", "cpu")
            if isinstance(_device_any, str):
                _device = torch.device(_device_any)
            else:
                try:
                    _device = (
                        _device_any
                        if isinstance(_device_any, torch.device)
                        else torch.device("cpu")
                    )
                except Exception:
                    _device = torch.device("cpu")
            t = _generate_timesteps_from_distribution(args, samples, _device)
            t = _apply_timestep_constraints(t, args, samples, _device)
        except Exception:
            t = torch.rand(int(20000))

        idx = (torch.clamp((t * 1000.0).round_(), 0, 999)).long().cpu()
        sampled_timesteps = torch.bincount(idx, minlength=1000).float().numpy()

        try:
            from utils.train_utils import compute_loss_weighting_for_sd3

            weights = []
            for i in range(1000):
                ts = torch.tensor([i + 1], device="cpu")
                w = compute_loss_weighting_for_sd3(
                    getattr(args, "weighting_scheme", "none"),
                    noise_scheduler,
                    ts,
                    "cpu",
                    torch.float16,
                )
                if w is None or (torch.isinf(w).any() or torch.isnan(w).any()):
                    weights.append(1.0)
                else:
                    weights.append(float(w.item()))
            sampled_weighting = _np.array(weights, dtype=_np.float32)
        except Exception:
            sampled_weighting = _np.ones((1000,), dtype=_np.float32)

        fig = plt.figure(figsize=(10, 5))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.bar(_np.arange(1000), sampled_timesteps, width=1.0)
        ax1.set_title("Sampled timesteps")
        ax1.set_xlabel("Timestep")
        ax1.set_ylabel("Count")

        ax2 = fig.add_subplot(1, 2, 2)
        ax2.bar(_np.arange(1000), sampled_weighting, width=1.0)
        ax2.set_title("Sampled loss weighting")
        ax2.set_xlabel("Timestep")
        ax2.set_ylabel("Weighting")
        fig.tight_layout()

        try:
            writer.add_figure("timestep/show_timesteps_chart", fig, global_step=0)
        finally:
            plt.close(fig)

        try:
            if marker_path is not None:
                with open(marker_path, "w", encoding="utf-8") as f:
                    f.write("logged\n")
        except Exception:
            pass
    finally:
        # Only close if we created a standalone writer that exposes close()
        if created_writer:
            try:
                # Close only if the dynamically created writer exposes close(); typing: ignore to satisfy protocol
                if hasattr(writer, "close"):
                    writer.close()  # type: ignore[attr-defined]
            except Exception:
                pass
</file>

<file path="scheduling/timestep_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/hv_train_network.py (Apache)

"""Timestep utilities for diffusion model training.

This module consolidates all timestep-related functionality including:
- Time shift transformations
- Linear function generation for spatial scaling
- Noisy model input generation with various sampling strategies
- Integration with pre-computed timestep distributions
"""

import argparse
import math
from typing import Callable, Optional, Tuple, Any

import torch
import numpy as np

from modules.scheduling_flow_match_discrete import FlowMatchDiscreteScheduler
from scheduling.timestep_distribution import (
    TimestepDistribution,
    should_use_precomputed_timesteps,
)
from utils.train_utils import get_sigmas, compute_density_for_timestep_sampling
from scheduling.fopp import (
    FoPPScheduler,
    get_alpha_bar_schedule,
    apply_asynchronous_noise,
)
from common.logger import get_logger

logger = get_logger(__name__)

# Guard flags to avoid spamming logs
_warned_double_constraint: bool = False


def time_shift(mu: float, sigma: float, t: torch.Tensor) -> torch.Tensor:
    """Apply a time shift transformation to the timestep tensor.

    Args:
        mu (float): Shift parameter (usually >0).
        sigma (float): Exponent parameter.
        t (torch.Tensor): Timesteps in [0, 1], shape (B,).

    Returns:
        torch.Tensor: Shifted timesteps, shape (B,).
    """
    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)


def get_lin_function(
    x1: float = 256,
    y1: float = 0.5,
    x2: float = 4096,
    y2: float = 1.15,
) -> Callable[[float], float]:
    """Return a linear function f(x) = m*x + b passing through (x1, y1) and (x2, y2).

    Args:
        x1 (float): First x-coordinate
        y1 (float): First y-coordinate
        x2 (float): Second x-coordinate
        y2 (float): Second y-coordinate

    Returns:
        Callable[[float], float]: Linear function f(x) = m*x + b
    """
    m = (y2 - y1) / (x2 - x1)
    b = y1 - m * x1
    return lambda x: m * x + b


def compute_sampled_timesteps_and_weighting(
    args: Any,
    timestep_distribution: Any,
    noise_scheduler: Any,
    num_samples: int = 100000,
    batch_size: int = 1000,
) -> tuple[list[int], list[float]]:
    """Compute sampled timestep counts (0..999) and per-index loss weighting.

    Returns:
    - sampled_timesteps: length-1000 list of counts
    - sampled_weighting: length-1000 list of weights
    """
    import torch

    # Ensure distribution is initialized once
    try:
        from scheduling.timestep_utils import initialize_timestep_distribution

        initialize_timestep_distribution(args, timestep_distribution)
    except Exception:
        pass

    BATCH_SIZE = max(1, int(batch_size))
    N_TRY = max(BATCH_SIZE, int(num_samples))

    latents = torch.zeros(BATCH_SIZE, 1, 1, 1, 1, dtype=torch.float16)
    noise = torch.ones_like(latents)

    sampled_timesteps = [0] * 1000

    try:
        from scheduling.timestep_utils import get_noisy_model_input_and_timesteps

        for _ in range(N_TRY // BATCH_SIZE):
            actual_timesteps, _, _ = get_noisy_model_input_and_timesteps(
                args,
                noise,
                latents,
                noise_scheduler,
                torch.device("cpu"),
                torch.float16,
                timestep_distribution,
            )
            actual_timesteps = actual_timesteps[:, 0, 0, 0, 0] * 1000
            for t in actual_timesteps:
                ti = int(t.item())
                if 0 <= ti < 1000:
                    sampled_timesteps[ti] += 1
    except Exception:
        # Fallback: uniform sampling if anything fails
        for i in range(1000):
            sampled_timesteps[i] = N_TRY // 1000

    # Compute per-index loss weighting
    sampled_weighting: list[float] = [1.0] * 1000
    try:
        from utils.train_utils import compute_loss_weighting_for_sd3

        for i in range(1000):
            ts = torch.tensor([i + 1], device="cpu")
            w = compute_loss_weighting_for_sd3(
                getattr(args, "weighting_scheme", "none"),
                noise_scheduler,
                ts,
                "cpu",
                torch.float16,
            )
            if w is None or (torch.isinf(w).any() or torch.isnan(w).any()):
                sampled_weighting[i] = 1.0
            else:
                sampled_weighting[i] = float(w.item())
    except Exception:
        pass

    return sampled_timesteps, sampled_weighting


def _sample_fopp_timesteps(
    args: argparse.Namespace,
    latents: torch.Tensor,
    device: torch.device,
) -> Tuple[torch.Tensor, np.ndarray]:
    """Sample FoPP AR-Diffusion timesteps for video training.

    Args:
        args: Training arguments
        latents: Input latents tensor
        device: Target device

    Returns:
        Tuple of (timesteps, alpha_bar_schedule)
    """
    batch_size = latents.shape[0]
    num_frames = latents.shape[1]
    num_timesteps = getattr(args, "fopp_num_timesteps", 1000)
    schedule_type = getattr(args, "fopp_schedule_type", "linear")
    beta_start = getattr(args, "fopp_beta_start", 0.0001)
    beta_end = getattr(args, "fopp_beta_end", 0.002)
    seed = getattr(args, "fopp_seed", None)

    # Sample per-frame timesteps for each video in the batch
    fopp_sched = FoPPScheduler(
        num_frames=num_frames,
        num_timesteps=num_timesteps,
        device=None if latents.device.type == "cpu" else latents.device,
        seed=seed,
    )
    timesteps_np = fopp_sched.sample_batch(batch_size)  # (B, F), np.int
    timesteps = torch.from_numpy(timesteps_np).to(device=device, dtype=torch.long)

    # Get alpha_bar schedule (configurable)
    alpha_bar = get_alpha_bar_schedule(
        num_timesteps,
        schedule_type=schedule_type,  # type: ignore
        beta_start=beta_start,
        beta_end=beta_end,
    )

    return timesteps, alpha_bar


def _normal_ppf(u: torch.Tensor) -> torch.Tensor:
    """Inverse CDF (percent point function) for standard normal using erfinv."""
    eps = 1e-7
    u = torch.clamp(u, eps, 1.0 - eps)
    return math.sqrt(2.0) * torch.erfinv(2.0 * u - 1.0)


def map_uniform_to_sampling(
    args: argparse.Namespace, t_uniform: torch.Tensor, latents: torch.Tensor
) -> torch.Tensor:
    """Map uniform-in-[0,1] samples to the current timestep_sampling distribution.

    This mirrors transformations in _generate_timesteps_from_distribution but uses
    inverse-CDF style mappings from provided uniform samples instead of RNG.
    """
    method = str(getattr(args, "timestep_sampling", "uniform")).lower()
    device = t_uniform.device
    t_uniform = t_uniform.clamp(0.0, 1.0)

    if method == "uniform":
        return t_uniform

    if method == "sigmoid":
        z = _normal_ppf(t_uniform)
        return torch.sigmoid(getattr(args, "sigmoid_scale", 1.0) * z)

    if method == "shift":
        z = _normal_ppf(t_uniform)
        t = torch.sigmoid(getattr(args, "sigmoid_scale", 1.0) * z)
        shift = float(getattr(args, "discrete_flow_shift", 1.0))
        return (t * shift) / (1 + (shift - 1) * t)

    if method in ("flux_shift", "qwen_shift"):
        z = _normal_ppf(t_uniform)
        t = torch.sigmoid(getattr(args, "sigmoid_scale", 1.0) * z)
        # compute base area
        if latents is not None and latents.ndim >= 4:
            h, w = latents.shape[-2:]
            base_area = float((h // 2) * (w // 2))
        else:
            base_area = 1024.0
        if method == "qwen_shift":
            m = (0.9 - 0.5) / (8192 - 256)
            b = 0.5 - m * 256
        else:
            m = (1.15 - 0.5) / (4096 - 256)
            b = 0.5 - m * 256
        mu = m * base_area + b
        shift = math.exp(mu)
        return (t * shift) / (1 + (shift - 1) * t)

    if method == "logit_normal":
        z = _normal_ppf(t_uniform)
        z = z * float(getattr(args, "sigmoid_scale", 1.0))
        return torch.sigmoid(z)

    if method == "bell_shaped":
        # Build bell density over x‚àà[0,1], invert its CDF at given uniform quantiles
        n = max(int(1e4), 2048)
        x = torch.linspace(0.0, 1.0, n, device=device)
        bell_std = float(getattr(args, "bell_std", 0.2))
        bell_center = float(getattr(args, "bell_center", 0.5))
        y = torch.exp(-0.5 * ((x - bell_center) / max(bell_std, 1e-6)) ** 2)
        y = torch.clamp(y, min=1e-12)
        cdf = torch.cumsum(y, dim=0)
        cdf = cdf / cdf[-1]
        # Invert CDF by interpolation
        idx = torch.searchsorted(cdf, t_uniform.clamp(0.0, 1.0), right=False)
        idx = idx.clamp(min=1, max=n - 1)
        cdf_lo = cdf[idx - 1]
        cdf_hi = cdf[idx]
        x_lo = x[idx - 1]
        x_hi = x[idx]
        denom = torch.clamp(cdf_hi - cdf_lo, min=1e-12)
        w = (t_uniform - cdf_lo) / denom
        return x_lo + w * (x_hi - x_lo)

    if method == "half_bell":
        n = max(int(1e4), 2048)
        x = torch.linspace(0.0, 1.0, n, device=device)
        bell_std = float(getattr(args, "bell_std", 0.2))
        y = torch.exp(-0.5 * ((x - 0.5) / max(bell_std, 1e-6)) ** 2)
        mid = n // 2
        y[mid:] = y[:mid].max()
        y = torch.clamp(y, min=1e-12)
        cdf = torch.cumsum(y, dim=0)
        cdf = cdf / cdf[-1]
        idx = torch.searchsorted(cdf, t_uniform.clamp(0.0, 1.0), right=False)
        idx = idx.clamp(min=1, max=n - 1)
        cdf_lo = cdf[idx - 1]
        cdf_hi = cdf[idx]
        x_lo = x[idx - 1]
        x_hi = x[idx]
        denom = torch.clamp(cdf_hi - cdf_lo, min=1e-12)
        w = (t_uniform - cdf_lo) / denom
        return x_lo + w * (x_hi - x_lo)

    if method == "logsnr":
        z = _normal_ppf(t_uniform)
        mean = float(getattr(args, "logit_mean", 0.0))
        std = float(getattr(args, "logit_std", 1.0))
        logsnr = mean + std * z
        return torch.sigmoid(-logsnr / 2.0)

    if method in ("qinglong_flux", "qinglong_qwen"):
        # Use mixture with thresholds on uniform sample
        # 0..0.80 -> mid_shift, 0.80..0.875 -> logsnr(mean,std), 0.875..1.0 -> logsnr2(5.36,1.0)
        u = t_uniform
        out = torch.zeros_like(u, device=device)
        # mid_shift
        mask0 = u < 0.80
        if mask0.any():
            # rescale to [0,1] within this segment to preserve shape
            u0 = (u[mask0] - 0.0) / 0.80
            z0 = _normal_ppf(u0)
            t0 = torch.sigmoid(getattr(args, "sigmoid_scale", 1.0) * z0)
            # area and mu
            if latents is not None and latents.ndim >= 4:
                h, w = latents.shape[-2:]
                base_area = float((h // 2) * (w // 2))
            else:
                base_area = 1024.0
            if method == "qinglong_qwen":
                m = (0.9 - 0.5) / (8192 - 256)
                b = 0.5 - m * 256
            else:
                m = (1.15 - 0.5) / (4096 - 256)
                b = 0.5 - m * 256
            mu = m * base_area + b
            shift = math.exp(mu)
            out[mask0] = (t0 * shift) / (1 + (shift - 1) * t0)
        # logsnr
        mask1 = (u >= 0.80) & (u < 0.875)
        if mask1.any():
            u1 = (u[mask1] - 0.80) / (0.075)
            z1 = _normal_ppf(u1)
            mean = float(getattr(args, "logit_mean", 0.0))
            std = float(getattr(args, "logit_std", 1.0))
            logsnr = mean + std * z1
            out[mask1] = torch.sigmoid(-logsnr / 2.0)
        # logsnr2 fixed mean
        mask2 = u >= 0.875
        if mask2.any():
            u2 = (u[mask2] - 0.875) / (0.125)
            z2 = _normal_ppf(u2)
            logsnr2 = 5.36 + 1.0 * z2
            out[mask2] = torch.sigmoid(-logsnr2 / 2.0)
        return out

    # Default fallback: return uniform
    return t_uniform


def _generate_timesteps_from_distribution(
    args: argparse.Namespace,
    batch_size: int,
    device: torch.device,
    latents: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """Generate timesteps using the specified distribution method.

    This is a shared method used by both _sample_standard_timesteps and
    _apply_timestep_constraints to avoid code duplication.

    Args:
        args: Training arguments
        batch_size: Number of samples to generate
        device: Target device
        latents: Optional latents tensor for flux_shift spatial calculations

    Returns:
        Timesteps tensor in [0, 1] range
    """
    if args.timestep_sampling == "uniform":
        t = torch.rand((batch_size,), device=device)

    elif args.timestep_sampling == "sigmoid":
        t = torch.sigmoid(
            args.sigmoid_scale * torch.randn((batch_size,), device=device)
        )

    elif args.timestep_sampling == "shift":
        shift = args.discrete_flow_shift
        logits_norm = torch.randn(batch_size, device=device)
        logits_norm = (
            logits_norm * args.sigmoid_scale
        )  # larger scale for more uniform sampling
        t = logits_norm.sigmoid()
        t = (t * shift) / (1 + (shift - 1) * t)

    elif args.timestep_sampling == "flux_shift":
        # https://github.com/kohya-ss/sd-scripts/pull/1541
        logits_norm = torch.randn(batch_size, device=device)
        logits_norm = logits_norm * args.sigmoid_scale
        t = logits_norm.sigmoid()

        # Compute mu as a function of spatial size (matching upstream implementation)
        if latents is not None:
            h, w = latents.shape[-2:] if latents.ndim >= 4 else (1, 1)
            # we are pre-packed so must adjust for packed size
            mu = get_lin_function(y1=0.5, y2=1.15)((h // 2) * (w // 2))
            # def time_shift(mu: float, sigma: float, t: torch.Tensor):
            #     return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma) # sigma=1.0
            shift = math.exp(mu)
            t = (t * shift) / (1 + (shift - 1) * t)
        else:
            # Fall back to simple sigmoid if latents not available
            logger.warning("flux_shift without latents, using simple sigmoid")

    elif args.timestep_sampling == "qwen_shift":
        # Qwen shift uses a different linear mapping for mu than flux
        logits_norm = torch.randn(batch_size, device=device)
        logits_norm = logits_norm * args.sigmoid_scale
        t = logits_norm.sigmoid()

        if latents is not None:
            h, w = latents.shape[-2:] if latents.ndim >= 4 else (1, 1)
            # Use upstream qwen mapping: (x1=256, y1=0.5) -> (x2=8192, y2=0.9)
            mu = get_lin_function(x1=256, y1=0.5, x2=8192, y2=0.9)((h // 2) * (w // 2))
            shift = math.exp(mu)
            t = (t * shift) / (1 + (shift - 1) * t)
        else:
            logger.warning("qwen_shift without latents, using simple sigmoid")

    elif args.timestep_sampling == "logit_normal":
        # Use logit-normal distribution
        dist = torch.distributions.normal.Normal(0, 1)
        t = dist.sample((batch_size,)).to(device)

        # Apply sigmoid scaling
        sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
        t = t * sigmoid_scale
        t = torch.sigmoid(t)

    elif args.timestep_sampling == "bell_shaped":
        # Bell-shaped distribution centered at `bell_center` with spread `bell_std`.
        # This focuses sampling around a specific region (e.g., near 0.95 ‚Üí timestep ~950).
        x = torch.rand(batch_size, device=device)
        bell_std = float(getattr(args, "bell_std", 0.2))
        bell_center = float(getattr(args, "bell_center", 0.5))
        # Gaussian-shaped bump around center
        y = torch.exp(-0.5 * ((x - bell_center) / max(bell_std, 1e-6)) ** 2)
        # Normalize to [0, 1]
        y_shifted = y - y.min()
        t = y_shifted / y_shifted.max()

    elif args.timestep_sampling == "half_bell":
        # Half Bell-Shaped (HBSMNTW) - bell curve for first half, flat for second half
        x = torch.rand(batch_size, device=device)
        bell_std = getattr(args, "bell_std", 0.2)
        y = torch.exp(-0.5 * ((x - 0.5) / bell_std) ** 2)
        y_shifted = y - y.min()

        # Flatten second half to max value
        mid_point = batch_size // 2
        y_shifted[mid_point:] = y_shifted[:mid_point].max()

        t = y_shifted / y_shifted.max()

    elif args.timestep_sampling == "lognorm_blend":
        # LogNormal Blend - combines lognormal distribution with linear sampling
        alpha = getattr(args, "lognorm_blend_alpha", 0.75)

        # Determine how many samples to use for each distribution
        t1_size = int(batch_size * alpha)
        t2_size = batch_size - t1_size

        if t1_size > 0:
            # LogNormal distribution for first portion
            lognormal = torch.distributions.LogNormal(loc=0, scale=0.333)
            t1 = lognormal.sample((t1_size,)).to(device)  # type: ignore
            t1_max = t1.max()
            if t1_max > 0:
                t1 = 1 - t1 / t1_max  # Scale to [0, 1]
            else:
                t1 = torch.zeros_like(t1)
        else:
            t1 = torch.empty(0, device=device)

        if t2_size > 0:
            # Linear distribution for remaining portion
            t2 = torch.rand(t2_size, device=device)
        else:
            t2 = torch.empty(0, device=device)

        # Combine and sort
        t = torch.cat([t1, t2])
        if len(t) > 0:
            t, _ = torch.sort(t)

    elif args.timestep_sampling == "enhanced_sigmoid":
        # Enhanced sigmoid with additional parameters
        sigmoid_scale = getattr(args, "sigmoid_scale", 1.0)
        sigmoid_bias = getattr(args, "sigmoid_bias", 0.0)
        t = torch.randn(batch_size, device=device)
        t = torch.sigmoid((t + sigmoid_bias) * sigmoid_scale)

    # https://github.com/kohya-ss/musubi-tuner/pull/407
    elif args.timestep_sampling == "logsnr":
        # https://arxiv.org/abs/2411.14793v3
        logsnr = torch.normal(
            mean=args.logit_mean, std=args.logit_std, size=(batch_size,), device=device
        )
        t = torch.sigmoid(-logsnr / 2)

    elif args.timestep_sampling in ("qinglong_flux", "qinglong_qwen"):
        # Qinglong triple hybrid sampling: mid_shift:logsnr:logsnr2 = .80:.075:.125
        # First decide which method to use for each sample independently
        decision_t = torch.rand((batch_size,), device=device)

        # Create masks based on decision_t: .80 for mid_shift, 0.075 for logsnr, and 0.125 for logsnr2
        flux_mask = decision_t < 0.80  # 80% for mid_shift (flux or qwen variant)
        logsnr_mask = (decision_t >= 0.80) & (decision_t < 0.875)  # 7.5% for logsnr
        logsnr_mask2 = decision_t >= 0.875  # 12.5% for logsnr with -logit_mean

        # Initialize output tensor
        t = torch.zeros((batch_size,), device=device)

        # Generate mid_shift samples for selected indices (80%)
        if flux_mask.any():
            flux_count = int(flux_mask.sum().item())
            h, w = latents.shape[-2:] if latents is not None else (1, 1)
            # Choose mu mapping: flux variant uses flux mapping; qwen variant uses alternate mapping
            if args.timestep_sampling == "qinglong_qwen":
                mu = get_lin_function(x1=256, y1=0.5, x2=8192, y2=0.9)(
                    (h // 2) * (w // 2)
                )
            else:
                # "qinglong_flux" uses flux mapping
                mu = get_lin_function(y1=0.5, y2=1.15)((h // 2) * (w // 2))
            shift = math.exp(mu)

            logits_norm_flux = torch.randn(flux_count, device=device)
            logits_norm_flux = logits_norm_flux * args.sigmoid_scale
            t_flux = logits_norm_flux.sigmoid()
            t_flux = (t_flux * shift) / (1 + (shift - 1) * t_flux)

            t[flux_mask] = t_flux

        # Generate logsnr samples for selected indices (7.5%)
        if logsnr_mask.any():
            logsnr_count = int(logsnr_mask.sum().item())
            logsnr = torch.normal(
                mean=args.logit_mean,
                std=args.logit_std,
                size=(logsnr_count,),
                device=device,
            )
            t_logsnr = torch.sigmoid(-logsnr / 2)

            t[logsnr_mask] = t_logsnr

        # Generate logsnr2 samples with -logit_mean for selected indices (12.5%)
        if logsnr_mask2.any():
            logsnr2_count = int(logsnr_mask2.sum().item())
            logsnr2 = torch.normal(
                mean=5.36, std=1.0, size=(logsnr2_count,), device=device
            )
            t_logsnr2 = torch.sigmoid(-logsnr2 / 2)

            t[logsnr_mask2] = t_logsnr2

    else:
        raise ValueError(f"Unknown timestep sampling method: {args.timestep_sampling}")

    return t


def _sample_standard_timesteps(
    args: argparse.Namespace,
    batch_size: int,
    device: torch.device,
    latents: torch.Tensor,
    timestep_distribution: Optional[TimestepDistribution] = None,
) -> torch.Tensor:
    """Sample standard timesteps using various sampling strategies.

    Args:
        args: Training arguments
        batch_size: Number of samples to generate
        device: Target device
        timestep_distribution: Optional pre-computed distribution

    Returns:
        Sampled timesteps tensor
    """
    # Check if we should use pre-computed timestep distribution
    if (
        should_use_precomputed_timesteps(args)
        and timestep_distribution is not None
        and timestep_distribution.is_initialized
    ):
        # Use pre-computed distribution for supported methods
        if args.timestep_sampling in [
            "uniform",
            "sigmoid",
            "shift",
            "flux_shift",
            "qwen_shift",
            "logit_normal",
            "bell_shaped",
            "half_bell",
            "lognorm_blend",
            "enhanced_sigmoid",
            "logsnr",
            "qinglong_flux",
            "qinglong_qwen",
        ]:
            t = timestep_distribution.sample(batch_size, device)
        else:
            # Fall back to original sampling for unsupported methods
            logger.debug(
                f"‚ö†Ô∏è  {args.timestep_sampling} does not support precomputed timesteps, using original sampling"
            )
            t = _generate_timesteps_from_distribution(args, batch_size, device, latents)
    else:
        # Use original sampling method
        if should_use_precomputed_timesteps(args):
            logger.debug(
                "‚ö†Ô∏è  Fallback to original sampling: precomputed distribution not initialized"
            )
        t = _generate_timesteps_from_distribution(args, batch_size, device, latents)

    return t


def _apply_timestep_constraints(
    t: torch.Tensor,
    args: argparse.Namespace,
    batch_size: int,
    device: torch.device,
    latents: Optional[torch.Tensor] = None,
    presampled_uniform: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """Apply min/max timestep constraints to the timestep tensor.

    Args:
        t: Timestep tensor in [0, 1]
        args: Training arguments containing min/max timestep settings
        batch_size: Number of samples to generate
        device: Target device
        latents: Optional latents tensor for flux_shift spatial calculations

    Returns:
        Constrained timestep tensor
    """
    t_min = args.min_timestep if args.min_timestep is not None else 0
    t_max = args.max_timestep if args.max_timestep is not None else 1000.0
    t_min /= 1000.0
    t_max /= 1000.0

    # Ensure t is a tensor before calling .view()
    if not isinstance(t, torch.Tensor):
        t = torch.tensor(t, device=device)

    # Optional: if timesteps already lie within [t_min, t_max], and user requested
    # to skip extra constraining, return as-is to avoid double scaling that
    # compresses the distribution toward the upper bound.
    if bool(getattr(args, "skip_extra_timestep_constraint", False)):
        eps = float(getattr(args, "timestep_constraint_epsilon", 1e-6))
        if torch.all(t >= (t_min - eps)) and torch.all(t <= (t_max + eps)):
            return t

    # Check if we should preserve the distribution shape
    if not getattr(args, "preserve_distribution_shape", False):
        # Simple scaling approach (original behavior)
        return t * (t_max - t_min) + t_min  # scale to [t_min, t_max], default [0, 1]
    else:
        # Rejection sampling to preserve distribution shape
        fast = bool(getattr(args, "fast_rejection_sampling", False))
        if fast:
            try:
                overdraw = float(getattr(args, "rejection_overdraw_factor", 4.0))
                max_iters = int(getattr(args, "rejection_max_iters", 10))
                available: list[torch.Tensor] = []

                # Use presampled uniforms first if available
                needed = int(batch_size)
                if presampled_uniform is not None and needed > 0:
                    mapped = map_uniform_to_sampling(
                        args,
                        presampled_uniform.to(device=device),
                        latents if latents is not None else torch.empty(0),
                    )
                    mask = (mapped >= t_min) & (mapped <= t_max)
                    selected = mapped[mask]
                    take = int(min(needed, selected.numel()))
                    if take > 0:
                        available.append(selected[:take])
                        needed -= take

                for _ in range(max_iters):
                    if needed <= 0:
                        break
                    n = max(int(math.ceil(needed * overdraw)), needed)
                    u = torch.rand((n,), device=device)
                    mapped = map_uniform_to_sampling(
                        args, u, latents if latents is not None else torch.empty(0)
                    )
                    mask = (mapped >= t_min) & (mapped <= t_max)
                    selected = mapped[mask]
                    if selected.numel() > 0:
                        take = int(min(needed, selected.numel()))
                        available.append(selected[:take])
                        needed -= take

                if needed > 0:
                    logger.warning(
                        "Preserve-shape(fast): insufficient in-range samples; falling back to linear scaling"
                    )
                    return t * (t_max - t_min) + t_min

                return torch.cat(available, dim=0)
            except Exception as e:
                logger.warning(
                    f"Fast rejection sampling failed ({e}); falling back to simple scaling"
                )
                return t * (t_max - t_min) + t_min
        else:
            max_loops = 1000
            available_t = []

            for i in range(max_loops):
                # Generate candidates:
                # - On first loop, if presampled_uniform provided, map those to the distribution
                # - Otherwise, generate fresh uniform and map
                try:
                    if i == 0 and presampled_uniform is not None:
                        new_t = map_uniform_to_sampling(
                            args,
                            presampled_uniform.to(device=device),
                            latents if latents is not None else torch.empty(0),
                        )
                    else:
                        u = torch.rand((batch_size,), device=device)
                        new_t = map_uniform_to_sampling(
                            args, u, latents if latents is not None else torch.empty(0)
                        )
                except Exception as e:
                    logger.warning(
                        f"Error mapping timesteps: {e}, falling back to simple scaling"
                    )
                    return t * (t_max - t_min) + t_min

                # Check which timesteps fall within bounds
                for t_i in new_t:
                    if t_min <= (t_i.item()) <= t_max:
                        available_t.append(t_i)
                    if len(available_t) == batch_size:
                        break
                if len(available_t) == batch_size:
                    break

            if len(available_t) < batch_size:
                logger.warning(
                    f"Could not sample {batch_size} valid timesteps in {max_loops} loops / {max_loops}„É´„Éº„Éó„Åß{batch_size}ÂÄã„ÅÆÊúâÂäπ„Å™„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü"
                )
                # Fall back to original timesteps with simple scaling
                return t * (t_max - t_min) + t_min
            else:
                return torch.stack(available_t, dim=0)  # [batch_size, ]


def _sample_sigma_timesteps(
    args: argparse.Namespace,
    batch_size: int,
    device: torch.device,
    noise_scheduler: FlowMatchDiscreteScheduler,
) -> torch.Tensor:
    """Sample timesteps for sigma-based weighting schemes.

    Args:
        args: Training arguments
        batch_size: Number of samples to generate
        device: Target device
        noise_scheduler: Noise scheduler for sigma computation

    Returns:
        Tuple of (timesteps, sigmas)
    """
    # Sample a random timestep for each image
    # for weighting schemes where we sample timesteps non-uniformly
    u = compute_density_for_timestep_sampling(
        weighting_scheme=args.weighting_scheme,
        batch_size=batch_size,
        logit_mean=args.logit_mean,
        logit_std=args.logit_std,
        mode_scale=args.mode_scale,
    )

    t_min = args.min_timestep if args.min_timestep is not None else 0
    t_max = args.max_timestep if args.max_timestep is not None else 1000
    indices = (u * (t_max - t_min) + t_min).long()

    timesteps = noise_scheduler.timesteps[indices].to(device=device)  # 1 to 1000

    # Ensure timesteps is a tensor
    if not isinstance(timesteps, torch.Tensor):
        timesteps = torch.tensor(timesteps, device=device)

    return timesteps


def get_noisy_model_input_and_timesteps(
    args: argparse.Namespace,
    noise: torch.Tensor,
    latents: torch.Tensor,
    noise_scheduler: FlowMatchDiscreteScheduler,
    device: torch.device,
    dtype: torch.dtype,
    timestep_distribution: Optional[TimestepDistribution] = None,
    presampled_uniform: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
    """Generate noisy model input and timesteps for training.

    This function handles various timestep sampling strategies including:
    - FoPP AR-Diffusion for video training
    - Standard sampling methods (uniform, sigmoid, shift, flux_shift)
    - Sigma-based weighting schemes
    - Pre-computed timestep distributions

    Args:
        args: Training arguments
        noise: Noise tensor
        latents: Input latents tensor
        noise_scheduler: Noise scheduler for sigma computation
        device: Target device
        dtype: Data type for computations
        timestep_distribution: Optional pre-computed timestep distribution

    Returns:
        Tuple of (noisy_model_input, timesteps, sigmas)
    """
    batch_size = noise.shape[0]

    # FoPP AR-Diffusion branch
    if getattr(args, "timestep_sampling", None) == "fopp":
        # Log if precomputed was requested but this method doesn't support it
        if should_use_precomputed_timesteps(args):
            logger.debug(
                "‚ö†Ô∏è  FoPP AR-Diffusion does not support precomputed timesteps (complex AR logic)"
            )

        # --- FoPP AR-Diffusion: asynchronous, non-decreasing per-frame timesteps ---
        assert latents.ndim >= 3, "Latents must be at least (B, F, ...) for FoPP."

        timesteps, alpha_bar = _sample_fopp_timesteps(args, latents, device)
        noisy_model_input = apply_asynchronous_noise(
            latents, timesteps, noise, alpha_bar
        )
        return noisy_model_input, timesteps, None

    else:
        sigmas = None

        if (
            args.timestep_sampling == "uniform"
            or args.timestep_sampling == "sigmoid"
            or args.timestep_sampling == "shift"
            or args.timestep_sampling == "flux_shift"
            or args.timestep_sampling == "qwen_shift"
            or args.timestep_sampling == "logit_normal"
            or args.timestep_sampling == "logsnr"
            or args.timestep_sampling == "qinglong_flux"
            or args.timestep_sampling == "qinglong_qwen"
        ):
            # Sample timesteps using standard methods
            if presampled_uniform is not None and not should_use_precomputed_timesteps(
                args
            ):
                # Map uniform [0,1] to selected sampling distribution
                t = map_uniform_to_sampling(
                    args, presampled_uniform.to(device=device), latents
                )
            else:
                t = _sample_standard_timesteps(
                    args, batch_size, device, latents, timestep_distribution
                )

            # Apply timestep constraints
            # Warn once if configuration may cause unintended compression when using precomputed timesteps
            try:
                if (
                    should_use_precomputed_timesteps(args)
                    and not bool(getattr(args, "preserve_distribution_shape", False))
                    and not bool(getattr(args, "skip_extra_timestep_constraint", False))
                ):
                    t_min = (getattr(args, "min_timestep", 0) or 0) / 1000.0
                    t_max = (getattr(args, "max_timestep", 1000) or 1000) / 1000.0
                    if t.numel() > 0 and t.min() >= t_min and t.max() <= t_max:
                        global _warned_double_constraint
                        if not _warned_double_constraint:
                            logger.warning(
                                "‚ö†Ô∏è Timesteps: precomputed distribution + linear constraint without skip flag will compress the range. Consider skip_extra_timestep_constraint=true or preserve_distribution_shape=true (or disable precomputed)."
                            )
                            _warned_double_constraint = True
            except Exception:
                pass

            t = _apply_timestep_constraints(
                t, args, batch_size, device, latents, presampled_uniform
            )

            # Convert to timestep indices and create noisy input
            timesteps = t * 1000.0
            t = t.view(-1, 1, 1, 1, 1) if latents.ndim == 5 else t.view(-1, 1, 1, 1)
            noisy_model_input = (1 - t) * latents + t * noise

            timesteps = timesteps + 1  # 1 to 1000
            # Optional: round training timesteps to nearest integer grid
            if getattr(args, "round_training_timesteps", False):
                try:
                    max_ts = int(
                        getattr(
                            getattr(noise_scheduler, "config", object()),
                            "num_train_timesteps",
                            1000,
                        )
                    )
                except Exception:
                    max_ts = 1000
                timesteps = timesteps.round().clamp_(1, max_ts)
            # Ensure timesteps is a tensor
            if not isinstance(timesteps, torch.Tensor):
                timesteps = torch.tensor(timesteps, device=device)

        else:  # sigma
            # Sample timesteps for sigma-based weighting schemes
            timesteps = _sample_sigma_timesteps(
                args, batch_size, device, noise_scheduler
            )

            # Add noise according to flow matching.
            sigmas = get_sigmas(
                noise_scheduler,
                timesteps,
                device,
                n_dim=latents.ndim,
                dtype=dtype,
                source="training/sigma-path",
            )
            noisy_model_input = sigmas * noise + (1.0 - sigmas) * latents

    # Ensure return types are correct
    assert isinstance(
        noisy_model_input, torch.Tensor
    ), "noisy_model_input must be a torch.Tensor"
    assert isinstance(timesteps, torch.Tensor), "timesteps must be a torch.Tensor"

    return noisy_model_input, timesteps, sigmas


def initialize_timestep_distribution(
    args: argparse.Namespace,
    timestep_distribution: TimestepDistribution,
) -> None:
    """Initialize pre-computed timestep distribution if enabled.

    Args:
        args: Training arguments
        timestep_distribution: TimestepDistribution instance to initialize
    """
    if (
        should_use_precomputed_timesteps(args)
        and not timestep_distribution.is_initialized
    ):
        timestep_distribution.initialize(args)
        # One-time usage message after initialization
        if (
            hasattr(timestep_distribution, "usage_logged")
            and not timestep_distribution.usage_logged
        ):
            stats = timestep_distribution.get_stats()
            logger.debug(
                f"üöÄ Using pre-computed timestep distribution: {stats['num_buckets']:,} buckets"
            )
            timestep_distribution.usage_logged = True
    elif should_use_precomputed_timesteps(args):
        # Only log once before the first step
        if (
            hasattr(timestep_distribution, "usage_logged")
            and not timestep_distribution.usage_logged
        ):
            stats = timestep_distribution.get_stats()
            logger.debug(
                f"üöÄ Using pre-computed timestep distribution: {stats['num_buckets']:,} buckets"
            )
            timestep_distribution.usage_logged = True
</file>

<file path="self_correction/manager.py">
from __future__ import annotations

import os
import random
from typing import Any, Dict, List, Optional

import torch
from accelerate import Accelerator

from common.logger import get_logger
from core.sampling_manager import SamplingManager
from dataset import config_utils


logger = get_logger(__name__)


class SelfCorrectionManager:
    """Generates lightweight correction clips and refreshes a small on-disk cache.

    Non-invasive draft implementation:
    - Generates short videos via SamplingManager using prompts from either config or dataset captions
    - Writes captions as .txt and lets the existing caching pipeline handle latents/TE
    - Keeps only up to `self_correction_cache_size` clips (FIFO style)
    """

    def __init__(
        self,
        args: Any,
        accelerator: Accelerator,
        sampling_manager: SamplingManager,
        blueprint: config_utils.Blueprint,
        vae_dtype: torch.dtype,
    ) -> None:
        self.args = args
        self.accelerator = accelerator
        self.sampling_manager = sampling_manager
        self.blueprint = blueprint
        self.vae_dtype = vae_dtype

        # Config keys (all prefixed with self_correction_)
        self.enabled: bool = bool(getattr(args, "self_correction_enabled", False))
        self.cache_dir: str = os.path.join(args.output_dir, "self_correction_cache")
        self.cache_size: int = int(getattr(args, "self_correction_cache_size", 200))
        self.clip_len: int = int(getattr(args, "self_correction_clip_len", 32))
        self.batch_ratio: float = float(
            getattr(args, "self_correction_batch_ratio", 0.2)
        )
        self.sample_steps: int = int(getattr(args, "self_correction_sample_steps", 16))
        self.sample_width: int = int(getattr(args, "self_correction_width", 256))
        self.sample_height: int = int(getattr(args, "self_correction_height", 256))
        self.guidance_scale: float = float(
            getattr(args, "self_correction_guidance_scale", 5.0)
        )

        os.makedirs(self.cache_dir, exist_ok=True)

    def _sample_prompts_from_dataset(self, max_count: int) -> List[Dict[str, Any]]:
        # Use the train dataset captions inferred from the blueprint and config utils
        train_group = config_utils.generate_dataset_group_by_blueprint(
            self.blueprint.train_dataset_group, training=False
        )
        prompts: List[Dict[str, Any]] = []
        try:
            # Access per-dataset datasource captions via retrieve_text_encoder_output_cache_batches path
            # Fallback: random empty strings when captions are unavailable
            num_workers = 0
            for dataset in train_group.datasets:  # type: ignore[attr-defined]
                collected = 0
                try:
                    for _, batch in dataset.retrieve_text_encoder_output_cache_batches(
                        num_workers
                    ):
                        # Guard typing in case linter infers wrong type
                        try:
                            iterable_batch = list(batch)  # type: ignore
                        except Exception:
                            iterable_batch = []
                        for item in iterable_batch:
                            # batch items are ItemInfo
                            try:
                                text = getattr(item, "caption", "") or ""
                            except Exception:
                                text = ""
                            prompts.append({"text": text, "enum": len(prompts)})
                            collected += 1
                            if len(prompts) >= max_count:
                                return prompts
                        if collected >= max_count:
                            break
                except Exception:
                    continue
        except Exception:
            pass
        # Pad with empty prompts if dataset captions were not available
        while len(prompts) < max_count:
            prompts.append({"text": "", "enum": len(prompts)})
        return prompts

    def _build_sampling_parameter(
        self, prompt_text: str, enum_idx: int
    ) -> Dict[str, Any]:
        return {
            "prompt": prompt_text,
            "height": self.sample_height,
            "width": self.sample_width,
            "frame_count": self.clip_len,
            "sample_steps": self.sample_steps,
            "guidance_scale": self.guidance_scale,
            "enum": enum_idx,
        }

    def _gather_prompts(self, count: int) -> List[Dict[str, Any]]:
        # Prefer inline prompts from args if provided
        inline_prompts = getattr(self.args, "self_correction_prompts", None)
        if isinstance(inline_prompts, list) and len(inline_prompts) > 0:
            return inline_prompts[:count]

        return self._sample_prompts_from_dataset(count)

    @torch.no_grad()
    def update_cache(self, transformer: Any) -> None:
        """Regenerate a small batch of correction clips and prune old ones.

        - Uses SamplingManager.do_inference through sample_images path for consistency
        - Saves .mp4 via existing helpers in SamplingManager
        - Triggers latent/text-encoder caching over the new folder
        """
        if not self.enabled:
            return

        logger.info("Updating self-correction cache ...")

        # 1) Prepare prompts
        count = max(1, int(self.cache_size))
        prompts = self._gather_prompts(count)

        # 2) Convert prompts to sampling parameter dicts (with embeds pre-encoded by SamplingManager)
        # Reuse SamplingManager.process_sample_prompts to get T5 embeddings for prompts
        prompt_dicts = [
            {"text": p.get("text", ""), "enum": p.get("enum", i)}
            for i, p in enumerate(prompts)
        ]
        sample_params = self.sampling_manager.process_sample_prompts(
            self.args, self.accelerator, prompt_dicts
        )
        if not sample_params:
            logger.warning(
                "No sample parameters produced; skipping self-correction update"
            )
            return

        # 3) Ensure VAE can be loaded lazily by SamplingManager
        vae_config = {
            "args": self.args,
            "vae_dtype": self.vae_dtype,
            "vae_path": getattr(self.args, "vae", None),
        }
        self.sampling_manager.set_vae_config(vae_config)

        # 4) Generate videos into cache_dir (reusing sample_images logic but overriding save_dir)
        save_root = self.cache_dir
        os.makedirs(save_root, exist_ok=True)

        device_dtype = (
            torch.bfloat16
            if str(getattr(self.args, "dit_dtype", "bf16")) in ("bfloat16", "bf16")
            else torch.float16
        )

        # Temporarily override output_dir to redirect saves into cache_dir
        old_output_dir = self.args.output_dir
        try:
            self.args.output_dir = save_root
            # Perform generation
            # Provide VAE via SamplingManager lazy path by passing None
            self.sampling_manager.sample_images(
                self.accelerator,
                self.args,
                epoch=None,
                steps=0,
                vae=None,  # type: ignore[arg-type]
                transformer=transformer,
                sample_parameters=sample_params,
                dit_dtype=device_dtype,
            )
        finally:
            self.args.output_dir = old_output_dir

        # 5) Prune oldest files to maintain cache_size (count .mp4 only)
        try:
            videos = [
                os.path.join(save_root, f)
                for f in os.listdir(save_root)
                if f.lower().endswith(".mp4")
            ]
            if len(videos) > self.cache_size:
                videos.sort(key=lambda p: os.path.getmtime(p))
                for path in videos[: len(videos) - self.cache_size]:
                    try:
                        os.remove(path)
                    except Exception:
                        pass
        except Exception:
            pass

        logger.info("Self-correction cache update complete")
</file>

<file path="self_correction/setup.py">
from __future__ import annotations

import os
from typing import Any

from common.logger import get_logger
from dataset import config_utils

logger = get_logger(__name__)


def maybe_wrap_with_self_correction(
    args: Any,
    blueprint_generator: Any,
    user_config: dict,
    model_config: Any,
    train_dataset_group: Any,
):
    """Optionally wrap the main dataset group with HybridDatasetGroup mixing a correction cache dataset.

    Returns the (possibly wrapped) dataset group.
    """
    try:
        from dataset.hybrid_group import HybridDatasetGroup  # type: ignore

        use_self_correction = bool(getattr(args, "self_correction_enabled", False))
        if not use_self_correction:
            return train_dataset_group

        correction_dir = os.path.join(args.output_dir, "self_correction_cache")
        if not os.path.exists(correction_dir):
            logger.info(
                "Self-correction enabled but no cache directory exists yet; training with main dataset only"
            )
            return train_dataset_group

        # Defaults if config doesn't expose fields
        default_w, default_h, default_frames = 512, 512, 17
        try:
            if hasattr(model_config, "train_img_size"):
                default_w = int(model_config.train_img_size[0])  # type: ignore
                default_h = int(model_config.train_img_size[1])  # type: ignore
            if hasattr(model_config, "train_img_frames"):
                default_frames = int(model_config.train_img_frames)  # type: ignore
        except Exception:
            pass

        correction_cfg = {
            "datasets": {
                "train": [
                    {
                        "video_directory": correction_dir,
                        "resolution": [default_w, default_h],
                        "batch_size": 1,
                        "num_repeats": 1,
                        "enable_bucket": False,
                        "bucket_no_upscale": False,
                        "target_frames": [default_frames],
                    }
                ]
            }
        }
        corr_blueprint = blueprint_generator.generate(correction_cfg, args)
        correction_group = config_utils.generate_dataset_group_by_blueprint(
            corr_blueprint.train_dataset_group,
            training=True,
            enable_control_lora=getattr(args, "enable_control_lora", False),
            prior_loss_weight=getattr(args, "prior_loss_weight", 1.0),
        )
        ratio = float(getattr(args, "self_correction_batch_ratio", 0.2))
        wrapped = HybridDatasetGroup(train_dataset_group, correction_group, ratio)
        logger.info(
            f"Self-correction enabled: mixing correction dataset at ratio={ratio}"
        )
        return wrapped
    except Exception as _sc_wrap_err:  # noqa: BLE001
        logger.warning(f"Self-correction hybrid setup skipped: {_sc_wrap_err}")
        return train_dataset_group


def maybe_attach_self_correction_manager(
    args: Any,
    accelerator: Any,
    sampling_manager: Any,
    blueprint: Any,
    vae_dtype: Any,
    transformer: Any,
):
    """Attach a SelfCorrectionManager instance to accelerator/model for periodic refresh, if enabled."""
    if not bool(getattr(args, "self_correction_enabled", False)):
        return
    if sampling_manager is None:
        return

    from self_correction.manager import SelfCorrectionManager

    sc_manager = SelfCorrectionManager(
        args=args,
        accelerator=accelerator,
        sampling_manager=sampling_manager,
        blueprint=blueprint,
        vae_dtype=vae_dtype,
    )
    try:
        setattr(accelerator.state, "_self_correction_manager", sc_manager)
    except Exception:
        pass
    try:
        setattr(transformer, "_self_correction_manager", sc_manager)
    except Exception:
        pass
</file>

<file path="takenoko.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan_train_network.py (Apache)

import argparse
import logging
import traceback

from dataset import config_utils
from wan.modules.vae import WanVAE
from utils.model_utils import str_to_dtype
from common.model_downloader import download_model_if_needed
import torch
import gc
from caching.cache_latents import (
    encode_and_save_batch,
    encode_datasets,
    show_datasets,
)

from caching.cache_text_encoder_outputs import (
    encode_and_save_text_encoder_output_batch,
    process_text_encoder_batches,
    post_process_cache_files,
    prepare_cache_files_and_paths,
)
from wan.modules.t5 import T5EncoderModel
from wan.configs import wan_t2v_14B


from core.wan_network_trainer import WanNetworkTrainer
from common.logger import get_logger
from common.performance_logger import (
    snapshot_gpu_memory,
    force_cuda_cleanup,
)
from common.global_seed import set_global_seed

logger = get_logger(__name__, level=logging.INFO)

from core.config import TrainerConfig
from common.dependencies import (
    setup_flash_attention,
    setup_sageattention,
    setup_xformers,
)
import toml
import os

import sys
import subprocess
import atexit
import time
from typing import Dict, Any, Optional, Tuple, List

from dataset.config_utils import (
    BlueprintGenerator,
    ConfigSanitizer,
    validate_dataset_config,
)

import accelerate
from utils.memory_utils import configure_cuda_allocator_from_config
from common.vram_estimator import (
    estimate_peak_vram_gb_from_config as shared_estimate_vram,
)


def load_training_config(config_path: str) -> Tuple[Dict[str, Any], str]:
    """Load training configuration from TOML file"""
    with open(config_path, "r", encoding="utf-8") as f:
        config_content = f.read()

    config = toml.loads(config_content)
    return config, config_content


def create_args_from_config(
    config: Dict[str, Any], config_path: str = None, config_content: str = None  # type: ignore
) -> argparse.Namespace:
    """Convert config dictionary to argparse.Namespace for compatibility"""
    # Removed pprint import - not needed

    args = argparse.Namespace()

    # Store original config information for state saving
    args.config_file = config_path
    args.config_content = config_content

    # Set default values for all possible arguments
    # Model settings
    args.task = config.get("task", "t2v-14B")

    # Map target_model to appropriate task if specified
    target_model = config.get("target_model", "wan21")  # for backwards compatibility
    args.target_model = (
        target_model  # Store target_model in args for dataset configuration
    )
    if target_model:
        target_model_mapping = {
            "wan21": "t2v-14B",
            "wan22": "t2v-A14B",
        }
        if target_model in target_model_mapping:
            args.task = target_model_mapping[target_model]
            logger.info(
                f"üìã Mapped target_model '{target_model}' to task '{args.task}'"
            )
        else:
            logger.warning(
                f"‚ö†Ô∏è  Unknown target_model '{target_model}'. Using task '{args.task}'"
            )
    else:
        logger.info(f"üìã Using task '{args.task}' (no target_model specified)")

    args.fp8_scaled = config.get("fp8_scaled", False)
    args.fp8_base = config.get("fp8_base", False)
    args.t5 = config.get("t5")
    args.fp8_t5 = config.get("fp8_t5", False)
    args.clip = config.get("clip")
    args.vae_cache_cpu = config.get("vae_cache_cpu", False)
    args.dit = config.get("dit")
    args.vae = config.get("vae")
    args.vae_dtype = config.get("vae_dtype")
    args.model_cache_dir = config.get("model_cache_dir")
    args.fp16_accumulation = config.get("fp16_accumulation", False)
    # Memory tracing (optional)
    args.trace_memory = config.get("trace_memory", False)

    # TREAD configuration (optional)
    # 1) Native TOML tables: tread_config.routes = [ {selection_ratio=..., start_layer_idx=..., end_layer_idx=...}, ... ]
    # 2) Shorthand strings: tread_config_route1 = "selection_ratio=0.2; start_layer_idx=2; end_layer_idx=-2"
    # Enable flag gates activation
    args.enable_tread = config.get("enable_tread", False)
    args.tread_mode = config.get("tread_mode", "full")  # "full" or "ffn"

    def _parse_route_kv_string(s: str) -> Dict[str, Any]:
        route: Dict[str, Any] = {}
        for part in s.split(";"):
            part = part.strip()
            if not part:
                continue
            if "=" not in part:
                continue
            k, v = [t.strip() for t in part.split("=", 1)]
            # try to cast to int, then float, else keep string
            try:
                if v.lower() == "true":
                    route[k] = True
                elif v.lower() == "false":
                    route[k] = False
                elif v.startswith("-") and v[1:].isdigit() or v.isdigit():
                    route[k] = int(v)
                else:
                    route[k] = float(v)
            except Exception:
                route[k] = v
        return route

    routes: list[Dict[str, Any]] = []
    # Collect from native TOML if present
    if isinstance(config.get("tread_config"), dict):
        routes.extend(config["tread_config"].get("routes", []) or [])
    # Collect shorthand: any top-level key like tread_config_routeX
    for key, val in list(config.items()):
        if isinstance(key, str) and key.lower().startswith("tread_config_route"):
            if isinstance(val, str):
                route = _parse_route_kv_string(val)
                if route:
                    routes.append(route)
    # Normalize: only set when enabled and we have routes
    args.tread_config = (
        {"routes": routes} if args.enable_tread and len(routes) > 0 else None
    )

    # Dataset config - set to the same as config file since it's included in main config
    args.dataset_config = config_path

    # Training settings
    args.max_train_steps = config.get("max_train_steps", 1600)
    args.prior_loss_weight = config.get("prior_loss_weight", 1.0)

    # START OF DOP ADDITION
    args.diff_output_preservation = config.get("diff_output_preservation", False)
    args.diff_output_preservation_trigger_word = config.get(
        "diff_output_preservation_trigger_word"
    )
    args.diff_output_preservation_class = config.get("diff_output_preservation_class")
    args.diff_output_preservation_multiplier = config.get(
        "diff_output_preservation_multiplier", 1.0
    )
    # END OF DOP ADDITION

    args.max_train_epochs = config.get("max_train_epochs")
    args.max_data_loader_n_workers = config.get("max_data_loader_n_workers", 8)
    args.persistent_data_loader_workers = config.get(
        "persistent_data_loader_workers", False
    )
    args.seed = config.get("seed", 42)
    args.gradient_checkpointing = config.get("gradient_checkpointing", False)
    args.gradient_accumulation_steps = config.get("gradient_accumulation_steps", 1)
    args.mixed_precision = config.get("mixed_precision", "no")

    # Optimizer settings
    args.optimizer_type = config.get("optimizer_type", "")
    args.optimizer_args = config.get("optimizer_args", [])
    args.learning_rate = config.get("learning_rate", 2.0e-6)
    args.max_grad_norm = config.get("max_grad_norm", 1.0)

    # LR Scheduler settings
    args.lr_scheduler = config.get("lr_scheduler", "constant")
    args.lr_warmup_steps = config.get("lr_warmup_steps", 0)
    args.lr_decay_steps = config.get("lr_decay_steps", 0)
    args.lr_scheduler_num_cycles = config.get("lr_scheduler_num_cycles", 1)
    args.lr_scheduler_power = config.get("lr_scheduler_power", 1.0)
    args.lr_scheduler_timescale = config.get("lr_scheduler_timescale", 0.0)
    args.lr_scheduler_min_lr_ratio = config.get("lr_scheduler_min_lr_ratio", 0.0)
    args.lr_scheduler_type = config.get("lr_scheduler_type", "")
    args.lr_scheduler_args = config.get("lr_scheduler_args", "")

    # Network settings
    args.network_module = config.get("network_module", "networks.lora_wan")
    args.network_dim = config.get("network_dim", 32)
    args.network_alpha = config.get("network_alpha", 32)
    args.network_weights = config.get("network_weights")
    args.network_dropout = config.get("network_dropout", 0)
    # Normalize network_args to a list of strings
    raw_net_args = config.get("network_args", [])
    if isinstance(raw_net_args, str):
        # Backward compatibility: allow single string or comma-separated
        if raw_net_args.strip() == "":
            args.network_args = []
        else:
            # split on commas if present, else wrap as single arg
            if "," in raw_net_args:
                args.network_args = [
                    s.strip() for s in raw_net_args.split(",") if s.strip()
                ]
            else:
                args.network_args = [raw_net_args.strip()]
    elif isinstance(raw_net_args, list):
        args.network_args = [str(v) for v in raw_net_args]
    else:
        args.network_args = []

    # Extract LoRA-GGPO parameters from network_args (e.g., "ggpo_sigma=0.03")
    args.ggpo_sigma = None
    args.ggpo_beta = None
    for net_arg in args.network_args:
        if isinstance(net_arg, str) and "=" in net_arg:
            key, value = net_arg.split("=", 1)
            k = key.strip().lower()
            v = value.strip()
            if k == "ggpo_sigma":
                try:
                    args.ggpo_sigma = float(v)
                except Exception:
                    args.ggpo_sigma = None
            elif k == "ggpo_beta":
                try:
                    args.ggpo_beta = float(v)
                except Exception:
                    args.ggpo_beta = None
    args.training_comment = config.get("training_comment", "trained with Takenoko")
    args.dim_from_weights = config.get("dim_from_weights", False)
    args.lycoris = config.get("lycoris", False)
    args.verbose_network = config.get("verbose_network", False)
    args.scale_weight_norms = config.get("scale_weight_norms", None)
    args.base_weights = config.get("base_weights")
    args.base_weights_multiplier = config.get("base_weights_multiplier", 1.0)

    # Reward LoRA settings (prefixed with reward_*)
    args.enable_reward_lora = config.get("enable_reward_lora", False)
    # prompts: either file path or enumerated keys like reward_prompt1, reward_prompt2, ...
    # We do not support reading external files for reward prompts; only TOML
    # derived keys are used. Keep these as None to avoid accidental use.
    args.reward_prompt_path = None
    args.reward_prompt_column = None
    # collect enumerated reward_promptN keys
    reward_prompts: list[str] = []
    for key, val in list(config.items()):
        if (
            isinstance(key, str)
            and key.lower().startswith("reward_prompt")
            and key.lower() != "reward_prompt_path"
        ):
            if isinstance(val, str) and val.strip():
                reward_prompts.append(val)
    args.reward_prompts = reward_prompts

    # core reward hyperparameters (defaults adapted from reference)
    args.reward_train_batch_size = config.get("reward_train_batch_size", 1)
    args.reward_train_sample_height = config.get("reward_train_sample_height", 256)
    args.reward_train_sample_width = config.get("reward_train_sample_width", 256)
    # support alias reward_num_frames -> reward_video_length
    args.reward_video_length = config.get(
        "reward_video_length", config.get("reward_num_frames", 81)
    )
    args.reward_num_inference_steps = config.get("reward_num_inference_steps", 50)
    args.reward_guidance_scale = config.get("reward_guidance_scale", 6.0)
    args.reward_num_decoded_latents = config.get("reward_num_decoded_latents", 1)
    args.reward_validation_steps = config.get("reward_validation_steps", 10000)
    args.reward_validation_prompt_path = config.get("reward_validation_prompt_path")

    # reward function selection
    args.reward_fn = config.get("reward_fn", "HPSReward")
    # keep kwargs as a raw json string to be parsed by the core
    rf_kwargs = config.get("reward_fn_kwargs", None)
    if isinstance(rf_kwargs, dict):
        import json as _json

        args.reward_fn_kwargs = _json.dumps(rf_kwargs)
    else:
        args.reward_fn_kwargs = rf_kwargs

    # backprop strategy
    args.reward_backprop = config.get("reward_backprop", True)
    args.reward_backprop_strategy = config.get("reward_backprop_strategy", "tail")
    args.reward_backprop_num_steps = config.get("reward_backprop_num_steps", 5)
    args.reward_backprop_step_list = config.get("reward_backprop_step_list")
    args.reward_backprop_random_start_step = config.get(
        "reward_backprop_random_start_step", 0
    )
    args.reward_backprop_random_end_step = config.get(
        "reward_backprop_random_end_step", 50
    )
    args.reward_stop_latent_model_input_gradient = config.get(
        "reward_stop_latent_model_input_gradient", False
    )

    # Enhanced progress bar and logging
    args.enhanced_progress_bar = config.get(
        "enhanced_progress_bar", True
    )  # Default to True for better UX
    args.logging_level = config.get("logging_level", "INFO")

    # Extra training metrics (periodic)
    args.log_extra_train_metrics = config.get("log_extra_train_metrics", True)
    args.train_metrics_interval = config.get("train_metrics_interval", 50)

    # Loss-vs-timestep scatter logging
    args.log_loss_scatterplot = config.get("log_loss_scatterplot", False)
    args.log_loss_scatterplot_interval = config.get(
        "log_loss_scatterplot_interval", 500
    )

    # Control LoRA settings
    args.enable_control_lora = config.get("enable_control_lora", False)
    args.control_lora_type = config.get("control_lora_type", "tile")
    args.control_preprocessing = config.get("control_preprocessing", "blur")
    args.control_blur_kernel_size = config.get("control_blur_kernel_size", 15)
    args.control_blur_sigma = config.get("control_blur_sigma", 4.0)
    args.control_scale_factor = config.get("control_scale_factor", 1.0)
    args.input_lr_scale = config.get("input_lr_scale", 1.0)
    # Match reference default (CFHW -> channel dim 0). Training (BCFHW) remaps to dim=1 at runtime.
    args.control_concatenation_dim = config.get("control_concatenation_dim", 0)
    args.load_control = config.get("load_control", False)
    args.control_suffix = config.get("control_suffix", "_control")
    args.control_inject_noise = config.get("control_inject_noise", 0.0)
    args.save_control_videos = config.get("save_control_videos", False)
    args.control_video_save_all = config.get("control_video_save_all", False)
    args.control_video_save_dir = config.get("control_video_save_dir", "control_videos")

    # ControlNet settings
    args.enable_controlnet = config.get("enable_controlnet", False)
    args.controlnet_weight = config.get("controlnet_weight", 1.0)
    args.controlnet_stride = config.get("controlnet_stride", 1)
    args.controlnet = config.get("controlnet", {})
    # Optional separate gradient clipping for ControlNet
    args.controlnet_max_grad_norm = config.get("controlnet_max_grad_norm")

    args.output_dir = config.get("output_dir", "output")
    args.output_name = config.get("output_name", "wan21_lora")
    args.resume = config.get("resume")
    args.auto_resume = config.get("auto_resume", True)
    args.save_every_n_epochs = config.get("save_every_n_epochs", None)
    args.save_every_n_steps = config.get("save_every_n_steps", 1000)
    args.save_last_n_epochs = config.get("save_last_n_epochs", None)
    args.save_last_n_epochs_state = config.get("save_last_n_epochs_state", None)
    args.save_last_n_steps = config.get("save_last_n_steps", None)
    args.save_last_n_steps_state = config.get("save_last_n_steps_state", None)
    args.save_state = config.get("save_state", True)
    args.save_state_on_train_end = config.get("save_state_on_train_end", False)

    # Sampling settings
    args.sample_every_n_steps = config.get("sample_every_n_steps", None)
    args.sample_at_first = config.get("sample_at_first", False)
    args.sample_every_n_epochs = config.get("sample_every_n_epochs", None)
    args.sample_prompts = config.get("sample_prompts", [])

    # Validation settings
    args.validate_every_n_steps = config.get("validate_every_n_steps", None)
    args.validate_on_epoch_end = config.get("validate_on_epoch_end", False)
    args.validation_timesteps = config.get("validation_timesteps", "500")
    args.use_unique_noise_per_batch = config.get("use_unique_noise_per_batch", True)

    # Logging settings
    args.logging_dir = config.get("logging_dir", "logs")

    args.log_with = config.get("log_with", "tensorboard")
    args.log_prefix = config.get("log_prefix", "")
    args.log_tracker_name = config.get("log_tracker_name", "")
    args.log_tracker_config = config.get("log_tracker_config", "")
    args.log_config = config.get("log_config", False)

    # Timestep distribution logging settings
    args.log_timestep_distribution = config.get("log_timestep_distribution", "off")
    args.log_timestep_distribution_interval = config.get(
        "log_timestep_distribution_interval", 1000
    )
    args.log_timestep_distribution_bins = config.get(
        "log_timestep_distribution_bins", 100
    )
    args.log_timestep_distribution_init = config.get(
        "log_timestep_distribution_init", True
    )
    args.log_timestep_distribution_samples = config.get(
        "log_timestep_distribution_samples", 20000
    )
    args.log_timestep_distribution_window = config.get(
        "log_timestep_distribution_window", 10000
    )
    args.log_timestep_distribution_bands = config.get(
        "log_timestep_distribution_bands", "0,100,200,300,400,500,600,700,800,900,1000"
    )
    args.log_timestep_distribution_init_once = config.get(
        "log_timestep_distribution_init_once", True
    )
    args.log_timestep_distribution_pmf = config.get(
        "log_timestep_distribution_pmf", False
    )

    # TensorBoard server settings
    args.launch_tensorboard_server = config.get("launch_tensorboard_server", False)
    args.tensorboard_host = config.get("tensorboard_host", "127.0.0.1")
    args.tensorboard_port = config.get("tensorboard_port", 6006)
    args.tensorboard_auto_reload = config.get("tensorboard_auto_reload", True)

    # Acceleration settings
    args.sdpa = config.get("sdpa", False)
    args.flash_attn = config.get("flash_attn", False)
    args.sage_attn = config.get("sage_attn", False)
    args.xformers = config.get("xformers", False)
    args.flash3 = config.get("flash3", False)
    args.split_attn = config.get("split_attn", False)

    # DDP settings
    args.ddp_timeout = config.get("ddp_timeout")
    args.ddp_gradient_as_bucket_view = config.get("ddp_gradient_as_bucket_view", False)
    args.ddp_static_graph = config.get("ddp_static_graph", False)

    # Dynamo settings
    args.dynamo_backend = config.get("dynamo_backend", "NO")
    args.dynamo_mode = config.get("dynamo_mode", "default")
    args.dynamo_fullgraph = config.get("dynamo_fullgraph", False)
    args.dynamo_dynamic = config.get("dynamo_dynamic", False)

    # Full precision settings (commented out in parser but used in code)
    args.full_fp16 = config.get("full_fp16", False)
    args.full_bf16 = config.get("full_bf16", False)

    # Timestep and flow matching settings
    args.timestep_sampling = config.get("timestep_sampling", "shift")

    # Parse new timestep optimization flags
    args.use_precomputed_timesteps = config.get("use_precomputed_timesteps", False)
    args.precomputed_timestep_buckets = config.get(
        "precomputed_timestep_buckets", 10000
    )
    args.discrete_flow_shift = config.get("discrete_flow_shift", 3.0)
    args.sigmoid_scale = config.get("sigmoid_scale", 1.0)
    # Enhanced sigmoid optional bias
    args.sigmoid_bias = config.get("sigmoid_bias", 0.0)
    # Bell-shaped distribution parameters
    args.bell_center = config.get("bell_center", 0.5)
    args.bell_std = config.get("bell_std", 0.2)
    # LogNormal blend control
    args.lognorm_blend_alpha = config.get("lognorm_blend_alpha", 0.75)
    args.weighting_scheme = config.get("weighting_scheme", "none")
    args.logit_mean = config.get("logit_mean", 0.0)
    args.logit_std = config.get("logit_std", 1.0)
    args.mode_scale = config.get("mode_scale", 1.29)
    args.min_timestep = config.get("min_timestep", 0)
    args.max_timestep = config.get("max_timestep", 1000)
    args.skip_extra_timestep_constraint = config.get(
        "skip_extra_timestep_constraint", True
    )
    args.fast_rejection_sampling = config.get("fast_rejection_sampling", False)
    # Fine-tuning knobs for fast rejection sampling
    args.rejection_overdraw_factor = config.get("rejection_overdraw_factor", 4.0)
    args.rejection_max_iters = config.get("rejection_max_iters", 10)
    args.timestep_constraint_epsilon = config.get("timestep_constraint_epsilon", 1e-6)
    # Optional: round training timesteps to the nearest integer schedule step
    args.round_training_timesteps = config.get("round_training_timesteps", False)
    args.preserve_distribution_shape = config.get("preserve_distribution_shape", False)
    # Optional override for precomputed mid-shift area (used by flux/qwen/qinglong precompute path)
    args.precomputed_midshift_area = config.get("precomputed_midshift_area")
    args.show_timesteps = config.get("show_timesteps")
    args.guidance_scale = config.get("guidance_scale", 1.0)

    # Offloading settings
    args.blocks_to_swap = config.get("blocks_to_swap", 0)
    args.allow_mixed_block_swap_offload = config.get(
        "allow_mixed_block_swap_offload", False
    )

    # Dual model training settings
    args.enable_dual_model_training = config.get("enable_dual_model_training", False)
    args.dit_high_noise = config.get("dit_high_noise")
    args.timestep_boundary = config.get("timestep_boundary", 875)
    args.offload_inactive_dit = config.get("offload_inactive_dit", True)

    # Dual-mode timestep bucketing strategy
    args.dual_timestep_bucket_strategy = config.get(
        "dual_timestep_bucket_strategy", "hybrid"
    )
    args.dual_timestep_bucket_max_retries = config.get(
        "dual_timestep_bucket_max_retries", 100
    )
    args.dual_timestep_bucket_eps = config.get("dual_timestep_bucket_eps", 1e-4)

    # Metadata settings
    args.no_metadata = config.get("no_metadata", False)
    args.embed_config_in_metadata = config.get("embed_config_in_metadata", True)
    args.metadata_title = config.get("metadata_title", "")
    args.metadata_author = config.get("metadata_author", "")
    args.metadata_description = config.get("metadata_description", "")
    args.metadata_license = config.get("metadata_license", "")
    args.metadata_tags = config.get("metadata_tags", "")

    # Device settings
    args.device = config.get("device")

    # Fluxflow settings
    args.enable_fluxflow = config.get("enable_fluxflow", False)
    args.fluxflow_mode = config.get("fluxflow_mode", "frame")
    args.fluxflow_frame_perturb_ratio = config.get("fluxflow_frame_perturb_ratio", 0.25)
    args.fluxflow_block_size = config.get("fluxflow_block_size", 4)
    args.fluxflow_block_perturb_prob = config.get("fluxflow_block_perturb_prob", 0.5)
    args.fluxflow_frame_dim_in_batch = config.get("fluxflow_frame_dim_in_batch", 2)

    # FVDM settings
    args.enable_fvdm = config.get("enable_fvdm", False)
    args.fvdm_ptss_p = config.get("fvdm_ptss_p", 0.2)

    # FOPP settings
    args.fopp_num_timesteps = config.get("fopp_num_timesteps", 1000)
    args.fopp_schedule_type = config.get("fopp_schedule_type", "linear")
    args.fopp_beta_start = config.get("fopp_beta_start", 0.0001)
    args.fopp_beta_end = config.get("fopp_beta_end", 0.002)
    args.fopp_seed = config.get("fopp_seed", None)

    # Nabla settings
    args.nabla_sparse_attention = config.get("nabla_sparse_attention", False)
    args.nabla_sparse_algo = config.get("nabla_sparse_algo", "nabla-0.7_sta-11-24-24")

    # Contrastive Flow Matching (ŒîFM) settings
    args.enable_contrastive_flow_matching = config.get(
        "enable_contrastive_flow_matching", False
    )
    args.contrastive_flow_lambda = config.get("contrastive_flow_lambda", 0.05)

    # Timestep bucketing (dataset-driven, per-epoch stratified uniform pool)
    # None disables bucketing; set to >=2 to enable
    args.num_timestep_buckets = config.get("num_timestep_buckets")

    # Dispersive Loss Regularization settings
    args.enable_dispersive_loss = config.get("enable_dispersive_loss", False)
    args.dispersive_loss_lambda = config.get("dispersive_loss_lambda", 0.0)
    args.dispersive_loss_tau = config.get("dispersive_loss_tau", 0.5)
    # None disables extraction, non-negative integer selects a block index
    args.dispersive_loss_target_block = config.get("dispersive_loss_target_block")
    args.dispersive_loss_metric = config.get(
        "dispersive_loss_metric", "l2_sq"
    )  # "l2_sq" or "cosine"
    # Optional: pool spatial tokens per frame before dispersion ("none" or "frame_mean")
    args.dispersive_loss_pooling = config.get("dispersive_loss_pooling", "none")

    # Optical Flow Loss (RAFT-based) settings
    args.enable_optical_flow_loss = config.get("enable_optical_flow_loss", False)
    args.lambda_optical_flow = config.get("lambda_optical_flow", 0.0)

    # REPA (Representation Alignment) settings
    args.enable_repa = config.get("enable_repa", False)
    args.repa_encoder_name = config.get("repa_encoder_name", "dinov2_vitb14")
    args.repa_alignment_depth = config.get("repa_alignment_depth", 8)
    args.repa_loss_lambda = config.get("repa_loss_lambda", 0.5)
    args.repa_similarity_fn = config.get("repa_similarity_fn", "cosine")

    # Use original config file directly - no need for temporary files!
    # The caching scripts can handle full config files and extract what they need

    # Validate that we have dataset configuration
    if "datasets" not in config and "val_datasets" not in config:
        raise ValueError(
            "No dataset configuration found in the config file. Please include [[datasets]] and/or [[val_datasets]] sections."
        )

    # Validate the dataset configuration
    logger.info("üîç Validating dataset configuration...")
    try:
        validate_dataset_config(args.dataset_config, test_dataset_creation=False)
        logger.info("‚úÖ Dataset configuration validation passed!")
    except Exception as e:
        logger.exception(f"‚ùå Dataset configuration validation failed: {e}")
        raise ValueError(f"Dataset configuration validation failed: {e}")

    # Set default values for compatibility
    args.dit_dtype = None  # automatically detected
    if args.vae_dtype is None:
        args.vae_dtype = "float16"  # make float16 as default for VAE

    # Read latent cache settings from config
    if "datasets" in config and "latent_cache" in config["datasets"]:
        latent_cache_config = config["datasets"]["latent_cache"]
        args.vae = latent_cache_config.get("vae", args.vae)
        args.vae_cache_cpu = latent_cache_config.get(
            "vae_cache_cpu", args.vae_cache_cpu
        )
        args.vae_dtype = latent_cache_config.get("vae_dtype", args.vae_dtype)
        args.latent_cache_device = latent_cache_config.get("device", args.device)
        args.latent_cache_batch_size = latent_cache_config.get("batch_size")
        args.latent_cache_num_workers = latent_cache_config.get("num_workers")
        args.latent_cache_skip_existing = latent_cache_config.get(
            "skip_existing", False
        )
        args.latent_cache_keep_cache = latent_cache_config.get("keep_cache", False)
        args.latent_cache_debug_mode = latent_cache_config.get("debug_mode")
        args.latent_cache_console_width = latent_cache_config.get("console_width", 80)
        args.latent_cache_console_back = latent_cache_config.get(
            "console_back", "black"
        )
        args.latent_cache_console_num_images = latent_cache_config.get(
            "console_num_images", 1
        )
    else:
        # Set defaults for latent cache if section not found
        args.latent_cache_device = args.device
        args.latent_cache_batch_size = None
        args.latent_cache_num_workers = None
        args.latent_cache_skip_existing = False
        args.latent_cache_keep_cache = False
        args.latent_cache_debug_mode = None
        args.latent_cache_console_width = 80
        args.latent_cache_console_back = "black"
        args.latent_cache_console_num_images = 1

    # Read text encoder cache settings from config
    if "datasets" in config and "text_encoder_cache" in config["datasets"]:
        text_encoder_cache_config = config["datasets"]["text_encoder_cache"]
        args.t5 = text_encoder_cache_config.get("t5", args.t5)
        args.fp8_t5 = text_encoder_cache_config.get("fp8_t5", args.fp8_t5)
        args.text_encoder_cache_device = text_encoder_cache_config.get(
            "device", args.device
        )
        args.text_encoder_cache_batch_size = text_encoder_cache_config.get("batch_size")
        args.text_encoder_cache_num_workers = text_encoder_cache_config.get(
            "num_workers"
        )
        args.text_encoder_cache_skip_existing = text_encoder_cache_config.get(
            "skip_existing", False
        )
        args.text_encoder_cache_keep_cache = text_encoder_cache_config.get(
            "keep_cache", False
        )
    else:
        # Set defaults for text encoder cache if section not found
        args.text_encoder_cache_device = args.device
        args.text_encoder_cache_batch_size = None
        args.text_encoder_cache_num_workers = None
        args.text_encoder_cache_skip_existing = False
        args.text_encoder_cache_keep_cache = False

    # No explicit load_mask toggle: mask_path presence implies mask loading

    # Self-correction (draft, fully gated by self_correction_enabled) - flat keys only
    args.self_correction_enabled = config.get("self_correction_enabled", False)
    args.self_correction_warmup_steps = config.get("self_correction_warmup_steps", 1000)
    args.self_correction_update_frequency = config.get(
        "self_correction_update_frequency", 1000
    )
    args.self_correction_cache_size = config.get("self_correction_cache_size", 200)
    args.self_correction_clip_len = config.get("self_correction_clip_len", 32)
    args.self_correction_batch_ratio = config.get("self_correction_batch_ratio", 0.2)
    args.self_correction_sample_steps = config.get("self_correction_sample_steps", 16)
    args.self_correction_width = config.get("self_correction_width", 256)
    args.self_correction_height = config.get("self_correction_height", 256)
    args.self_correction_guidance_scale = config.get(
        "self_correction_guidance_scale", 5.0
    )

    # Inline self-correction prompts in config (preferred over external files)
    args.self_correction_prompts = None
    if "self_correction_prompts" in config:
        try:
            sc_prompts = config.get("self_correction_prompts", [])
            if isinstance(sc_prompts, list):
                normalized: list[dict] = []
                for i, p in enumerate(sc_prompts):
                    if isinstance(p, str):
                        normalized.append({"text": p, "enum": i})
                    elif isinstance(p, dict):
                        d = dict(p)
                        if "text" not in d:
                            # If a dict without text appears, skip it gracefully
                            continue
                        d.setdefault("enum", i)
                        normalized.append(d)
                args.self_correction_prompts = normalized
        except Exception:
            args.self_correction_prompts = None

    return args


# Removed cleanup_temp_files - no longer needed since we don't create temporary files


# Global variable to store TensorBoard process
_tensorboard_process: Optional[subprocess.Popen] = None


def find_tensorboard_executable() -> Optional[str]:
    """Find the correct TensorBoard executable path"""
    # Try different common locations and methods
    candidates = []

    # Method 1: Direct 'tensorboard' command
    candidates.append("tensorboard")

    # Method 2: In virtual environment Scripts directory (Windows)
    if hasattr(sys, "real_prefix") or (
        hasattr(sys, "base_prefix") and sys.base_prefix != sys.prefix
    ):
        # We're in a virtual environment
        if os.name == "nt":  # Windows
            venv_tensorboard = os.path.join(
                os.path.dirname(sys.executable), "Scripts", "tensorboard.exe"
            )
            candidates.append(venv_tensorboard)
        else:  # Unix-like
            venv_tensorboard = os.path.join(
                os.path.dirname(sys.executable), "tensorboard"
            )
            candidates.append(venv_tensorboard)

    # Method 3: Alongside Python executable
    if os.name == "nt":  # Windows
        python_dir_tensorboard = os.path.join(
            os.path.dirname(sys.executable), "tensorboard.exe"
        )
        candidates.append(python_dir_tensorboard)

    # Test each candidate
    for candidate in candidates:
        try:
            result = subprocess.run(
                [candidate, "--help"], capture_output=True, timeout=10, text=True
            )
            if result.returncode == 0:
                return candidate
        except (
            subprocess.TimeoutExpired,
            subprocess.CalledProcessError,
            FileNotFoundError,
        ):
            continue

    return None


def launch_tensorboard_server(
    logdir: str, host: str = "127.0.0.1", port: int = 6006, auto_reload: bool = True
) -> Optional[subprocess.Popen]:
    """Launch TensorBoard server in background"""
    global _tensorboard_process

    # Check if TensorBoard is already running on this port
    try:
        import socket

        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex((host, port))
        sock.close()
        if result == 0:
            logger.warning(
                f"TensorBoard appears to be already running on {host}:{port}"
            )
            return None
    except Exception:
        pass  # Ignore socket check errors

    try:
        # Find the correct TensorBoard executable
        tensorboard_exe = find_tensorboard_executable()
        if not tensorboard_exe:
            logger.error("‚ùå Could not find TensorBoard executable")
            return None

        # Build the command
        cmd = [tensorboard_exe, "--logdir", logdir, "--host", host, "--port", str(port)]

        if auto_reload:
            cmd.extend(["--reload_interval", "30"])

        logger.info("Launching TensorBoard server...")
        logger.debug(f"TensorBoard command: {' '.join(cmd)}")
        logger.info(f"TensorBoard will be accessible at: http://{host}:{port}")

        # Start the process with minimal output
        _tensorboard_process = subprocess.Popen(
            cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True
        )

        # Give TensorBoard a moment to start
        time.sleep(3)

        # Check if process is still running (didn't immediately crash)
        if _tensorboard_process.poll() is None:
            logger.info("‚úì TensorBoard server launched successfully!")
            logger.info(f"  Access it at: http://{host}:{port}")
            logger.info(f"  Logdir: {logdir}")

            # Register cleanup function to run on exit
            atexit.register(stop_tensorboard_server)

            return _tensorboard_process
        else:
            # Process crashed, get error output
            _, stderr = _tensorboard_process.communicate()
            logger.error(f"Failed to launch TensorBoard: {stderr}")
            _tensorboard_process = None
            return None

    except Exception as e:
        logger.exception(f"Error launching TensorBoard: {e}")
        return None


def stop_tensorboard_server():
    """Stop the TensorBoard server if running"""
    global _tensorboard_process

    if _tensorboard_process is not None:
        try:
            logger.info("Stopping TensorBoard server...")
            _tensorboard_process.terminate()

            # Give it a moment to terminate gracefully
            try:
                _tensorboard_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                # Force kill if it doesn't terminate gracefully
                _tensorboard_process.kill()
                _tensorboard_process.wait()

            logger.info("‚úì TensorBoard server stopped")
        except Exception as e:
            logger.exception(f"Error stopping TensorBoard: {e}")
        finally:
            _tensorboard_process = None


def check_tensorboard_installation() -> bool:
    """Check if TensorBoard is properly installed"""
    try:
        import tensorboard

        return True
    except ImportError:
        return False


def get_tensorboard_install_instructions() -> str:
    """Get platform-specific TensorBoard installation instructions"""
    instructions = """
TensorBoard Installation Instructions:
=====================================

Option 1 (Recommended): Install via pip
  pip install tensorboard

Option 2: Install via conda  
  conda install tensorboard

Option 3: Install with TensorFlow
  pip install tensorflow  # includes tensorboard

After installation, restart your terminal/IDE and try again.
"""
    return instructions


def setup_tensorboard_if_enabled(args: argparse.Namespace):
    """Setup TensorBoard server if enabled in config"""
    # Only rank 0 should attempt to launch the server in distributed runs
    try:
        import accelerate  # type: ignore

        state = accelerate.PartialState()  # type: ignore
        is_main_process = bool(getattr(state, "is_main_process", True))
    except Exception:
        is_main_process = True

    if (
        hasattr(args, "launch_tensorboard_server")
        and args.launch_tensorboard_server
        and is_main_process
    ):
        # Check if TensorBoard is installed
        if not check_tensorboard_installation():
            logger.error("‚ùå TensorBoard is not installed!")
            logger.info(get_tensorboard_install_instructions())
            logger.warning(
                "TensorBoard server launch is disabled due to missing installation."
            )
            return

        # Ensure logging directory exists
        os.makedirs(args.logging_dir, exist_ok=True)

        # Launch TensorBoard server
        process = launch_tensorboard_server(
            logdir=args.logging_dir,
            host=args.tensorboard_host,
            port=args.tensorboard_port,
            auto_reload=args.tensorboard_auto_reload,
        )

        if process:
            logger.info(
                f"TensorBoard is running in the background (PID: {process.pid})"
            )
        else:
            logger.error("‚ùå Failed to launch TensorBoard server")
            logger.info(
                f"Try launching TensorBoard manually with: tensorboard --logdir {args.logging_dir} --host {args.tensorboard_host} --port {args.tensorboard_port}"
            )
    else:
        if (
            hasattr(args, "launch_tensorboard_server")
            and args.launch_tensorboard_server
        ):
            logger.info("TensorBoard server launch skipped on non-main process")
        else:
            logger.info("TensorBoard server launch is disabled")


def _estimate_peak_vram_gb_from_config(
    config: Dict[str, Any],
) -> Tuple[float, Dict[str, Any]]:
    return shared_estimate_vram(config)


class UnifiedTrainer:
    """Unified trainer that handles caching and training operations"""

    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config, self.config_content = load_training_config(config_path)
        self.args = create_args_from_config(
            self.config, config_path, self.config_content
        )

        # Configure CUDA allocator from TOML before any CUDA initialization
        try:
            configure_cuda_allocator_from_config(self.config, logger)
        except Exception as _cuda_alloc_err:
            # Non-fatal: proceed with default allocator if misconfigured
            logger.debug(f"CUDA allocator config skipped: {_cuda_alloc_err}")

        # Apply logging level from config
        try:
            level_str = str(getattr(self.args, "logging_level", "INFO")).upper()
            level_val = getattr(logging, level_str, logging.INFO)
            logger.setLevel(level_val)
            for h in logger.handlers:
                h.setLevel(level_val)
        except Exception:
            pass

        # Set global seed for reproducibility
        try:
            set_global_seed(int(getattr(self.args, "seed", 42)))
            logger.info(f"üîí Global seed set to {getattr(self.args, 'seed', 42)}")
        except Exception as seed_err:
            logger.warning(f"Failed to set global seed: {seed_err}")

        flash_attn, _flash_attn_forward, flash_attn_varlen_func, flash_attn_func = (
            setup_flash_attention()
        )
        sageattn_varlen, sageattn = setup_sageattention()
        xops = setup_xformers()

        # Setup TensorBoard server if enabled
        setup_tensorboard_if_enabled(self.args)

    def show_menu(self) -> str:
        """Display the main menu and get user choice"""
        print("\n" + "=" * 50)
        print("Takenoko - Unified Operations Menu")
        print("=" * 50)
        print("1. Cache Latents")
        print("2. Cache Text Encoder Outputs")
        print("3. Train Model")
        print("4. Estimate VRAM Usage (from current config)")
        print("5. Reload Config File")
        print("6. Free VRAM (aggressive)")
        print("7. Return to Config Selection")
        print("=" * 50)

        while True:
            choice = input("Enter your choice (1-7): ").strip()
            if choice in ["1", "2", "3", "4", "5", "6", "7"]:
                return choice
            else:
                print("Invalid choice. Please enter 1, 2, 3, 4, 5, 6, or 7.")

    def cache_latents(self) -> bool:
        """Run latent caching operation"""
        logger.info("Starting Latent Caching...")

        try:
            # Optional memory trace gate
            trace_memory: bool = bool(
                getattr(self, "args", argparse.Namespace()).__dict__.get(
                    "trace_memory", False
                )
            )
            if trace_memory:
                snapshot_gpu_memory("cache_latents/before")
            # Create args namespace with the required arguments from config
            cache_args = argparse.Namespace()
            cache_args.dataset_config = self.args.dataset_config
            cache_args.vae = self.args.vae
            cache_args.vae_dtype = self.args.vae_dtype
            cache_args.vae_cache_cpu = self.args.vae_cache_cpu
            cache_args.clip = self.args.clip
            cache_args.device = self.args.latent_cache_device
            cache_args.debug_mode = self.args.latent_cache_debug_mode
            cache_args.console_width = self.args.latent_cache_console_width
            cache_args.console_back = self.args.latent_cache_console_back
            cache_args.console_num_images = self.args.latent_cache_console_num_images
            cache_args.batch_size = self.args.latent_cache_batch_size
            cache_args.num_workers = self.args.latent_cache_num_workers
            cache_args.skip_existing = self.args.latent_cache_skip_existing
            cache_args.keep_cache = self.args.latent_cache_keep_cache

            # Add control LoRA caching arguments
            cache_args.control_lora_type = self.args.control_lora_type
            cache_args.control_preprocessing = self.args.control_preprocessing
            cache_args.control_blur_kernel_size = self.args.control_blur_kernel_size
            cache_args.control_blur_sigma = self.args.control_blur_sigma

            # Add target_model for proper cache extension
            cache_args.target_model = self.args.target_model

            # Set default values for any missing arguments
            if not hasattr(cache_args, "vae") or cache_args.vae is None:
                raise ValueError("VAE checkpoint is required for latent caching")

            logger.info(f"Running latent caching with VAE: {cache_args.vae}")
            logger.info(f"Device: {cache_args.device}")
            logger.debug(f"Debug mode: {cache_args.debug_mode}")
            logger.info(f"Batch size: {cache_args.batch_size}")
            logger.info(f"Num workers: {cache_args.num_workers}")
            logger.info(f"Skip existing: {cache_args.skip_existing}")
            logger.info(f"Keep cache: {cache_args.keep_cache}")

            # Run latent caching using the unified functions

            device = (
                cache_args.device
                if cache_args.device is not None
                else "cuda" if torch.cuda.is_available() else "cpu"
            )
            if isinstance(device, str):
                device = torch.device(device)

            # Load dataset config
            blueprint_generator = BlueprintGenerator(ConfigSanitizer())
            logger.info(f"Load dataset config from {cache_args.dataset_config}")
            user_config = config_utils.load_user_config(cache_args.dataset_config)

            blueprint = blueprint_generator.generate(user_config, cache_args)

            # Combine training and validation dataset blueprints
            all_dataset_blueprints = list(blueprint.train_dataset_group.datasets)
            if len(blueprint.val_dataset_group.datasets) > 0:
                all_dataset_blueprints.extend(blueprint.val_dataset_group.datasets)

            combined_dataset_group_blueprint = config_utils.DatasetGroupBlueprint(
                all_dataset_blueprints
            )

            dataset_group = config_utils.generate_dataset_group_by_blueprint(
                combined_dataset_group_blueprint,
                training=False,
                prior_loss_weight=getattr(cache_args, "prior_loss_weight", 1.0),
            )

            datasets = dataset_group.datasets

            if cache_args.debug_mode is not None:
                show_datasets(
                    datasets,  # type: ignore
                    cache_args.debug_mode,
                    cache_args.console_width,
                    cache_args.console_back,
                    cache_args.console_num_images,
                    fps=16,
                )
                return True

            assert cache_args.vae is not None, "vae checkpoint is required"

            vae_path = cache_args.vae

            # Download model if it's a URL
            if vae_path.startswith(("http://", "https://")):
                logger.info(f"Detected URL for VAE model, downloading: {vae_path}")
                cache_dir = getattr(cache_args, "model_cache_dir", None)
                vae_path = download_model_if_needed(vae_path, cache_dir=cache_dir)
                logger.info(f"Downloaded VAE model to: {vae_path}")

            logger.info(f"Loading VAE model from {vae_path}")
            # Default to float16 for consistency unless explicitly overridden
            vae_dtype = (
                torch.float16
                if cache_args.vae_dtype is None
                else str_to_dtype(cache_args.vae_dtype)
            )
            cache_device = torch.device("cpu") if cache_args.vae_cache_cpu else None
            vae = WanVAE(
                vae_path=vae_path,
                device=str(device),
                dtype=vae_dtype,
                cache_device=cache_device,
            )
            # Convert device string to torch.device for compatibility with encode_and_save_batch
            vae.device = torch.device(vae.device)

            if trace_memory:
                snapshot_gpu_memory("cache_latents/after_load")

            # Encode images
            def encode(one_batch):
                encode_and_save_batch(vae, one_batch, cache_args)

            encode_datasets(datasets, encode, cache_args)  # type: ignore

            # Clean up VAE model from memory
            logger.info("Cleaning up VAE model from memory...")
            if trace_memory:
                snapshot_gpu_memory("cache_latents/before_cleanup")
            del vae
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            if trace_memory:
                force_cuda_cleanup("cache_latents")

            logger.info("Latent caching completed successfully!")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Error during latent caching: {e}")
            return False

    def cache_text_encoder_outputs(self) -> bool:
        """Run text encoder output caching operation - simplified without temporary files"""
        logger.info("Starting Text Encoder Output Caching...")

        try:
            trace_memory: bool = bool(
                getattr(self, "args", argparse.Namespace()).__dict__.get(
                    "trace_memory", False
                )
            )
            if trace_memory:
                snapshot_gpu_memory("cache_t5/before")
            # Create args namespace with the required arguments from config
            cache_args = argparse.Namespace()
            cache_args.dataset_config = self.args.dataset_config
            cache_args.t5 = self.args.t5
            cache_args.fp8_t5 = self.args.fp8_t5
            cache_args.device = self.args.text_encoder_cache_device
            cache_args.num_workers = self.args.text_encoder_cache_num_workers
            cache_args.skip_existing = self.args.text_encoder_cache_skip_existing
            cache_args.batch_size = self.args.text_encoder_cache_batch_size
            cache_args.keep_cache = self.args.text_encoder_cache_keep_cache

            # Add target_model for proper cache extension
            cache_args.target_model = self.args.target_model

            # Set default values for any missing arguments
            if not hasattr(cache_args, "t5") or cache_args.t5 is None:
                raise ValueError(
                    "T5 checkpoint is required for text encoder output caching"
                )

            logger.info(f"Running text encoder caching with T5: {cache_args.t5}")
            logger.info(f"Dataset config: {cache_args.dataset_config}")
            logger.info(f"Device: {cache_args.device}")
            logger.info(f"Batch size: {cache_args.batch_size}")
            logger.info(f"Num workers: {cache_args.num_workers}")
            logger.info(f"Skip existing: {cache_args.skip_existing}")
            logger.info(f"Keep cache: {cache_args.keep_cache}")
            logger.info(f"FP8 T5: {cache_args.fp8_t5}")

            # Run text encoder caching using the imported functions

            device = (
                cache_args.device
                if cache_args.device is not None
                else "cuda" if torch.cuda.is_available() else "cpu"
            )
            if isinstance(device, str):
                device = torch.device(device)

            # Load dataset config
            blueprint_generator = BlueprintGenerator(ConfigSanitizer())
            logger.info(f"Load dataset config from {cache_args.dataset_config}")
            user_config = config_utils.load_user_config(cache_args.dataset_config)

            blueprint = blueprint_generator.generate(user_config, cache_args)

            # Combine training and validation dataset blueprints
            all_dataset_blueprints = list(blueprint.train_dataset_group.datasets)
            if len(blueprint.val_dataset_group.datasets) > 0:
                all_dataset_blueprints.extend(blueprint.val_dataset_group.datasets)

            combined_dataset_group_blueprint = config_utils.DatasetGroupBlueprint(
                all_dataset_blueprints
            )

            dataset_group = config_utils.generate_dataset_group_by_blueprint(
                combined_dataset_group_blueprint,
                training=False,
                prior_loss_weight=getattr(cache_args, "prior_loss_weight", 1.0),
            )

            datasets = dataset_group.datasets

            # define accelerator for fp8 inference
            # Select T5 config based on mapped task
            if getattr(self.args, "task", "t2v-14B") == "t2v-A14B":
                from wan.configs import wan_t2v_A14B as _wan_cfg

                config = _wan_cfg.t2v_A14B  # type: ignore
            else:
                config = wan_t2v_14B.t2v_14B
            accelerator = None
            if cache_args.fp8_t5:
                mixed_precision = getattr(cache_args, "mixed_precision", None)
                if not mixed_precision or mixed_precision == "no":
                    mixed_precision = "bf16" if config.t5_dtype == torch.bfloat16 else "fp16"  # type: ignore
                accelerator = accelerate.Accelerator(mixed_precision=mixed_precision)

            # prepare cache files and paths
            all_cache_files_for_dataset, all_cache_paths_for_dataset = (
                prepare_cache_files_and_paths(datasets)  # type: ignore
            )

            # Download T5 model if it's a URL
            t5_path = cache_args.t5
            if t5_path.startswith(("http://", "https://")):
                logger.info(f"Detected URL for T5 model, downloading: {t5_path}")
                cache_dir = getattr(cache_args, "model_cache_dir", None)
                t5_path = download_model_if_needed(t5_path, cache_dir=cache_dir)
                logger.info(f"Downloaded T5 model to: {t5_path}")

            # Load T5
            logger.info(f"Loading T5: {t5_path}")
            text_encoder = T5EncoderModel(
                text_len=config.text_len,  # type: ignore
                dtype=config.t5_dtype,  # type: ignore
                device=device,  # type: ignore
                weight_path=t5_path,
                fp8=cache_args.fp8_t5,
            )

            if trace_memory:
                snapshot_gpu_memory("cache_t5/after_load")

            # Encode with T5
            logger.info("Encoding with T5")

            def encode_for_text_encoder(batch):
                encode_and_save_text_encoder_output_batch(
                    text_encoder,
                    batch,
                    device,
                    accelerator,
                    cache_args,  # <-- ADD cache_args
                )

            process_text_encoder_batches(
                cache_args.num_workers,
                cache_args.skip_existing,
                cache_args.batch_size,
                datasets,  # type: ignore
                all_cache_files_for_dataset,
                all_cache_paths_for_dataset,
                encode_for_text_encoder,
            )

            # Clean up text encoder model from memory
            logger.info("Cleaning up text encoder model from memory...")
            if trace_memory:
                snapshot_gpu_memory("cache_t5/before_cleanup")
            del text_encoder
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            if trace_memory:
                force_cuda_cleanup("cache_t5")

            # remove cache files not in dataset
            post_process_cache_files(
                datasets,  # type: ignore
                all_cache_files_for_dataset,
                all_cache_paths_for_dataset,
                cache_args.keep_cache,
            )

            logger.info("‚úÖ Text encoder output caching completed successfully!")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Error during text encoder output caching: {e}")
            return False

    def free_vram_aggressively(self) -> bool:
        """Attempt to free as much VRAM as possible using safe mechanisms."""
        try:
            try:
                snapshot_gpu_memory("free_vram/before")
            except Exception:
                pass
            # CPU garbage collect
            try:
                gc.collect()
            except Exception:
                pass
            # CUDA cleanup
            if torch.cuda.is_available():
                try:
                    torch.cuda.synchronize()
                except Exception:
                    pass
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
                # ipc_collect may not exist on all builds; ignore if missing
                try:
                    torch.cuda.ipc_collect()  # type: ignore[attr-defined]
                except Exception:
                    pass
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
            # Use existing utility to enforce cleanup
            try:
                force_cuda_cleanup("manual_free")
            except Exception:
                pass
            try:
                snapshot_gpu_memory("free_vram/after")
            except Exception:
                pass
            logger.info("Attempted aggressive VRAM cleanup.")
            return True
        except Exception as e:
            logger.exception(f"‚ùå Error during VRAM cleanup: {e}")
            return False

    def _log_timestep_configuration(self) -> None:
        """Log comprehensive timestep configuration information."""
        args = self.args
        # Honor configured logging level
        level_str = str(getattr(args, "logging_level", "INFO")).upper()
        level_val = getattr(logging, level_str, logging.INFO)
        logger = get_logger(__name__, level=level_val)

        logger.info("Timestep Sampling Configuration:")
        logger.info(f"   Method: '{args.timestep_sampling}'")

        # Strategy summary (explicit)
        use_precomputed = bool(getattr(args, "use_precomputed_timesteps", False))
        num_buckets = getattr(args, "num_timestep_buckets", None)
        has_bucketting = isinstance(num_buckets, int) and num_buckets >= 2

        strategy_parts: list[str] = [str(getattr(args, "timestep_sampling", "unknown"))]
        if use_precomputed:
            strategy_parts.append("precomputed-quantiles")
        if has_bucketting:
            strategy_parts.append(f"dataset-bucketing({num_buckets})")
        logger.info(f"   Strategy: {' + '.join(strategy_parts)}")

        # Dataset-driven timestep bucketing
        if has_bucketting:
            logger.info(f"   Dataset bucketing: ENABLED ‚Äî num_buckets={num_buckets}")
        else:
            logger.info("   Dataset bucketing: DISABLED")

        # Show method-specific parameters
        if args.timestep_sampling in ["sigmoid", "shift", "logit_normal"]:
            logger.info(f"   Sigmoid scale: {getattr(args, 'sigmoid_scale', 1.0)}")
        if args.timestep_sampling == "shift":
            logger.info(f"   Flow shift: {getattr(args, 'discrete_flow_shift', 3.0)}")

        # Show optimization settings
        if use_precomputed:
            logger.info(f"   Precomputed quantiles: ENABLED")
            logger.info(
                f"   Bucket count: {getattr(args, 'precomputed_timestep_buckets', 10000):,}"
            )
        else:
            logger.info(f"   Optimization: Original random sampling")

        # Show timestep constraints
        min_timestep = getattr(args, "min_timestep", None)
        max_timestep = getattr(args, "max_timestep", None)
        if min_timestep is not None or max_timestep is not None:
            min_val = min_timestep if min_timestep is not None else 0
            max_val = max_timestep if max_timestep is not None else 1000
            logger.info(f"   Range: [{min_val}, {max_val}] (out of [0, 1000])")
        else:
            logger.info(f"   Range: [0, 1000] (full range)")

        # Additional sampling toggles
        logger.info(
            f"   Round to schedule steps: {bool(getattr(args, 'round_training_timesteps', False))}"
        )
        logger.info(
            f"   Preserve distribution shape: {bool(getattr(args, 'preserve_distribution_shape', False))}"
        )
        logger.info(
            f"   Skip extra in-range constraint: {bool(getattr(args, 'skip_extra_timestep_constraint', False))}"
        )
        logger.info(
            f"   Constraint epsilon: {float(getattr(args, 'timestep_constraint_epsilon', 1e-6))}"
        )
        if bool(getattr(args, "preserve_distribution_shape", False)):
            logger.info(
                "   Shape preservation settings:"
                + f" weighting='{getattr(args, 'weighting_scheme', 'none')}',"
                + f" mode_scale={getattr(args, 'mode_scale', 1.29)},"
                + f" bell_center={getattr(args, 'bell_center', 0.5)},"
                + f" bell_std={getattr(args, 'bell_std', 0.2)},"
                + f" logit_mean={getattr(args, 'logit_mean', 0.0)},"
                + f" logit_std={getattr(args, 'logit_std', 1.0)}"
            )

        # Show compatibility status
        supported_methods = ["uniform", "sigmoid", "shift", "logit_normal"]
        if use_precomputed:
            if args.timestep_sampling in supported_methods:
                logger.info(f"   Compatibility: fully supported")
            else:
                logger.info(f"   Compatibility: will fall back to original sampling")
                if args.timestep_sampling == "flux_shift":
                    logger.info(
                        f"      Reason: Spatial dependency (image size affects timesteps)"
                    )
                elif args.timestep_sampling == "fopp":
                    logger.info(f"      Reason: Complex AR-Diffusion logic")

        logger.info("")  # Empty line for readability

    def _log_acceleration_configuration(self) -> None:
        """Log comprehensive acceleration configuration information."""
        args = self.args
        # Honor configured logging level
        level_str = str(getattr(args, "logging_level", "INFO")).upper()
        level_val = getattr(logging, level_str, logging.INFO)
        logger = get_logger(__name__, level=level_val)

        logger.info("üöÄ Acceleration Configuration:")

        # Check which acceleration techniques are enabled
        acceleration_methods = []

        if getattr(args, "sdpa", False):
            acceleration_methods.append("SDPA (Scaled Dot-Product Attention)")

        if getattr(args, "flash_attn", False):
            acceleration_methods.append("FlashAttention")

        if getattr(args, "sage_attn", False):
            acceleration_methods.append("SageAttention")

        if getattr(args, "xformers", False):
            acceleration_methods.append("Xformers")

        if getattr(args, "flash3", False):
            acceleration_methods.append("FlashAttention 3.0")

        if getattr(args, "split_attn", False):
            acceleration_methods.append("Split Attention")

        # Check mixed precision settings
        mixed_precision = getattr(args, "mixed_precision", "no")
        if mixed_precision != "no":
            acceleration_methods.append(f"Mixed Precision ({mixed_precision})")

        # Check full precision settings
        if getattr(args, "full_fp16", False):
            acceleration_methods.append("Full FP16")
        if getattr(args, "full_bf16", False):
            acceleration_methods.append("Full BF16")

        # Check FP8 settings
        if getattr(args, "fp8_scaled", False):
            acceleration_methods.append("FP8 Scaled")
        if getattr(args, "fp8_base", False):
            acceleration_methods.append("FP8 Base")
        if getattr(args, "fp8_t5", False):
            acceleration_methods.append("FP8 T5")

        # Check gradient checkpointing
        if getattr(args, "gradient_checkpointing", False):
            acceleration_methods.append("Gradient Checkpointing")

        # Check Dynamo settings
        dynamo_backend = getattr(args, "dynamo_backend", "NO")
        if dynamo_backend != "NO":
            acceleration_methods.append(f"Dynamo ({dynamo_backend})")

        # Log the results
        if acceleration_methods:
            logger.info("   ‚úÖ Enabled acceleration techniques:")
            for method in acceleration_methods:
                logger.info(f"      ‚Ä¢ {method}")
        else:
            logger.info("   ‚ö†Ô∏è  No acceleration techniques enabled")

        # Check for potential conflicts
        conflicts = []
        if getattr(args, "flash_attn", False) and getattr(args, "xformers", False):
            conflicts.append("FlashAttention and Xformers may conflict")
        if getattr(args, "flash_attn", False) and getattr(args, "sage_attn", False):
            conflicts.append("FlashAttention and SageAttention may conflict")
        if getattr(args, "xformers", False) and getattr(args, "sage_attn", False):
            conflicts.append("Xformers and SageAttention may conflict")

        if conflicts:
            logger.info("   ‚ö†Ô∏è  Potential conflicts detected:")
            for conflict in conflicts:
                logger.info(f"      ‚Ä¢ {conflict}")

        # Log device information
        device = getattr(args, "device", None)
        if device:
            logger.info(f"   üñ•Ô∏è  Target device: {device}")

        logger.info("")  # Empty line for readability

    def train_model(self) -> bool:
        """Run training operation"""
        logger.info("Starting Model Training...")

        # Log comprehensive timestep configuration
        self._log_timestep_configuration()

        # Log acceleration configuration
        self._log_acceleration_configuration()

        try:
            # Use the unified trainer directly
            trainer = WanNetworkTrainer()
            # Store config content in trainer for state saving
            trainer.original_config_content = self.config_content  # type: ignore
            trainer.original_config_path = self.config_path  # type: ignore
            trainer.train(self.args)

            logger.info("Training completed successfully!")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Error during training: {e}")
            return False

    def reload_config(self) -> bool:
        """Reload configuration from file"""
        logger.info("Reloading Configuration...")

        try:
            # Clean up existing temporary files and stop TensorBoard
            self.cleanup()

            # Reload configuration
            self.config, self.config_content = load_training_config(self.config_path)
            self.args = create_args_from_config(
                self.config, self.config_path, self.config_content
            )

            # Setup TensorBoard server with new configuration
            setup_tensorboard_if_enabled(self.args)

            logger.info(f"Configuration reloaded successfully from: {self.config_path}")
            logger.info("All settings have been updated.")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Error reloading configuration: {e}")
            return False

    def run(self):
        """Main loop for the unified trainer"""
        logger.info(f"Loaded configuration from: {self.config_path}")

        while True:
            choice = self.show_menu()

            if choice == "1":
                success = self.cache_latents()
                if not success:
                    logger.error(
                        "Latent caching failed. Please check the error messages above."
                    )
                    input("Press Enter to continue...")

            elif choice == "2":
                success = self.cache_text_encoder_outputs()
                if not success:
                    logger.error(
                        "Text encoder output caching failed. Please check the error messages above."
                    )
                    input("Press Enter to continue...")

            elif choice == "3":
                success = self.train_model()
                if not success:
                    logger.error(
                        "Training failed. Please check the error messages above."
                    )
                    input("Press Enter to continue...")

            elif choice == "4":
                try:
                    gb, details = _estimate_peak_vram_gb_from_config(self.config)
                    logger.info(
                        "üß† Estimated peak VRAM usage (per device): {:.2f} GB".format(
                            gb
                        )
                    )
                    logger.info(
                        "   Shape: B={} F={} H={} W={} ‚Üí lat {}x{} tokens={}".format(
                            details["batch_size"],
                            details["frames"],
                            details["height"],
                            details["width"],
                            details["lat_h"],
                            details["lat_w"],
                            details["tokens_per_sample"],
                        )
                    )
                    logger.info(
                        "   Precision={} ({} bytes/elem), checkpointing={}, control_lora={}, dual={} (offload_inactive_dit={})".format(
                            details["mixed_precision"],
                            details["bytes_per_elem"],
                            details["gradient_checkpointing"],
                            details["enable_control_lora"],
                            details["enable_dual_model_training"],
                            details["offload_inactive_dit"],
                        )
                    )
                    logger.info(
                        "   Breakdown: activations={:.2f} GB, latents={:.2f} GB, text={:.2f} GB, overhead‚âà{:.2f} GB".format(
                            details["activations_gb"],
                            details["latents_gb"],
                            details["text_gb"],
                            details["overhead_gb"],
                        )
                    )
                except Exception as e:
                    logger.exception(f"‚ùå Error estimating VRAM usage: {e}")
                input("Press Enter to continue...")

            elif choice == "5":
                success = self.reload_config()
                if not success:
                    logger.error(
                        "Config reload failed. Please check the error messages above."
                    )
                    input("Press Enter to continue...")

            elif choice == "6":
                success = self.free_vram_aggressively()
                if not success:
                    logger.error("VRAM cleanup failed.")
                input("Press Enter to continue...")

            elif choice == "7":
                logger.info("Returning to config selection menu...")
                sys.exit(100)

    def cleanup(self):
        """Clean up temporary files"""
        # No temp files to clean up anymore

        # Stop TensorBoard server if running
        stop_tensorboard_server()


def main():
    """Entry point: preserves interactive menu by default, adds non-interactive flags."""
    parser = argparse.ArgumentParser(
        description="Takenoko unified operations (interactive by default)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Config path: support positional or --config for convenience
    parser.add_argument("config", nargs="?", help="Path to training config TOML file")
    parser.add_argument(
        "--config", dest="config_opt", help="Path to training config TOML file"
    )

    # Non-interactive operation flags
    parser.add_argument(
        "--cache-latents", action="store_true", help="Run latent caching and exit"
    )
    parser.add_argument(
        "--cache-text-encoder",
        action="store_true",
        help="Run text encoder output caching and exit",
    )
    parser.add_argument("--train", action="store_true", help="Run training and exit")
    parser.add_argument(
        "--all",
        action="store_true",
        help="Run cache-latents, cache-text-encoder, then train",
    )
    parser.add_argument(
        "--non-interactive",
        action="store_true",
        help="Disable interactive menu; requires at least one action flag or --all",
    )

    args = parser.parse_args()

    # Resolve config path
    config_path = args.config_opt or args.config
    if not config_path:
        parser.error("missing config path (positional 'config' or --config)")

    # Validate config path early
    if not os.path.exists(config_path):
        logger.error(f"Error: Configuration file not found: {config_path}")
        sys.exit(1)

    # Determine requested actions
    action_flags = [args.cache_latents, args.cache_text_encoder, args.train, args.all]
    non_interactive_requested = args.non_interactive or any(action_flags)

    try:
        trainer = UnifiedTrainer(config_path)

        if non_interactive_requested:
            # Build ordered action list
            actions: List[str] = []
            if args.all:
                actions = ["cache_latents", "cache_text_encoder", "train"]
            else:
                if args.cache_latents:
                    actions.append("cache_latents")
                if args.cache_text_encoder:
                    actions.append("cache_text_encoder")
                if args.train:
                    actions.append("train")

            if len(actions) == 0:
                logger.error(
                    "--non-interactive requires at least one action flag or --all"
                )
                sys.exit(2)

            overall_success = True
            for action in actions:
                if action == "cache_latents":
                    success = trainer.cache_latents()
                elif action == "cache_text_encoder":
                    success = trainer.cache_text_encoder_outputs()
                elif action == "train":
                    success = trainer.train_model()
                else:
                    logger.error(f"Unknown action: {action}")
                    success = False

                if not success:
                    overall_success = False
                    break

            sys.exit(0 if overall_success else 1)
        else:
            # Preserve existing behavior: interactive menu
            trainer.run()

    except SystemExit:
        # bubble up argparse or explicit exits
        raise
    except Exception as e:
        logger.exception(f"üí• CRITICAL ERROR: {e}")
        sys.exit(1)
    finally:
        if "trainer" in locals():
            trainer.cleanup()


if __name__ == "__main__":
    main()
</file>

<file path="utils/device_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/device_utils.py (Apache)

import torch


def clean_memory_on_device(device):
    if device.type == "cuda":
        torch.cuda.empty_cache()
    elif device.type == "cpu":
        pass
    elif device.type == "mps":  # not tested
        torch.mps.empty_cache()


def synchronize_device(device: torch.device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "xpu":
        torch.xpu.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()
</file>

<file path="utils/ema.py">
## Based on: https://github.com/ostris/ai-toolkit/blob/main/toolkit/ema.py (MIT)

from __future__ import division
from __future__ import unicode_literals

from typing import Iterable, Optional
import weakref
import copy
import contextlib


import torch

from optimizers.optimizer_utils import copy_stochastic


# Partially based on:
# https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py
class ExponentialMovingAverage:
    """
    Maintains (exponential) moving average of a set of parameters.

    Args:
        parameters: Iterable of `torch.nn.Parameter` (typically from
            `model.parameters()`).
            Note that EMA is computed on *all* provided parameters,
            regardless of whether or not they have `requires_grad = True`;
            this allows a single EMA object to be consistantly used even
            if which parameters are trainable changes step to step.

            If you want to some parameters in the EMA, do not pass them
            to the object in the first place. For example:

                ExponentialMovingAverage(
                    parameters=[p for p in model.parameters() if p.requires_grad],
                    decay=0.9
                )

            will ignore parameters that do not require grad.

        decay: The exponential decay.

        use_num_updates: Whether to use number of updates when computing
            averages.
    """

    def __init__(
        self,
        parameters: Iterable[torch.nn.Parameter] = None,  # type: ignore
        decay: float = 0.995,
        use_num_updates: bool = False,
        # feeds back the decat to the parameter
        use_feedback: bool = False,
        param_multiplier: float = 1.0,
    ):
        if parameters is None:
            raise ValueError("parameters must be provided")
        if decay < 0.0 or decay > 1.0:
            raise ValueError("Decay must be between 0 and 1")
        self.decay = decay
        self.num_updates = 0 if use_num_updates else None
        self.use_feedback = use_feedback
        self.param_multiplier = param_multiplier
        parameters = list(parameters)
        self.shadow_params = [p.clone().detach() for p in parameters]
        self.collected_params = None
        self._is_train_mode = True
        # By maintaining only a weakref to each parameter,
        # we maintain the old GC behaviour of ExponentialMovingAverage:
        # if the model goes out of scope but the ExponentialMovingAverage
        # is kept, no references to the model or its parameters will be
        # maintained, and the model will be cleaned up.
        self._params_refs = [weakref.ref(p) for p in parameters]

    def _get_parameters(
        self, parameters: Optional[Iterable[torch.nn.Parameter]]
    ) -> Iterable[torch.nn.Parameter]:
        if parameters is None:
            parameters = [p() for p in self._params_refs]  # type: ignore
            if any(p is None for p in parameters):  # type: ignore
                raise ValueError(
                    "(One of) the parameters with which this "
                    "ExponentialMovingAverage "
                    "was initialized no longer exists (was garbage collected);"
                    " please either provide `parameters` explicitly or keep "
                    "the model to which they belong from being garbage "
                    "collected."
                )
            return parameters  # type: ignore
        else:
            parameters = list(parameters)
            if len(parameters) != len(self.shadow_params):
                raise ValueError(
                    "Number of parameters passed as argument is different "
                    "from number of shadow parameters maintained by this "
                    "ExponentialMovingAverage"
                )
            return parameters

    def update(self, parameters: Optional[Iterable[torch.nn.Parameter]] = None) -> None:
        """
        Update currently maintained parameters.

        Call this every time the parameters are updated, such as the result of
        the `optimizer.step()` call.

        Args:
            parameters: Iterable of `torch.nn.Parameter`; usually the same set of
                parameters used to initialize this object. If `None`, the
                parameters with which this `ExponentialMovingAverage` was
                initialized will be used.
        """
        parameters = self._get_parameters(parameters)
        decay = self.decay
        if self.num_updates is not None:
            self.num_updates += 1
            decay = min(decay, (1 + self.num_updates) / (10 + self.num_updates))
        one_minus_decay = 1.0 - decay
        with torch.no_grad():
            for s_param, param in zip(self.shadow_params, parameters):
                s_param_float = s_param.float()
                if s_param.dtype != torch.float32:
                    s_param_float = s_param_float.to(torch.float32)
                param_float = param
                if param.dtype != torch.float32:
                    param_float = param_float.to(torch.float32)
                tmp = s_param_float - param_float
                # tmp will be a new tensor so we can do in-place
                tmp.mul_(one_minus_decay)
                s_param_float.sub_(tmp)

                update_param = False
                if self.use_feedback:
                    # make feedback 10x decay
                    param_float.add_(tmp * 10)
                    update_param = True

                if self.param_multiplier != 1.0:
                    param_float.mul_(self.param_multiplier)
                    update_param = True

                if s_param.dtype != torch.float32:
                    copy_stochastic(s_param, s_param_float)

                if update_param and param.dtype != torch.float32:
                    copy_stochastic(param, param_float)

    def copy_to(
        self, parameters: Optional[Iterable[torch.nn.Parameter]] = None
    ) -> None:
        """
        Copy current averaged parameters into given collection of parameters.

        Args:
            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
                updated with the stored moving averages. If `None`, the
                parameters with which this `ExponentialMovingAverage` was
                initialized will be used.
        """
        parameters = self._get_parameters(parameters)
        for s_param, param in zip(self.shadow_params, parameters):
            param.data.copy_(s_param.data)

    def store(self, parameters: Optional[Iterable[torch.nn.Parameter]] = None) -> None:
        """
        Save the current parameters for restoring later.

        Args:
            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
                temporarily stored. If `None`, the parameters of with which this
                `ExponentialMovingAverage` was initialized will be used.
        """
        parameters = self._get_parameters(parameters)
        self.collected_params = [param.clone() for param in parameters]

    def restore(
        self, parameters: Optional[Iterable[torch.nn.Parameter]] = None
    ) -> None:
        """
        Restore the parameters stored with the `store` method.
        Useful to validate the model with EMA parameters without affecting the
        original optimization process. Store the parameters before the
        `copy_to` method. After validation (or model saving), use this to
        restore the former parameters.

        Args:
            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
                updated with the stored parameters. If `None`, the
                parameters with which this `ExponentialMovingAverage` was
                initialized will be used.
        """
        if self.collected_params is None:
            raise RuntimeError(
                "This ExponentialMovingAverage has no `store()`ed weights "
                "to `restore()`"
            )
        parameters = self._get_parameters(parameters)
        for c_param, param in zip(self.collected_params, parameters):
            param.data.copy_(c_param.data)

    @contextlib.contextmanager
    def average_parameters(
        self, parameters: Optional[Iterable[torch.nn.Parameter]] = None
    ):
        r"""
        Context manager for validation/inference with averaged parameters.

        Equivalent to:

            ema.store()
            ema.copy_to()
            try:
                ...
            finally:
                ema.restore()

        Args:
            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
                updated with the stored parameters. If `None`, the
                parameters with which this `ExponentialMovingAverage` was
                initialized will be used.
        """
        parameters = self._get_parameters(parameters)
        self.store(parameters)
        self.copy_to(parameters)
        try:
            yield
        finally:
            self.restore(parameters)

    def to(self, device=None, dtype=None) -> None:
        r"""Move internal buffers of the ExponentialMovingAverage to `device`.

        Args:
            device: like `device` argument to `torch.Tensor.to`
        """
        # .to() on the tensors handles None correctly
        self.shadow_params = [
            (
                p.to(device=device, dtype=dtype)
                if p.is_floating_point()
                else p.to(device=device)
            )
            for p in self.shadow_params
        ]
        if self.collected_params is not None:
            self.collected_params = [
                (
                    p.to(device=device, dtype=dtype)
                    if p.is_floating_point()
                    else p.to(device=device)
                )
                for p in self.collected_params
            ]
        return

    def state_dict(self) -> dict:
        r"""Returns the state of the ExponentialMovingAverage as a dict."""
        # Following PyTorch conventions, references to tensors are returned:
        # "returns a reference to the state and not its copy!" -
        # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict
        return {
            "decay": self.decay,
            "num_updates": self.num_updates,
            "shadow_params": self.shadow_params,
            "collected_params": self.collected_params,
        }

    def load_state_dict(self, state_dict: dict) -> None:
        r"""Loads the ExponentialMovingAverage state.

        Args:
            state_dict (dict): EMA state. Should be an object returned
                from a call to :meth:`state_dict`.
        """
        # deepcopy, to be consistent with module API
        state_dict = copy.deepcopy(state_dict)
        self.decay = state_dict["decay"]
        if self.decay < 0.0 or self.decay > 1.0:
            raise ValueError("Decay must be between 0 and 1")
        self.num_updates = state_dict["num_updates"]
        assert self.num_updates is None or isinstance(
            self.num_updates, int
        ), "Invalid num_updates"

        self.shadow_params = state_dict["shadow_params"]
        assert isinstance(self.shadow_params, list), "shadow_params must be a list"
        assert all(
            isinstance(p, torch.Tensor) for p in self.shadow_params
        ), "shadow_params must all be Tensors"

        self.collected_params = state_dict["collected_params"]
        if self.collected_params is not None:
            assert isinstance(
                self.collected_params, list
            ), "collected_params must be a list"
            assert all(
                isinstance(p, torch.Tensor) for p in self.collected_params
            ), "collected_params must all be Tensors"
            assert len(self.collected_params) == len(
                self.shadow_params
            ), "collected_params and shadow_params had different lengths"

        if len(self.shadow_params) == len(self._params_refs):
            # Consistant with torch.optim.Optimizer, cast things to consistant
            # device and dtype with the parameters
            params = [p() for p in self._params_refs]
            # If parameters have been garbage collected, just load the state
            # we were given without change.
            if not any(p is None for p in params):
                # ^ parameter references are still good
                for i, p in enumerate(params):
                    self.shadow_params[i] = self.shadow_params[i].to(
                        device=p.device, dtype=p.dtype  # type: ignore
                    )
                    if self.collected_params is not None:
                        self.collected_params[i] = self.collected_params[i].to(
                            device=p.device, dtype=p.dtype  # type: ignore
                        )
        else:
            raise ValueError(
                "Tried to `load_state_dict()` with the wrong number of "
                "parameters in the saved state."
            )

    def eval(self):
        if self._is_train_mode:
            with torch.no_grad():
                self.store()
                self.copy_to()
                self._is_train_mode = False

    def train(self):
        if not self._is_train_mode:
            with torch.no_grad():
                self.restore()
                self._is_train_mode = True
</file>

<file path="utils/fluxflow_augmentation.py">
## Based on: https://arxiv.org/abs/2503.15417

import torch
import random
import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def _fluxflow_perturb_single_video(
    video_frames: torch.Tensor,  # Expected shape (F, C, H, W) or (F, other_dims...)
    mode: str,
    frame_perturb_ratio: float,
    block_size: int,
    block_perturb_prob: float,
) -> torch.Tensor:
    """
    Applies FLUXFLOW temporal perturbation to a single video tensor.
    The first dimension is assumed to be the frame/time dimension.

    Args:
        video_frames: Input video tensor with shape (F, ...)
        mode: "frame" or "block"
        frame_perturb_ratio: Alpha - ratio of frames to shuffle in frame mode
        block_size: k - size of blocks in block mode
        block_perturb_prob: Beta - ratio of blocks to reorder in block mode (NOT probability!)
    """
    num_frames = video_frames.shape[0]
    if num_frames <= 1:  # Cannot perturb if 0 or 1 frame
        logger.info(f"FLUXFLOW: Skipping single video - only {num_frames} frame(s)")
        return video_frames

    if mode == "frame":
        # --- Frame-Level Perturbations (following paper Algorithm 1) ---
        num_to_shuffle = int(num_frames * frame_perturb_ratio)
        if num_to_shuffle <= 1:  # Need at least 2 frames to shuffle
            logger.info(
                f"FLUXFLOW: Frame mode - not enough frames to shuffle ({num_to_shuffle} calculated from {num_frames} frames)"
            )
            return video_frames

        logger.info(
            f"FLUXFLOW: Frame mode - shuffling {num_to_shuffle}/{num_frames} frames"
        )

        # Create index permutation following paper's approach
        all_indices = list(range(num_frames))
        indices_to_permute = sorted(random.sample(all_indices, num_to_shuffle))
        shuffled_subset = random.sample(indices_to_permute, len(indices_to_permute))

        new_order = list(range(num_frames))
        for i in range(num_to_shuffle):
            original_pos = indices_to_permute[i]
            new_val = shuffled_subset[i]
            new_order[original_pos] = new_val

        return video_frames[new_order]

    elif mode == "block":
        # --- Block-Level Perturbations (following paper Algorithm 1) ---
        if block_size <= 0:
            logger.warning(
                f"FLUXFLOW: Invalid block_size {block_size}, returning original video."
            )
            return video_frames
        if num_frames < block_size:  # Not enough frames for even one block
            logger.info(
                f"FLUXFLOW: Block mode - not enough frames ({num_frames}) for block size {block_size}"
            )
            return video_frames

        num_blocks = num_frames // block_size
        if num_blocks <= 1:  # Need at least 2 blocks to shuffle their order
            logger.info(
                f"FLUXFLOW: Block mode - only {num_blocks} block(s), need at least 2 for shuffling"
            )
            return video_frames

        # Calculate number of blocks to reorder (following paper's beta usage)
        num_blocks_to_reorder = int(num_blocks * block_perturb_prob)
        if num_blocks_to_reorder <= 1:
            logger.info(
                f"FLUXFLOW: Block mode - not enough blocks to reorder ({num_blocks_to_reorder} calculated from {num_blocks} blocks)"
            )
            return video_frames

        logger.info(
            f"FLUXFLOW: Block mode - reordering {num_blocks_to_reorder}/{num_blocks} blocks of size {block_size}"
        )

        # Create block index permutation following paper's approach
        all_block_indices = list(range(num_blocks))
        block_indices_to_permute = sorted(
            random.sample(all_block_indices, num_blocks_to_reorder)
        )
        shuffled_block_subset = random.sample(
            block_indices_to_permute, len(block_indices_to_permute)
        )

        new_block_order = list(range(num_blocks))
        for i in range(num_blocks_to_reorder):
            original_pos = block_indices_to_permute[i]
            new_val = shuffled_block_subset[i]
            new_block_order[original_pos] = new_val

        # Reconstruct video from reordered blocks
        v_perturbed_blocks = []
        for block_idx in new_block_order:
            start_frame = block_idx * block_size
            end_frame = start_frame + block_size
            v_perturbed_blocks.append(video_frames[start_frame:end_frame])

        # Concatenate reordered blocks
        perturbed_video = torch.cat(v_perturbed_blocks, dim=0)

        # Handle any remaining frames (if num_frames % block_size != 0)
        remaining_frames = num_frames % block_size
        if remaining_frames > 0:
            remaining_start = num_blocks * block_size
            remaining_frames_tensor = video_frames[remaining_start:]
            perturbed_video = torch.cat(
                [perturbed_video, remaining_frames_tensor], dim=0
            )

        return perturbed_video

    else:
        raise ValueError(
            f"FLUXFLOW: Unknown mode '{mode}'. Must be 'frame' or 'block'."
        )


def apply_fluxflow_to_batch(
    video_batch_tensor: torch.Tensor, fluxflow_config: dict
) -> torch.Tensor:
    """
    Applies FLUXFLOW temporal perturbation to a batch of video tensors.

    Args:
        video_batch_tensor: The input batch of video latents/frames.
        fluxflow_config: A dictionary with FLUXFLOW parameters:
            'mode': str, "frame" or "block"
            'frame_perturb_ratio': float, alpha - ratio for frame mode
            'block_size': int, k - block size for block mode
            'block_perturb_prob': float, beta - ratio of blocks to reorder (NOT probability!)
            'frame_dim_in_batch': int, index of the frame/time dimension in the batch tensor.
                                      Assumes batch dim is 0.

    Returns:
        The batch of temporally perturbed video tensors.
    """
    mode = fluxflow_config.get("mode", "frame")
    frame_perturb_ratio = fluxflow_config.get("frame_perturb_ratio", 0.25)
    block_size = fluxflow_config.get("block_size", 4)
    block_perturb_prob = fluxflow_config.get("block_perturb_prob", 0.5)
    frame_dim_in_batch = fluxflow_config.get(
        "frame_dim_in_batch", 2
    )  # Default for B,C,F,H,W

    logger.info(
        f"FLUXFLOW: apply_fluxflow_to_batch called - batch shape={video_batch_tensor.shape}, "
        f"mode={mode}, frame_dim={frame_dim_in_batch}, "
        f"frame_perturb_ratio={frame_perturb_ratio}, block_size={block_size}, block_perturb_ratio={block_perturb_prob}"
    )

    if not (0 <= frame_dim_in_batch < video_batch_tensor.ndim):
        raise ValueError(
            f"FLUXFLOW: frame_dim_in_batch {frame_dim_in_batch} is out of bounds "
            f"for tensor with {video_batch_tensor.ndim} dims."
        )

    # Permute to (B, F, ...) for easier iteration if frames are not already the second dimension (after batch dim 0)
    original_dims = list(range(video_batch_tensor.ndim))
    if (
        frame_dim_in_batch != 1
    ):  # If frame dimension is not at index 1 (after batch dim 0)
        permuted_dims = [original_dims[0]]  # Batch dim always first
        permuted_dims.append(
            original_dims[frame_dim_in_batch]
        )  # Bring frame dim to second position
        for i in range(
            1, len(original_dims)
        ):  # Add other dimensions, skipping original frame_dim_in_batch if it wasn't 0
            if i != frame_dim_in_batch:
                permuted_dims.append(original_dims[i])

        video_batch_bf_ellipsis = video_batch_tensor.permute(*permuted_dims)
    else:  # Frame dimension is already at index 1
        permuted_dims = original_dims  # No actual permutation needed for processing
        video_batch_bf_ellipsis = video_batch_tensor

    perturbed_video_list = []
    for i in range(video_batch_bf_ellipsis.shape[0]):  # Iterate over batch
        single_video_f_ellipsis = video_batch_bf_ellipsis[i]  # Shape (F, ...)
        perturbed_single_video = _fluxflow_perturb_single_video(
            single_video_f_ellipsis,
            mode,
            frame_perturb_ratio,
            block_size,
            block_perturb_prob,
        )
        perturbed_video_list.append(perturbed_single_video)

    perturbed_batch_bf_ellipsis = torch.stack(
        perturbed_video_list, dim=0
    )  # Stack along batch dimension

    # Permute back to original batch dimension order if a permutation occurred
    if frame_dim_in_batch != 1:
        # To get the inverse permutation:
        inv_permuted_dims = [0] * len(permuted_dims)
        for original_pos, permuted_val in enumerate(permuted_dims):
            inv_permuted_dims[permuted_val] = original_pos

        perturbed_batch_original_dims = perturbed_batch_bf_ellipsis.permute(
            *inv_permuted_dims
        )
    else:  # No permutation was done, so no inverse needed
        perturbed_batch_original_dims = perturbed_batch_bf_ellipsis

    return perturbed_batch_original_dims


def get_fluxflow_config_from_args(args) -> dict:
    """
    Extracts FLUXFLOW configuration parameters from args namespace
    and returns them as a dictionary.
    """
    return {
        "enable_fluxflow": getattr(args, "enable_fluxflow", False),
        "mode": getattr(args, "fluxflow_mode", "frame"),
        "frame_perturb_ratio": getattr(args, "fluxflow_frame_perturb_ratio", 0.25),
        "block_size": getattr(args, "fluxflow_block_size", 4),
        "block_perturb_prob": getattr(args, "fluxflow_block_perturb_prob", 0.5),
        "frame_dim_in_batch": getattr(args, "fluxflow_frame_dim_in_batch", 2),
    }


if __name__ == "__main__":
    # Example Usage and Basic Tests
    print("Running FLUXFLOW Augmentation tests...")

    # Test _fluxflow_perturb_single_video
    print("\n--- Testing _fluxflow_perturb_single_video ---")
    test_video_frames = torch.arange(10).view(10, 1)  # 10 frames, 1 channel
    print(f"Original video (frames 0-9):\n{test_video_frames.squeeze().tolist()}")

    # Frame mode test
    perturbed_frame_mode = _fluxflow_perturb_single_video(
        test_video_frames, "frame", 0.5, 4, 0.5
    )
    print(f"Frame mode (50% shuffle):\n{perturbed_frame_mode.squeeze().tolist()}")
    assert (
        not torch.equal(test_video_frames, perturbed_frame_mode)
        or test_video_frames.shape[0] <= 1
        or int(test_video_frames.shape[0] * 0.5) <= 1
    ), "Frame mode did not change video"

    # Block mode test (now using beta as ratio, not probability)
    # Use block_size=2 and beta=1.0 to ensure we get enough blocks to reorder
    perturbed_block_mode = _fluxflow_perturb_single_video(
        test_video_frames, "block", 0.25, 2, 1.0  # beta=1.0 means reorder all blocks
    )
    print(
        f"Block mode (block_size=2, beta=1.0):\n{perturbed_block_mode.squeeze().tolist()}"
    )
    num_blocks = test_video_frames.shape[0] // 2  # 10 // 2 = 5 blocks
    assert not torch.equal(test_video_frames, perturbed_block_mode) or (
        num_blocks <= 1
    ), "Block mode did not change video"

    # Test with fewer frames than block size
    short_video = torch.arange(2).view(2, 1)
    perturbed_short_block = _fluxflow_perturb_single_video(
        short_video, "block", 0.25, 3, 1.0
    )
    assert torch.equal(
        short_video, perturbed_short_block
    ), "Block mode changed video with < block_size frames"
    print(
        f"Block mode (short video, block_size=3): {perturbed_short_block.squeeze().tolist()}"
    )

    # Test apply_fluxflow_to_batch
    print("\n--- Testing apply_fluxflow_to_batch ---")
    # B, C, F, H, W -> Frame dim = 2
    test_batch_bcfhw = torch.stack(
        [torch.arange(i * 10, (i + 1) * 10).view(1, 10, 1, 1) for i in range(2)], dim=0
    )  # Batch=2, C=1, F=10, H=1, W=1
    print(
        f"Original batch (B,C,F,H,W), frame_dim=2, Video 0 (0-9), Video 1 (10-19):\n{test_batch_bcfhw.squeeze().tolist()}"
    )

    config_bcfhw = {
        "mode": "frame",
        "frame_perturb_ratio": 0.5,
        "block_size": 2,
        "block_perturb_prob": 1.0,  # Now interpreted as ratio, not probability
        "frame_dim_in_batch": 2,
    }
    perturbed_batch_bcfhw = apply_fluxflow_to_batch(
        test_batch_bcfhw.clone(), config_bcfhw
    )
    print(
        f"Perturbed batch (B,C,F,H,W), frame_dim=2:\n{perturbed_batch_bcfhw.squeeze().tolist()}"
    )
    assert not torch.equal(
        test_batch_bcfhw, perturbed_batch_bcfhw
    ), "Batch B,C,F,H,W did not change"

    # B, F, C, H, W -> Frame dim = 1
    test_batch_bfchw = test_batch_bcfhw.permute(0, 2, 1, 3, 4)  # Permute to B,F,C,H,W
    print(
        f"Original batch (B,F,C,H,W), frame_dim=1, Video 0 (0-9), Video 1 (10-19):\n{test_batch_bfchw.squeeze().tolist()}"
    )
    config_bfchw = {
        "mode": "block",
        "frame_perturb_ratio": 0.3,
        "block_size": 2,
        "block_perturb_prob": 1.0,  # Now interpreted as ratio, not probability
        "frame_dim_in_batch": 1,
    }
    perturbed_batch_bfchw = apply_fluxflow_to_batch(
        test_batch_bfchw.clone(), config_bfchw
    )
    print(
        f"Perturbed batch (B,F,C,H,W), frame_dim=1:\n{perturbed_batch_bfchw.squeeze().tolist()}"
    )
    assert not torch.equal(
        test_batch_bfchw, perturbed_batch_bfchw
    ), "Batch B,F,C,H,W did not change"
    assert (
        perturbed_batch_bfchw.shape == test_batch_bfchw.shape
    ), "Shape changed for B,F,C,H,W"

    print("\nAll basic tests completed.")
</file>

<file path="utils/lora_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/lora_utils.py (Apache)

import os
import re
from typing import Dict, List, Optional, Union
import torch

import logging
from modules.fp8_optimization_utils import (
    load_safetensors_with_fp8_optimization,
    optimize_state_dict_with_fp8,
)
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

from tqdm import tqdm

from utils.safetensors_utils import (
    MemoryEfficientSafeOpen,
    load_safetensors,
)


def filter_lora_state_dict(
    weights_sd: Dict[str, torch.Tensor],
    include_pattern: Optional[str] = None,
    exclude_pattern: Optional[str] = None,
) -> Dict[str, torch.Tensor]:
    # apply include/exclude patterns
    original_key_count = len(weights_sd.keys())
    if include_pattern is not None:
        regex_include = re.compile(include_pattern)
        weights_sd = {k: v for k, v in weights_sd.items() if regex_include.search(k)}
        logger.info(
            f"Filtered keys with include pattern {include_pattern}: {original_key_count} -> {len(weights_sd.keys())}"
        )

    if exclude_pattern is not None:
        original_key_count_ex = len(weights_sd.keys())
        regex_exclude = re.compile(exclude_pattern)
        weights_sd = {
            k: v for k, v in weights_sd.items() if not regex_exclude.search(k)
        }
        logger.info(
            f"Filtered keys with exclude pattern {exclude_pattern}: {original_key_count_ex} -> {len(weights_sd.keys())}"
        )

    if len(weights_sd) != original_key_count:
        remaining_keys = list(set([k.split(".", 1)[0] for k in weights_sd.keys()]))
        remaining_keys.sort()
        logger.info(f"Remaining LoRA modules after filtering: {remaining_keys}")
        if len(weights_sd) == 0:
            logger.warning(f"No keys left after filtering.")

    return weights_sd


def load_safetensors_with_lora_and_fp8(
    model_files: Union[str, List[str]],
    lora_weights_list: Optional[Dict[str, torch.Tensor]],
    lora_multipliers: Optional[List[float]],
    fp8_optimization: bool,
    calc_device: torch.device,
    move_to_device: bool = False,
    dit_weight_dtype: Optional[torch.dtype] = None,
    target_keys: Optional[List[str]] = None,
    exclude_keys: Optional[List[str]] = None,
) -> dict[str, torch.Tensor]:
    """
    Merge LoRA weights into the state dict of a model with fp8 optimization if needed.

    Args:
        model_files (Union[str, List[str]]): Path to the model file or list of paths. If the path matches a pattern like `00001-of-00004`, it will load all files with the same prefix.
        lora_weights_list (Optional[Dict[str, torch.Tensor]]): Dictionary of LoRA weight tensors to load.
        lora_multipliers (Optional[List[float]]): List of multipliers for LoRA weights.
        fp8_optimization (bool): Whether to apply FP8 optimization.
        calc_device (torch.device): Device to calculate on.
        move_to_device (bool): Whether to move tensors to the calculation device after loading.
        target_keys (Optional[List[str]]): Keys to target for optimization.
        exclude_keys (Optional[List[str]]): Keys to exclude from optimization.
    """

    # if the file name ends with 00001-of-00004 etc, we need to load the files with the same prefix
    if isinstance(model_files, str):
        model_files = [model_files]

    extended_model_files = []
    for model_file in model_files:
        basename = os.path.basename(model_file)
        match = re.match(r"^(.*?)(\d+)-of-(\d+)\.safetensors$", basename)
        if match:
            prefix = basename[: match.start(2)]
            count = int(match.group(3))
            state_dict = {}
            for i in range(count):
                filename = f"{prefix}{i+1:05d}-of-{count:05d}.safetensors"
                filepath = os.path.join(os.path.dirname(model_file), filename)
                if os.path.exists(filepath):
                    extended_model_files.append(filepath)
                else:
                    raise FileNotFoundError(f"File {filepath} not found")
        else:
            extended_model_files.append(model_file)
    model_files = extended_model_files
    logger.info(f"Loading model files: {model_files}")

    # load LoRA weights
    weight_hook = None
    if lora_weights_list is None or len(lora_weights_list) == 0:
        lora_weights_list = []  # type: ignore
        lora_multipliers = []
        list_of_lora_weight_keys = []
    else:
        list_of_lora_weight_keys = []
        for lora_sd in lora_weights_list:
            lora_weight_keys = set(lora_sd.keys())  # type: ignore
            list_of_lora_weight_keys.append(lora_weight_keys)

        if lora_multipliers is None:
            lora_multipliers = [1.0] * len(lora_weights_list)
        while len(lora_multipliers) < len(lora_weights_list):
            lora_multipliers.append(1.0)
        if len(lora_multipliers) > len(lora_weights_list):
            lora_multipliers = lora_multipliers[: len(lora_weights_list)]

        # Merge LoRA weights into the state dict
        logger.info(
            f"Merging LoRA weights into state dict. multipliers: {lora_multipliers}"
        )

        # make hook for LoRA merging
        def weight_hook_func(model_weight_key, model_weight):
            nonlocal list_of_lora_weight_keys, lora_weights_list, lora_multipliers, calc_device

            if not model_weight_key.endswith(".weight"):
                return model_weight

            original_device = model_weight.device
            if original_device != calc_device:
                model_weight = model_weight.to(
                    calc_device
                )  # to make calculation faster

            for lora_weight_keys, lora_sd, multiplier in zip(
                list_of_lora_weight_keys, lora_weights_list, lora_multipliers  # type: ignore
            ):
                # check if this weight has LoRA weights
                lora_name = model_weight_key.rsplit(".", 1)[
                    0
                ]  # remove trailing ".weight"
                lora_name = "lora_unet_" + lora_name.replace(".", "_")
                down_key = lora_name + ".lora_down.weight"
                up_key = lora_name + ".lora_up.weight"
                alpha_key = lora_name + ".alpha"
                if down_key not in lora_weight_keys or up_key not in lora_weight_keys:
                    continue

                # get LoRA weights
                down_weight = lora_sd[down_key]
                up_weight = lora_sd[up_key]

                dim = down_weight.size()[0]
                alpha = lora_sd.get(alpha_key, dim)
                scale = alpha / dim

                down_weight = down_weight.to(calc_device)
                up_weight = up_weight.to(calc_device)

                # W <- W + U * D
                if len(model_weight.size()) == 2:
                    # linear
                    if len(up_weight.size()) == 4:  # use linear projection mismatch
                        up_weight = up_weight.squeeze(3).squeeze(2)
                        down_weight = down_weight.squeeze(3).squeeze(2)
                    model_weight = (
                        model_weight + multiplier * (up_weight @ down_weight) * scale
                    )
                elif down_weight.size()[2:4] == (1, 1):
                    # conv2d 1x1
                    model_weight = (
                        model_weight
                        + multiplier
                        * (
                            up_weight.squeeze(3).squeeze(2)
                            @ down_weight.squeeze(3).squeeze(2)
                        )
                        .unsqueeze(2)
                        .unsqueeze(3)
                        * scale
                    )
                else:
                    # conv2d 3x3
                    conved = torch.nn.functional.conv2d(
                        down_weight.permute(1, 0, 2, 3), up_weight
                    ).permute(1, 0, 2, 3)
                    # logger.info(conved.size(), weight.size(), module.stride, module.padding)
                    model_weight = model_weight + multiplier * conved * scale

                # remove LoRA keys from set
                lora_weight_keys.remove(down_key)
                lora_weight_keys.remove(up_key)
                if alpha_key in lora_weight_keys:
                    lora_weight_keys.remove(alpha_key)

            model_weight = model_weight.to(
                original_device
            )  # move back to original device
            return model_weight

        weight_hook = weight_hook_func

    state_dict = load_safetensors_with_fp8_optimization_and_hook(
        model_files,
        fp8_optimization,
        calc_device,
        move_to_device,
        dit_weight_dtype,
        target_keys,
        exclude_keys,
        weight_hook=weight_hook,
    )

    for lora_weight_keys in list_of_lora_weight_keys:
        # check if all LoRA keys are used
        if len(lora_weight_keys) > 0:
            # if there are still LoRA keys left, it means they are not used in the model
            # this is a warning, not an error
            logger.warning(
                f"Warning: not all LoRA keys are used: {', '.join(lora_weight_keys)}"
            )

    return state_dict


def load_safetensors_with_fp8_optimization_and_hook(
    model_files: list[str],
    fp8_optimization: bool,
    calc_device: torch.device,
    move_to_device: bool = False,
    dit_weight_dtype: Optional[torch.dtype] = None,
    target_keys: Optional[List[str]] = None,
    exclude_keys: Optional[List[str]] = None,
    weight_hook: callable = None,  # type: ignore
) -> dict[str, torch.Tensor]:
    """
    Load state dict from safetensors files and merge LoRA weights into the state dict with fp8 optimization if needed.
    """
    if fp8_optimization:
        logger.info(
            f"Loading state dict with FP8 optimization. Hook enabled: {weight_hook is not None}"
        )

        # dit_weight_dtype is not used because we use fp8 optimization
        state_dict = load_safetensors_with_fp8_optimization(
            model_files,
            calc_device,
            target_keys,
            exclude_keys,
            move_to_device=move_to_device,
            weight_hook=weight_hook,
        )
    else:
        logger.info(
            f"Loading state dict without FP8 optimization. Hook enabled: {weight_hook is not None}"
        )
        state_dict = {}
        for model_file in model_files:
            with MemoryEfficientSafeOpen(model_file) as f:
                for key in tqdm(
                    f.keys(),
                    desc=f"Loading {os.path.basename(model_file)}",
                    leave=False,
                ):
                    value = f.get_tensor(key)
                    if weight_hook is not None:
                        value = weight_hook(key, value)
                    if move_to_device:
                        if dit_weight_dtype is None:
                            value = value.to(calc_device)
                        else:
                            value = value.to(calc_device, dtype=dit_weight_dtype)
                    elif dit_weight_dtype is not None:
                        value = value.to(dit_weight_dtype)
                    state_dict[key] = value

    return state_dict
</file>

<file path="utils/memory_utils.py">
from __future__ import annotations

import os
import logging
from typing import Any, Dict, Optional


def configure_cuda_allocator_from_config(
    config: Dict[str, Any], logger: Optional[logging.Logger] = None
) -> None:
    """Configure PyTorch CUDA caching allocator via environment variable.

    Reads top-level keys (recommended) usually placed under the OPTIMIZATION section
    of the TOML. Legacy [cuda_allocator] table is no longer supported.

    Supported keys (top-level):
      - cuda_allocator_enable: bool
      - cuda_allocator_max_split_size_mb: int > 0
      - cuda_allocator_expandable_segments: bool

    If PYTORCH_CUDA_ALLOC_CONF is already set in the environment, this function
    will not override it.
    """

    log = logger or logging.getLogger(__name__)

    # Respect existing environment
    if os.environ.get("PYTORCH_CUDA_ALLOC_CONF"):
        log.info("Using existing PYTORCH_CUDA_ALLOC_CONF from environment")
        return

    # Preferred: top-level keys
    enable = bool(config.get("cuda_allocator_enable", False))
    max_split_raw = config.get("cuda_allocator_max_split_size_mb")
    expandable_raw = config.get("cuda_allocator_expandable_segments")

    if not enable:
        return

    options: list[str] = []

    # expandable_segments
    if expandable_raw is not None:
        try:
            expandable_val = bool(expandable_raw)
            options.append(
                f"expandable_segments:{'True' if expandable_val else 'False'}"
            )
        except Exception:
            # ignore invalid value
            pass

    # max_split_size_mb
    if max_split_raw is not None:
        try:
            msz = int(max_split_raw)  # type: ignore[arg-type]
            if msz > 0:
                options.append(f"max_split_size_mb:{msz}")
        except Exception:
            # ignore invalid value
            pass

    if not options:
        return

    env_value = ",".join(options)
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = env_value
    log.info(f"Set PYTORCH_CUDA_ALLOC_CONF='{env_value}' from config")
</file>

<file path="utils/model_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/model_utils.py (Apache)

import hashlib
from io import BytesIO
from typing import Optional

import safetensors.torch
import torch


def model_hash(filename):
    """Old model hash used by stable-diffusion-webui"""
    try:
        with open(filename, "rb") as file:
            m = hashlib.sha256()

            file.seek(0x100000)
            m.update(file.read(0x10000))
            return m.hexdigest()[0:8]
    except FileNotFoundError:
        return "NOFILE"
    except IsADirectoryError:  # Linux?
        return "IsADirectory"
    except PermissionError:  # Windows
        return "IsADirectory"


def calculate_sha256(filename):
    """New model hash used by stable-diffusion-webui"""
    try:
        hash_sha256 = hashlib.sha256()
        blksize = 1024 * 1024

        with open(filename, "rb") as f:
            for chunk in iter(lambda: f.read(blksize), b""):
                hash_sha256.update(chunk)

        return hash_sha256.hexdigest()
    except FileNotFoundError:
        return "NOFILE"
    except IsADirectoryError:  # Linux?
        return "IsADirectory"
    except PermissionError:  # Windows
        return "IsADirectory"


def addnet_hash_legacy(b):
    """Old model hash used by sd-webui-additional-networks for .safetensors format files"""
    m = hashlib.sha256()

    b.seek(0x100000)
    m.update(b.read(0x10000))
    return m.hexdigest()[0:8]


def addnet_hash_safetensors(b):
    """New model hash used by sd-webui-additional-networks for .safetensors format files"""
    hash_sha256 = hashlib.sha256()
    blksize = 1024 * 1024

    b.seek(0)
    header = b.read(8)
    n = int.from_bytes(header, "little")

    offset = n + 8
    b.seek(offset)
    for chunk in iter(lambda: b.read(blksize), b""):
        hash_sha256.update(chunk)

    return hash_sha256.hexdigest()


def precalculate_safetensors_hashes(tensors, metadata):
    """Precalculate the model hashes needed by sd-webui-additional-networks to
    save time on indexing the model later."""

    # Because writing user metadata to the file can change the result of
    # sd_models.model_hash(), only retain the training metadata for purposes of
    # calculating the hash, as they are meant to be immutable
    metadata = {k: v for k, v in metadata.items() if k.startswith("ss_")}

    bytes = safetensors.torch.save(tensors, metadata)
    b = BytesIO(bytes)

    model_hash = addnet_hash_safetensors(b)
    legacy_hash = addnet_hash_legacy(b)
    return model_hash, legacy_hash


def dtype_to_str(dtype: torch.dtype) -> str:
    # get name of the dtype
    dtype_name = str(dtype).split(".")[-1]
    return dtype_name


def str_to_dtype(
    s: Optional[str], default_dtype: Optional[torch.dtype] = None
) -> torch.dtype:
    """
    Convert a string to a torch.dtype

    Args:
        s: string representation of the dtype
        default_dtype: default dtype to return if s is None

    Returns:
        torch.dtype: the corresponding torch.dtype

    Raises:
        ValueError: if the dtype is not supported

    Examples:
        >>> str_to_dtype("float32")
        torch.float32
        >>> str_to_dtype("fp32")
        torch.float32
        >>> str_to_dtype("float16")
        torch.float16
        >>> str_to_dtype("fp16")
        torch.float16
        >>> str_to_dtype("bfloat16")
        torch.bfloat16
        >>> str_to_dtype("bf16")
        torch.bfloat16
        >>> str_to_dtype("fp8")
        torch.float8_e4m3fn
        >>> str_to_dtype("fp8_e4m3fn")
        torch.float8_e4m3fn
        >>> str_to_dtype("fp8_e4m3fnuz")
        torch.float8_e4m3fnuz
        >>> str_to_dtype("fp8_e5m2")
        torch.float8_e5m2
        >>> str_to_dtype("fp8_e5m2fnuz")
        torch.float8_e5m2fnuz
    """
    if s is None:
        return default_dtype  # type: ignore
    if s in ["bf16", "bfloat16"]:
        return torch.bfloat16
    elif s in ["fp16", "float16"]:
        return torch.float16
    elif s in ["fp32", "float32", "float"]:
        return torch.float32
    elif s in ["fp8_e4m3fn", "e4m3fn", "float8_e4m3fn"]:
        return torch.float8_e4m3fn
    elif s in ["fp8_e4m3fnuz", "e4m3fnuz", "float8_e4m3fnuz"]:
        return torch.float8_e4m3fnuz
    elif s in ["fp8_e5m2", "e5m2", "float8_e5m2"]:
        return torch.float8_e5m2
    elif s in ["fp8_e5m2fnuz", "e5m2fnuz", "float8_e5m2fnuz"]:
        return torch.float8_e5m2fnuz
    elif s in ["fp8", "float8"]:
        return torch.float8_e4m3fn  # default fp8
    else:
        raise ValueError(f"Unsupported dtype: {s}")
</file>

<file path="utils/regularization_utils.py">
"""
Regularization utilities for training with regularization datasets.

This module provides utilities for handling regularization datasets and loss weighting
in the training process. It follows the battle-tested implementation from kohya-ss/sd-scripts.
"""

import logging
from typing import Dict, Any, Optional
import torch

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def apply_regularization_weights(
    loss: torch.Tensor, batch: Dict[str, Any], device: torch.device, dtype: torch.dtype
) -> torch.Tensor:
    """
    Apply regularization weights to the loss tensor.

    Args:
        loss: The computed loss tensor
        batch: The batch dictionary containing weights
        device: Target device for the weights
        dtype: Target dtype for the weights

    Returns:
        The weighted loss tensor
    """
    # Get the per-sample weights from the batch
    sample_weights = batch.get("weight", None)

    if sample_weights is not None:
        sample_weights = sample_weights.to(device=device, dtype=dtype)

        # Reshape weights to broadcast with loss
        while sample_weights.dim() < loss.dim():
            sample_weights = sample_weights.unsqueeze(-1)

        # Apply weights to loss
        loss = loss * sample_weights

        # Log regularization usage if weights are not all 1.0
        if torch.any(sample_weights != 1.0):
            reg_count = torch.sum(sample_weights != 1.0).item()
            total_count = sample_weights.numel()
            logger.debug(
                f"Applied regularization weights to {reg_count}/{total_count} samples"
            )

    return loss


def log_regularization_info(dataset_group: Any) -> None:
    """
    Log information about regularization datasets in the dataset group.

    Args:
        dataset_group: The dataset group to analyze
    """
    if not hasattr(dataset_group, "datasets"):
        return

    reg_datasets = []
    normal_datasets = []

    for dataset in dataset_group.datasets:
        if hasattr(dataset, "is_reg") and dataset.is_reg:
            reg_datasets.append(dataset)
        else:
            normal_datasets.append(dataset)

    if reg_datasets:
        logger.info("üìä Regularization Dataset Configuration:")
        logger.info(f"   üéØ Regularization datasets: {len(reg_datasets)}")
        logger.info(f"   üìö Normal datasets: {len(normal_datasets)}")

        for i, dataset in enumerate(reg_datasets):
            dataset_id = getattr(
                dataset, "get_dataset_identifier", lambda: f"Dataset_{i}"
            )()
            logger.info(f"      ‚Ä¢ {dataset_id} (regularization)")

        for i, dataset in enumerate(normal_datasets):
            dataset_id = getattr(
                dataset, "get_dataset_identifier", lambda: f"Dataset_{i}"
            )()
            logger.info(f"      ‚Ä¢ {dataset_id} (normal)")
    else:
        logger.info("üìä No regularization datasets found - using standard training")


def validate_regularization_config(args: Any) -> None:
    """
    Validate regularization configuration.

    Args:
        args: Training arguments containing regularization settings
    """
    prior_loss_weight = getattr(args, "prior_loss_weight", 1.0)

    if prior_loss_weight != 1.0:
        logger.info(f"üîß Regularization weight: {prior_loss_weight}")

        if prior_loss_weight < 0:
            logger.warning(
                "‚ö†Ô∏è  Negative regularization weight detected - this may cause training instability"
            )
        elif prior_loss_weight > 10:
            logger.warning(
                "‚ö†Ô∏è  High regularization weight detected - this may cause over-regularization"
            )
    else:
        logger.info("üîß Using default regularization weight (1.0)")
</file>

<file path="utils/safetensors_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/safetensors_utils.py (Apache)

import os
import re
import torch
import json
import struct
from typing import Dict, Any, Union, Optional

from safetensors.torch import load_file


def mem_eff_save_file(
    tensors: Dict[str, torch.Tensor], filename: str, metadata: Dict[str, Any] = None  # type: ignore
):
    """
    memory efficient save file
    """

    _TYPES = {
        torch.float64: "F64",
        torch.float32: "F32",
        torch.float16: "F16",
        torch.bfloat16: "BF16",
        torch.int64: "I64",
        torch.int32: "I32",
        torch.int16: "I16",
        torch.int8: "I8",
        torch.uint8: "U8",
        torch.bool: "BOOL",
        getattr(torch, "float8_e5m2", None): "F8_E5M2",
        getattr(torch, "float8_e4m3fn", None): "F8_E4M3",
    }
    _ALIGN = 256

    def validate_metadata(metadata: Dict[str, Any]) -> Dict[str, str]:
        validated = {}
        for key, value in metadata.items():
            if not isinstance(key, str):
                raise ValueError(f"Metadata key must be a string, got {type(key)}")
            if not isinstance(value, str):
                print(
                    f"Warning: Metadata value for key '{key}' is not a string. Converting to string."
                )
                validated[key] = str(value)
            else:
                validated[key] = value
        return validated

    # print(f"Using memory efficient save file: {filename}")

    header = {}
    offset = 0
    if metadata:
        header["__metadata__"] = validate_metadata(metadata)
    for k, v in tensors.items():
        if v.numel() == 0:  # empty tensor
            header[k] = {
                "dtype": _TYPES[v.dtype],
                "shape": list(v.shape),
                "data_offsets": [offset, offset],
            }
        else:
            size = v.numel() * v.element_size()
            header[k] = {
                "dtype": _TYPES[v.dtype],
                "shape": list(v.shape),
                "data_offsets": [offset, offset + size],
            }
            offset += size

    hjson = json.dumps(header).encode("utf-8")
    hjson += b" " * (-(len(hjson) + 8) % _ALIGN)

    with open(filename, "wb") as f:
        f.write(struct.pack("<Q", len(hjson)))
        f.write(hjson)

        for k, v in tensors.items():
            if v.numel() == 0:
                continue
            if v.is_cuda:
                # Direct GPU to disk save
                with torch.cuda.device(v.device):
                    if (
                        v.dim() == 0
                    ):  # if scalar, need to add a dimension to work with view
                        v = v.unsqueeze(0)
                    tensor_bytes = v.contiguous().view(torch.uint8)
                    tensor_bytes.cpu().numpy().tofile(f)
            else:
                # CPU tensor save
                if v.dim() == 0:  # if scalar, need to add a dimension to work with view
                    v = v.unsqueeze(0)
                v.contiguous().view(torch.uint8).numpy().tofile(f)


class MemoryEfficientSafeOpen:
    # does not support metadata loading
    def __init__(self, filename):
        self.filename = filename
        self.file = open(filename, "rb")
        self.header, self.header_size = self._read_header()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.file.close()

    def keys(self):
        return [k for k in self.header.keys() if k != "__metadata__"]

    def metadata(self) -> Dict[str, str]:
        return self.header.get("__metadata__", {})

    def get_tensor(self, key):
        if key not in self.header:
            raise KeyError(f"Tensor '{key}' not found in the file")

        metadata = self.header[key]
        offset_start, offset_end = metadata["data_offsets"]

        if offset_start == offset_end:
            tensor_bytes = None
        else:
            # adjust offset by header size
            self.file.seek(self.header_size + 8 + offset_start)
            tensor_bytes = self.file.read(offset_end - offset_start)

        return self._deserialize_tensor(tensor_bytes, metadata)

    def _read_header(self):
        header_size = struct.unpack("<Q", self.file.read(8))[0]
        header_json = self.file.read(header_size).decode("utf-8")
        return json.loads(header_json), header_size

    def _deserialize_tensor(self, tensor_bytes, metadata):
        dtype = self._get_torch_dtype(metadata["dtype"])
        shape = metadata["shape"]

        if tensor_bytes is None:
            byte_tensor = torch.empty(0, dtype=torch.uint8)
        else:
            tensor_bytes = bytearray(tensor_bytes)  # make it writable
            byte_tensor = torch.frombuffer(tensor_bytes, dtype=torch.uint8)

        # process float8 types
        if metadata["dtype"] in ["F8_E5M2", "F8_E4M3"]:
            return self._convert_float8(byte_tensor, metadata["dtype"], shape)

        # convert to the target dtype and reshape
        return byte_tensor.view(dtype).reshape(shape)  # type: ignore

    @staticmethod
    def _get_torch_dtype(dtype_str):
        dtype_map = {
            "F64": torch.float64,
            "F32": torch.float32,
            "F16": torch.float16,
            "BF16": torch.bfloat16,
            "I64": torch.int64,
            "I32": torch.int32,
            "I16": torch.int16,
            "I8": torch.int8,
            "U8": torch.uint8,
            "BOOL": torch.bool,
        }
        # add float8 types if available
        if hasattr(torch, "float8_e5m2"):
            dtype_map["F8_E5M2"] = torch.float8_e5m2
        if hasattr(torch, "float8_e4m3fn"):
            dtype_map["F8_E4M3"] = torch.float8_e4m3fn
        return dtype_map.get(dtype_str)

    @staticmethod
    def _convert_float8(byte_tensor, dtype_str, shape):
        if dtype_str == "F8_E5M2" and hasattr(torch, "float8_e5m2"):
            return byte_tensor.view(torch.float8_e5m2).reshape(shape)
        elif dtype_str == "F8_E4M3" and hasattr(torch, "float8_e4m3fn"):
            return byte_tensor.view(torch.float8_e4m3fn).reshape(shape)
        else:
            # # convert to float16 if float8 is not supported
            # print(f"Warning: {dtype_str} is not supported in this PyTorch version. Converting to float16.")
            # return byte_tensor.view(torch.uint8).to(torch.float16).reshape(shape)
            raise ValueError(
                f"Unsupported float8 type: {dtype_str} (upgrade PyTorch to support float8 types)"
            )


def load_safetensors(
    path: str,
    device: Union[str, torch.device],
    disable_mmap: bool = False,
    dtype: Optional[torch.dtype] = None,
) -> dict[str, torch.Tensor]:
    if disable_mmap:
        # return safetensors.torch.load(open(path, "rb").read())
        # use experimental loader
        # logger.info(f"Loading without mmap (experimental)")
        state_dict = {}
        with MemoryEfficientSafeOpen(path) as f:
            for key in f.keys():
                state_dict[key] = f.get_tensor(key).to(device, dtype=dtype)
        return state_dict
    else:
        try:
            state_dict = load_file(path, device=device)  # type: ignore
        except:
            state_dict = load_file(path)  # prevent device invalid Error
        if dtype is not None:
            for key in state_dict.keys():
                state_dict[key] = state_dict[key].to(dtype=dtype)
        return state_dict


def load_split_weights(
    file_path: str, device: Union[str, torch.device] = "cpu", disable_mmap: bool = False
) -> Dict[str, torch.Tensor]:
    """
    Load split weights from a file. If the file name ends with 00001-of-00004 etc, it will load all files with the same prefix.
    dtype is as is, no conversion is done.
    """
    device = torch.device(device)

    # if the file name ends with 00001-of-00004 etc, we need to load the files with the same prefix
    basename = os.path.basename(file_path)
    match = re.match(r"^(.*?)(\d+)-of-(\d+)\.safetensors$", basename)
    if match:
        prefix = basename[: match.start(2)]
        count = int(match.group(3))
        state_dict = {}
        for i in range(count):
            filename = f"{prefix}{i+1:05d}-of-{count:05d}.safetensors"
            filepath = os.path.join(os.path.dirname(file_path), filename)
            if os.path.exists(filepath):
                state_dict.update(
                    load_safetensors(filepath, device=device, disable_mmap=disable_mmap)
                )
            else:
                raise FileNotFoundError(f"File {filepath} not found")
    else:
        state_dict = load_safetensors(
            file_path, device=device, disable_mmap=disable_mmap
        )
    return state_dict


def extract_config_from_metadata(metadata: Dict[str, str]) -> Optional[str]:
    """Extract the original config content from safetensors metadata."""
    return metadata.get("takenoko_config_content")


def extract_config_file_from_metadata(metadata: Dict[str, str]) -> Optional[str]:
    """Extract the original config filename from safetensors metadata."""
    return metadata.get("takenoko_config_file")


def load_metadata_from_safetensors_file(file_path: str) -> Dict[str, str]:
    """Load metadata from a safetensors file."""
    try:
        with open(file_path, "rb") as f:
            # Read the header length (first 8 bytes)
            header_length_bytes = f.read(8)
            header_length = int.from_bytes(header_length_bytes, "little")

            # Read the header
            header_bytes = f.read(header_length)
            header_str = header_bytes.decode("utf-8")

            # Parse the header JSON
            header_data = json.loads(header_str)

            # Extract metadata
            metadata = header_data.get("__metadata__", {})
            return metadata
    except Exception as e:
        print(f"Error loading metadata from {file_path}: {e}")
        return {}


def save_config_from_safetensors(
    file_path: str, output_path: Optional[str] = None
) -> bool:
    """Extract and save the config content from a safetensors file."""
    metadata = load_metadata_from_safetensors_file(file_path)

    config_content = extract_config_from_metadata(metadata)
    if config_content is None:
        print(f"No config content found in metadata for {file_path}")
        return False

    config_filename = extract_config_file_from_metadata(metadata)
    if config_filename is None:
        config_filename = "extracted_config.toml"

    if output_path is None:
        # Save in the same directory as the safetensors file
        base_dir = os.path.dirname(file_path)
        output_path = os.path.join(base_dir, config_filename)

    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(config_content)
        print(f"Config saved to: {output_path}")
        return True
    except Exception as e:
        print(f"Error saving config to {output_path}: {e}")
        return False
</file>

<file path="utils/tensorboard_utils.py">
"""TensorBoard utilities for non-intrusive metric description registration.

If TensorFlow is available, TensorBoard supports per-scalar descriptions that
render directly under charts. This module provides a safe helper to register
those descriptions without impacting the training loop if the environment
doesn't support it.
"""

from __future__ import annotations

from typing import Any, Dict


def get_default_metric_descriptions() -> Dict[str, str]:
    """Return a default set of metric tag descriptions.

    Users can extend or override this mapping before calling
    register_metric_descriptions_non_intrusive.
    """
    return {
        # Basic loss metrics
        "loss/current": "Current step training loss (lower is better).",
        "loss/average": "Moving average of training loss (lower is better).",
        "loss/ema": "Bias-corrected EMA of loss (lower is better).",
        "loss/mse": "Base flow-matching MSE per step (lower is better).",
        # Training loss components and diagnostics
        "train/loss_p50": "Median per-sample velocity loss (lower is better).",
        "train/loss_p90": "90th percentile per-sample velocity loss (lower is better).",
        "train/loss_cv_in_batch": "Coefficient of variation of per-sample loss within batch; measures training stability (lower is better).",
        "train/direct_noise_loss_mean": "Mean direct noise-prediction loss per batch (lower is better).",
        "train/loss_snr_correlation_batch": "Correlation between loss and SNR within batch; negative values often indicate proper noise scheduling (closer to -1 is better).",
        # Training loss components (optional)
        "train/base_loss": "Base flow-matching loss component before additional terms (lower is better).",
        "train/dop_loss": "Differential Output Preservation loss; maintains base model behavior (lower is better).",
        "train/dispersive_loss": "InfoNCE-style dispersive loss for representation diversity (lower is better).",
        "train/optical_flow_loss": "Temporal consistency loss using optical flow (lower is better).",
        "train/repa_loss": "REPA alignment loss for visual quality (lower is better).",
        # Validation metrics - velocity prediction
        "val/velocity_loss_avg": "Average velocity loss across validation timesteps (lower is better).",
        "val/velocity_loss_avg_weighted": "Sample-weighted average velocity loss across all validation data (lower is better).",
        "val/velocity_loss_std": "Standard deviation of velocity loss across timesteps; measures consistency (lower is better).",
        "val/velocity_loss_range": "Range (max-min) of velocity loss across timesteps; measures stability (lower is better).",
        "val/velocity_loss_cv_across_timesteps": "Coefficient of variation of velocity loss across timesteps (lower is better).",
        "val/velocity_loss_avg_p25": "25th percentile of per-timestep velocity loss (lower is better).",
        "val/velocity_loss_avg_p50": "Median of per-timestep velocity loss (lower is better).",
        "val/velocity_loss_avg_p75": "75th percentile of per-timestep velocity loss (lower is better).",
        "val/velocity_loss_snr_correlation": "Correlation between velocity loss and SNR across timesteps; indicates noise scheduling effectiveness (negative preferred).",
        "val/velocity_loss_timestep_correlation": "Correlation between velocity loss and timestep values; shows temporal consistency (closer to 0 is better).",
        "val/best_velocity_loss": "Best (lowest) velocity loss among all validation timesteps (lower is better).",
        "val/worst_velocity_loss": "Worst (highest) velocity loss among all validation timesteps (lower is better).",
        "val/best_timestep_velocity": "Timestep with the best velocity loss performance (informational).",
        "val/worst_timestep_velocity": "Timestep with the worst velocity loss performance (informational).",
        # Validation metrics - direct noise prediction
        "val/direct_noise_loss_avg": "Average direct noise-prediction loss across validation timesteps (lower is better).",
        "val/direct_noise_loss_avg_weighted": "Sample-weighted average direct noise loss across all validation data (lower is better).",
        "val/direct_noise_loss_std": "Standard deviation of direct noise loss across timesteps (lower is better).",
        "val/direct_noise_loss_range": "Range (max-min) of direct noise loss across timesteps (lower is better).",
        "val/direct_noise_loss_cv_across_timesteps": "Coefficient of variation of direct noise loss across timesteps (lower is better).",
        "val/direct_noise_loss_avg_p25": "25th percentile of per-timestep direct noise loss (lower is better).",
        "val/direct_noise_loss_avg_p50": "Median of per-timestep direct noise loss (lower is better).",
        "val/direct_noise_loss_avg_p75": "75th percentile of per-timestep direct noise loss (lower is better).",
        "val/noise_loss_snr_correlation": "Correlation between direct noise loss and SNR across timesteps (negative preferred).",
        "val/noise_loss_timestep_correlation": "Correlation between direct noise loss and timestep values (closer to 0 is better).",
        "val/best_direct_loss": "Best (lowest) direct noise loss among all validation timesteps (lower is better).",
        "val/worst_direct_loss": "Worst (highest) direct noise loss among all validation timesteps (lower is better).",
        "val/best_timestep_direct": "Timestep with the best direct noise loss performance (informational).",
        "val/worst_timestep_direct": "Timestep with the worst direct noise loss performance (informational).",
        # Validation comparison metrics
        "val/loss_ratio": "Ratio of velocity loss to direct noise loss; indicates relative performance (closer to 1.0 is better).",
        # Per-timestep validation metrics (dynamic based on timesteps)
        # Note: These are generated dynamically in validation code as:
        # f"val_timesteps/velocity_loss_mean_t{timestep}"
        # f"val_timesteps/velocity_loss_std_t{timestep}"
        # f"val_timesteps/velocity_loss_min_t{timestep}"
        # f"val_timesteps/velocity_loss_max_t{timestep}"
        # f"val_timesteps/velocity_loss_p50_t{timestep}"
        # f"val_timesteps/velocity_loss_p90_t{timestep}"
        # f"val_timesteps/direct_noise_loss_mean_t{timestep}"
        # f"val_timesteps/direct_noise_loss_std_t{timestep}"
        # f"val_timesteps/direct_noise_loss_min_t{timestep}"
        # f"val_timesteps/direct_noise_loss_max_t{timestep}"
        # f"val_timesteps/direct_noise_loss_p50_t{timestep}"
        # f"val_timesteps/direct_noise_loss_p90_t{timestep}"
        # f"val_timesteps/snr_t{timestep}"
        # Generic timestep patterns for common validation timesteps
        "val_timesteps/velocity_loss_mean": "Mean velocity loss at specific timestep (lower is better).",
        "val_timesteps/velocity_loss_std": "Standard deviation of velocity loss at specific timestep (lower is better).",
        "val_timesteps/velocity_loss_min": "Minimum velocity loss at specific timestep (lower is better).",
        "val_timesteps/velocity_loss_max": "Maximum velocity loss at specific timestep (lower is better).",
        "val_timesteps/velocity_loss_p50": "Median velocity loss at specific timestep (lower is better).",
        "val_timesteps/velocity_loss_p90": "90th percentile velocity loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_mean": "Mean direct noise loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_std": "Standard deviation of direct noise loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_min": "Minimum direct noise loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_max": "Maximum direct noise loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_p50": "Median direct noise loss at specific timestep (lower is better).",
        "val_timesteps/direct_noise_loss_p90": "90th percentile direct noise loss at specific timestep (lower is better).",
        "val_timesteps/snr": "Signal-to-Noise Ratio at specific timestep; higher values indicate less noise (higher is better).",
        # Per-source loss metrics (video vs image)
        "loss/video": "Training loss for video data sources only (lower is better).",
        "loss/image": "Training loss for image data sources only (lower is better).",
        "loss/source": "Training loss for specific data source type (lower is better).",
        # Gradient monitoring
        "grad_norm": "Global gradient norm across all model parameters; indicates gradient flow strength (moderate values preferred).",
        # Learning rate tracking
        "lr/unet": "Learning rate for UNet/main model parameters (informational).",
        "lr/textencoder": "Learning rate for text encoder parameters (informational).",
        "lr/group0": "Learning rate for parameter group 0 (informational).",
        "lr/group1": "Learning rate for parameter group 1 (informational).",
        "lr/d*lr": "Effective learning rate (d*lr) for adaptive optimizers like Prodigy (informational).",
        "lr/d*lr/unet": "Effective learning rate for UNet parameters with adaptive optimizers (informational).",
        "lr/d*lr/textencoder": "Effective learning rate for text encoder with adaptive optimizers (informational).",
        "lr/d*lr/group0": "Effective learning rate for parameter group 0 with adaptive optimizers (informational).",
        "lr/d*lr/group1": "Effective learning rate for parameter group 1 with adaptive optimizers (informational).",
        # Parameter statistics (when log_param_stats=True)
        "param_stats/total_param_norm": "Sum of norms of all model parameters; tracks overall parameter scale (stable values preferred).",
        "param_stats/avg_param_norm": "Average norm across all model parameters (stable values preferred).",
        "param_stats/total_grad_norm": "Sum of gradient norms across all parameters; indicates total gradient magnitude (moderate values preferred).",
        "param_stats/avg_grad_norm": "Average gradient norm across all parameters (moderate values preferred).",
        "param_stats/num_params": "Number of trainable parameters (informational).",
        "param_stats/largest_param_norm": "Largest parameter norm in the model; helps detect parameter explosion (stable values preferred).",
        "param_stats/largest_grad_norm": "Largest gradient norm in the model; helps detect gradient explosion (moderate values preferred).",
        # Individual parameter tracking (top-K parameters by norm)
        "param_norm": "L2 norm of specific parameter tensor; tracks parameter drift (stable values preferred).",
        "grad_norm": "L2 norm of gradients for specific parameter tensor (moderate values preferred).",
    }


def register_metric_descriptions_non_intrusive(
    accelerator: Any, args: Any, tag_to_desc: Dict[str, str]
) -> None:
    """Attempt to register TensorBoard scalar descriptions; fail silently.

    - No-ops unless logging with TensorBoard (args.log_with in ["tensorboard", "all"]).
    - Requires TensorFlow to be importable; otherwise returns quietly.
    - Writes a single scalar per tag with a description at step=0 so that the
      description is rendered under the corresponding chart in TensorBoard.
    """
    try:
        if getattr(args, "log_with", None) not in ["tensorboard", "all"]:
            return

        # Find TensorBoard writer log_dir
        log_dir = None
        try:
            for t in getattr(accelerator, "trackers", []):
                if getattr(t, "name", "") == "tensorboard" and hasattr(
                    t.writer, "log_dir"
                ):
                    log_dir = t.writer.log_dir
                    break
        except Exception:
            log_dir = None
        if log_dir is None:
            return

        try:
            import tensorflow as tf  # type: ignore
        except Exception:
            # TensorFlow not available: cannot attach per-scalar descriptions
            return

        try:
            writer = tf.summary.create_file_writer(log_dir)
            with writer.as_default():
                for tag, desc in tag_to_desc.items():
                    try:
                        tf.summary.scalar(name=tag, data=0.0, step=0, description=desc)
                    except Exception:
                        # Skip tags that fail; continue registering others
                        pass
                writer.flush()
        except Exception:
            # Do not raise; remain non-intrusive
            return
    except Exception:
        # Fully non-intrusive
        return
</file>

<file path="utils/train_utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/utils/train_utils.py (Apache)

import argparse

import os
import shutil

import accelerate

from datetime import datetime, timedelta
import gc
import argparse
import math
import re
import time
import json
from typing import Dict, Optional
import accelerate
from packaging.version import Version

import toml

import torch
from accelerate.utils import TorchDynamoPlugin, DynamoBackend
from accelerate import (
    Accelerator,
    InitProcessGroupKwargs,
    DistributedDataParallelKwargs,
)


import logging
from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

# Guard to emit certain warnings only once per run (per source)
_warned_missing_timesteps_sources: set[str] = set()


TAKENOKO_METADATA_KEY_BASE_MODEL_VERSION = "takenoko_base_model_version"
TAKENOKO_METADATA_KEY_NETWORK_MODULE = "takenoko_network_module"
TAKENOKO_METADATA_KEY_NETWORK_DIM = "takenoko_network_dim"
TAKENOKO_METADATA_KEY_NETWORK_ALPHA = "takenoko_network_alpha"
TAKENOKO_METADATA_KEY_NETWORK_ARGS = "takenoko_network_args"

TAKENOKO_METADATA_MINIMUM_KEYS = [
    TAKENOKO_METADATA_KEY_BASE_MODEL_VERSION,
    TAKENOKO_METADATA_KEY_NETWORK_MODULE,
    TAKENOKO_METADATA_KEY_NETWORK_DIM,
    TAKENOKO_METADATA_KEY_NETWORK_ALPHA,
    TAKENOKO_METADATA_KEY_NETWORK_ARGS,
]


# checkpoint
EPOCH_STATE_NAME = "{}-{:06d}-state"
EPOCH_FILE_NAME = "{}-{:06d}"
EPOCH_DIFFUSERS_DIR_NAME = "{}-{:06d}"
LAST_STATE_NAME = "{}-state"
STEP_STATE_NAME = "{}-step{:08d}-state"
STEP_FILE_NAME = "{}-step{:08d}"
STEP_DIFFUSERS_DIR_NAME = "{}-step{:08d}"


def get_time_flag():
    return datetime.fromtimestamp(time.time()).strftime("%Y%m%d-%H%M%S-%f")[:-3]


def get_sanitized_config_or_none(args: argparse.Namespace):
    # if `--log_config` is enabled, return args for logging. if not, return None.
    # when `--log_config is enabled, filter out sensitive values from args
    # if wandb is not enabled, the log is not exposed to the public, but it is fine to filter out sensitive values to be safe

    if not args.log_config:
        return None

    sensitive_args = []
    sensitive_path_args = [
        "dit",
        "vae",
        "text_encoder1",
        "text_encoder2",
        "image_encoder",
        "base_weights",
        "network_weights",
        "output_dir",
        "logging_dir",
    ]
    filtered_args = {}
    for k, v in vars(args).items():
        # filter out sensitive values and convert to string if necessary
        if k not in sensitive_args + sensitive_path_args:
            # Accelerate values need to have type `bool`,`str`, `float`, `int`, or `None`.
            if (
                v is None
                or isinstance(v, bool)
                or isinstance(v, str)
                or isinstance(v, float)
                or isinstance(v, int)
            ):
                filtered_args[k] = v
            # accelerate does not support lists
            elif isinstance(v, list):
                filtered_args[k] = f"{v}"
            # accelerate does not support objects
            elif isinstance(v, object):
                filtered_args[k] = f"{v}"

    return filtered_args


class LossRecorder:
    def __init__(self):
        self.loss_list: list[float] = []
        self.loss_total: float = 0.0

    def add(self, *, epoch: int, step: int, loss: float) -> None:
        if epoch == 0:
            self.loss_list.append(loss)
        else:
            while len(self.loss_list) <= step:
                self.loss_list.append(0.0)
            self.loss_total -= self.loss_list[step]
            self.loss_list[step] = loss
        self.loss_total += loss

    @property
    def moving_average(self) -> float:
        return self.loss_total / len(self.loss_list)


def get_epoch_ckpt_name(model_name, epoch_no: int):
    return EPOCH_FILE_NAME.format(model_name, epoch_no) + ".safetensors"


def get_step_ckpt_name(model_name, step_no: int):
    return STEP_FILE_NAME.format(model_name, step_no) + ".safetensors"


def get_last_ckpt_name(model_name):
    return model_name + ".safetensors"


def get_remove_epoch_no(args: argparse.Namespace, epoch_no: int):
    if args.save_last_n_epochs is None:
        return None

    remove_epoch_no = epoch_no - args.save_every_n_epochs * args.save_last_n_epochs
    if remove_epoch_no < 0:
        return None
    return remove_epoch_no


def get_remove_step_no(args: argparse.Namespace, step_no: int):
    if args.save_last_n_steps is None:
        return None

    # calculate the step number to remove from the last_n_steps and save_every_n_steps
    # e.g. if save_every_n_steps=10, save_last_n_steps=30, at step 50, keep 30 steps and remove step 10
    remove_step_no = step_no - args.save_last_n_steps - 1
    remove_step_no = remove_step_no - (remove_step_no % args.save_every_n_steps)
    if remove_step_no < 0:
        return None
    return remove_step_no


def save_and_remove_state_on_epoch_end(
    args: argparse.Namespace, accelerator: accelerate.Accelerator, epoch_no: int
):
    model_name = args.output_name

    logger.info(f"üíæ Saving state at epoch {epoch_no}")
    os.makedirs(args.output_dir, exist_ok=True)

    state_dir = os.path.join(
        args.output_dir, EPOCH_STATE_NAME.format(model_name, epoch_no)
    )
    accelerator.save_state(state_dir)

    # Save original config file to state directory
    save_original_config_to_state_dir(state_dir, args)

    last_n_epochs = (
        args.save_last_n_epochs_state
        if args.save_last_n_epochs_state
        else args.save_last_n_epochs
    )
    if last_n_epochs is not None:
        remove_epoch_no = epoch_no - args.save_every_n_epochs * last_n_epochs
        state_dir_old = os.path.join(
            args.output_dir, EPOCH_STATE_NAME.format(model_name, remove_epoch_no)
        )
        if os.path.exists(state_dir_old):
            logger.info(f"removing old state: {state_dir_old}")
            shutil.rmtree(state_dir_old)


def save_original_config_to_state_dir(state_dir: str, args: argparse.Namespace) -> None:
    """Save the original config file content to the state directory for reproducibility."""
    try:
        if hasattr(args, "config_content") and args.config_content is not None:
            config_filename = "original_config.toml"
            if hasattr(args, "config_file") and args.config_file is not None:
                # Extract just the filename from the path
                import os

                config_filename = os.path.basename(args.config_file)
                if not config_filename.endswith(".toml"):
                    config_filename += ".toml"

            config_file_path = os.path.join(state_dir, config_filename)
            with open(config_file_path, "w", encoding="utf-8") as f:
                f.write(args.config_content)
            logger.info(f"Saved original config to: {config_file_path}")
        else:
            logger.debug("No original config content available to save")
    except Exception as e:
        logger.warning(f"Failed to save original config to state directory: {e}")


def save_and_remove_state_stepwise(
    args: argparse.Namespace, accelerator: accelerate.Accelerator, step_no: int
):
    model_name = args.output_name

    logger.info(f"üíæ Saving state at step {step_no}")
    os.makedirs(args.output_dir, exist_ok=True)

    state_dir = os.path.join(
        args.output_dir, STEP_STATE_NAME.format(model_name, step_no)
    )
    accelerator.save_state(state_dir)

    # Save original config file to state directory
    save_original_config_to_state_dir(state_dir, args)

    # Save step number to step.txt
    step_file = os.path.join(state_dir, "step.txt")
    try:
        with open(step_file, "w") as f:
            f.write(str(step_no))
    except Exception as e:
        logger.warning(f"Failed to write step.txt: {e}")

    last_n_steps = (
        args.save_last_n_steps_state
        if args.save_last_n_steps_state
        else args.save_last_n_steps
    )
    if last_n_steps is not None:
        remove_step_no = step_no - last_n_steps - 1
        remove_step_no = remove_step_no - (remove_step_no % args.save_every_n_steps)

        if remove_step_no > 0:
            state_dir_old = os.path.join(
                args.output_dir, STEP_STATE_NAME.format(model_name, remove_step_no)
            )
            if os.path.exists(state_dir_old):
                logger.info(f"removing old state: {state_dir_old}")
                shutil.rmtree(state_dir_old)


def save_state_on_train_end(
    args: argparse.Namespace, accelerator: accelerate.Accelerator
):
    model_name = args.output_name

    logger.info("üíæ Saving final state")
    os.makedirs(args.output_dir, exist_ok=True)

    state_dir = os.path.join(args.output_dir, LAST_STATE_NAME.format(model_name))
    accelerator.save_state(state_dir)

    # Save original config file to state directory
    save_original_config_to_state_dir(state_dir, args)


def clean_memory_on_device(device: torch.device):
    r"""
    Clean memory on the specified device, will be called from training scripts.
    """
    gc.collect()

    # device may "cuda" or "cuda:0", so we need to check the type of device
    if device.type == "cuda":
        torch.cuda.empty_cache()
    if device.type == "xpu":
        torch.xpu.empty_cache()
    if device.type == "mps":
        torch.mps.empty_cache()


# for collate_fn: epoch and step is multiprocessing.Value
class collator_class:
    def __init__(self, epoch, step, dataset):
        self.current_epoch = epoch
        self.current_step = step
        self.dataset = (
            dataset  # not used if worker_info is not None, in case of multiprocessing
        )

    def __call__(self, examples):
        worker_info = torch.utils.data.get_worker_info()
        # worker_info is None in the main process
        if worker_info is not None:
            dataset = worker_info.dataset
            context_reason = "dataloader_worker"
        else:
            dataset = self.dataset
            context_reason = "main_process"

        # Conservative epoch setting with explicit shuffle control
        try:
            # Build reason string for better logging context
            reason = f"collator_{context_reason}"

            # CRITICAL: Always disable shuffling for collator calls
            # Collator calls are synchronization operations, not training progression
            # Shuffling should only happen during genuine epoch progression in training loops
            dataset.set_current_epoch(  # type: ignore
                self.current_epoch.value,
                force_shuffle=False,  # Explicitly disable shuffling for all collator calls
                reason=reason,
            )
        except (AttributeError, TypeError) as e:
            # Fallback for datasets that don't support the new signature
            try:
                dataset.set_current_epoch(self.current_epoch.value)  # type: ignore
            except AttributeError:
                pass  # Dataset doesn't have set_current_epoch method

        try:
            dataset.set_current_step(self.current_step.value)  # type: ignore
        except AttributeError:
            pass  # Dataset doesn't have set_current_step method

        return examples[0]


def prepare_accelerator(args: argparse.Namespace) -> Accelerator:
    """
    DeepSpeed is not supported in this script currently.
    """
    if args.logging_dir is None:
        logging_dir = None
    else:
        log_prefix = "" if args.log_prefix is None else args.log_prefix
        logging_dir = (
            args.logging_dir
            + "/"
            + log_prefix
            + time.strftime("%Y%m%d%H%M%S", time.localtime())
        )

    if args.log_with is None:
        if logging_dir is not None:
            log_with = "tensorboard"
        else:
            log_with = None
    else:
        requested = str(args.log_with).lower()
        if not requested in ["tensorboard"]:
            logger.warning(
                "only tensorboard logging is supported; using TensorBoard instead"
            )
            log_with = "tensorboard" if logging_dir is not None else None
        else:
            log_with = requested
        if log_with == "tensorboard" and logging_dir is None:
            raise ValueError(
                "logging_dir must be specified when using TensorBoard logging."
            )

    kwargs_handlers = [
        (
            InitProcessGroupKwargs(
                backend=(
                    "gloo"
                    if os.name == "nt" or not torch.cuda.is_available()
                    else "nccl"
                ),
                init_method=(
                    "env://?use_libuv=False"
                    if os.name == "nt"
                    and Version(torch.__version__) >= Version("2.4.0")
                    else None
                ),
                timeout=(
                    timedelta(minutes=args.ddp_timeout) if args.ddp_timeout else None
                ),
            )
            if torch.cuda.device_count() > 1
            else None
        ),
        (
            DistributedDataParallelKwargs(
                gradient_as_bucket_view=args.ddp_gradient_as_bucket_view,
                static_graph=args.ddp_static_graph,
            )
            if args.ddp_gradient_as_bucket_view or args.ddp_static_graph
            else None
        ),
    ]
    kwargs_handlers = [i for i in kwargs_handlers if i is not None]

    dynamo_plugin = None
    if args.dynamo_backend.upper() != "NO":
        dynamo_plugin = TorchDynamoPlugin(
            backend=DynamoBackend(args.dynamo_backend.upper()),
            mode=args.dynamo_mode,
            fullgraph=args.dynamo_fullgraph,
            dynamic=args.dynamo_dynamic,
        )

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=log_with,
        project_dir=logging_dir,
        dynamo_plugin=dynamo_plugin,
        kwargs_handlers=kwargs_handlers,
    )
    return accelerator


def line_to_prompt_dict(line: str) -> dict:
    # subset of gen_img_diffusers
    prompt_args = line.split(" --")
    prompt_dict = {}
    prompt_dict["prompt"] = prompt_args[0]

    for parg in prompt_args:
        try:
            m = re.match(r"w (\d+)", parg, re.IGNORECASE)
            if m:
                prompt_dict["width"] = int(m.group(1))
                continue

            m = re.match(r"h (\d+)", parg, re.IGNORECASE)
            if m:
                prompt_dict["height"] = int(m.group(1))
                continue

            m = re.match(r"f (\d+)", parg, re.IGNORECASE)
            if m:
                prompt_dict["frame_count"] = int(m.group(1))
                continue

            m = re.match(r"d (\d+)", parg, re.IGNORECASE)
            if m:
                prompt_dict["seed"] = int(m.group(1))
                continue

            m = re.match(r"s (\d+)", parg, re.IGNORECASE)
            if m:  # steps
                prompt_dict["sample_steps"] = max(1, min(1000, int(m.group(1))))
                continue

            m = re.match(r"g ([\d\.]+)", parg, re.IGNORECASE)
            if m:  # scale
                prompt_dict["guidance_scale"] = float(m.group(1))
                continue

            m = re.match(r"fs ([\d\.]+)", parg, re.IGNORECASE)
            if m:  # scale
                prompt_dict["discrete_flow_shift"] = float(m.group(1))
                continue

            m = re.match(r"l ([\d\.]+)", parg, re.IGNORECASE)
            if m:  # scale
                prompt_dict["cfg_scale"] = float(m.group(1))
                continue

            m = re.match(r"n (.+)", parg, re.IGNORECASE)
            if m:  # negative prompt
                prompt_dict["negative_prompt"] = m.group(1)
                continue

            m = re.match(r"i (.+)", parg, re.IGNORECASE)
            if m:  # negative prompt
                prompt_dict["image_path"] = m.group(1)
                continue

            m = re.match(r"cn (.+)", parg, re.IGNORECASE)
            if m:
                prompt_dict["control_video_path"] = m.group(1)
                continue

            m = re.match(r"ci (.+)", parg, re.IGNORECASE)
            if m:
                # can be multiple control images
                control_image_path = m.group(1)
                if "control_image_path" not in prompt_dict:
                    prompt_dict["control_image_path"] = []
                prompt_dict["control_image_path"].append(control_image_path)
                continue

            m = re.match(r"of (.+)", parg, re.IGNORECASE)
            if m:  # output folder
                prompt_dict["one_frame"] = m.group(1)
                continue

        except ValueError as ex:
            logger.error(f"Exception occurred while parsing prompt argument: {parg}")
            logger.error(ex)

    return prompt_dict


def load_prompts(prompt_file: str) -> list[Dict]:
    # Validate input
    if not prompt_file or not prompt_file.strip():
        raise ValueError("prompt_file cannot be empty or None")

    logger.info(f"load_prompts called with prompt_file='{prompt_file}'")

    # read prompts
    if prompt_file.endswith(".txt"):
        logger.info(f"Loading from .txt file")
        with open(prompt_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        prompts = [
            line.strip() for line in lines if len(line.strip()) > 0 and line[0] != "#"
        ]
    elif prompt_file.endswith(".toml"):
        logger.info(f"Loading from .toml file")
        with open(prompt_file, "r", encoding="utf-8") as f:
            data = toml.load(f)

        # Handle new TOML structure with sample_prompts array
        if "sample_prompts" in data:
            logger.info(
                f"Found sample_prompts in TOML, count: {len(data['sample_prompts'])}"
            )
            prompts = []
            for prompt_dict in data["sample_prompts"]:
                # Convert the new structure to the expected format
                converted_dict = {}
                if "text" in prompt_dict:
                    converted_dict["prompt"] = prompt_dict["text"]
                if "width" in prompt_dict:
                    converted_dict["width"] = prompt_dict["width"]
                if "height" in prompt_dict:
                    converted_dict["height"] = prompt_dict["height"]
                if "frames" in prompt_dict:
                    converted_dict["frame_count"] = prompt_dict["frames"]
                if "seed" in prompt_dict:
                    converted_dict["seed"] = prompt_dict["seed"]
                if "step" in prompt_dict:
                    converted_dict["sample_steps"] = prompt_dict["step"]
                if "control_path" in prompt_dict:
                    converted_dict["control_path"] = prompt_dict["control_path"]
                # Add other fields as needed
                prompts.append(converted_dict)
        else:
            logger.info(f"Using old TOML structure")
            # Handle old TOML structure
            prompts = [
                dict(**data["prompt"], **subset) for subset in data["prompt"]["subset"]
            ]
    elif prompt_file.endswith(".json"):
        logger.info(f"Loading from .json file")
        with open(prompt_file, "r", encoding="utf-8") as f:
            prompts = json.load(f)
    else:
        raise ValueError(
            f"Unsupported file format: {prompt_file}. Supported formats: .txt, .toml, .json"
        )

    logger.info(f"Loaded {len(prompts)} prompts from {prompt_file}")

    # preprocess prompts
    for i in range(len(prompts)):
        prompt_dict = prompts[i]
        if isinstance(prompt_dict, str):
            prompt_dict = line_to_prompt_dict(prompt_dict)
            prompts[i] = prompt_dict  # type: ignore
        assert isinstance(prompt_dict, dict)

        # Adds an enumerator to the dict based on prompt position. Used later to name image files. Also cleanup of extra data in original prompt dict.
        prompt_dict["enum"] = i
        prompt_dict.pop("subset", None)

    logger.info(f"Final processed prompts count: {len(prompts)}")
    return prompts  # type: ignore


def compute_density_for_timestep_sampling(
    weighting_scheme: str,
    batch_size: int,
    logit_mean: float = None,  # type: ignore
    logit_std: float = None,  # type: ignore
    mode_scale: float = None,  # type: ignore
):
    """Compute the density for sampling the timesteps when doing SD3 training.

    Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.

    SD3 paper reference: https://arxiv.org/abs/2403.03206v1.
    """
    if weighting_scheme == "logit_normal":
        # See 3.1 in the SD3 paper ($rf/lognorm(0.00,1.00)$).
        u = torch.normal(
            mean=logit_mean, std=logit_std, size=(batch_size,), device="cpu"
        )
        u = torch.nn.functional.sigmoid(u)
    elif weighting_scheme == "mode":
        u = torch.rand(size=(batch_size,), device="cpu")
        u = 1 - u - mode_scale * (torch.cos(math.pi * u / 2) ** 2 - 1 + u)
    else:
        u = torch.rand(size=(batch_size,), device="cpu")
    return u


def get_sigmas(
    noise_scheduler,
    timesteps,
    device,
    n_dim=4,
    dtype=torch.float32,
    *,
    source: str | None = None,
):
    sigmas = noise_scheduler.sigmas.to(device=device, dtype=dtype)
    schedule_timesteps = noise_scheduler.timesteps.to(device)
    timesteps = timesteps.to(device)

    # if sum([(schedule_timesteps == t) for t in timesteps]) < len(timesteps):
    if any([(schedule_timesteps == t).sum() == 0 for t in timesteps]):
        # round to nearest timestep
        global _warned_missing_timesteps_sources
        src = source or "unspecified"
        if src not in _warned_missing_timesteps_sources:
            if source:
                logger.warning(
                    f"‚ö†Ô∏è Some timesteps are not present in the noise schedule (source: {source}). Rounding to the nearest available timestep. This is expected when using continuous timesteps; training is not affected."
                )
            else:
                logger.warning(
                    "‚ö†Ô∏è Some timesteps are not present in the noise schedule. Rounding to the nearest available timestep. This is expected when using continuous timesteps; training is not affected."
                )
            _warned_missing_timesteps_sources.add(src)
        step_indices = [
            torch.argmin(torch.abs(schedule_timesteps - t)).item() for t in timesteps
        ]
    else:
        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]

    sigma = sigmas[step_indices].flatten()
    while len(sigma.shape) < n_dim:
        sigma = sigma.unsqueeze(-1)
    return sigma


def compute_loss_weighting_for_sd3(
    weighting_scheme: str, noise_scheduler, timesteps, device, dtype
):
    """Computes loss weighting scheme for SD3 training.

    Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.

    SD3 paper reference: https://arxiv.org/abs/2403.03206v1.
    """
    if weighting_scheme == "sigma_sqrt" or weighting_scheme == "cosmap":
        sigmas = get_sigmas(noise_scheduler, timesteps, device, n_dim=5, dtype=dtype)
        if weighting_scheme == "sigma_sqrt":
            weighting = (sigmas**-2.0).float()
        else:
            bot = 1 - 2 * sigmas + 2 * sigmas**2
            weighting = 2 / (math.pi * bot)
    else:
        weighting = None  # torch.ones_like(sigmas)
    return weighting


def should_sample_images(args, steps, epoch=None):
    if steps == 0:
        if not args.sample_at_first:
            return False
        else:
            return True
    else:
        should_sample_by_steps = (
            args.sample_every_n_steps is not None
            and args.sample_every_n_steps > 0
            and steps % args.sample_every_n_steps == 0
        )
        should_sample_by_epochs = (
            args.sample_every_n_epochs is not None
            and args.sample_every_n_epochs > 0
            and epoch is not None
            and epoch % args.sample_every_n_epochs == 0
        )

        if not should_sample_by_steps and not should_sample_by_epochs:
            return False

        return True


# Utility to read step from step.txt in state dir
def read_step_from_state_dir(state_dir: str) -> Optional[int]:
    step_file = os.path.join(state_dir, "step.txt")
    if os.path.exists(step_file):
        try:
            with open(step_file, "r") as f:
                return int(f.read().strip())
        except Exception as e:
            logger.warning(f"Failed to read step.txt: {e}")
    return None
</file>

<file path="utils/tread.py">
## Based on https://github.com/bghira/SimpleTuner/blob/main/helpers/training/tread.py (AGPLv3)

import torch
from dataclasses import dataclass
from typing import Optional, Any, cast


@dataclass
class MaskInfo:
    """Book‚Äëkeeping for one TREAD routing window."""

    mask: torch.Tensor  # bool tensor, True where token was dropped
    ids_keep: torch.Tensor  # long tensor (B, num_keep)
    ids_mask: torch.Tensor  # long tensor (B, num_mask)
    ids_shuffle: torch.Tensor  # long tensor, permutation that packs kept tokens first
    ids_restore: torch.Tensor  # long tensor, inverse permutation


class TREADRouter:
    """
    Minimal implementation of the token router used in TREAD.

    Public API
    ----------
    get_mask(x, mask_ratio, ...) -> MaskInfo
        Sample a permutation & binary mask that decides which tokens are kept.
    start_route(x, mask_info) -> x_small
        Apply the permutation and truncate to kept tokens.
    end_route(x_small, mask_info, original_x=None) -> x_full
        Undo the permutation, re‚Äëinsert the skipped tokens (either the
        original representations or a mask token), and return a full‚Äëlength
        sequence so gradients can flow everywhere.
    """

    def __init__(self, seed: int = 42, device: Any = None):
        self.generator = torch.Generator(device=device)
        self.generator.manual_seed(seed)

    # --------------------------------------------------------------------- #
    # helpers
    # --------------------------------------------------------------------- #
    @staticmethod
    def _importance(x: torch.Tensor) -> torch.Tensor:
        """
        Per‚Äëtoken importance score  ‚àù  L1‚Äënorm of the feature vector.

        Normalised to [0,‚ÄØ1] **per sample** to avoid scale drift.
        """
        # x: (B, S, D)
        mags = x.abs().sum(-1)  # (B,S)
        rng = mags.max(dim=1, keepdim=True)[0] - mags.min(dim=1, keepdim=True)[0]
        mags = (mags - mags.min(dim=1, keepdim=True)[0]) / (rng + 1e-8)
        return mags

    # --------------------------------------------------------------------- #
    # public API
    # --------------------------------------------------------------------- #
    @torch.no_grad()
    def get_mask(
        self,
        x: torch.Tensor,  # (B, S, D)
        mask_ratio: float = 0.0,
        l1_reg: float = 0.0,
        inverse: bool = False,
        force_keep: Optional[torch.Tensor] = None,  # (B, S) bool
    ) -> MaskInfo:
        """
        Decide which tokens to keep.

        * `mask_ratio`¬†=¬†0.75  ‚Üí keep 25‚ÄØ% of the *image* tokens.
        * `force_keep` marks tokens that are **never** allowed to drop.
        """
        B, S, _ = x.shape

        # ---------------------------------------------------------------------
        # 1) book‚Äëkeeping for "must‚Äëkeep" tokens
        # ---------------------------------------------------------------------
        if force_keep is None:
            force_keep = torch.zeros(B, S, dtype=torch.bool, device=x.device)
        # narrow type for checkers
        assert force_keep is not None
        num_force = force_keep.sum(1)  # (B,)  how many per sample

        # overall keep budget (one scalar K for the whole batch)
        base_keep = S - int(round(S * float(mask_ratio)))
        keep_budget = max(base_keep, int(num_force.max()))
        K = keep_budget  # make name explicit

        # ---------------------------------------------------------------------
        # 2) importance + randomness mix
        # ---------------------------------------------------------------------
        score = self._importance(x)  # in [0,1]
        if inverse:
            score = 1.0 - score

        noise = torch.rand(
            score.shape,
            dtype=score.dtype,
            device=score.device,
            generator=self.generator,
        )

        mix = (1.0 - l1_reg) * noise + l1_reg * score  # convex combination
        mix = mix.masked_fill(force_keep, -1.0)  # force‚Äëkeep ‚áí lowest rank

        # ---------------------------------------------------------------------
        # 3) build permutations
        # ---------------------------------------------------------------------
        ids_shuffle = torch.argsort(mix, dim=1)  # (B, S) smallest first
        ids_keep = ids_shuffle[:, :K]  # (B, K)
        ids_mask = ids_shuffle[:, K:]  # (B, S-K)
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # bool mask where True¬†=¬†*masked* (dropped) token
        mask = torch.ones(B, S, dtype=torch.bool, device=x.device)
        mask.scatter_(1, ids_keep, False)

        # runtime dtype assertions for clarity
        assert mask.dtype == torch.bool
        assert ids_keep.dtype == torch.long
        assert ids_mask.dtype == torch.long
        assert ids_shuffle.dtype == torch.long
        assert ids_restore.dtype == torch.long

        return MaskInfo(mask, ids_keep, ids_mask, ids_shuffle, ids_restore)

    def start_route(self, x: torch.Tensor, info: MaskInfo) -> torch.Tensor:
        """
        Permute tokens so the kept ones come first, then truncate.
        """
        # x: (B,S,D)
        x_shuf = torch.take_along_dim(
            x, info.ids_shuffle.unsqueeze(-1).expand_as(x), dim=1
        )
        return x_shuf[:, : info.ids_keep.size(1), :]

    def end_route(
        self,
        routed_x: torch.Tensor,  # (B, num_keep, D)
        info: MaskInfo,
        original_x: Optional[torch.Tensor] = None,
        mask_token: float | int = 0.0,
    ) -> torch.Tensor:
        """
        Rebuild a sequence of length S so gradients can flow everywhere.

        If `original_x` is provided we copy the *actual* skipped
        representations back in (recommended for training).  Otherwise we
        fill them with `mask_token`.
        """
        B, S = info.mask.shape
        D = routed_x.shape[2]

        # Create buffer for all tokens in shuffled order
        x_shuf = torch.empty(B, S, D, device=routed_x.device, dtype=routed_x.dtype)
        x_shuf[:, : routed_x.size(1), :] = routed_x

        if original_x is not None:
            # put original skipped tokens back behind the kept ones
            orig_shuf = torch.take_along_dim(
                original_x,
                info.ids_shuffle.unsqueeze(-1).expand_as(original_x),
                dim=1,
            )
            x_shuf[:, routed_x.size(1) :, :] = orig_shuf[:, routed_x.size(1) :, :]
        else:
            x_shuf[:, routed_x.size(1) :, :].fill_(mask_token)

        # Undo the permutation
        x_full = torch.take_along_dim(
            x_shuf, info.ids_restore.unsqueeze(-1).expand_as(x_shuf), dim=1
        )
        return x_full
</file>

<file path="utils/vae_utils.py">
import os
import torch
from typing import Optional

from wan.modules.vae import WanVAE
from common.model_downloader import download_model_if_needed
from common.logger import get_logger

logger = get_logger(__name__)


def load_vae(args, config, device: torch.device, dtype: torch.dtype) -> WanVAE:
    """load VAE model

    Args:
        args: command line arguments
        config: model configuration
        device: device to use
        dtype: data type for the model

    Returns:
        WanVAE: loaded VAE model
    """
    vae_path = (
        args.vae
        if args.vae is not None
        else os.path.join(args.ckpt_dir, config.vae_checkpoint)
    )

    # Download model if it's a URL
    if vae_path.startswith(("http://", "https://")):
        logger.info(f"Detected URL for VAE model, downloading: {vae_path}")
        cache_dir = getattr(args, "model_cache_dir", None)
        vae_path = download_model_if_needed(vae_path, cache_dir=cache_dir)
        logger.info(f"Downloaded VAE model to: {vae_path}")

    logger.info(f"Loading VAE model from {vae_path}")
    cache_device = torch.device("cpu") if args.vae_cache_cpu else None
    vae = WanVAE(
        vae_path=vae_path, device=device, dtype=dtype, cache_device=cache_device  # type: ignore
    )
    return vae
</file>

<file path="wan/configs/config.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/configs/__init__.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import copy
import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"

from wan.configs.wan_t2v_14B import t2v_14B
from wan.configs.wan_t2v_A14B import t2v_A14B

WAN_CONFIGS = {
    "t2v-14B": t2v_14B,
    "t2v-A14B": t2v_A14B,
}

SIZE_CONFIGS = {
    "720*1280": (720, 1280),
    "1280*720": (1280, 720),
    "480*832": (480, 832),
    "832*480": (832, 480),
    "1024*1024": (1024, 1024),
}

MAX_AREA_CONFIGS = {
    "720*1280": 720 * 1280,
    "1280*720": 1280 * 720,
    "480*832": 480 * 832,
    "832*480": 832 * 480,
    "704*1280": 704 * 1280,
    "1280*704": 1280 * 704,
}

SUPPORTED_SIZES = {
    "t2v-14B": ("720*1280", "1280*720", "480*832", "832*480"),
    "t2v-A14B": ("720*1280", "1280*720", "480*832", "832*480"),
}
</file>

<file path="wan/configs/shared_config.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/configs/shared_config.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import torch
from easydict import EasyDict

# ------------------------ Wan shared config ------------------------#
wan_shared_cfg = EasyDict()

# t5
wan_shared_cfg.t5_model = "umt5_xxl"
wan_shared_cfg.t5_dtype = torch.bfloat16
wan_shared_cfg.text_len = 512

# transformer
wan_shared_cfg.param_dtype = torch.bfloat16
wan_shared_cfg.out_dim = 16

# inference
wan_shared_cfg.num_train_timesteps = 1000
wan_shared_cfg.sample_fps = 16
wan_shared_cfg.sample_neg_prompt = (
    "Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºå"
    "ËøáÊõùÔºå"
    "ÈùôÊÄÅÔºå"
    "ÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºå"
    "Â≠óÂπïÔºå"
    "È£éÊ†ºÔºå"
    "‰ΩúÂìÅÔºå"
    "Áîª‰ΩúÔºå"
    "ÁîªÈù¢Ôºå"
    "ÈùôÊ≠¢Ôºå"
    "Êï¥‰ΩìÂèëÁÅ∞Ôºå"
    "ÊúÄÂ∑ÆË¥®ÈáèÔºå"
    "‰ΩéË¥®ÈáèÔºå"
    "JPEGÂéãÁº©ÊÆãÁïôÔºå"
    "‰∏ëÈôãÁöÑÔºå"
    "ÊÆãÁº∫ÁöÑÔºå"
    "Â§ö‰ΩôÁöÑÊâãÊåáÔºå"
    "ÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®Ôºå"
    "ÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®Ôºå"
    "Áï∏ÂΩ¢ÁöÑÔºå"
    "ÊØÅÂÆπÁöÑÔºå"
    "ÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºå"
    "ÊâãÊåáËûçÂêàÔºå"
    "ÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢Ôºå"
    "ÊùÇ‰π±ÁöÑËÉåÊôØÔºå"
    "‰∏âÊù°ËÖøÔºå"
    "ËÉåÊôØ‰∫∫ÂæàÂ§öÔºå"
    "ÂÄíÁùÄËµ∞"
)

wan_shared_cfg.frame_num = 81
</file>

<file path="wan/configs/wan_t2v_14B.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/configs/wan_t2v_14B.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
from easydict import EasyDict

from wan.configs.shared_config import wan_shared_cfg

# ------------------------ Wan T2V 14B ------------------------#

t2v_14B = EasyDict(__name__="Config: Wan T2V 14B")
t2v_14B.update(wan_shared_cfg)
t2v_14B.v2_2 = False

# t5
t2v_14B.t5_checkpoint = "models_t5_umt5-xxl-enc-bf16.pth"
t2v_14B.t5_tokenizer = "google/umt5-xxl"

# vae
t2v_14B.vae_checkpoint = "Wan2.1_VAE.pth"
t2v_14B.vae_stride = (4, 8, 8)

# transformer
t2v_14B.patch_size = (1, 2, 2)
t2v_14B.dim = 5120
t2v_14B.ffn_dim = 13824
t2v_14B.freq_dim = 256
t2v_14B.in_dim = 16  # not in original
t2v_14B.num_heads = 40
t2v_14B.num_layers = 40
t2v_14B.window_size = (-1, -1)
t2v_14B.qk_norm = True
t2v_14B.cross_attn_norm = True
t2v_14B.eps = 1e-6

# inference
t2v_14B.sample_shift = 5.0
t2v_14B.sample_steps = 50
t2v_14B.boundary = None
t2v_14B.sample_guide_scale = (5.0,)
</file>

<file path="wan/configs/wan_t2v_A14B.py">
## Based on: https://github.com/Wan-Video/Wan2.2/blob/main/wan/configs/wan_t2v_A14B.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
from easydict import EasyDict

from wan.configs.shared_config import wan_shared_cfg

# ------------------------ Wan T2V A14B ------------------------#

t2v_A14B = EasyDict(__name__="Config: Wan T2V A14B")
t2v_A14B.update(wan_shared_cfg)
t2v_A14B.v2_2 = True

# t5
t2v_A14B.t5_checkpoint = "models_t5_umt5-xxl-enc-bf16.pth"
t2v_A14B.t5_tokenizer = "google/umt5-xxl"

# vae
t2v_A14B.vae_checkpoint = "Wan2.1_VAE.pth"
t2v_A14B.vae_stride = (4, 8, 8)

# transformer
t2v_A14B.patch_size = (1, 2, 2)
t2v_A14B.dim = 5120
t2v_A14B.ffn_dim = 13824
t2v_A14B.freq_dim = 256
t2v_A14B.in_dim = 16  # not in original
t2v_A14B.num_heads = 40
t2v_A14B.num_layers = 40
t2v_A14B.window_size = (-1, -1)
t2v_A14B.qk_norm = True
t2v_A14B.cross_attn_norm = True
t2v_A14B.eps = 1e-6

t2v_A14B.low_noise_checkpoint = "low_noise_model"
t2v_A14B.high_noise_checkpoint = "high_noise_model"

# inference
t2v_A14B.sample_shift = 12.0
t2v_A14B.sample_steps = 40
t2v_A14B.boundary = 0.875
t2v_A14B.sample_guide_scale = (3.0, 4.0)  # low noise, high noise
</file>

<file path="wan/modules/attention.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/modules/attention.py (Apache)
## based on: https://github.com/gen-ai-team/Wan2.1-NABLA (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
from typing import Optional, List
import torch
import math
from torch import Tensor
from torch.nn.attention.flex_attention import BlockMask
from einops import rearrange

try:
    import flash_attn_interface  # type: ignore

    FLASH_ATTN_3_AVAILABLE = True
except ModuleNotFoundError:
    FLASH_ATTN_3_AVAILABLE = False

try:
    import flash_attn  # type: ignore

    FLASH_ATTN_2_AVAILABLE = True
except ModuleNotFoundError:
    FLASH_ATTN_2_AVAILABLE = False

try:
    import sageattention  # type: ignore

    SAGE_ATTN_AVAILABLE = True
except ModuleNotFoundError:
    SAGE_ATTN_AVAILABLE = False

try:
    import xformers.ops as xops  # type: ignore

    XFORMERS_AVAILABLE = True
except ImportError:
    XFORMERS_AVAILABLE = False


import warnings


def flash_attention(
    qkv,
    q_lens=None,
    k_lens=None,
    dropout_p=0.0,
    softmax_scale=None,
    q_scale=None,
    causal=False,
    window_size=(-1, -1),
    deterministic=False,
    dtype=torch.bfloat16,
    version=None,
    attn_mode: Optional[str] = "torch",
    split_attn: bool = False,
    batched_rotary: Optional[torch.Tensor] = None,
):
    """
    q:              [B, Lq, Nq, C1].
    k:              [B, Lk, Nk, C1].
    v:              [B, Lk, Nk, C2]. Nq must be divisible by Nk.
    q_lens:         [B].
    k_lens:         [B].
    dropout_p:      float. Dropout probability.
    softmax_scale:  float. The scaling of QK^T before applying softmax.
    causal:         bool. Whether to apply causal attention mask.
    window_size:    (left right). If not (-1, -1), apply sliding window local attention.
    deterministic:  bool. If True, slightly slower and uses more memory.
    dtype:          torch.dtype. Apply when dtype of q/k/v is not float16/bfloat16.
    """
    q, k, v = qkv
    qkv.clear()

    half_dtypes = (torch.float16, torch.bfloat16)
    assert dtype in half_dtypes
    # assert q.device.type == "cuda" and q.size(-1) <= 256

    # params
    b, lq, lk, out_dtype = q.size(0), q.size(1), k.size(1), q.dtype

    def half(x):
        return x if x.dtype in half_dtypes else x.to(dtype)

    # Flash attention 3 not tested, so keep the original code.
    # Customized code (except for flash attention 3) is not supported q_lens and k_lens.
    if attn_mode != "flash3" and attn_mode != "sageattn":
        assert q_lens is None, "q_lens is not supported except for flash attention 3."
        assert k_lens is None or (
            min(k_lens) == max(k_lens) and k_lens[0] == lk
        ), "k_lens is not supported except for flash attention 3."

    # SDPA
    if attn_mode == "torch" or attn_mode == "sdpa":
        assert (
            not deterministic
        ), "deterministic is not supported in scaled_dot_product_attention."
        if q_scale is not None:
            q = q * q_scale
        q = half(q.transpose(1, 2))
        k = half(k.transpose(1, 2))
        v = half(v.transpose(1, 2))

        # Apply batched rotary embeddings if provided (for routed attention)
        if batched_rotary is not None:
            # batched_rotary expected shape: (B, S, D) applied to last dim pairs
            def apply_rot(x, rot):
                # x: (B, L, N, D), rot: (B, L, D)
                B, L, N, D = x.shape
                x_c = torch.view_as_complex(x.float().reshape(B, L, N, D // 2, 2))
                rot_c = torch.view_as_complex(rot.float().reshape(B, L, 1, D // 2, 2))
                out = torch.view_as_real(x_c * rot_c).reshape(B, L, N, D).type_as(x)
                return out

            q = apply_rot(q, batched_rotary)
            k = apply_rot(k, batched_rotary)

        if not split_attn:
            q = torch.nn.functional.scaled_dot_product_attention(
                q, k, v, is_causal=causal, dropout_p=dropout_p, scale=softmax_scale
            )
            x = q
        else:
            x = torch.empty_like(q)
            for i in range(q.size(0)):
                x[i : i + 1] = torch.nn.functional.scaled_dot_product_attention(
                    q[i : i + 1],
                    k[i : i + 1],
                    v[i : i + 1],
                    is_causal=causal,
                    dropout_p=dropout_p,
                    scale=softmax_scale,
                )

        del q, k, v
        x = x.transpose(1, 2).contiguous()
        return x.type(out_dtype)

    # flash attention 2
    if attn_mode == "flash" or attn_mode == "flash2":
        if not FLASH_ATTN_2_AVAILABLE:
            raise RuntimeError(
                "Flash Attention 2 is not available. Please install flash-attn or use a different attention mode."
            )

        if q_scale is not None:
            q = q * q_scale
        q = half(q)
        k = half(k)
        v = half(v)

        if not split_attn:
            q = flash_attn.flash_attn_func(
                q,
                k,
                v,
                dropout_p,
                softmax_scale,
                causal,
                window_size,
                deterministic=deterministic,
            )
            x = q
        else:
            x = torch.empty_like(q)
            for i in range(q.size(0)):
                x[i : i + 1] = flash_attn.flash_attn_func(  # type: ignore
                    q[i : i + 1],
                    k[i : i + 1],
                    v[i : i + 1],
                    dropout_p,
                    softmax_scale,
                    causal,
                    window_size,
                    deterministic=deterministic,
                )
        del q, k, v
        return x.type(out_dtype)  # type: ignore

    # xformers
    if attn_mode == "xformers":
        if not XFORMERS_AVAILABLE:
            raise RuntimeError(
                "Xformers is not available. Please install xformers or use a different attention mode."
            )

        assert not deterministic, "deterministic is not supported in xformers."
        assert not causal, "causal is not supported in xformers."
        if q_scale is not None:
            q = q * q_scale
        q = half(q)
        k = half(k)
        v = half(v)

        if not split_attn:
            q = xops.memory_efficient_attention(
                q, k, v, p=dropout_p, scale=softmax_scale
            )
            x = q
        else:
            x = torch.empty_like(q)
            for i in range(q.size(0)):
                x[i : i + 1] = xops.memory_efficient_attention(
                    q[i : i + 1],
                    k[i : i + 1],
                    v[i : i + 1],
                    p=dropout_p,
                    scale=softmax_scale,
                )

        del q, k, v
        return x.type(out_dtype)

    # sage attention with fixed length seems to cause NaN in I2V inference.
    # # sage attention
    # if attn_mode == "sageattn":
    #     print("Using sage attention")
    #     assert not deterministic, "deterministic is not supported in sage attention."
    #     if q_scale is not None:
    #         q = q * q_scale
    #     q, k, v = half(q), half(k), half(v)
    #     x = sageattention.sageattn(q, k, v, "NHD", is_causal=causal, sm_scale=softmax_scale)
    #     del q, k, v
    #     return x.type(out_dtype)

    assert (
        not split_attn
    ), "split_attn is not supported in flash attention 3 or sage attention."

    # preprocess query: in Wan 2.1, q_lens is always None.
    if q_lens is None:
        q = half(q.flatten(0, 1))
        q_lens = torch.tensor([lq] * b, dtype=torch.int32).to(
            device=q.device, non_blocking=True
        )
    else:
        q = half(torch.cat([u[:v] for u, v in zip(q, q_lens)]))

    # preprocess key, value
    if k_lens is None:
        k = half(k.flatten(0, 1))
        v = half(v.flatten(0, 1))
        k_lens = torch.tensor([lk] * b, dtype=torch.int32).to(
            device=k.device, non_blocking=True
        )
    else:
        # Note: in Wan 2.1, all k_lens are same if we have same image size in the batch.
        if min(k_lens) == max(k_lens) and k.shape[1] == k_lens[0]:
            # B, L, N, C -> BN, L, C
            k = half(k.flatten(0, 1))
            v = half(v.flatten(0, 1))
        else:
            k = half(torch.cat([u[:v] for u, v in zip(k, k_lens)]))
            v = half(torch.cat([u[:v] for u, v in zip(v, k_lens)]))

    q = q.to(v.dtype)
    k = k.to(v.dtype)

    if q_scale is not None:
        q = q * q_scale

    # if version is not None and version == 3 and not FLASH_ATTN_3_AVAILABLE:
    #     warnings.warn("Flash attention 3 is not available, use flash attention 2 instead.")

    # apply attention
    # if (version is None or version == 3) and FLASH_ATTN_3_AVAILABLE:
    if attn_mode == "flash3":
        if not FLASH_ATTN_3_AVAILABLE:
            raise RuntimeError(
                "Flash Attention 3 is not available. Please install flash-attn>=3.0 or use a different attention mode."
            )

        # Not tested yet
        # Note: dropout_p, window_size are not supported in FA3 now.
        x = flash_attn_interface.flash_attn_varlen_func(
            q=q,
            k=k,
            v=v,
            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens])
            .cumsum(0, dtype=torch.int32)
            .to(q.device, non_blocking=True),
            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens])
            .cumsum(0, dtype=torch.int32)
            .to(q.device, non_blocking=True),
            seqused_q=None,
            seqused_k=None,
            max_seqlen_q=lq,
            max_seqlen_k=lk,
            softmax_scale=softmax_scale,
            causal=causal,
            deterministic=deterministic,
        )[0].unflatten(0, (b, lq))
    # elif (version is None or version == 2) and FLASH_ATTN_2_AVAILABLE:
    #     # assert FLASH_ATTN_2_AVAILABLE
    #     x = flash_attn.flash_attn_varlen_func(
    #         q=q,
    #         k=k,
    #         v=v,
    #         cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(0, dtype=torch.int32).to(q.device, non_blocking=True),
    #         cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(0, dtype=torch.int32).to(q.device, non_blocking=True),
    #         max_seqlen_q=lq,
    #         max_seqlen_k=lk,
    #         dropout_p=dropout_p,
    #         softmax_scale=softmax_scale,
    #         causal=causal,
    #         window_size=window_size,
    #         deterministic=deterministic,
    #     ).unflatten(0, (b, lq))
    # elif version is None and SAGE_ATTN_AVAILABLE:
    elif attn_mode == "sageattn":
        if not SAGE_ATTN_AVAILABLE:
            raise RuntimeError(
                "SageAttention is not available. Please install sageattention or use a different attention mode."
            )

        # print("Using sage attention")
        assert not causal, "SAGE attention does not support causal attention."
        x = sageattention.sageattn_varlen(
            q=q,
            k=k,
            v=v,
            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens])
            .cumsum(0, dtype=torch.int32)
            .to(q.device, non_blocking=True),
            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens])
            .cumsum(0, dtype=torch.int32)
            .to(q.device, non_blocking=True),
            max_seqlen_q=lq,
            max_seqlen_k=lk,
            sm_scale=softmax_scale,
        ).unflatten(0, (b, lq))
    else:
        raise ValueError(f"Unknown attention mode: {attn_mode}")

    # output
    return x.type(out_dtype)


def attention(
    q,
    k,
    v,
    q_lens=None,
    k_lens=None,
    dropout_p=0.0,
    softmax_scale=None,
    q_scale=None,
    causal=False,
    window_size=(-1, -1),
    deterministic=False,
    dtype=torch.bfloat16,
    fa_version=None,
    batched_rotary: Optional[torch.Tensor] = None,
):
    if FLASH_ATTN_2_AVAILABLE or FLASH_ATTN_3_AVAILABLE:
        return flash_attention(
            q=q,  # type: ignore
            k=k,  # type: ignore
            v=v,  # type: ignore
            q_lens=q_lens,
            k_lens=k_lens,
            dropout_p=dropout_p,
            softmax_scale=softmax_scale,
            q_scale=q_scale,
            causal=causal,
            window_size=window_size,
            deterministic=deterministic,
            dtype=dtype,
            version=fa_version,
            batched_rotary=batched_rotary,
        )
    else:
        if q_lens is not None or k_lens is not None:
            warnings.warn(
                "Padding mask is disabled when using scaled_dot_product_attention. It can have a significant impact on performance."
            )
        attn_mask = None

        q = q.transpose(1, 2).to(dtype)
        k = k.transpose(1, 2).to(dtype)
        v = v.transpose(1, 2).to(dtype)

        out = torch.nn.functional.scaled_dot_product_attention(
            q, k, v, attn_mask=attn_mask, is_causal=causal, dropout_p=dropout_p
        )

        out = out.transpose(1, 2).contiguous()
        return out


def local_patching(x, height, width, group_size):
    """
    Applies fractal flattening to group tokens from the same spatial patch together.
    This is crucial for block-wise attention to be effective.
    """
    if group_size > 0:
        x = rearrange(
            x,
            "b t (h g1) (w g2) c -> b t (h w) (g1 g2) c",
            h=height // group_size,
            w=width // group_size,
            g1=group_size,
            g2=group_size,
        )
    else:
        x = rearrange(x, "b c t h w -> b c t (h w)", h=height, w=width)
    return x


def local_merge(x, height, width, group_size):
    """
    Reverses the local_patching operation to restore the original token order.
    """
    if group_size > 0:
        x = rearrange(
            x,
            "b t (h w) (g1 g2) c -> b t (h g1) (w g2) c ",
            h=height // group_size,
            w=width // group_size,
            g1=group_size,
            g2=group_size,
        )
    else:
        x = rearrange(x, "b c (h w) -> b c h w", h=height, w=width)
    return x


@torch.no_grad()
def sta(
    T: int, H: int, W: int, wT: int = 3, wH: int = 3, wW: int = 3, device: str = "cuda"
) -> BlockMask:
    l = torch.Tensor([T, H, W]).max()
    r = torch.arange(0, l, 1, dtype=torch.int16, device=device)  # type: ignore
    mat = (r.unsqueeze(1) - r.unsqueeze(0)).abs()
    sta_t, sta_h, sta_w = (
        mat[:T, :T].flatten(),
        mat[:H, :H].flatten(),
        mat[:W, :W].flatten(),
    )
    sta_t = sta_t <= wT // 2
    sta_h = sta_h <= wH // 2
    sta_w = sta_w <= wW // 2
    sta_hw = (
        (sta_h.unsqueeze(1) * sta_w.unsqueeze(0))
        .reshape(H, H, W, W)
        .transpose(1, 2)
        .flatten()
    )
    sta = (
        (sta_t.unsqueeze(1) * sta_hw.unsqueeze(0))
        .reshape(T, T, H * W, H * W)
        .transpose(1, 2)
    )
    sta = sta.reshape(T * H * W, T * H * W).unsqueeze_(0).unsqueeze_(0)

    # BlockMask creation
    kv_nb = sta.sum(-1).to(torch.int32)
    kv_inds = sta.argsort(dim=-1, descending=True).to(torch.int32)
    return BlockMask.from_kv_blocks(
        torch.zeros_like(kv_nb), kv_inds, kv_nb, kv_inds, BLOCK_SIZE=64, mask_mod=None
    )


@torch.no_grad()
def sta_nabla(
    T: int, H: int, W: int, wT: int = 3, wH: int = 3, wW: int = 3, device: str = "cuda"
) -> Tensor:
    l = torch.Tensor([T, H, W]).max()
    r = torch.arange(0, l, 1, dtype=torch.int16, device=device)  # type: ignore
    mat = (r.unsqueeze(1) - r.unsqueeze(0)).abs()
    sta_t, sta_h, sta_w = (
        mat[:T, :T].flatten(),
        mat[:H, :H].flatten(),
        mat[:W, :W].flatten(),
    )
    sta_t = sta_t <= wT // 2
    sta_h = sta_h <= wH // 2
    sta_w = sta_w <= wW // 2
    sta_hw = (
        (sta_h.unsqueeze(1) * sta_w.unsqueeze(0))
        .reshape(H, H, W, W)
        .transpose(1, 2)
        .flatten()
    )
    sta = (
        (sta_t.unsqueeze(1) * sta_hw.unsqueeze(0))
        .reshape(T, T, H * W, H * W)
        .transpose(1, 2)
    )
    return sta.reshape(T * H * W, T * H * W)


@torch.no_grad()
def nablaT(
    q: Tensor,
    k: Tensor,
    seq: Tensor,
    T: int,
    H: int,
    W: int,
    wT: int = 3,
    wH: int = 3,
    wW: int = 3,
    thr: float = 0.9,
    sta_att=1,
    device: str = "cuda",
) -> BlockMask:
    # Map estimation
    B, h, S, D = q.shape
    qa = q.reshape(B, h, S // 64, 64, D).mean(-2)
    ka = k.reshape(B, h, S // 64, 64, D).mean(-2).transpose(-2, -1)
    map = qa @ ka

    d = torch.diff(seq)
    doc = (
        torch.eye(d.numel(), dtype=torch.bool, device=device)
        .repeat_interleave(d * H * W, dim=0)
        .repeat_interleave(d * H * W, dim=1)
    )
    map += doc.log()
    map = torch.softmax(map / math.sqrt(D), dim=-1)

    # Map binarization
    vals, inds = map.sort(-1)
    cvals = vals.cumsum_(-1)
    mask = (cvals >= 1 - thr).int()
    mask = mask.gather(-1, inds.argsort(-1))
    if sta_att > 0:
        sta = sta_nabla(T, H, W, wT, wH, wW, device=device).unsqueeze_(0).unsqueeze_(0)
        mask = torch.logical_or(mask, sta)
    mask = torch.logical_and(mask, doc)

    # BlockMask creation
    kv_nb = mask.sum(-1).to(torch.int32)
    kv_inds = mask.argsort(dim=-1, descending=True).to(torch.int32)
    return BlockMask.from_kv_blocks(
        torch.zeros_like(kv_nb), kv_inds, kv_nb, kv_inds, BLOCK_SIZE=64, mask_mod=None
    )
</file>

<file path="wan/modules/model.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/modules/model.py (Apache)
## Based on: https://github.com/gen-ai-team/Wan2.1-NABLA/blob/main/wan/modules/model.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import math
from typing import Dict, List, Optional, Union

import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint
from accelerate import init_empty_weights
from torch.distributed.tensor.parallel import (
    PrepareModuleInput,
    PrepareModuleOutput,
    parallelize_module,
)
from torch.distributed._tensor import Replicate, Shard  # type: ignore

from utils.lora_utils import load_safetensors_with_lora_and_fp8
from utils.safetensors_utils import MemoryEfficientSafeOpen, load_safetensors

import logging

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)

from wan.modules.attention import flash_attention
from utils.device_utils import clean_memory_on_device
from modules.custom_offloading_utils import ModelOffloader
from modules.fp8_optimization_utils import (
    apply_fp8_monkey_patch,
    optimize_state_dict_with_fp8,
)

from .attention import local_patching, local_merge, nablaT, sta, sta_nabla
from utils.tread import TREADRouter, MaskInfo  # minimal router integration
from torch.nn.attention.flex_attention import flex_attention

try:
    flex = torch.compile(
        flex_attention, mode="max-autotune-no-cudagraphs", dynamic=True
    )
except:
    logger.warning("torch.compile failed to compile flex_attention")
    logger.warning(
        "Using original Neighborhood Adaptive Block-Level Attention (NABLA) implementation"
    )
    flex = flex_attention


def sinusoidal_embedding_1d(dim, position):
    # preprocess
    assert dim % 2 == 0
    half = dim // 2
    position = position.type(torch.float64)

    # calculation
    sinusoid = torch.outer(
        position, torch.pow(10000, -torch.arange(half).to(position).div(half))
    )
    x = torch.cat([torch.cos(sinusoid), torch.sin(sinusoid)], dim=1)
    return x


# @amp.autocast(enabled=False)
# no autocast is needed for rope_apply, because it is already in float64
def rope_params(max_seq_len, dim, theta=10000):
    assert dim % 2 == 0
    freqs = torch.outer(
        torch.arange(max_seq_len),
        1.0 / torch.pow(theta, torch.arange(0, dim, 2).to(torch.float64).div(dim)),
    )
    freqs = torch.polar(torch.ones_like(freqs), freqs)
    return freqs


# @amp.autocast(enabled=False)
def rope_apply(x, grid_sizes, freqs, fractal=False):
    device_type = x.device.type
    with torch.amp.autocast(device_type=device_type, enabled=False):  # type: ignore
        n, c = x.size(2), x.size(3) // 2

        # split freqs
        freqs = freqs.split([c - 2 * (c // 3), c // 3, c // 3], dim=1)

        # loop over samples
        output = []
        for i, (f, h, w) in enumerate(grid_sizes.tolist()):
            seq_len = f * h * w

            # precompute multipliers
            x_i = torch.view_as_complex(
                x[i, :seq_len].to(torch.float64).reshape(seq_len, n, -1, 2)
            )
            freqs_i = torch.cat(
                [
                    freqs[0][:f].view(f, 1, 1, -1).expand(f, h, w, -1),
                    freqs[1][:h].view(1, h, 1, -1).expand(f, h, w, -1),
                    freqs[2][:w].view(1, 1, w, -1).expand(f, h, w, -1),
                ],
                dim=-1,
            )

            if fractal:
                freqs_i = local_patching(freqs_i.unsqueeze(0), h, w, 8)
            freqs_i = freqs_i.reshape(seq_len, 1, -1)

            # apply rotary embedding
            x_i = torch.view_as_real(x_i * freqs_i).flatten(2)
            x_i = torch.cat([x_i, x[i, seq_len:]])

            # append to collection
            output.append(x_i)
        return torch.stack(output).float()


def calculate_freqs_i(fhw, c, freqs):
    f, h, w = fhw
    freqs = freqs.split([c - 2 * (c // 3), c // 3, c // 3], dim=1)
    freqs_f = freqs[0][:f]
    freqs_i = torch.cat(
        [
            freqs_f.view(f, 1, 1, -1).expand(f, h, w, -1),
            freqs[1][:h].view(1, h, 1, -1).expand(f, h, w, -1),
            freqs[2][:w].view(1, 1, w, -1).expand(f, h, w, -1),
        ],
        dim=-1,
    ).reshape(f * h * w, 1, -1)
    return freqs_i


# inplace version of rope_apply
def rope_apply_inplace_cached(x, grid_sizes, freqs_list):
    # with torch.amp.autocast(device_type=device_type, enabled=False):
    rope_dtype = torch.float64  # float32 does not reduce memory usage significantly

    n, c = x.size(2), x.size(3) // 2

    # loop over samples
    for i, (f, h, w) in enumerate(grid_sizes.tolist()):
        seq_len = f * h * w

        # precompute multipliers
        x_i = torch.view_as_complex(
            x[i, :seq_len].to(rope_dtype).reshape(seq_len, n, -1, 2)
        )
        freqs_i = freqs_list[i]

        # apply rotary embedding
        x_i = torch.view_as_real(x_i * freqs_i).flatten(2)
        # x_i = torch.cat([x_i, x[i, seq_len:]])

        # inplace update
        x[i, :seq_len] = x_i.to(x.dtype)

    return x


class WanRMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.dim = dim
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        r"""
        Args:
            x(Tensor): Shape [B, L, C]
        """
        # return self._norm(x.float()).type_as(x) * self.weight
        # support fp8
        return self._norm(x.float()).type_as(x) * self.weight.to(x.dtype)

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)

    # def forward(self, x):
    #     r"""
    #     Args:
    #         x(Tensor): Shape [B, L, C]
    #     """
    #     # inplace version, also supports fp8 -> does not have significant performance improvement
    #     original_dtype = x.dtype
    #     x = x.float()
    #     y = x.pow(2).mean(dim=-1, keepdim=True)
    #     y.add_(self.eps)
    #     y.rsqrt_()
    #     x *= y
    #     x = x.to(original_dtype)
    #     x *= self.weight.to(original_dtype)
    #     return x


class WanLayerNorm(nn.LayerNorm):
    def __init__(self, dim, eps=1e-6, elementwise_affine=False):
        super().__init__(dim, elementwise_affine=elementwise_affine, eps=eps)

    def forward(self, x):
        r"""
        Args:
            x(Tensor): Shape [B, L, C]
        """
        return super().forward(x.float()).type_as(x)


class WanSelfAttention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        window_size=(-1, -1),
        qk_norm=True,
        eps=1e-6,
        attn_mode="torch",
        split_attn=False,
        sparse_algo=None,
    ):
        assert dim % num_heads == 0
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.window_size = window_size
        self.qk_norm = qk_norm
        self.eps = eps
        self.attn_mode = attn_mode
        self.split_attn = split_attn

        # layers
        self.q = nn.Linear(dim, dim)
        self.k = nn.Linear(dim, dim)
        self.v = nn.Linear(dim, dim)
        self.o = nn.Linear(dim, dim)
        self.norm_q = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()
        self.norm_k = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()

        self.sparse_algo = sparse_algo
        self.mask_func = self.construct_mask_func(sparse_algo) if sparse_algo else None

    def construct_mask_func(self, sparse_algo):
        if "_" in sparse_algo:
            nabla_cfg, sta_cfg = sparse_algo.split("_")
            thr = float(nabla_cfg.split("-")[-1])
            wT, wH, wW = [int(cfg) for cfg in sta_cfg.split("-")[1:]]
            return lambda q, k, seq, T, H, W: nablaT(
                q, k, seq, T, H, W, thr=thr, wT=wT, wH=wH, wW=wW
            )

        if "nabla" in sparse_algo:
            thr = float(sparse_algo.split("-")[-1])
            return lambda q, k, seq, T, H, W: nablaT(
                q, k, seq, T, H, W, thr=thr, sta_att=0
            )
        elif "sta" in sparse_algo:
            wT, wH, wW = [int(cfg) for cfg in sparse_algo.split("-")[1:]]
            return lambda q, k, seq, T, H, W: sta(T, H, W, wT=wT, wH=wH, wW=wW)
        else:
            raise ValueError(f"Invalid sparse algorithm: {sparse_algo}")

    def forward(
        self,
        x,
        seq_lens,
        grid_sizes,
        freqs,
        sparse_attention: bool = False,
        batched_rotary: Optional[torch.Tensor] = None,
    ):
        r"""
        Args:
            x(Tensor): Shape [B, L, num_heads, C / num_heads]
            seq_lens(Tensor): Shape [B]
            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)
            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]
            sparse_attention (bool): Whether to use sparse attention (default: False)
        """
        # # query, key, value function
        # def qkv_fn(x):
        #     q = self.norm_q(self.q(x)).view(b, s, n, d)
        #     k = self.norm_k(self.k(x)).view(b, s, n, d)
        #     v = self.v(x).view(b, s, n, d)
        #     return q, k, v
        # q, k, v = qkv_fn(x)
        # del x
        # query, key, value function

        if sparse_attention:
            b, _, n, d = *x.shape[:2], self.num_heads, self.head_dim

            def qkv_fn(x):
                q = self.norm_q(self.q(x)).view(b, -1, n, d)
                k = self.norm_k(self.k(x)).view(b, -1, n, d)
                v = self.v(x).view(b, -1, n, d)
                return q, k, v

            q, k, v = qkv_fn(x)
            # This block is only executed if sparse attention is configured
            q_rope = (
                rope_apply(q, grid_sizes, freqs, fractal=True)
                .to(dtype=torch.bfloat16)
                .transpose(1, 2)
            )
            k_rope = (
                rope_apply(k, grid_sizes, freqs, fractal=True)
                .to(dtype=torch.bfloat16)
                .transpose(1, 2)
            )

            T, H, W = grid_sizes.tolist()[0]
            H, W = H // 8, W // 8
            seq = torch.tensor([0, T], dtype=torch.int32).to(q_rope.device)
            block_mask = self.mask_func(q_rope, k_rope, seq, T, H, W)  # type: ignore

            x = flex(
                q_rope, k_rope, v.transpose(1, 2), block_mask=block_mask
            ).transpose(  # type: ignore
                1, 2
            )  # type: ignore
        else:
            b, s, n, d = *x.shape[:2], self.num_heads, self.head_dim

            q = self.q(x)
            k = self.k(x)
            v = self.v(x)
            del x
            q = self.norm_q(q)
            k = self.norm_k(k)
            q = q.view(b, s, n, d)
            k = k.view(b, s, n, d)
            v = v.view(b, s, n, d)

            # Only apply standard RoPE when not routing with batched rotary
            if batched_rotary is None:
                rope_apply_inplace_cached(q, grid_sizes, freqs)
                rope_apply_inplace_cached(k, grid_sizes, freqs)
            qkv = [q, k, v]
            del q, k, v
            x = flash_attention(
                qkv,
                k_lens=seq_lens,
                window_size=self.window_size,
                attn_mode=self.attn_mode,
                split_attn=self.split_attn,
                batched_rotary=batched_rotary,
            )

        # output
        x = x.flatten(2)
        x = self.o(x)
        return x


class WanCrossAttention(WanSelfAttention):
    def forward(self, x, context, context_lens):
        r"""
        Args:
            x(Tensor): Shape [B, L1, C]
            context(Tensor): Shape [B, L2, C]
            context_lens(Tensor): Shape [B]
        """
        b, n, d = x.size(0), self.num_heads, self.head_dim

        # compute query, key, value
        # q = self.norm_q(self.q(x)).view(b, -1, n, d)
        # k = self.norm_k(self.k(context)).view(b, -1, n, d)
        # v = self.v(context).view(b, -1, n, d)
        q = self.q(x)
        del x
        k = self.k(context)
        v = self.v(context)
        del context
        q = self.norm_q(q)
        k = self.norm_k(k)
        q = q.view(b, -1, n, d)
        k = k.view(b, -1, n, d)
        v = v.view(b, -1, n, d)

        # compute attention
        qkv = [q, k, v]
        del q, k, v
        x = flash_attention(
            qkv,
            k_lens=context_lens,
            attn_mode=self.attn_mode,
            split_attn=self.split_attn,
        )

        # output
        x = x.flatten(2)
        x = self.o(x)
        return x


WAN_CROSSATTENTION_CLASSES = {
    "t2v_cross_attn": WanCrossAttention,
}


class WanAttentionBlock(nn.Module):
    def __init__(
        self,
        cross_attn_type,
        dim,
        ffn_dim,
        num_heads,
        window_size=(-1, -1),
        qk_norm=True,
        cross_attn_norm=False,
        eps=1e-6,
        attn_mode="torch",
        split_attn=False,
        sparse_algo=None,
        model_version="2.1",
    ):
        super().__init__()
        self.dim = dim
        self.ffn_dim = ffn_dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.qk_norm = qk_norm
        self.cross_attn_norm = cross_attn_norm
        self.eps = eps
        self.sparse_algo = sparse_algo
        self.model_version = model_version

        # layers
        self.norm1 = WanLayerNorm(dim, eps)
        self.self_attn = WanSelfAttention(
            dim,
            num_heads,
            window_size,
            qk_norm,
            eps,
            attn_mode,
            split_attn,
            sparse_algo=sparse_algo,
        )
        self.norm3 = (
            WanLayerNorm(dim, eps, elementwise_affine=True)
            if cross_attn_norm
            else nn.Identity()
        )
        self.cross_attn = WAN_CROSSATTENTION_CLASSES[cross_attn_type](
            dim, num_heads, (-1, -1), qk_norm, eps, attn_mode, split_attn
        )
        self.norm2 = WanLayerNorm(dim, eps)
        self.ffn = nn.Sequential(
            nn.Linear(dim, ffn_dim),
            nn.GELU(approximate="tanh"),
            nn.Linear(ffn_dim, dim),
        )

        # modulation
        self.modulation = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)

        self.gradient_checkpointing = False

    def enable_gradient_checkpointing(self):
        self.gradient_checkpointing = True

    def disable_gradient_checkpointing(self):
        self.gradient_checkpointing = False

    def _forward(
        self,
        x,
        e,
        seq_lens,
        grid_sizes,
        freqs,
        context,
        context_lens,
        sparse_attention: bool = False,
        batched_rotary: torch.Tensor | None = None,
    ):
        r"""
        Args:
            x(Tensor): Shape [B, L, C]
            e(Tensor): Shape [B, 6, C] (v2.1) or [B, L, 6, C] (per-token, v2.2)
            seq_lens(Tensor): Shape [B], length of each sequence in batch
            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)
            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]
            sparse_attention (bool): Whether to use sparse attention (default: False)
        """
        if self.model_version == "2.1":
            e = self.modulation.to(torch.float32) + e
            e = e.chunk(6, dim=1)
            assert e[0].dtype == torch.float32

            # self-attention
            y = self.self_attn(
                self.norm1(x).float() * (1 + e[1]) + e[0],
                seq_lens,
                grid_sizes,
                freqs if batched_rotary is None else None,
                sparse_attention=sparse_attention,
                # pass through when provided during routing
                # type: ignore[arg-type]
                batched_rotary=batched_rotary,
            )
            x = x + y.to(torch.float32) * e[2]
            del y

            # cross-attention & ffn
            x = x + self.cross_attn(self.norm3(x), context, context_lens)
            del context
            y = self.ffn(self.norm2(x).float() * (1 + e[4]) + e[3])
            x = x + y.to(torch.float32) * e[5]
            del y
        else:  # For Wan2.2
            e = self.modulation.to(torch.float32) + e
            e = e.chunk(6, dim=2)  # e is [B, L, 6, C] for 2.2
            assert e[0].dtype == torch.float32

            # self-attention
            y = self.self_attn(
                self.norm1(x).float() * (1 + e[1].squeeze(2)) + e[0].squeeze(2),
                seq_lens,
                grid_sizes,
                freqs if batched_rotary is None else None,
                sparse_attention=sparse_attention,
                batched_rotary=batched_rotary,  # type: ignore[arg-type]
            )
            x = x + y.to(torch.float32) * e[2].squeeze(2)
            del y

            # cross-attention & ffn
            x = x + self.cross_attn(self.norm3(x), context, context_lens)
            del context
            y = self.ffn(
                self.norm2(x).float() * (1 + e[4].squeeze(2)) + e[3].squeeze(2)
            )
            x = x + y.to(torch.float32) * e[5].squeeze(2)

            del y
        return x

    def forward(
        self,
        x,
        e,
        seq_lens,
        grid_sizes,
        freqs,
        context,
        context_lens,
        sparse_attention: bool = False,
        batched_rotary: torch.Tensor | None = None,
    ):
        """
        Forward pass for WanAttentionBlock.

        Args:
            x (Tensor): Input tensor of shape [B, L, C]
            e (Tensor): Modulation tensor of shape [B, 6, C]
            seq_lens (Tensor): Sequence lengths [B]
            grid_sizes (Tensor): Grid sizes [B, 3]
            freqs (Tensor): Rotary embedding frequencies
            context (Tensor): Context tensor
            context_lens (Tensor): Context lengths
            sparse_attention (bool): Whether to use sparse attention (default: False)
        """
        if self.training and self.gradient_checkpointing:
            return checkpoint(
                self._forward,
                x,
                e,
                seq_lens,
                grid_sizes,
                freqs,
                context,
                context_lens,
                sparse_attention,
                batched_rotary,
                use_reentrant=False,
            )
        return self._forward(
            x,
            e,
            seq_lens,
            grid_sizes,
            freqs,
            context,
            context_lens,
            sparse_attention,
            batched_rotary,
        )


class Head(nn.Module):
    def __init__(self, dim, out_dim, patch_size, eps=1e-6, model_version="2.1"):
        super().__init__()
        self.dim = dim
        self.out_dim = out_dim
        self.patch_size = patch_size
        self.eps = eps
        self.model_version = model_version

        # layers
        out_dim = math.prod(patch_size) * out_dim
        self.norm = WanLayerNorm(dim, eps)
        self.head = nn.Linear(dim, out_dim)

        # modulation
        self.modulation = nn.Parameter(torch.randn(1, 2, dim) / dim**0.5)

    def forward(self, x, e):
        r"""
        Args:
            x(Tensor): Shape [B, L, C]
            e(Tensor): Shape [B, C] for 2.1, [B, L, 6, C] for 2.2
        """
        assert e.dtype == torch.float32

        if self.model_version == "2.1":
            e = (self.modulation.to(torch.float32) + e.unsqueeze(1)).chunk(2, dim=1)
            x = self.head(self.norm(x) * (1 + e[1]) + e[0])
        else:  # For Wan2.2
            e = (self.modulation.unsqueeze(0).to(torch.float32) + e.unsqueeze(2)).chunk(
                2, dim=2
            )
            x = self.head(self.norm(x) * (1 + e[1].squeeze(2)) + e[0].squeeze(2))

        return x


class MLPProj(torch.nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()

        self.proj = torch.nn.Sequential(
            torch.nn.LayerNorm(in_dim),
            torch.nn.Linear(in_dim, in_dim),
            torch.nn.GELU(),
            torch.nn.Linear(in_dim, out_dim),
            torch.nn.LayerNorm(out_dim),
        )

    def forward(self, image_embeds):
        clip_extra_context_tokens = self.proj(image_embeds)
        return clip_extra_context_tokens


FP8_OPTIMIZATION_TARGET_KEYS = ["blocks"]
FP8_OPTIMIZATION_EXCLUDE_KEYS = [
    "norm",
    "patch_embedding",
    "text_embedding",
    "time_embedding",
    "time_projection",
    "head",
    "modulation",
    "img_emb",
]


class WanModel(nn.Module):  # ModelMixin, ConfigMixin):
    r"""
    Wan diffusion backbone supporting both text-to-video and image-to-video.
    """

    ignore_for_config = [
        "patch_size",
        "cross_attn_norm",
        "qk_norm",
        "text_dim",
        "window_size",
    ]
    _no_split_modules = ["WanAttentionBlock"]

    # @register_to_config
    def __init__(
        self,
        model_version="2.1",
        patch_size=(1, 2, 2),
        text_len=512,
        in_dim=16,
        dim=2048,
        ffn_dim=8192,
        freq_dim=256,
        text_dim=4096,
        out_dim=16,
        num_heads=16,
        num_layers=32,
        window_size=(-1, -1),
        qk_norm=True,
        cross_attn_norm=True,
        eps=1e-6,
        attn_mode=None,
        split_attn=False,
        sparse_algo=None,
        use_fvdm: bool = False,
    ):
        r"""
        Initialize the diffusion model backbone.

        Args:
            model_version (`str`, *optional*, defaults to '2.1'):
                Version of the model, e.g., '2.1' or '2.2'. This is used to determine the modulation strategy.
            patch_size (`tuple`, *optional*, defaults to (1, 2, 2)):
                3D patch dimensions for video embedding (t_patch, h_patch, w_patch)
            text_len (`int`, *optional*, defaults to 512):
                Fixed length for text embeddings
            in_dim (`int`, *optional*, defaults to 16):
                Input video channels (C_in)
            dim (`int`, *optional*, defaults to 2048):
                Hidden dimension of the transformer
            ffn_dim (`int`, *optional*, defaults to 8192):
                Intermediate dimension in feed-forward network
            freq_dim (`int`, *optional*, defaults to 256):
                Dimension for sinusoidal time embeddings
            text_dim (`int`, *optional*, defaults to 4096):
                Input dimension for text embeddings
            out_dim (`int`, *optional*, defaults to 16):
                Output video channels (C_out)
            num_heads (`int`, *optional*, defaults to 16):
                Number of attention heads
            num_layers (`int`, *optional*, defaults to 32):
                Number of transformer blocks
            window_size (`tuple`, *optional*, defaults to (-1, -1)):
                Window size for local attention (-1 indicates global attention)
            qk_norm (`bool`, *optional*, defaults to True):
                Enable query/key normalization
            cross_attn_norm (`bool`, *optional*, defaults to False):
                Enable cross-attention normalization
            eps (`float`, *optional*, defaults to 1e-6):
                Epsilon value for normalization layers
            sparse_algo (`str`, *optional*, defaults to None):
                Sparse attention algorithm, e.g. "nabla-0.5_sta-5-5-11"

        """

        super().__init__()

        self.patch_size = patch_size
        self.text_len = text_len
        self.in_dim = in_dim
        self.dim = dim
        self.ffn_dim = ffn_dim
        self.freq_dim = freq_dim
        self.text_dim = text_dim
        self.out_dim = out_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.window_size = window_size
        self.qk_norm = qk_norm
        self.cross_attn_norm = cross_attn_norm
        self.eps = eps
        self.attn_mode = attn_mode if attn_mode is not None else "torch"
        self.split_attn = split_attn
        self.model_version = model_version
        # FVDM uses per-token modulation; force effective version to 2.2 when enabled
        self.use_fvdm = use_fvdm
        self.effective_model_version = "2.2" if self.use_fvdm else self.model_version

        # embeddings
        self.patch_embedding = nn.Conv3d(
            in_dim, dim, kernel_size=patch_size, stride=patch_size
        )
        self.text_embedding = nn.Sequential(
            nn.Linear(text_dim, dim), nn.GELU(approximate="tanh"), nn.Linear(dim, dim)
        )

        self.time_embedding = nn.Sequential(
            nn.Linear(freq_dim, dim), nn.SiLU(), nn.Linear(dim, dim)
        )
        self.time_projection = nn.Sequential(nn.SiLU(), nn.Linear(dim, dim * 6))

        # blocks
        cross_attn_type = "t2v_cross_attn"
        self.sparse_algo = sparse_algo

        self.blocks = nn.ModuleList(
            [
                WanAttentionBlock(
                    cross_attn_type,
                    dim,
                    ffn_dim,
                    num_heads,
                    window_size,
                    qk_norm,
                    cross_attn_norm,
                    eps,
                    attn_mode,  # type: ignore
                    split_attn,
                    sparse_algo=sparse_algo,
                    model_version=self.effective_model_version,
                )
                for _ in range(num_layers)
            ]
        )

        # head
        self.head = Head(
            dim, out_dim, patch_size, eps, model_version=self.effective_model_version
        )

        # buffers (don't use register_buffer otherwise dtype will be changed in to())
        assert (dim % num_heads) == 0 and (dim // num_heads) % 2 == 0
        d = dim // num_heads
        self.freqs = torch.cat(
            [
                rope_params(1024, d - 4 * (d // 6)),
                rope_params(1024, 2 * (d // 6)),
                rope_params(1024, 2 * (d // 6)),
            ],
            dim=1,
        )
        self.freqs_fhw = {}

        # initialize weights
        self.init_weights()

        self.gradient_checkpointing = False

        # offloading
        self.blocks_to_swap = None
        self.offloader = None

        # TREAD routing (optional; set via set_router)
        self._tread_router: TREADRouter | None = None
        self._tread_routes: list[dict] | None = None

    def set_router(self, router: TREADRouter, routes: list[dict]) -> None:
        """Enable TREAD routing for this model.

        Args:
            router: TREADRouter instance
            routes: list of dicts with keys selection_ratio, start_layer_idx, end_layer_idx
        """
        self._tread_router = router
        self._tread_routes = routes or []

    @property
    def dtype(self):
        return self.patch_embedding.weight.dtype

    @property
    def device(self):
        return self.patch_embedding.weight.device

    def fp8_optimization(
        self,
        state_dict: dict[str, torch.Tensor],
        device: torch.device,
        move_to_device: bool,
        use_scaled_mm: bool = False,
    ) -> int:
        """
        Optimize the model state_dict with fp8.

        Args:
            state_dict (dict[str, torch.Tensor]):
                The state_dict of the model.
            device (torch.device):
                The device to calculate the weight.
            move_to_device (bool):
                Whether to move the weight to the device after optimization.
        """

        # inplace optimization
        state_dict = optimize_state_dict_with_fp8(
            state_dict,
            device,
            FP8_OPTIMIZATION_TARGET_KEYS,
            FP8_OPTIMIZATION_EXCLUDE_KEYS,
            move_to_device=move_to_device,
        )

        # apply monkey patching
        apply_fp8_monkey_patch(self, state_dict, use_scaled_mm=use_scaled_mm)

        return state_dict  # type: ignore

    def enable_gradient_checkpointing(self):
        self.gradient_checkpointing = True

        for block in self.blocks:  # type: ignore
            block.enable_gradient_checkpointing()  # type: ignore

        print(f"WanModel: Gradient checkpointing enabled.")

    def disable_gradient_checkpointing(self):
        self.gradient_checkpointing = False

        for block in self.blocks:  # type: ignore
            block.disable_gradient_checkpointing()  # type: ignore

        print(f"WanModel: Gradient checkpointing disabled.")

    def enable_block_swap(
        self, blocks_to_swap: int, device: torch.device, supports_backward: bool
    ):
        self.blocks_to_swap = blocks_to_swap
        self.num_blocks = len(self.blocks)  # type: ignore

        assert (
            self.blocks_to_swap <= self.num_blocks - 1
        ), f"Cannot swap more than {self.num_blocks - 1} blocks. Requested {self.blocks_to_swap} blocks to swap."

        self.offloader = ModelOffloader(
            "wan_attn_block",
            self.blocks,  # type: ignore
            self.num_blocks,
            self.blocks_to_swap,
            supports_backward,
            device,  # , debug=True
        )
        print(
            f"WanModel: Block swap enabled. Swapping {self.blocks_to_swap} blocks out of {self.num_blocks} blocks. Supports backward: {supports_backward}"
        )

    def switch_block_swap_for_inference(self):
        if self.blocks_to_swap:
            self.offloader.set_forward_only(True)  # type: ignore
            self.prepare_block_swap_before_forward()
            print(f"WanModel: Block swap set to forward only.")

    def switch_block_swap_for_training(self):
        if self.blocks_to_swap:
            self.offloader.set_forward_only(False)  # type: ignore
            self.prepare_block_swap_before_forward()
            print(f"WanModel: Block swap set to forward and backward.")

    def move_to_device_except_swap_blocks(self, device: torch.device):
        # assume model is on cpu. do not move blocks to device to reduce temporary memory usage
        if self.blocks_to_swap:
            save_blocks = self.blocks
            self.blocks = None

        self.to(device)

        if self.blocks_to_swap:
            self.blocks = save_blocks

    def prepare_block_swap_before_forward(self):
        if self.blocks_to_swap is None or self.blocks_to_swap == 0:
            return
        self.offloader.prepare_block_devices_before_forward(self.blocks)  # type: ignore

    def forward(
        self,
        x,
        t,
        context,
        seq_len,
        clip_fea=None,  # not used
        y=None,
        skip_block_indices=None,
        sparse_attention=False,
        force_keep_mask: torch.Tensor | None = None,
        controlnet_states: tuple[torch.Tensor, ...] | list[torch.Tensor] | None = None,
        controlnet_weight: float = 1.0,
        controlnet_stride: int = 1,
        dispersive_loss_target_block: int | None = None,
        return_intermediate: bool = False,
    ):
        r"""
        Forward pass through the diffusion model

        Args:
            x (List[Tensor]):
                List of input video tensors, each with shape [C_in, F, H, W]
            t (Tensor):
                Diffusion timesteps tensor of shape [B]
            context (List[Tensor]):
                List of text embeddings each with shape [L, C]
            seq_len (`int`):
                Maximum sequence length for positional encoding
            y (List[Tensor], *optional*):
                Conditional video inputs for image-to-video mode, same shape as x
            skip_block_indices (List[int], *optional*):
                Indices of blocks to skip during forward pass
            sparse_attention (bool, *optional*):
                Whether to use sparse attention
        Returns:
            List[Tensor]:
                List of denoised video tensors with original input shapes [C_out, F, H / 8, W / 8]
        """
        # remove assertions to work with Fun-Control T2V
        # if self.model_type == "i2v":
        #     assert clip_fea is not None and y is not None
        # params
        device = self.patch_embedding.weight.device
        if self.freqs.device != device:
            self.freqs = self.freqs.to(device)

        if y is not None:
            x = [torch.cat([u, v], dim=0) for u, v in zip(x, y)]
            y = None

        # embeddings
        x = [
            self.patch_embedding(u.unsqueeze(0)) for u in x
        ]  # x[0].shape = [1, 5120, F, H, W]

        if sparse_attention:
            T, H, W = x[0].shape[2:5]

        grid_sizes = torch.stack(
            [torch.tensor(u.shape[2:], dtype=torch.long) for u in x]
        )  # list of [F, H, W]

        freqs_list = []
        for i, fhw in enumerate(grid_sizes):
            fhw = tuple(fhw.tolist())
            if fhw not in self.freqs_fhw:
                c = self.dim // self.num_heads // 2
                self.freqs_fhw[fhw] = calculate_freqs_i(fhw, c, self.freqs)
            freqs_list.append(self.freqs_fhw[fhw])

        x = [u.flatten(2).transpose(1, 2) for u in x]
        seq_lens = torch.tensor([u.size(1) for u in x], dtype=torch.long)
        assert (
            seq_lens.max() <= seq_len
        ), f"Sequence length exceeds maximum allowed length {seq_len}. Got {seq_lens.max()}"
        x = torch.cat(
            [
                torch.cat([u, u.new_zeros(1, seq_len - u.size(1), u.size(2))], dim=1)
                for u in x
            ]
        )

        # time embeddings
        # with amp.autocast(dtype=torch.float32):
        with torch.amp.autocast(device_type=device.type, dtype=torch.float32):  # type: ignore
            if self.use_fvdm:
                # Vectorized timesteps path (FVDM/PUSA). Accept t of shape [B] or [B, F].
                # Build per-token embeddings without per-sample Python loops.
                B = seq_lens.numel()
                Fp = grid_sizes[:, 0]  # [B]
                Hp = grid_sizes[:, 1]
                Wp = grid_sizes[:, 2]
                patches_per_frame = Hp * Wp  # [B]
                L = Fp * patches_per_frame  # [B]

                # Construct per-frame timesteps flattened across batch
                if t.dim() == 1:
                    # Repeat each scalar timestep by the number of frames in that sample
                    t_frames_flat = t.repeat_interleave(Fp)
                elif t.dim() == 2 and (Fp == t.size(1)).all():
                    # Already per-frame aligned for all samples
                    t_frames_flat = t.reshape(-1)
                else:
                    # Rare mismatch: fall back to safe per-sample handling
                    # This path is uncommon and preserves correctness for ragged inputs
                    t_list = []
                    for i in range(B):
                        Fi = int(Fp[i].item())
                        if t.dim() == 1:
                            t_i = t[i].view(1).expand(Fi)
                        else:
                            t_i_full = t[i]
                            F_in = t_i_full.numel()
                            if F_in == Fi:
                                t_i = t_i_full
                            elif F_in > Fi:
                                t_i = t_i_full[:Fi]
                            else:
                                rep = (Fi + F_in - 1) // F_in
                                t_i = t_i_full.repeat(rep)[:Fi]
                        t_list.append(t_i)
                    t_frames_flat = torch.cat(t_list, dim=0)

                # Expand per-frame to per-token using per-frame repeats = patches_per_frame
                repeats_frames = patches_per_frame.repeat_interleave(Fp)
                t_tokens_flat = t_frames_flat.repeat_interleave(repeats_frames)

                # Compute time embeddings for all tokens at once
                e_tokens_flat = self.time_embedding(
                    sinusoidal_embedding_1d(self.freq_dim, t_tokens_flat).float()
                )  # [sum(L), dim]
                e0_tokens_flat = self.time_projection(e_tokens_flat).unflatten(
                    1, (6, self.dim)
                )  # [sum(L), 6, dim]

                # Prepare padded batch tensors [B, seq_len, ...]
                e = x.new_zeros((B, seq_len, self.dim), dtype=e_tokens_flat.dtype)
                e0 = x.new_zeros((B, seq_len, 6, self.dim), dtype=e_tokens_flat.dtype)

                # Build indices to scatter without Python loops
                # batch indices per token
                batch_idx_flat = torch.arange(B, device=x.device).repeat_interleave(L)
                # token positions within each sample
                token_pos_list = [
                    torch.arange(int(L[i].item()), device=x.device) for i in range(B)
                ]
                token_pos_flat = torch.cat(token_pos_list, dim=0)

                e[batch_idx_flat, token_pos_flat, :] = e_tokens_flat
                e0[batch_idx_flat, token_pos_flat, :, :] = e0_tokens_flat
            elif self.effective_model_version == "2.1":
                e = self.time_embedding(
                    sinusoidal_embedding_1d(self.freq_dim, t).float()
                )
                e0 = self.time_projection(e).unflatten(1, (6, self.dim))
            else:  # For Wan2.2 (standard per-token path)
                if t.dim() == 1:
                    t = t.unsqueeze(1).expand(-1, seq_len)

                bt = t.size(0)
                t = t.flatten()
                e = self.time_embedding(
                    sinusoidal_embedding_1d(self.freq_dim, t)
                    .unflatten(0, (bt, seq_len))
                    .float()
                )
                e0 = self.time_projection(e).unflatten(2, (6, self.dim))
        assert e.dtype == torch.float32 and e0.dtype == torch.float32

        # context
        context_lens = None
        if type(context) is list:
            context = torch.stack(
                [
                    torch.cat([u, u.new_zeros(self.text_len - u.size(0), u.size(1))])
                    for u in context
                ]
            )
        context = self.text_embedding(context)

        # i2v is not used

        # if clip_fea is not None:
        #     context_clip = self.img_emb(clip_fea)  # bs x 257 x dim
        #     context = torch.concat([context_clip, context], dim=1)
        #     clip_fea = None
        #     context_clip = None

        # arguments
        kwargs = dict(
            e=e0,
            seq_lens=seq_lens,
            grid_sizes=grid_sizes,
            freqs=freqs_list,
            context=context,
            context_lens=context_lens,
            sparse_attention=sparse_attention,
        )

        if sparse_attention:
            P = 8
            x = x.reshape(1, T, H, W, -1)
            x = local_patching(x, H, W, P)
            x = x.reshape(1, H * W * T, -1)  # type: ignore

        if self.blocks_to_swap:
            clean_memory_on_device(device)

        # print(f"x: {x.shape}, e: {e0.shape}, context: {context.shape}, seq_lens: {seq_lens}")
        # --- TREAD routing state ---
        routes = self._tread_routes or []
        router = self._tread_router
        use_routing = self.training and torch.is_grad_enabled() and len(routes) > 0
        route_ptr = 0
        routing_now = False
        tread_mask_info: MaskInfo | None = None
        saved_tokens = None

        # Normalize negative indices in routes
        if use_routing and routes:
            total_layers = len(self.blocks)  # type: ignore

            def _to_pos(idx: int) -> int:
                return idx if idx >= 0 else total_layers + idx

            routes = [
                {
                    **r,
                    "start_layer_idx": _to_pos(int(r["start_layer_idx"])),
                    "end_layer_idx": _to_pos(int(r["end_layer_idx"])),
                }
                for r in routes
            ]

        # Normalize control states container to a list for indexing
        if controlnet_states is not None and isinstance(controlnet_states, tuple):
            controlnet_states = list(controlnet_states)

        # Optional intermediate capture for dispersive loss
        intermediate_z = None

        for block_idx, block in enumerate(self.blocks):  # type: ignore
            is_block_skipped = (
                skip_block_indices is not None and block_idx in skip_block_indices
            )

            if self.blocks_to_swap and not is_block_skipped:
                self.offloader.wait_for_block(block_idx)  # type: ignore

            # TREAD: START route at configured layer index
            if (
                use_routing
                and route_ptr < len(routes)
                and block_idx == int(routes[route_ptr]["start_layer_idx"])  # type: ignore
            ):
                assert router is not None
                mask_ratio = float(routes[route_ptr]["selection_ratio"])  # type: ignore
                # Only route video tokens (x). Text/context stays full
                tread_mask_info = router.get_mask(
                    x, mask_ratio=mask_ratio, force_keep=force_keep_mask  # type: ignore
                )
                saved_tokens = x.clone()
                x = router.start_route(x, tread_mask_info)
                routing_now = True

                # Build a single batched rotary tensor for the routed tokens
                # Collapse per-sample freqs_list[(L,1,D)] -> (B, S, D) after shuffle, then slice keep_len
                B = x.size(0)
                S_keep = x.size(1)
                # Concatenate per-sample to a tensor (B, S_full, D), then shuffle per batch
                full_rope = []
                for f in freqs_list:
                    full_rope.append(f.squeeze(1))  # (L,D)
                full_rope = torch.stack(full_rope, dim=0)  # (B, S_full, D)
                shuf = torch.take_along_dim(
                    full_rope,
                    tread_mask_info.ids_shuffle.unsqueeze(-1).expand(
                        B, -1, full_rope.size(-1)
                    ),
                    dim=1,
                )
                batched_rotary = shuf[:, :S_keep, :]
                kwargs["batched_rotary"] = batched_rotary

            if not is_block_skipped:
                # Switch attention to use batched_rotary when routing
                if (
                    routing_now
                    and "batched_rotary" in kwargs
                    and getattr(self, "_tread_mode", "full") == "full"
                ):
                    # We pass batched_rotary down via the attention wrapper
                    # by temporarily overriding freqs with None and adding batched_rotary
                    x = block(
                        x,
                        e=kwargs["e"],
                        seq_lens=kwargs["seq_lens"],
                        grid_sizes=kwargs["grid_sizes"],
                        freqs=None,
                        context=kwargs["context"],
                        context_lens=kwargs["context_lens"],
                        sparse_attention=kwargs["sparse_attention"],
                        batched_rotary=kwargs["batched_rotary"],
                    )
                else:
                    x = block(x, **kwargs)

                # Inject ControlNet states if provided
                if controlnet_states is not None and controlnet_weight != 0.0:
                    if controlnet_stride <= 0:
                        controlnet_idx = block_idx
                    else:
                        # downsample states along layers by stride
                        if block_idx % max(1, int(controlnet_stride)) == 0:
                            controlnet_idx = block_idx // int(max(1, controlnet_stride))
                        else:
                            controlnet_idx = None
                    if controlnet_idx is not None and controlnet_idx < len(
                        controlnet_states
                    ):
                        cn = controlnet_states[controlnet_idx]
                        # Expect shape: (B, L_tokens, dim)
                        if cn is not None and isinstance(cn, torch.Tensor):
                            try:
                                x = x + cn.to(dtype=x.dtype, device=x.device) * float(
                                    controlnet_weight
                                )
                            except Exception:
                                # Best-effort: ignore shape mismatch to avoid breaking non-control runs
                                pass

                # Capture intermediate representation if requested
                if (
                    return_intermediate
                    and dispersive_loss_target_block is not None
                    and block_idx == int(dispersive_loss_target_block)
                ):
                    # x shape is (B, L, C). Keep as-is for downstream flattening.
                    intermediate_z = x

            # TREAD: END route
            if routing_now and route_ptr < len(routes) and block_idx == int(routes[route_ptr]["end_layer_idx"]):  # type: ignore
                assert tread_mask_info is not None and saved_tokens is not None
                x = router.end_route(x, tread_mask_info, original_x=saved_tokens)  # type: ignore
                routing_now = False
                route_ptr += 1
                kwargs.pop("batched_rotary", None)
                kwargs["freqs"] = freqs_list  # restore full rotary embeddings

            if self.blocks_to_swap:
                self.offloader.submit_move_blocks_forward(self.blocks, block_idx)  # type: ignore

        # head
        x = self.head(x, e)

        if sparse_attention:
            P = 8
            x = x.reshape(1, T, (H * W) // (P * P), P * P, -1)  # type: ignore
            x = local_merge(x, H, W, P)
            x = x.reshape(1, T * H * W, -1)  # type: ignore

        # unpatchify
        x = self.unpatchify(x, grid_sizes)
        outputs = [u.float() for u in x]
        if return_intermediate:
            return outputs, intermediate_z
        return outputs

    def unpatchify(self, x, grid_sizes):
        r"""
        Reconstruct video tensors from patch embeddings.

        Args:
            x (List[Tensor]):
                List of patchified features, each with shape [L, C_out * prod(patch_size)]
            grid_sizes (Tensor):
                Original spatial-temporal grid dimensions before patching,
                    shape [B, 3] (3 dimensions correspond to F_patches, H_patches, W_patches)
        Returns:
            List[Tensor]:
                Reconstructed video tensors with shape [C_out, F, H / 8, W / 8]
        """

        c = self.out_dim
        out = []
        for u, v in zip(x, grid_sizes.tolist()):
            u = u[: math.prod(v)].view(*v, *self.patch_size, c)
            u = torch.einsum("fhwpqrc->cfphqwr", u)
            u = u.reshape(c, *[i * j for i, j in zip(v, self.patch_size)])
            out.append(u)
        return out

    def init_weights(self):
        r"""
        Initialize model parameters using Xavier initialization.
        """

        # basic init
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

        # init embeddings
        nn.init.xavier_uniform_(self.patch_embedding.weight.flatten(1))
        for m in self.text_embedding.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=0.02)
        for m in self.time_embedding.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=0.02)

        # init output layer
        nn.init.zeros_(self.head.head.weight)


def detect_wan_sd_dtype(path: str) -> torch.dtype:
    # get dtype from model weights
    with MemoryEfficientSafeOpen(path) as f:
        keys = set(f.keys())
        key1 = "model.diffusion_model.blocks.0.cross_attn.k.weight"  # 1.3B
        key2 = "blocks.0.cross_attn.k.weight"  # 14B
        if key1 in keys:
            dit_dtype = f.get_tensor(key1).dtype
        elif key2 in keys:
            dit_dtype = f.get_tensor(key2).dtype
        else:
            raise ValueError(f"Could not find the dtype in the model weights: {path}")
    logger.info(f"Detected DiT dtype: {dit_dtype}")
    return dit_dtype


def load_wan_model(
    config: any,  # type: ignore
    device: Union[str, torch.device],
    dit_path: str,
    attn_mode: str,
    split_attn: bool,
    loading_device: Union[str, torch.device],
    dit_weight_dtype: Optional[torch.dtype],
    fp8_scaled: bool = False,
    lora_weights_list: Optional[Dict[str, torch.Tensor]] = None,
    lora_multipliers: Optional[List[float]] = None,
    sparse_algo: Optional[str] = None,
    use_scaled_mm: bool = False,
    use_fvdm: bool = False,
) -> WanModel:
    """
    Load a WAN model from the specified checkpoint.

    Args:
        config (any): Configuration object containing model parameters.
        device (Union[str, torch.device]): Device to load the model on.
        dit_path (str): Path to the DiT model checkpoint.
        attn_mode (str): Attention mode to use, e.g., "torch", "flash", etc.
        split_attn (bool): Whether to use split attention.
        loading_device (Union[str, torch.device]): Device to load the model weights on.
        dit_weight_dtype (Optional[torch.dtype]): Data type of the DiT weights. If None, it will be loaded as is (same as the state_dict) or scaled for fp8. if not None, model weights will be casted to this dtype.
        fp8_scaled (bool): Whether to use fp8 scaling for the model weights.
        lora_weights_list (Optional[Dict[str, torch.Tensor]]): LoRA weights to apply, if any.
        lora_multipliers (Optional[List[float]]): LoRA multipliers for the weights, if any.
        sparse_algo (Optional[str]): Sparse attention algorithm to use, if any.
    """

    # dit_weight_dtype is None for fp8_scaled
    assert (not fp8_scaled and dit_weight_dtype is not None) or (
        fp8_scaled and dit_weight_dtype is None
    )

    device = torch.device(device)
    loading_device = torch.device(loading_device)

    with init_empty_weights():
        logger.info(
            f"Creating WanModel, V2.2: {config.v2_2}, device: {device}, loading_device: {loading_device}, fp8_scaled: {fp8_scaled}"
        )

        model = WanModel(
            model_version="2.1" if not config.v2_2 else "2.2",
            dim=config.dim,
            eps=config.eps,
            ffn_dim=config.ffn_dim,
            freq_dim=config.freq_dim,
            in_dim=config.in_dim,
            num_heads=config.num_heads,
            num_layers=config.num_layers,
            out_dim=config.out_dim,
            text_len=config.text_len,
            attn_mode=attn_mode,
            split_attn=split_attn,
            sparse_algo=sparse_algo,
            use_fvdm=use_fvdm,
        )
        if dit_weight_dtype is not None:
            model.to(dit_weight_dtype)

    # load model weights with dynamic fp8 optimization and LoRA merging if needed
    logger.info(f"Loading DiT model from {dit_path}, device={loading_device}")

    sd = load_safetensors_with_lora_and_fp8(
        model_files=dit_path,
        lora_weights_list=lora_weights_list,
        lora_multipliers=lora_multipliers,
        fp8_optimization=fp8_scaled,
        calc_device=device,
        move_to_device=(loading_device == device),
        target_keys=FP8_OPTIMIZATION_TARGET_KEYS,
        exclude_keys=FP8_OPTIMIZATION_EXCLUDE_KEYS,
    )

    # remove "model.diffusion_model." prefix: 1.3B model has this prefix
    for key in list(sd.keys()):
        if key.startswith("model.diffusion_model."):
            sd[key[22:]] = sd.pop(key)

    if fp8_scaled:
        apply_fp8_monkey_patch(model, sd, use_scaled_mm=use_scaled_mm)

        if loading_device.type != "cpu":
            # make sure all the model weights are on the loading_device
            logger.info(f"Moving weights to {loading_device}")
            for key in sd.keys():  # type: ignore
                sd[key] = sd[key].to(loading_device)

    info = model.load_state_dict(sd, strict=True, assign=True)
    if dit_weight_dtype is not None:
        # cast model weights to the specified dtype. This makes sure that the model is in the correct dtype
        logger.info(f"Casting model weights to {dit_weight_dtype}")
        model = model.to(dit_weight_dtype)

    logger.info(f"Loaded DiT model from {dit_path}, info={info}")

    return model


def parallelize_seq_T2V(model, tp_mesh):
    if tp_mesh.size() > 1:
        for i, block in enumerate(model.blocks):
            plan = {
                "self_attn.norm_q": PrepareModuleOutput(
                    output_layouts=(Shard(1)), desired_output_layouts=(Shard(-1))
                ),
                "self_attn.norm_k": PrepareModuleOutput(
                    output_layouts=(Shard(1)), desired_output_layouts=(Shard(-1))
                ),
                "self_attn.v": PrepareModuleOutput(
                    output_layouts=(Shard(1)), desired_output_layouts=(Shard(-1))
                ),
                "self_attn.o": PrepareModuleInput(
                    input_layouts=(Shard(-1)),
                    desired_input_layouts=(Shard(1)),
                    use_local_output=True,
                ),
                "cross_attn.norm_q": PrepareModuleOutput(
                    output_layouts=(Shard(1)), desired_output_layouts=(Shard(-1))
                ),
                "cross_attn.norm_k": PrepareModuleOutput(
                    output_layouts=(Replicate()), desired_output_layouts=(Shard(-1))
                ),
                "cross_attn.v": PrepareModuleOutput(
                    output_layouts=(Replicate()), desired_output_layouts=(Shard(-1))
                ),
                "cross_attn.o": PrepareModuleInput(
                    input_layouts=(Shard(-1)),
                    desired_input_layouts=(Shard(1)),
                    use_local_output=True,
                ),
            }
            self_attn = block.self_attn
            self_attn.num_heads = self_attn.num_heads // tp_mesh.size()
            cross_attn = block.cross_attn
            cross_attn.num_heads = cross_attn.num_heads // tp_mesh.size()
            parallelize_module(block, tp_mesh, plan)

            if i == 0:
                parallelize_module(
                    block,
                    tp_mesh,
                    PrepareModuleInput(
                        input_layouts=(Replicate()),
                        desired_input_layouts=(Shard(1)),
                        use_local_output=True,
                    ),
                )

        plan = {
            "head": PrepareModuleOutput(
                output_layouts=(Shard(1)), desired_output_layouts=(Replicate())
            ),
        }
        parallelize_module(model, tp_mesh, plan)  # type: ignore
    return model
</file>

<file path="wan/modules/t5.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/modules/t5.py (Apache)

# Modified from transformers.models.t5.modeling_t5
# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.

import math
import os

import torch
import torch.nn as nn
import torch.nn.functional as F

from wan.modules.tokenizers import HuggingfaceTokenizer
from accelerate import init_empty_weights
from safetensors.torch import load_file

import logging

from common.logger import get_logger

logger = get_logger(__name__, level=logging.INFO)


def fp16_clamp(x):
    if x.dtype == torch.float16 and torch.isinf(x).any():
        clamp = torch.finfo(x.dtype).max - 1000
        x = torch.clamp(x, min=-clamp, max=clamp)
    return x


def init_weights(m):
    if isinstance(m, T5LayerNorm):
        nn.init.ones_(m.weight)
    elif isinstance(m, T5Model):
        nn.init.normal_(m.token_embedding.weight, std=1.0)
    elif isinstance(m, T5FeedForward):
        nn.init.normal_(m.gate[0].weight, std=m.dim**-0.5)  # type: ignore
        nn.init.normal_(m.fc1.weight, std=m.dim**-0.5)
        nn.init.normal_(m.fc2.weight, std=m.dim_ffn**-0.5)
    elif isinstance(m, T5Attention):
        nn.init.normal_(m.q.weight, std=(m.dim * m.dim_attn) ** -0.5)
        nn.init.normal_(m.k.weight, std=m.dim**-0.5)
        nn.init.normal_(m.v.weight, std=m.dim**-0.5)
        nn.init.normal_(m.o.weight, std=(m.num_heads * m.dim_attn) ** -0.5)
    elif isinstance(m, T5RelativeEmbedding):
        nn.init.normal_(
            m.embedding.weight, std=(2 * m.num_buckets * m.num_heads) ** -0.5
        )


class GELU(nn.Module):
    def forward(self, x):
        return (
            0.5
            * x
            * (
                1.0
                + torch.tanh(
                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))
                )
            )
        )


class T5LayerNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super(T5LayerNorm, self).__init__()
        self.dim = dim
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        x = x * torch.rsqrt(x.float().pow(2).mean(dim=-1, keepdim=True) + self.eps)
        if self.weight.dtype in [torch.float16, torch.bfloat16]:
            x = x.type_as(self.weight)
        return self.weight * x


class T5Attention(nn.Module):
    def __init__(self, dim, dim_attn, num_heads, dropout=0.1):
        assert dim_attn % num_heads == 0
        super(T5Attention, self).__init__()
        self.dim = dim
        self.dim_attn = dim_attn
        self.num_heads = num_heads
        self.head_dim = dim_attn // num_heads

        # layers
        self.q = nn.Linear(dim, dim_attn, bias=False)
        self.k = nn.Linear(dim, dim_attn, bias=False)
        self.v = nn.Linear(dim, dim_attn, bias=False)
        self.o = nn.Linear(dim_attn, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, context=None, mask=None, pos_bias=None):
        """
        x:          [B, L1, C].
        context:    [B, L2, C] or None.
        mask:       [B, L2] or [B, L1, L2] or None.
        """
        # check inputs
        context = x if context is None else context
        b, n, c = x.size(0), self.num_heads, self.head_dim

        # compute query, key, value
        q = self.q(x).view(b, -1, n, c)
        k = self.k(context).view(b, -1, n, c)
        v = self.v(context).view(b, -1, n, c)

        # attention bias
        attn_bias = x.new_zeros(b, n, q.size(1), k.size(1))
        if pos_bias is not None:
            attn_bias += pos_bias
        if mask is not None:
            assert mask.ndim in [2, 3]
            mask = mask.view(b, 1, 1, -1) if mask.ndim == 2 else mask.unsqueeze(1)
            attn_bias.masked_fill_(mask == 0, torch.finfo(x.dtype).min)

        # compute attention (T5 does not use scaling)
        attn = torch.einsum("binc,bjnc->bnij", q, k) + attn_bias
        attn = F.softmax(attn.float(), dim=-1).type_as(attn)
        x = torch.einsum("bnij,bjnc->binc", attn, v)

        # output
        x = x.reshape(b, -1, n * c)
        x = self.o(x)
        x = self.dropout(x)
        return x


class T5FeedForward(nn.Module):
    def __init__(self, dim, dim_ffn, dropout=0.1):
        super(T5FeedForward, self).__init__()
        self.dim = dim
        self.dim_ffn = dim_ffn

        # layers
        self.gate = nn.Sequential(nn.Linear(dim, dim_ffn, bias=False), GELU())
        self.fc1 = nn.Linear(dim, dim_ffn, bias=False)
        self.fc2 = nn.Linear(dim_ffn, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x) * self.gate(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x


class T5SelfAttention(nn.Module):
    def __init__(
        self,
        dim,
        dim_attn,
        dim_ffn,
        num_heads,
        num_buckets,
        shared_pos=True,
        dropout=0.1,
    ):
        super(T5SelfAttention, self).__init__()
        self.dim = dim
        self.dim_attn = dim_attn
        self.dim_ffn = dim_ffn
        self.num_heads = num_heads
        self.num_buckets = num_buckets
        self.shared_pos = shared_pos

        # layers
        self.norm1 = T5LayerNorm(dim)
        self.attn = T5Attention(dim, dim_attn, num_heads, dropout)
        self.norm2 = T5LayerNorm(dim)
        self.ffn = T5FeedForward(dim, dim_ffn, dropout)
        self.pos_embedding = (
            None
            if shared_pos
            else T5RelativeEmbedding(num_buckets, num_heads, bidirectional=True)
        )

    def forward(self, x, mask=None, pos_bias=None):
        e = pos_bias if self.shared_pos else self.pos_embedding(x.size(1), x.size(1))  # type: ignore
        x = fp16_clamp(x + self.attn(self.norm1(x), mask=mask, pos_bias=e))
        x = fp16_clamp(x + self.ffn(self.norm2(x)))
        return x


class T5CrossAttention(nn.Module):
    def __init__(
        self,
        dim,
        dim_attn,
        dim_ffn,
        num_heads,
        num_buckets,
        shared_pos=True,
        dropout=0.1,
    ):
        super(T5CrossAttention, self).__init__()
        self.dim = dim
        self.dim_attn = dim_attn
        self.dim_ffn = dim_ffn
        self.num_heads = num_heads
        self.num_buckets = num_buckets
        self.shared_pos = shared_pos

        # layers
        self.norm1 = T5LayerNorm(dim)
        self.self_attn = T5Attention(dim, dim_attn, num_heads, dropout)
        self.norm2 = T5LayerNorm(dim)
        self.cross_attn = T5Attention(dim, dim_attn, num_heads, dropout)
        self.norm3 = T5LayerNorm(dim)
        self.ffn = T5FeedForward(dim, dim_ffn, dropout)
        self.pos_embedding = (
            None
            if shared_pos
            else T5RelativeEmbedding(num_buckets, num_heads, bidirectional=False)
        )

    def forward(
        self, x, mask=None, encoder_states=None, encoder_mask=None, pos_bias=None
    ):
        e = pos_bias if self.shared_pos else self.pos_embedding(x.size(1), x.size(1))  # type: ignore
        x = fp16_clamp(x + self.self_attn(self.norm1(x), mask=mask, pos_bias=e))
        x = fp16_clamp(
            x
            + self.cross_attn(self.norm2(x), context=encoder_states, mask=encoder_mask)
        )
        x = fp16_clamp(x + self.ffn(self.norm3(x)))
        return x


class T5RelativeEmbedding(nn.Module):
    def __init__(self, num_buckets, num_heads, bidirectional, max_dist=128):
        super(T5RelativeEmbedding, self).__init__()
        self.num_buckets = num_buckets
        self.num_heads = num_heads
        self.bidirectional = bidirectional
        self.max_dist = max_dist

        # layers
        self.embedding = nn.Embedding(num_buckets, num_heads)

    def forward(self, lq, lk):
        device = self.embedding.weight.device
        # rel_pos = torch.arange(lk).unsqueeze(0).to(device) - \
        #     torch.arange(lq).unsqueeze(1).to(device)
        rel_pos = torch.arange(lk, device=device).unsqueeze(0) - torch.arange(
            lq, device=device
        ).unsqueeze(1)
        rel_pos = self._relative_position_bucket(rel_pos)
        rel_pos_embeds = self.embedding(rel_pos)
        rel_pos_embeds = rel_pos_embeds.permute(2, 0, 1).unsqueeze(0)  # [1, N, Lq, Lk]
        return rel_pos_embeds.contiguous()

    def _relative_position_bucket(self, rel_pos):
        # preprocess
        if self.bidirectional:
            num_buckets = self.num_buckets // 2
            rel_buckets = (rel_pos > 0).long() * num_buckets
            rel_pos = torch.abs(rel_pos)
        else:
            num_buckets = self.num_buckets
            rel_buckets = 0
            rel_pos = -torch.min(rel_pos, torch.zeros_like(rel_pos))

        # embeddings for small and large positions
        max_exact = num_buckets // 2
        rel_pos_large = (
            max_exact
            + (
                torch.log(rel_pos.float() / max_exact)
                / math.log(self.max_dist / max_exact)
                * (num_buckets - max_exact)
            ).long()
        )
        rel_pos_large = torch.min(
            rel_pos_large, torch.full_like(rel_pos_large, num_buckets - 1)
        )
        rel_buckets += torch.where(rel_pos < max_exact, rel_pos, rel_pos_large)
        return rel_buckets


class T5Encoder(nn.Module):
    def __init__(
        self,
        vocab,
        dim,
        dim_attn,
        dim_ffn,
        num_heads,
        num_layers,
        num_buckets,
        shared_pos=True,
        dropout=0.1,
    ):
        super(T5Encoder, self).__init__()
        self.dim = dim
        self.dim_attn = dim_attn
        self.dim_ffn = dim_ffn
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_buckets = num_buckets
        self.shared_pos = shared_pos

        # layers
        self.token_embedding = (
            vocab if isinstance(vocab, nn.Embedding) else nn.Embedding(vocab, dim)
        )
        self.pos_embedding = (
            T5RelativeEmbedding(num_buckets, num_heads, bidirectional=True)
            if shared_pos
            else None
        )
        self.dropout = nn.Dropout(dropout)
        self.blocks = nn.ModuleList(
            [
                T5SelfAttention(
                    dim, dim_attn, dim_ffn, num_heads, num_buckets, shared_pos, dropout
                )
                for _ in range(num_layers)
            ]
        )
        self.norm = T5LayerNorm(dim)

        # initialize weights
        self.apply(init_weights)

    def prepare_fp8(self, target_dtype=torch.bfloat16):
        def forward_hook(module):
            def forward(hidden_states):
                hidden_gelu = module.act(module.wi_0(hidden_states))
                hidden_linear = module.wi_1(hidden_states)
                hidden_states = hidden_gelu * hidden_linear
                hidden_states = module.dropout(hidden_states)

                hidden_states = module.wo(hidden_states)
                return hidden_states

            return forward

        for module in self.modules():
            if module.__class__.__name__ in ["T5LayerNorm", "Embedding"]:
                # print("set", module.__class__.__name__, "to", target_dtype)
                module.to(target_dtype)
            if module.__class__.__name__ in ["T5DenseGatedActDense"]:
                # print("set", module.__class__.__name__, "hooks")
                module.forward = forward_hook(module)

    def forward(self, ids, mask=None):
        x = self.token_embedding(ids)
        x = self.dropout(x)
        e = self.pos_embedding(x.size(1), x.size(1)) if self.shared_pos else None  # type: ignore
        for block in self.blocks:
            x = block(x, mask, pos_bias=e)
        x = self.norm(x)
        x = self.dropout(x)
        return x


class T5Decoder(nn.Module):
    def __init__(
        self,
        vocab,
        dim,
        dim_attn,
        dim_ffn,
        num_heads,
        num_layers,
        num_buckets,
        shared_pos=True,
        dropout=0.1,
    ):
        super(T5Decoder, self).__init__()
        self.dim = dim
        self.dim_attn = dim_attn
        self.dim_ffn = dim_ffn
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_buckets = num_buckets
        self.shared_pos = shared_pos

        # layers
        self.token_embedding = (
            vocab if isinstance(vocab, nn.Embedding) else nn.Embedding(vocab, dim)
        )
        self.pos_embedding = (
            T5RelativeEmbedding(num_buckets, num_heads, bidirectional=False)
            if shared_pos
            else None
        )
        self.dropout = nn.Dropout(dropout)
        self.blocks = nn.ModuleList(
            [
                T5CrossAttention(
                    dim, dim_attn, dim_ffn, num_heads, num_buckets, shared_pos, dropout
                )
                for _ in range(num_layers)
            ]
        )
        self.norm = T5LayerNorm(dim)

        # initialize weights
        self.apply(init_weights)

    def forward(self, ids, mask=None, encoder_states=None, encoder_mask=None):
        b, s = ids.size()

        # causal mask
        if mask is None:
            mask = torch.tril(torch.ones(1, s, s).to(ids.device))
        elif mask.ndim == 2:
            mask = torch.tril(mask.unsqueeze(1).expand(-1, s, -1))

        # layers
        x = self.token_embedding(ids)
        x = self.dropout(x)
        e = self.pos_embedding(x.size(1), x.size(1)) if self.shared_pos else None  # type: ignore
        for block in self.blocks:
            x = block(x, mask, encoder_states, encoder_mask, pos_bias=e)
        x = self.norm(x)
        x = self.dropout(x)
        return x


class T5Model(nn.Module):
    def __init__(
        self,
        vocab_size,
        dim,
        dim_attn,
        dim_ffn,
        num_heads,
        encoder_layers,
        decoder_layers,
        num_buckets,
        shared_pos=True,
        dropout=0.1,
    ):
        super(T5Model, self).__init__()
        self.vocab_size = vocab_size
        self.dim = dim
        self.dim_attn = dim_attn
        self.dim_ffn = dim_ffn
        self.num_heads = num_heads
        self.encoder_layers = encoder_layers
        self.decoder_layers = decoder_layers
        self.num_buckets = num_buckets

        # layers
        self.token_embedding = nn.Embedding(vocab_size, dim)
        self.encoder = T5Encoder(
            self.token_embedding,
            dim,
            dim_attn,
            dim_ffn,
            num_heads,
            encoder_layers,
            num_buckets,
            shared_pos,
            dropout,
        )
        self.decoder = T5Decoder(
            self.token_embedding,
            dim,
            dim_attn,
            dim_ffn,
            num_heads,
            decoder_layers,
            num_buckets,
            shared_pos,
            dropout,
        )
        self.head = nn.Linear(dim, vocab_size, bias=False)

        # initialize weights
        self.apply(init_weights)

    def forward(self, encoder_ids, encoder_mask, decoder_ids, decoder_mask):
        x = self.encoder(encoder_ids, encoder_mask)
        x = self.decoder(decoder_ids, decoder_mask, x, encoder_mask)
        x = self.head(x)
        return x


def _t5(
    name,
    encoder_only=False,
    decoder_only=False,
    return_tokenizer=False,
    tokenizer_kwargs={},
    **kwargs,
):
    # dtype=torch.float32,
    # device="cpu",
    # sanity check
    assert not (encoder_only and decoder_only)

    # params
    if encoder_only:
        model_cls = T5Encoder
        kwargs["vocab"] = kwargs.pop("vocab_size")
        kwargs["num_layers"] = kwargs.pop("encoder_layers")
        _ = kwargs.pop("decoder_layers")
    elif decoder_only:
        model_cls = T5Decoder
        kwargs["vocab"] = kwargs.pop("vocab_size")
        kwargs["num_layers"] = kwargs.pop("decoder_layers")
        _ = kwargs.pop("encoder_layers")
    else:
        model_cls = T5Model

    # # init model
    # with torch.device(device):
    model = model_cls(**kwargs)

    # # set device
    # model = model.to(dtype=dtype, device=device)

    # init tokenizer
    if return_tokenizer:
        from wan.modules.tokenizers import HuggingfaceTokenizer

        tokenizer = HuggingfaceTokenizer(f"google/{name}", **tokenizer_kwargs)
        return model, tokenizer
    else:
        return model


def umt5_xxl(**kwargs):
    cfg = dict(
        vocab_size=256384,
        dim=4096,
        dim_attn=4096,
        dim_ffn=10240,
        num_heads=64,
        encoder_layers=24,
        decoder_layers=24,
        num_buckets=32,
        shared_pos=False,
        dropout=0.1,
    )
    cfg.update(**kwargs)
    return _t5("umt5-xxl", **cfg)  # type: ignore


class T5EncoderModel:
    def __init__(
        self,
        text_len,
        dtype=torch.bfloat16,
        device=torch.cuda.current_device(),
        checkpoint_path=None,
        tokenizer_path=None,
        shard_fn=None,
        weight_path=None,
        fp8=False,
    ):
        self.text_len = text_len
        self.dtype = dtype if not fp8 else torch.float8_e4m3fn
        self.device = device
        self.checkpoint_path = checkpoint_path
        self.tokenizer_path = tokenizer_path

        # init model
        with init_empty_weights():
            model = umt5_xxl(encoder_only=True, return_tokenizer=False)

        model = model.eval().requires_grad_(False)  # type: ignore
        if checkpoint_path is not None:
            logger.info(f"loading {checkpoint_path}")
            model.load_state_dict(torch.load(checkpoint_path, map_location="cpu"))
        else:
            logger.info(f"loading weights from {weight_path}")
            if os.path.splitext(weight_path)[1] == ".safetensors":  # type: ignore
                sd = load_file(weight_path)  # type: ignore
            else:
                sd = torch.load(weight_path, map_location="cpu", weights_only=True)  # type: ignore
            # remove prefix "encoder." from the state dict
            sd = {k.replace("encoder.", ""): v for k, v in sd.items()}
            model.load_state_dict(sd, strict=True, assign=True)

        logger.info(f"moving model to {device} and casting to {self.dtype}")
        model = model.to(device, dtype=self.dtype)

        if fp8:
            logger.info("preparing model for fp8")
            model.prepare_fp8(dtype)  # type: ignore

        self.model = model
        # if shard_fn is not None:
        #     self.model = shard_fn(self.model, sync_module_states=False)
        # else:
        #     self.model.to(self.device)
        # init tokenizer
        if tokenizer_path is None:
            tokenizer_path = "Wan-AI/Wan2.1-T2V-14B"
            subfolder = "google/umt5-xxl"
        else:
            subfolder = None
        self.tokenizer = HuggingfaceTokenizer(
            name=tokenizer_path,
            seq_len=text_len,
            clean="whitespace",
            subfolder=subfolder,
        )

    def __call__(self, texts, device):
        ids, mask = self.tokenizer(texts, return_mask=True, add_special_tokens=True)
        ids = ids.to(device)
        mask = mask.to(device)
        seq_lens = mask.gt(0).sum(dim=1).long()
        context = self.model(ids, mask)
        return [u[:v] for u, v in zip(context, seq_lens)]
</file>

<file path="wan/modules/tokenizers.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/modules/tokenizers.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import html
import string

import ftfy
import regex as re
from transformers import AutoTokenizer


def basic_clean(text):
    text = ftfy.fix_text(text)
    text = html.unescape(html.unescape(text))
    return text.strip()


def whitespace_clean(text):
    text = re.sub(r"\s+", " ", text)
    text = text.strip()
    return text


def canonicalize(text, keep_punctuation_exact_string=None):
    text = text.replace("_", " ")
    if keep_punctuation_exact_string:
        text = keep_punctuation_exact_string.join(
            part.translate(str.maketrans("", "", string.punctuation))
            for part in text.split(keep_punctuation_exact_string)
        )
    else:
        text = text.translate(str.maketrans("", "", string.punctuation))
    text = text.lower()
    text = re.sub(r"\s+", " ", text)
    return text.strip()


class HuggingfaceTokenizer:

    def __init__(self, name, seq_len=None, clean=None, **kwargs):
        assert clean in (None, "whitespace", "lower", "canonicalize")
        self.name = name
        self.seq_len = seq_len
        self.clean = clean

        # init tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(name, **kwargs)
        self.vocab_size = self.tokenizer.vocab_size

    def __call__(self, sequence, **kwargs):
        return_mask = kwargs.pop("return_mask", False)

        # arguments
        _kwargs = {"return_tensors": "pt"}
        if self.seq_len is not None:
            _kwargs.update(
                {
                    "padding": "max_length",
                    "truncation": True,
                    "max_length": self.seq_len,
                }  # type: ignore
            )
        _kwargs.update(**kwargs)

        # tokenization
        if isinstance(sequence, str):
            sequence = [sequence]
        if self.clean:
            sequence = [self._clean(u) for u in sequence]
        ids = self.tokenizer(sequence, **_kwargs)  # type: ignore

        # output
        if return_mask:
            return ids.input_ids, ids.attention_mask
        else:
            return ids.input_ids

    def _clean(self, text):
        if self.clean == "whitespace":
            text = whitespace_clean(basic_clean(text))
        elif self.clean == "lower":
            text = whitespace_clean(basic_clean(text)).lower()
        elif self.clean == "canonicalize":
            text = canonicalize(basic_clean(text))
        return text
</file>

<file path="wan/modules/vae.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/modules/vae.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import logging
import os
from typing import Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

from safetensors.torch import load_file

CACHE_T = 2


class CausalConv3d(nn.Conv3d):
    """
    Causal 3d convolusion.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._padding = (
            self.padding[2],
            self.padding[2],
            self.padding[1],
            self.padding[1],
            2 * self.padding[0],
            0,
        )
        self.padding = (0, 0, 0)

    def forward(self, x, cache_x=None):
        padding = list(self._padding)
        if cache_x is not None and self._padding[4] > 0:  # type: ignore
            cache_x = cache_x.to(x.device)
            x = torch.cat([cache_x, x], dim=2)
            padding[4] -= cache_x.shape[2]
        x = F.pad(x, padding)  # type: ignore

        return super().forward(x)


class RMS_norm(nn.Module):
    def __init__(self, dim, channel_first=True, images=True, bias=False):
        super().__init__()
        broadcastable_dims = (1, 1, 1) if not images else (1, 1)
        shape = (dim, *broadcastable_dims) if channel_first else (dim,)

        self.channel_first = channel_first
        self.scale = dim**0.5
        self.gamma = nn.Parameter(torch.ones(shape))
        self.bias = nn.Parameter(torch.zeros(shape)) if bias else 0.0

    def forward(self, x):
        return (
            F.normalize(x, dim=(1 if self.channel_first else -1))
            * self.scale
            * self.gamma
            + self.bias
        )


class Upsample(nn.Upsample):
    def forward(self, x):
        """
        Fix bfloat16 support for nearest neighbor interpolation.
        """
        return super().forward(x.float()).type_as(x)


class Resample(nn.Module):
    def __init__(self, dim, mode):
        assert mode in (
            "none",
            "upsample2d",
            "upsample3d",
            "downsample2d",
            "downsample3d",
        )
        super().__init__()
        self.dim = dim
        self.mode = mode

        # layers
        if mode == "upsample2d":
            self.resample = nn.Sequential(
                Upsample(scale_factor=(2.0, 2.0), mode="nearest-exact"),
                nn.Conv2d(dim, dim // 2, 3, padding=1),
            )
        elif mode == "upsample3d":
            self.resample = nn.Sequential(
                Upsample(scale_factor=(2.0, 2.0), mode="nearest-exact"),
                nn.Conv2d(dim, dim // 2, 3, padding=1),
            )
            self.time_conv = CausalConv3d(dim, dim * 2, (3, 1, 1), padding=(1, 0, 0))

        elif mode == "downsample2d":
            self.resample = nn.Sequential(
                nn.ZeroPad2d((0, 1, 0, 1)), nn.Conv2d(dim, dim, 3, stride=(2, 2))
            )
        elif mode == "downsample3d":
            self.resample = nn.Sequential(
                nn.ZeroPad2d((0, 1, 0, 1)), nn.Conv2d(dim, dim, 3, stride=(2, 2))
            )
            self.time_conv = CausalConv3d(
                dim, dim, (3, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0)
            )

        else:
            self.resample = nn.Identity()

        self.cache_device = None

    def set_cache_device(self, device):
        self.cache_device = device

    def forward(self, x, feat_cache=None, feat_idx=[0]):
        cache_device = self.cache_device if self.cache_device is not None else x.device

        b, c, t, h, w = x.size()
        if self.mode == "upsample3d":
            if feat_cache is not None:
                idx = feat_idx[0]
                if feat_cache[idx] is None:
                    feat_cache[idx] = "Rep"
                    feat_idx[0] += 1
                else:

                    cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
                    if (
                        cache_x.shape[2] < 2
                        and feat_cache[idx] is not None
                        and feat_cache[idx] != "Rep"
                    ):
                        # cache last frame of last two chunk
                        cache_x = torch.cat(
                            [
                                feat_cache[idx][:, :, -1, :, :]
                                .unsqueeze(2)
                                .to(cache_x.device),
                                cache_x,
                            ],
                            dim=2,
                        )
                    if (
                        cache_x.shape[2] < 2
                        and feat_cache[idx] is not None
                        and feat_cache[idx] == "Rep"
                    ):
                        cache_x = torch.cat(
                            [torch.zeros_like(cache_x).to(cache_x.device), cache_x],
                            dim=2,
                        )
                    if feat_cache[idx] == "Rep":
                        x = self.time_conv(x)
                    else:
                        x = self.time_conv(
                            x,
                            (
                                feat_cache[idx].to(x.device)
                                if feat_cache[idx] is not None
                                else None
                            ),
                        )
                    feat_cache[idx] = cache_x
                    feat_idx[0] += 1

                    x = x.reshape(b, 2, c, t, h, w)
                    x = torch.stack((x[:, 0, :, :, :, :], x[:, 1, :, :, :, :]), 3)
                    x = x.reshape(b, c, t * 2, h, w)
        t = x.shape[2]
        x = rearrange(x, "b c t h w -> (b t) c h w")
        x = self.resample(x)
        x = rearrange(x, "(b t) c h w -> b c t h w", t=t)

        if self.mode == "downsample3d":
            if feat_cache is not None:
                idx = feat_idx[0]
                if feat_cache[idx] is None:
                    feat_cache[idx] = x.clone().to(cache_device)
                    feat_idx[0] += 1
                else:

                    cache_x = x[:, :, -1:, :, :].clone().to(cache_device)
                    # if cache_x.shape[2] < 2 and feat_cache[idx] is not None and feat_cache[idx]!='Rep':
                    #     # cache last frame of last two chunk
                    #     cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)

                    x = self.time_conv(
                        torch.cat([feat_cache[idx][:, :, -1:, :, :].to(x.device), x], 2)
                    )
                    feat_cache[idx] = cache_x
                    feat_idx[0] += 1
        return x

    def init_weight(self, conv):
        conv_weight = conv.weight
        nn.init.zeros_(conv_weight)
        c1, c2, t, h, w = conv_weight.size()
        one_matrix = torch.eye(c1, c2)
        init_matrix = one_matrix
        nn.init.zeros_(conv_weight)
        # conv_weight.data[:,:,-1,1,1] = init_matrix * 0.5
        conv_weight.data[:, :, 1, 0, 0] = init_matrix  # * 0.5
        conv.weight.data.copy_(conv_weight)
        nn.init.zeros_(conv.bias.data)

    def init_weight2(self, conv):
        conv_weight = conv.weight.data
        nn.init.zeros_(conv_weight)
        c1, c2, t, h, w = conv_weight.size()
        init_matrix = torch.eye(c1 // 2, c2)
        # init_matrix = repeat(init_matrix, 'o ... -> (o 2) ...').permute(1,0,2).contiguous().reshape(c1,c2)
        conv_weight[: c1 // 2, :, -1, 0, 0] = init_matrix
        conv_weight[c1 // 2 :, :, -1, 0, 0] = init_matrix
        conv.weight.data.copy_(conv_weight)
        nn.init.zeros_(conv.bias.data)


class ResidualBlock(nn.Module):
    def __init__(self, in_dim, out_dim, dropout=0.0):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim

        # layers
        self.residual = nn.Sequential(
            RMS_norm(in_dim, images=False),
            nn.SiLU(),
            CausalConv3d(in_dim, out_dim, 3, padding=1),
            RMS_norm(out_dim, images=False),
            nn.SiLU(),
            nn.Dropout(dropout),
            CausalConv3d(out_dim, out_dim, 3, padding=1),
        )
        self.shortcut = (
            CausalConv3d(in_dim, out_dim, 1) if in_dim != out_dim else nn.Identity()
        )

        self.cache_device = None

    def set_cache_device(self, device):
        self.cache_device = device

    def forward(self, x, feat_cache=None, feat_idx=[0]):
        cache_device = self.cache_device if self.cache_device is not None else x.device

        h = self.shortcut(x)
        for layer in self.residual:
            if isinstance(layer, CausalConv3d) and feat_cache is not None:
                idx = feat_idx[0]
                cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
                if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                    # cache last frame of last two chunk
                    cache_x = torch.cat(
                        [
                            feat_cache[idx][:, :, -1, :, :]
                            .unsqueeze(2)
                            .to(cache_x.device),
                            cache_x,
                        ],
                        dim=2,
                    )
                x = layer(
                    x,
                    (
                        feat_cache[idx].to(x.device)
                        if feat_cache[idx] is not None
                        else None
                    ),
                )
                feat_cache[idx] = cache_x
                feat_idx[0] += 1
            else:
                x = layer(x)
        return x + h


class AttentionBlock(nn.Module):
    """
    Causal self-attention with a single head.
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

        # layers
        self.norm = RMS_norm(dim)
        self.to_qkv = nn.Conv2d(dim, dim * 3, 1)
        self.proj = nn.Conv2d(dim, dim, 1)

        # zero out the last layer params
        nn.init.zeros_(self.proj.weight)

    def forward(self, x):
        identity = x
        b, c, t, h, w = x.size()
        x = rearrange(x, "b c t h w -> (b t) c h w")
        x = self.norm(x)
        # compute query, key, value
        q, k, v = (
            self.to_qkv(x)
            .reshape(b * t, 1, c * 3, -1)
            .permute(0, 1, 3, 2)
            .contiguous()
            .chunk(3, dim=-1)
        )

        # apply attention
        x = F.scaled_dot_product_attention(
            q,
            k,
            v,
        )
        x = x.squeeze(1).permute(0, 2, 1).reshape(b * t, c, h, w)

        # output
        x = self.proj(x)
        x = rearrange(x, "(b t) c h w-> b c t h w", t=t)
        return x + identity


class Encoder3d(nn.Module):
    def __init__(
        self,
        dim=128,
        z_dim=4,
        dim_mult=[1, 2, 4, 4],
        num_res_blocks=2,
        attn_scales=[],
        temperal_downsample=[True, True, False],
        dropout=0.0,
    ):
        super().__init__()
        self.dim = dim
        self.z_dim = z_dim
        self.dim_mult = dim_mult
        self.num_res_blocks = num_res_blocks
        self.attn_scales = attn_scales
        self.temperal_downsample = temperal_downsample

        # dimensions
        dims = [dim * u for u in [1] + dim_mult]
        scale = 1.0

        # init block
        self.conv1 = CausalConv3d(3, dims[0], 3, padding=1)

        # downsample blocks
        downsamples = []
        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):
            # residual (+attention) blocks
            for _ in range(num_res_blocks):
                downsamples.append(ResidualBlock(in_dim, out_dim, dropout))
                if scale in attn_scales:
                    downsamples.append(AttentionBlock(out_dim))
                in_dim = out_dim

            # downsample block
            if i != len(dim_mult) - 1:
                mode = "downsample3d" if temperal_downsample[i] else "downsample2d"
                downsamples.append(Resample(out_dim, mode=mode))
                scale /= 2.0
        self.downsamples = nn.Sequential(*downsamples)

        # middle blocks
        self.middle = nn.Sequential(
            ResidualBlock(out_dim, out_dim, dropout),
            AttentionBlock(out_dim),
            ResidualBlock(out_dim, out_dim, dropout),
        )

        # output blocks
        self.head = nn.Sequential(
            RMS_norm(out_dim, images=False),
            nn.SiLU(),
            CausalConv3d(out_dim, z_dim, 3, padding=1),
        )

        self.cache_device = None

    def set_cache_device(self, device):
        self.cache_device = device

        # set cache device for all layers
        for layer in self.downsamples + self.middle + self.head:
            if isinstance(layer, Resample) or isinstance(layer, ResidualBlock):
                layer.set_cache_device(device)

    def forward(self, x, feat_cache=None, feat_idx=[0]):
        cache_device = self.cache_device if self.cache_device is not None else x.device

        if feat_cache is not None:
            idx = feat_idx[0]
            cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
            if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                # cache last frame of last two chunk
                cache_x = torch.cat(
                    [
                        feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device),
                        cache_x,
                    ],
                    dim=2,
                )
            x = self.conv1(
                x, feat_cache[idx].to(x.device) if feat_cache[idx] is not None else None
            )
            feat_cache[idx] = cache_x
            feat_idx[0] += 1
        else:
            x = self.conv1(x)

        ## downsamples
        for layer in self.downsamples:
            if feat_cache is not None:
                x = layer(x, feat_cache, feat_idx)
            else:
                x = layer(x)

        ## middle
        for layer in self.middle:
            if isinstance(layer, ResidualBlock) and feat_cache is not None:
                x = layer(x, feat_cache, feat_idx)
            else:
                x = layer(x)

        ## head
        for layer in self.head:
            if isinstance(layer, CausalConv3d) and feat_cache is not None:
                idx = feat_idx[0]
                cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
                if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                    # cache last frame of last two chunk
                    cache_x = torch.cat(
                        [
                            feat_cache[idx][:, :, -1, :, :]
                            .unsqueeze(2)
                            .to(cache_x.device),
                            cache_x,
                        ],
                        dim=2,
                    )
                x = layer(
                    x,
                    (
                        feat_cache[idx].to(x.device)
                        if feat_cache[idx] is not None
                        else None
                    ),
                )
                feat_cache[idx] = cache_x
                feat_idx[0] += 1
            else:
                x = layer(x)
        return x


class Decoder3d(nn.Module):
    def __init__(
        self,
        dim=128,
        z_dim=4,
        dim_mult=[1, 2, 4, 4],
        num_res_blocks=2,
        attn_scales=[],
        temperal_upsample=[False, True, True],
        dropout=0.0,
    ):
        super().__init__()
        self.dim = dim
        self.z_dim = z_dim
        self.dim_mult = dim_mult
        self.num_res_blocks = num_res_blocks
        self.attn_scales = attn_scales
        self.temperal_upsample = temperal_upsample

        # dimensions
        dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]
        scale = 1.0 / 2 ** (len(dim_mult) - 2)

        # init block
        self.conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1)

        # middle blocks
        self.middle = nn.Sequential(
            ResidualBlock(dims[0], dims[0], dropout),
            AttentionBlock(dims[0]),
            ResidualBlock(dims[0], dims[0], dropout),
        )

        # upsample blocks
        upsamples = []
        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):
            # residual (+attention) blocks
            if i == 1 or i == 2 or i == 3:
                in_dim = in_dim // 2
            for _ in range(num_res_blocks + 1):
                upsamples.append(ResidualBlock(in_dim, out_dim, dropout))
                if scale in attn_scales:
                    upsamples.append(AttentionBlock(out_dim))
                in_dim = out_dim

            # upsample block
            if i != len(dim_mult) - 1:
                mode = "upsample3d" if temperal_upsample[i] else "upsample2d"
                upsamples.append(Resample(out_dim, mode=mode))
                scale *= 2.0
        self.upsamples = nn.Sequential(*upsamples)

        # output blocks
        self.head = nn.Sequential(
            RMS_norm(out_dim, images=False),
            nn.SiLU(),
            CausalConv3d(out_dim, 3, 3, padding=1),
        )

        self.cache_device = None

    def set_cache_device(self, device):
        self.cache_device = device

        # set cache device for all layers
        for layer in self.middle + self.upsamples + self.head:
            if isinstance(layer, Resample) or isinstance(layer, ResidualBlock):
                layer.set_cache_device(device)

    def forward(self, x, feat_cache=None, feat_idx=[0]):
        cache_device = self.cache_device if self.cache_device is not None else x.device

        ## conv1
        if feat_cache is not None:
            idx = feat_idx[0]
            cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
            if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                # cache last frame of last two chunk
                cache_x = torch.cat(
                    [
                        feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device),
                        cache_x,
                    ],
                    dim=2,
                )
            x = self.conv1(
                x, feat_cache[idx].to(x.device) if feat_cache[idx] is not None else None
            )
            feat_cache[idx] = cache_x
            feat_idx[0] += 1
        else:
            x = self.conv1(x)

        ## middle
        for layer in self.middle:
            if isinstance(layer, ResidualBlock) and feat_cache is not None:
                x = layer(x, feat_cache, feat_idx)
            else:
                x = layer(x)

        ## upsamples
        for layer in self.upsamples:
            if feat_cache is not None:
                x = layer(x, feat_cache, feat_idx)
            else:
                x = layer(x)

        ## head
        for layer in self.head:
            if isinstance(layer, CausalConv3d) and feat_cache is not None:
                idx = feat_idx[0]
                cache_x = x[:, :, -CACHE_T:, :, :].clone().to(cache_device)
                if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                    # cache last frame of last two chunk
                    cache_x = torch.cat(
                        [
                            feat_cache[idx][:, :, -1, :, :]
                            .unsqueeze(2)
                            .to(cache_x.device),
                            cache_x,
                        ],
                        dim=2,
                    )
                x = layer(
                    x,
                    (
                        feat_cache[idx].to(x.device)
                        if feat_cache[idx] is not None
                        else None
                    ),
                )
                feat_cache[idx] = cache_x
                feat_idx[0] += 1
            else:
                x = layer(x)
        return x


def count_conv3d(model):
    count = 0
    for m in model.modules():
        if isinstance(m, CausalConv3d):
            count += 1
    return count


class WanVAE_(nn.Module):

    def __init__(
        self,
        dim=128,
        z_dim=4,
        dim_mult=[1, 2, 4, 4],
        num_res_blocks=2,
        attn_scales=[],
        temperal_downsample=[True, True, False],
        dropout=0.0,
    ):
        super().__init__()
        self.dim = dim
        self.z_dim = z_dim
        self.dim_mult = dim_mult
        self.num_res_blocks = num_res_blocks
        self.attn_scales = attn_scales
        self.temperal_downsample = temperal_downsample
        self.temperal_upsample = temperal_downsample[::-1]

        # modules
        self.encoder = Encoder3d(
            dim,
            z_dim * 2,
            dim_mult,
            num_res_blocks,
            attn_scales,
            self.temperal_downsample,
            dropout,
        )
        self.conv1 = CausalConv3d(z_dim * 2, z_dim * 2, 1)
        self.conv2 = CausalConv3d(z_dim, z_dim, 1)
        self.decoder = Decoder3d(
            dim,
            z_dim,
            dim_mult,
            num_res_blocks,
            attn_scales,
            self.temperal_upsample,
            dropout,
        )

        self.cache_device = None

    @property
    def dtype(self):
        return self.conv1.weight.dtype

    @property
    def device(self):
        return self.conv1.weight.device

    def set_cache_device(self, device):
        # set cache device
        self.cache_device = device
        self.encoder.set_cache_device(device)
        self.decoder.set_cache_device(device)

    def forward(self, x):
        mu, log_var = self.encode(x)  # type: ignore
        z = self.reparameterize(mu, log_var)
        x_recon = self.decode(z)  # type: ignore
        return x_recon, mu, log_var

    def encode(self, x, scale):
        self.clear_cache()
        ## cache
        t = x.shape[2]
        iter_ = 1 + (t - 1) // 4
        # ## ÂØπencodeËæìÂÖ•ÁöÑxÔºåÊåâÊó∂Èó¥ÊãÜÂàÜ‰∏∫1„ÄÅ4„ÄÅ4„ÄÅ4....

        # if self.cache_device is None:
        for i in range(iter_):
            self._enc_conv_idx = [0]
            if i == 0:
                out = self.encoder(
                    x[:, :, :1, :, :],
                    feat_cache=self._enc_feat_map,
                    feat_idx=self._enc_conv_idx,
                )
            else:
                out_ = self.encoder(
                    x[:, :, 1 + 4 * (i - 1) : 1 + 4 * i, :, :],
                    feat_cache=self._enc_feat_map,
                    feat_idx=self._enc_conv_idx,
                )
                out = torch.cat([out, out_], 2)
        # else:
        #     # VRAM optimization
        #     device = x.device
        #     clean_memory_on_device(device)
        #     outs = []
        #     for i in range(iter_):
        #         self._enc_conv_idx = [0]
        #         if i == 0:
        #             out = self.encoder(x[:, :, :1, :, :], feat_cache=self._enc_feat_map, feat_idx=self._enc_conv_idx)
        #         else:
        #             out = self.encoder(
        #                 x[:, :, 1 + 4 * (i - 1) : 1 + 4 * i, :, :], feat_cache=self._enc_feat_map, feat_idx=self._enc_conv_idx
        #             )
        #         outs.append(out.to(self.cache_device))
        #     out = torch.cat(outs, 2).to(device)
        mu, log_var = self.conv1(out).chunk(2, dim=1)
        if isinstance(scale[0], torch.Tensor):
            mu = (mu - scale[0].view(1, self.z_dim, 1, 1, 1)) * scale[1].view(
                1, self.z_dim, 1, 1, 1
            )
        else:
            mu = (mu - scale[0]) * scale[1]
        self.clear_cache()
        return mu

    def decode(self, z, scale):
        self.clear_cache()
        # z: [b,c,t,h,w]
        if isinstance(scale[0], torch.Tensor):
            z = z / scale[1].view(1, self.z_dim, 1, 1, 1) + scale[0].view(
                1, self.z_dim, 1, 1, 1
            )
        else:
            z = z / scale[1] + scale[0]
        iter_ = z.shape[2]
        x = self.conv2(z)

        # if self.cache_device is None:
        for i in range(iter_):
            self._conv_idx = [0]
            if i == 0:
                out = self.decoder(
                    x[:, :, i : i + 1, :, :],
                    feat_cache=self._feat_map,
                    feat_idx=self._conv_idx,
                )
            else:
                out_ = self.decoder(
                    x[:, :, i : i + 1, :, :],
                    feat_cache=self._feat_map,
                    feat_idx=self._conv_idx,
                )
                out = torch.cat([out, out_], 2)
        # else:
        #     # VRAM optimization
        #     device = z.device
        #     x = x.to("cpu")
        #     clean_memory_on_device(device)
        #     outs = []
        #     for i in range(iter_):
        #         self._conv_idx = [0]
        #         out = self.decoder(x[:, :, i : i + 1, :, :].to(device), feat_cache=self._feat_map, feat_idx=self._conv_idx).to(
        #             self.cache_device
        #         )
        #         outs.append(out)
        #     out = torch.cat(outs, 2)  # on cache_device
        self.clear_cache()
        return out

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return eps * std + mu

    def sample(self, imgs, deterministic=False):
        mu, log_var = self.encode(imgs)  # type: ignore
        if deterministic:
            return mu
        std = torch.exp(0.5 * log_var.clamp(-30.0, 20.0))
        return mu + std * torch.randn_like(std)

    def clear_cache(self):
        self._conv_num = count_conv3d(self.decoder)
        self._conv_idx = [0]
        self._feat_map = [None] * self._conv_num
        # cache encode
        self._enc_conv_num = count_conv3d(self.encoder)
        self._enc_conv_idx = [0]
        self._enc_feat_map = [None] * self._enc_conv_num


def _video_vae(pretrained_path=None, z_dim=None, device="cpu", **kwargs):
    """
    Autoencoder3d adapted from Stable Diffusion 1.x, 2.x and XL.
    """
    # params
    cfg = dict(
        dim=96,
        z_dim=z_dim,
        dim_mult=[1, 2, 4, 4],
        num_res_blocks=2,
        attn_scales=[],
        temperal_downsample=[False, True, True],
        dropout=0.0,
    )
    cfg.update(**kwargs)

    # init model
    with torch.device("meta"):
        model = WanVAE_(**cfg)  # type: ignore

    # load checkpoint
    logging.info(f"loading {pretrained_path}")
    if os.path.splitext(pretrained_path)[-1] == ".safetensors":  # type: ignore
        sd = load_file(pretrained_path)  # type: ignore
        model.load_state_dict(sd, strict=False, assign=True)
    else:
        model.load_state_dict(
            torch.load(pretrained_path, map_location=device, weights_only=True),  # type: ignore
            assign=True,
        )

    return model


class WanVAE:

    def __init__(
        self,
        z_dim=16,
        vae_path="cache/vae_step_411000.pth",
        dtype=torch.float,
        device="cuda",
        cache_device=None,
    ):
        self.dtype = dtype
        self.device = device

        mean = [
            -0.7571,
            -0.7089,
            -0.9113,
            0.1075,
            -0.1745,
            0.9653,
            -0.1517,
            1.5508,
            0.4134,
            -0.0715,
            0.5517,
            -0.3632,
            -0.1922,
            -0.9497,
            0.2503,
            -0.2921,
        ]
        std = [
            2.8184,
            1.4541,
            2.3275,
            2.6558,
            1.2196,
            1.7708,
            2.6052,
            2.0743,
            3.2687,
            2.1526,
            2.8652,
            1.5579,
            1.6382,
            1.1253,
            2.8251,
            1.9160,
        ]
        self.mean = torch.tensor(mean, dtype=dtype, device=device)
        self.std = torch.tensor(std, dtype=dtype, device=device)
        self.scale = [self.mean, 1.0 / self.std]

        # init model
        self.model = (
            _video_vae(
                pretrained_path=vae_path,
                z_dim=z_dim,
            )
            .eval()
            .requires_grad_(False)
            .to(device, dtype=dtype)
        )
        if cache_device is not None:
            self.model.set_cache_device(torch.device(cache_device))

    def to_device(self, device):
        self.device = device
        self.model.to(device)
        self.mean = self.mean.to(device)
        self.std = self.std.to(device)
        self.scale = [t.to(device) for t in self.scale]

    def to_dtype(self, dtype):
        self.dtype = dtype
        self.model.to(dtype=dtype)
        self.mean = self.mean.to(dtype)
        self.std = self.std.to(dtype)
        self.scale = [t.to(dtype) for t in self.scale]

    def eval(self):
        self.model.eval()

    def train(self, mode: bool = True):
        self.model.train(mode)

    def requires_grad_(self, requires_grad: bool = True):
        self.model.requires_grad_(requires_grad)

    def to(
        self,
        device_or_dtype: Union[torch.device, torch.dtype, str],
        dtype: Optional[torch.dtype] = None,
    ):
        """
        Add nn.Module.to() support for device and dtype.
        """
        if isinstance(device_or_dtype, str) or isinstance(
            device_or_dtype, torch.device
        ):
            self.to_device(device_or_dtype)
        else:
            self.to_dtype(device_or_dtype)

        if dtype is not None:
            self.to_dtype(dtype)

    def encode(self, videos):
        """
        videos: A list of videos each with shape [C, T, H, W].
        """
        # with amp.autocast(dtype=self.dtype):
        return [
            self.model.encode(u.unsqueeze(0), self.scale).float().squeeze(0)
            for u in videos
        ]

    def decode(self, zs):
        # with amp.autocast(dtype=self.dtype):
        return [
            self.model.decode(u.unsqueeze(0), self.scale)
            .float()
            .clamp_(-1, 1)
            .squeeze(0)
            for u in zs
        ]
</file>

<file path="wan/utils/fm_solvers_unipc.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/utils/fm_solvers_unipc.py (Apache)

# Copied from https://github.com/huggingface/diffusers/blob/v0.31.0/src/diffusers/schedulers/scheduling_unipc_multistep.py
# Convert unipc for flow matching
# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.

import math
from typing import List, Optional, Tuple, Union

import numpy as np
import torch
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.schedulers.scheduling_utils import (
    KarrasDiffusionSchedulers,
    SchedulerMixin,
    SchedulerOutput,
)
from diffusers.utils.deprecation_utils import deprecate
from diffusers.utils.import_utils import is_scipy_available

if is_scipy_available():
    import scipy.stats


class FlowUniPCMultistepScheduler(SchedulerMixin, ConfigMixin):
    """
    `UniPCMultistepScheduler` is a training-free framework designed for the fast sampling of diffusion models.

    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic
    methods the library implements for all schedulers such as loading and saving.

    Args:
        num_train_timesteps (`int`, defaults to 1000):
            The number of diffusion steps to train the model.
        solver_order (`int`, default `2`):
            The UniPC order which can be any positive integer. The effective order of accuracy is `solver_order + 1`
            due to the UniC. It is recommended to use `solver_order=2` for guided sampling, and `solver_order=3` for
            unconditional sampling.
        prediction_type (`str`, defaults to "flow_prediction"):
            Prediction type of the scheduler function; must be `flow_prediction` for this scheduler, which predicts
            the flow of the diffusion process.
        thresholding (`bool`, defaults to `False`):
            Whether to use the "dynamic thresholding" method. This is unsuitable for latent-space diffusion models such
            as Stable Diffusion.
        dynamic_thresholding_ratio (`float`, defaults to 0.995):
            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.
        sample_max_value (`float`, defaults to 1.0):
            The threshold value for dynamic thresholding. Valid only when `thresholding=True` and `predict_x0=True`.
        predict_x0 (`bool`, defaults to `True`):
            Whether to use the updating algorithm on the predicted x0.
        solver_type (`str`, default `bh2`):
            Solver type for UniPC. It is recommended to use `bh1` for unconditional sampling when steps < 10, and `bh2`
            otherwise.
        lower_order_final (`bool`, default `True`):
            Whether to use lower-order solvers in the final steps. Only valid for < 15 inference steps. This can
            stabilize the sampling of DPMSolver for steps < 15, especially for steps <= 10.
        disable_corrector (`list`, default `[]`):
            Decides which step to disable the corrector to mitigate the misalignment between `epsilon_theta(x_t, c)`
            and `epsilon_theta(x_t^c, c)` which can influence convergence for a large guidance scale. Corrector is
            usually disabled during the first few steps.
        solver_p (`SchedulerMixin`, default `None`):
            Any other scheduler that if specified, the algorithm becomes `solver_p + UniC`.
        use_karras_sigmas (`bool`, *optional*, defaults to `False`):
            Whether to use Karras sigmas for step sizes in the noise schedule during the sampling process. If `True`,
            the sigmas are determined according to a sequence of noise levels {œÉi}.
        use_exponential_sigmas (`bool`, *optional*, defaults to `False`):
            Whether to use exponential sigmas for step sizes in the noise schedule during the sampling process.
        timestep_spacing (`str`, defaults to `"linspace"`):
            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and
            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.
        steps_offset (`int`, defaults to 0):
            An offset added to the inference steps, as required by some model families.
        final_sigmas_type (`str`, defaults to `"zero"`):
            The final `sigma` value for the noise schedule during the sampling process. If `"sigma_min"`, the final
            sigma is the same as the last sigma in the training schedule. If `zero`, the final sigma is set to 0.
    """

    _compatibles = [e.name for e in KarrasDiffusionSchedulers]
    order = 1

    @register_to_config
    def __init__(
        self,
        num_train_timesteps: int = 1000,
        solver_order: int = 2,
        prediction_type: str = "flow_prediction",
        shift: Optional[float] = 1.0,
        use_dynamic_shifting=False,
        thresholding: bool = False,
        dynamic_thresholding_ratio: float = 0.995,
        sample_max_value: float = 1.0,
        predict_x0: bool = True,
        solver_type: str = "bh2",
        lower_order_final: bool = True,
        disable_corrector: List[int] = [],
        solver_p: SchedulerMixin = None,  # type: ignore
        timestep_spacing: str = "linspace",
        steps_offset: int = 0,
        final_sigmas_type: Optional[str] = "zero",  # "zero", "sigma_min"
    ):

        if solver_type not in ["bh1", "bh2"]:
            if solver_type in ["midpoint", "heun", "logrho"]:
                self.register_to_config(solver_type="bh2")
            else:
                raise NotImplementedError(
                    f"{solver_type} is not implemented for {self.__class__}"
                )

        self.predict_x0 = predict_x0
        # setable values
        self.num_inference_steps = None
        alphas = np.linspace(1, 1 / num_train_timesteps, num_train_timesteps)[
            ::-1
        ].copy()
        sigmas = 1.0 - alphas
        sigmas = torch.from_numpy(sigmas).to(dtype=torch.float32)

        if not use_dynamic_shifting:
            # when use_dynamic_shifting is True, we apply the timestep shifting on the fly based on the image resolution
            sigmas = shift * sigmas / (1 + (shift - 1) * sigmas)  # pyright: ignore

        self.sigmas = sigmas
        self.timesteps = sigmas * num_train_timesteps

        self.model_outputs = [None] * solver_order
        self.timestep_list = [None] * solver_order
        self.lower_order_nums = 0
        self.disable_corrector = disable_corrector
        self.solver_p = solver_p
        self.last_sample = None
        self._step_index = None
        self._begin_index = None

        self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication
        self.sigma_min = self.sigmas[-1].item()
        self.sigma_max = self.sigmas[0].item()

    @property
    def step_index(self):
        """
        The index counter for current timestep. It will increase 1 after each scheduler step.
        """
        return self._step_index

    @property
    def begin_index(self):
        """
        The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
        """
        return self._begin_index

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.set_begin_index
    def set_begin_index(self, begin_index: int = 0):
        """
        Sets the begin index for the scheduler. This function should be run from pipeline before the inference.

        Args:
            begin_index (`int`):
                The begin index for the scheduler.
        """
        self._begin_index = begin_index

    # Modified from diffusers.schedulers.scheduling_flow_match_euler_discrete.FlowMatchEulerDiscreteScheduler.set_timesteps
    def set_timesteps(
        self,
        num_inference_steps: Union[int, None] = None,
        device: Union[str, torch.device] = None,  # type: ignore
        sigmas: Optional[List[float]] = None,
        mu: Optional[Union[float, None]] = None,
        shift: Optional[Union[float, None]] = None,
    ):
        """
        Sets the discrete timesteps used for the diffusion chain (to be run before inference).
        Args:
            num_inference_steps (`int`):
                Total number of the spacing of the time steps.
            device (`str` or `torch.device`, *optional*):
                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        """

        if self.config.use_dynamic_shifting and mu is None:  # type: ignore
            raise ValueError(
                " you have to pass a value for `mu` when `use_dynamic_shifting` is set to be `True`"
            )

        if sigmas is None:
            sigmas = np.linspace(
                self.sigma_max, self.sigma_min, num_inference_steps + 1  # type: ignore
            ).copy()[
                :-1
            ]  # pyright: ignore

        if self.config.use_dynamic_shifting:  # type: ignore
            sigmas = self.time_shift(mu, 1.0, sigmas)  # pyright: ignore
        else:
            if shift is None:
                shift = self.config.shift  # type: ignore
            sigmas = shift * sigmas / (1 + (shift - 1) * sigmas)  # pyright: ignore

        if self.config.final_sigmas_type == "sigma_min":  # type: ignore
            sigma_last = ((1 - self.alphas_cumprod[0]) / self.alphas_cumprod[0]) ** 0.5
        elif self.config.final_sigmas_type == "zero":  # type: ignore
            sigma_last = 0
        else:
            raise ValueError(
                f"`final_sigmas_type` must be one of 'zero', or 'sigma_min', but got {self.config.final_sigmas_type}"  # type: ignore
            )

        timesteps = sigmas * self.config.num_train_timesteps  # type: ignore
        sigmas = np.concatenate([sigmas, [sigma_last]]).astype(  # type: ignore
            np.float32
        )  # pyright: ignore

        self.sigmas = torch.from_numpy(sigmas)
        self.timesteps = torch.from_numpy(timesteps).to(
            device=device, dtype=torch.int64
        )

        self.num_inference_steps = len(timesteps)

        self.model_outputs = [
            None,
        ] * self.config.solver_order  # type: ignore
        self.lower_order_nums = 0
        self.last_sample = None
        if self.solver_p:
            self.solver_p.set_timesteps(self.num_inference_steps, device=device)  # type: ignore

        # add an index counter for schedulers that allow duplicated timesteps
        self._step_index = None
        self._begin_index = None
        self.sigmas = self.sigmas.to("cpu")  # to avoid too much CPU/GPU communication

    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler._threshold_sample
    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:
        """
        "Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the
        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by
        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing
        pixels from saturation at each step. We find that dynamic thresholding results in significantly better
        photorealism as well as better image-text alignment, especially when using very large guidance weights."

        https://arxiv.org/abs/2205.11487
        """
        dtype = sample.dtype
        batch_size, channels, *remaining_dims = sample.shape

        if dtype not in (torch.float32, torch.float64):
            sample = (
                sample.float()
            )  # upcast for quantile calculation, and clamp not implemented for cpu half

        # Flatten sample for doing quantile calculation along each image
        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))  # type: ignore

        abs_sample = sample.abs()  # "a certain percentile absolute pixel value"

        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)  # type: ignore
        s = torch.clamp(
            s, min=1, max=self.config.sample_max_value  # type: ignore
        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]
        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0
        sample = (
            torch.clamp(sample, -s, s) / s
        )  # "we threshold xt0 to the range [-s, s] and then divide by s"

        sample = sample.reshape(batch_size, channels, *remaining_dims)
        sample = sample.to(dtype)

        return sample

    # Copied from diffusers.schedulers.scheduling_flow_match_euler_discrete.FlowMatchEulerDiscreteScheduler._sigma_to_t
    def _sigma_to_t(self, sigma):
        return sigma * self.config.num_train_timesteps  # type: ignore

    def _sigma_to_alpha_sigma_t(self, sigma):
        return 1 - sigma, sigma

    # Copied from diffusers.schedulers.scheduling_flow_match_euler_discrete.set_timesteps
    def time_shift(self, mu: float, sigma: float, t: torch.Tensor):
        return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)

    def convert_model_output(
        self,
        model_output: torch.Tensor,
        *args,
        sample: torch.Tensor = None,  # type: ignore
        **kwargs,
    ) -> torch.Tensor:
        r"""
        Convert the model output to the corresponding type the UniPC algorithm needs.

        Args:
            model_output (`torch.Tensor`):
                The direct output from the learned diffusion model.
            timestep (`int`):
                The current discrete timestep in the diffusion chain.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.

        Returns:
            `torch.Tensor`:
                The converted model output.
        """
        timestep = args[0] if len(args) > 0 else kwargs.pop("timestep", None)
        if sample is None:
            if len(args) > 1:
                sample = args[1]
            else:
                raise ValueError("missing `sample` as a required keyward argument")
        if timestep is not None:
            deprecate(
                "timesteps",
                "1.0.0",
                "Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        sigma = self.sigmas[self.step_index]  # type: ignore
        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)

        if self.predict_x0:
            if self.config.prediction_type == "flow_prediction":  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                x0_pred = sample - sigma_t * model_output
            else:
                raise ValueError(
                    f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`,"  # type: ignore
                    " `v_prediction` or `flow_prediction` for the UniPCMultistepScheduler."
                )

            if self.config.thresholding:  # type: ignore
                x0_pred = self._threshold_sample(x0_pred)

            return x0_pred
        else:
            if self.config.prediction_type == "flow_prediction":  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                epsilon = sample - (1 - sigma_t) * model_output
            else:
                raise ValueError(
                    f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`,"  # type: ignore
                    " `v_prediction` or `flow_prediction` for the UniPCMultistepScheduler."
                )

            if self.config.thresholding:  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                x0_pred = sample - sigma_t * model_output
                x0_pred = self._threshold_sample(x0_pred)
                epsilon = model_output + x0_pred

            return epsilon

    def multistep_uni_p_bh_update(
        self,
        model_output: torch.Tensor,
        *args,
        sample: torch.Tensor = None,  # type: ignore
        order: int = None,  # pyright: ignore
        **kwargs,
    ) -> torch.Tensor:
        """
        One step for the UniP (B(h) version). Alternatively, `self.solver_p` is used if is specified.

        Args:
            model_output (`torch.Tensor`):
                The direct output from the learned diffusion model at the current timestep.
            prev_timestep (`int`):
                The previous discrete timestep in the diffusion chain.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
            order (`int`):
                The order of UniP at this timestep (corresponds to the *p* in UniPC-p).

        Returns:
            `torch.Tensor`:
                The sample tensor at the previous timestep.
        """
        prev_timestep = args[0] if len(args) > 0 else kwargs.pop("prev_timestep", None)
        if sample is None:
            if len(args) > 1:
                sample = args[1]
            else:
                raise ValueError(" missing `sample` as a required keyward argument")
        if order is None:
            if len(args) > 2:
                order = args[2]
            else:
                raise ValueError(" missing `order` as a required keyward argument")
        if prev_timestep is not None:
            deprecate(
                "prev_timestep",
                "1.0.0",
                "Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )
        model_output_list = self.model_outputs

        s0 = self.timestep_list[-1]
        m0 = model_output_list[-1]
        x = sample

        if self.solver_p:
            x_t = self.solver_p.step(model_output, s0, x).prev_sample  # type: ignore
            return x_t

        sigma_t, sigma_s0 = (
            self.sigmas[self.step_index + 1],  # type: ignore
            self.sigmas[self.step_index],  # type: ignore
        )  # pyright: ignore
        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)
        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)

        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)
        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)

        h = lambda_t - lambda_s0
        device = sample.device

        rks = []
        D1s = []
        for i in range(1, order):
            si = self.step_index - i  # pyright: ignore
            mi = model_output_list[-(i + 1)]
            alpha_si, sigma_si = self._sigma_to_alpha_sigma_t(self.sigmas[si])  # type: ignore
            lambda_si = torch.log(alpha_si) - torch.log(sigma_si)
            rk = (lambda_si - lambda_s0) / h
            rks.append(rk)
            D1s.append((mi - m0) / rk)  # pyright: ignore

        rks.append(1.0)
        rks = torch.tensor(rks, device=device)

        R = []
        b = []

        hh = -h if self.predict_x0 else h
        h_phi_1 = torch.expm1(hh)  # h\phi_1(h) = e^h - 1
        h_phi_k = h_phi_1 / hh - 1

        factorial_i = 1

        if self.config.solver_type == "bh1":  # type: ignore
            B_h = hh
        elif self.config.solver_type == "bh2":  # type: ignore
            B_h = torch.expm1(hh)
        else:
            raise NotImplementedError()

        for i in range(1, order + 1):
            R.append(torch.pow(rks, i - 1))
            b.append(h_phi_k * factorial_i / B_h)
            factorial_i *= i + 1
            h_phi_k = h_phi_k / hh - 1 / factorial_i

        R = torch.stack(R)
        b = torch.tensor(b, device=device)

        if len(D1s) > 0:
            D1s = torch.stack(D1s, dim=1)  # (B, K)
            # for order 2, we use a simplified version
            if order == 2:
                rhos_p = torch.tensor([0.5], dtype=x.dtype, device=device)
            else:
                rhos_p = torch.linalg.solve(R[:-1, :-1], b[:-1]).to(device).to(x.dtype)
        else:
            D1s = None

        if self.predict_x0:
            x_t_ = sigma_t / sigma_s0 * x - alpha_t * h_phi_1 * m0
            if D1s is not None:
                pred_res = torch.einsum(
                    "k,bkc...->bc...", rhos_p, D1s
                )  # pyright: ignore
            else:
                pred_res = 0
            x_t = x_t_ - alpha_t * B_h * pred_res
        else:
            x_t_ = alpha_t / alpha_s0 * x - sigma_t * h_phi_1 * m0
            if D1s is not None:
                pred_res = torch.einsum(
                    "k,bkc...->bc...", rhos_p, D1s
                )  # pyright: ignore
            else:
                pred_res = 0
            x_t = x_t_ - sigma_t * B_h * pred_res

        x_t = x_t.to(x.dtype)
        return x_t

    def multistep_uni_c_bh_update(
        self,
        this_model_output: torch.Tensor,
        *args,
        last_sample: torch.Tensor = None,  # type: ignore
        this_sample: torch.Tensor = None,  # type: ignore
        order: int = None,  # pyright: ignore
        **kwargs,
    ) -> torch.Tensor:
        """
        One step for the UniC (B(h) version).

        Args:
            this_model_output (`torch.Tensor`):
                The model outputs at `x_t`.
            this_timestep (`int`):
                The current timestep `t`.
            last_sample (`torch.Tensor`):
                The generated sample before the last predictor `x_{t-1}`.
            this_sample (`torch.Tensor`):
                The generated sample after the last predictor `x_{t}`.
            order (`int`):
                The `p` of UniC-p at this step. The effective order of accuracy should be `order + 1`.

        Returns:
            `torch.Tensor`:
                The corrected sample tensor at the current timestep.
        """
        this_timestep = args[0] if len(args) > 0 else kwargs.pop("this_timestep", None)
        if last_sample is None:
            if len(args) > 1:
                last_sample = args[1]
            else:
                raise ValueError(" missing`last_sample` as a required keyward argument")
        if this_sample is None:
            if len(args) > 2:
                this_sample = args[2]
            else:
                raise ValueError(" missing`this_sample` as a required keyward argument")
        if order is None:
            if len(args) > 3:
                order = args[3]
            else:
                raise ValueError(" missing`order` as a required keyward argument")
        if this_timestep is not None:
            deprecate(
                "this_timestep",
                "1.0.0",
                "Passing `this_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        model_output_list = self.model_outputs

        m0 = model_output_list[-1]
        x = last_sample
        x_t = this_sample
        model_t = this_model_output

        sigma_t, sigma_s0 = (
            self.sigmas[self.step_index],  # type: ignore
            self.sigmas[self.step_index - 1],  # type: ignore
        )  # pyright: ignore
        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)
        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)

        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)
        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)

        h = lambda_t - lambda_s0
        device = this_sample.device

        rks = []
        D1s = []
        for i in range(1, order):
            si = self.step_index - (i + 1)  # pyright: ignore
            mi = model_output_list[-(i + 1)]
            alpha_si, sigma_si = self._sigma_to_alpha_sigma_t(self.sigmas[si])  # type: ignore
            lambda_si = torch.log(alpha_si) - torch.log(sigma_si)
            rk = (lambda_si - lambda_s0) / h
            rks.append(rk)
            D1s.append((mi - m0) / rk)  # pyright: ignore

        rks.append(1.0)
        rks = torch.tensor(rks, device=device)

        R = []
        b = []

        hh = -h if self.predict_x0 else h
        h_phi_1 = torch.expm1(hh)  # h\phi_1(h) = e^h - 1
        h_phi_k = h_phi_1 / hh - 1

        factorial_i = 1

        if self.config.solver_type == "bh1":  # type: ignore
            B_h = hh
        elif self.config.solver_type == "bh2":  # type: ignore
            B_h = torch.expm1(hh)
        else:
            raise NotImplementedError()

        for i in range(1, order + 1):
            R.append(torch.pow(rks, i - 1))
            b.append(h_phi_k * factorial_i / B_h)
            factorial_i *= i + 1
            h_phi_k = h_phi_k / hh - 1 / factorial_i

        R = torch.stack(R)
        b = torch.tensor(b, device=device)

        if len(D1s) > 0:
            D1s = torch.stack(D1s, dim=1)
        else:
            D1s = None

        # for order 1, we use a simplified version
        if order == 1:
            rhos_c = torch.tensor([0.5], dtype=x.dtype, device=device)
        else:
            rhos_c = torch.linalg.solve(R, b).to(device).to(x.dtype)

        if self.predict_x0:
            x_t_ = sigma_t / sigma_s0 * x - alpha_t * h_phi_1 * m0
            if D1s is not None:
                corr_res = torch.einsum("k,bkc...->bc...", rhos_c[:-1], D1s)
            else:
                corr_res = 0
            D1_t = model_t - m0  # type: ignore
            x_t = x_t_ - alpha_t * B_h * (corr_res + rhos_c[-1] * D1_t)
        else:
            x_t_ = alpha_t / alpha_s0 * x - sigma_t * h_phi_1 * m0
            if D1s is not None:
                corr_res = torch.einsum("k,bkc...->bc...", rhos_c[:-1], D1s)
            else:
                corr_res = 0
            D1_t = model_t - m0  # type: ignore
            x_t = x_t_ - sigma_t * B_h * (corr_res + rhos_c[-1] * D1_t)
        x_t = x_t.to(x.dtype)
        return x_t

    def index_for_timestep(self, timestep, schedule_timesteps=None):
        if schedule_timesteps is None:
            schedule_timesteps = self.timesteps

        indices = (schedule_timesteps == timestep).nonzero()

        # The sigma index that is taken for the **very** first `step`
        # is always the second index (or the last index if there is only 1)
        # This way we can ensure we don't accidentally skip a sigma in
        # case we start in the middle of the denoising schedule (e.g. for image-to-image)
        pos = 1 if len(indices) > 1 else 0

        return indices[pos].item()

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler._init_step_index
    def _init_step_index(self, timestep):
        """
        Initialize the step_index counter for the scheduler.
        """

        if self.begin_index is None:
            if isinstance(timestep, torch.Tensor):
                timestep = timestep.to(self.timesteps.device)
            self._step_index = self.index_for_timestep(timestep)
        else:
            self._step_index = self._begin_index

    def step(
        self,
        model_output: torch.Tensor,
        timestep: Union[int, torch.Tensor],
        sample: torch.Tensor,
        return_dict: bool = True,
        generator=None,
    ) -> Union[SchedulerOutput, Tuple]:
        """
        Predict the sample from the previous timestep by reversing the SDE. This function propagates the sample with
        the multistep UniPC.

        Args:
            model_output (`torch.Tensor`):
                The direct output from learned diffusion model.
            timestep (`int`):
                The current discrete timestep in the diffusion chain.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
            return_dict (`bool`):
                Whether or not to return a [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`.

        Returns:
            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:
                If return_dict is `True`, [`~schedulers.scheduling_utils.SchedulerOutput`] is returned, otherwise a
                tuple is returned where the first element is the sample tensor.

        """
        if self.num_inference_steps is None:
            raise ValueError(
                "Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler"
            )

        if self.step_index is None:
            self._init_step_index(timestep)

        use_corrector = (
            self.step_index > 0  # type: ignore
            and self.step_index - 1 not in self.disable_corrector  # type: ignore
            and self.last_sample is not None  # pyright: ignore
        )

        model_output_convert = self.convert_model_output(model_output, sample=sample)
        if use_corrector:
            sample = self.multistep_uni_c_bh_update(
                this_model_output=model_output_convert,
                last_sample=self.last_sample,  # type: ignore
                this_sample=sample,
                order=self.this_order,  # type: ignore
            )

        for i in range(self.config.solver_order - 1):  # type: ignore
            self.model_outputs[i] = self.model_outputs[i + 1]
            self.timestep_list[i] = self.timestep_list[i + 1]

        self.model_outputs[-1] = model_output_convert  # type: ignore
        self.timestep_list[-1] = timestep  # pyright: ignore

        if self.config.lower_order_final:  # type: ignore
            this_order = min(
                self.config.solver_order, len(self.timesteps) - self.step_index  # type: ignore
            )  # pyright: ignore
        else:
            this_order = self.config.solver_order  # type: ignore

        self.this_order = min(
            this_order, self.lower_order_nums + 1
        )  # warmup for multistep
        assert self.this_order > 0

        self.last_sample = sample
        prev_sample = self.multistep_uni_p_bh_update(
            model_output=model_output,  # pass the original non-converted model output, in case solver-p is used
            sample=sample,
            order=self.this_order,  # type: ignore
        )

        if self.lower_order_nums < self.config.solver_order:  # type: ignore
            self.lower_order_nums += 1

        # upon completion increase step index by one
        self._step_index += 1  # pyright: ignore

        if not return_dict:
            return (prev_sample,)

        return SchedulerOutput(prev_sample=prev_sample)

    def scale_model_input(self, sample: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """
        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
        current timestep.

        Args:
            sample (`torch.Tensor`):
                The input sample.

        Returns:
            `torch.Tensor`:
                A scaled input sample.
        """
        return sample

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.add_noise
    def add_noise(
        self,
        original_samples: torch.Tensor,
        noise: torch.Tensor,
        timesteps: torch.IntTensor,
    ) -> torch.Tensor:
        # Make sure sigmas and timesteps have the same device and dtype as original_samples
        sigmas = self.sigmas.to(
            device=original_samples.device, dtype=original_samples.dtype
        )
        if original_samples.device.type == "mps" and torch.is_floating_point(timesteps):
            # mps does not support float64
            schedule_timesteps = self.timesteps.to(
                original_samples.device, dtype=torch.float32
            )
            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)  # type: ignore
        else:
            schedule_timesteps = self.timesteps.to(original_samples.device)
            timesteps = timesteps.to(original_samples.device)  # type: ignore

        # begin_index is None when the scheduler is used for training or pipeline does not implement set_begin_index
        if self.begin_index is None:
            step_indices = [
                self.index_for_timestep(t, schedule_timesteps) for t in timesteps
            ]
        elif self.step_index is not None:
            # add_noise is called after first denoising step (for inpainting)
            step_indices = [self.step_index] * timesteps.shape[0]
        else:
            # add noise is called before first denoising step to create initial latent(img2img)
            step_indices = [self.begin_index] * timesteps.shape[0]

        sigma = sigmas[step_indices].flatten()  # type: ignore
        while len(sigma.shape) < len(original_samples.shape):
            sigma = sigma.unsqueeze(-1)

        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)
        noisy_samples = alpha_t * original_samples + sigma_t * noise
        return noisy_samples

    def __len__(self):
        return self.config.num_train_timesteps  # type: ignore
</file>

<file path="wan/utils/fm_solvers.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/utils/fm_solvers.py (Apache)

# Copied from https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py
# Convert dpm solver for flow matching
# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.

import inspect
import math
from typing import List, Optional, Tuple, Union

import numpy as np
import torch
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.schedulers.scheduling_utils import (
    KarrasDiffusionSchedulers,
    SchedulerMixin,
    SchedulerOutput,
)
from diffusers.utils.deprecation_utils import deprecate
from diffusers.utils.import_utils import is_scipy_available
from diffusers.utils.torch_utils import randn_tensor

if is_scipy_available():
    pass


def get_sampling_sigmas(sampling_steps, shift):
    sigma = np.linspace(1, 0, sampling_steps + 1)[:sampling_steps]
    sigma = shift * sigma / (1 + (shift - 1) * sigma)

    return sigma


def retrieve_timesteps(
    scheduler,
    num_inference_steps=None,
    device=None,
    timesteps=None,
    sigmas=None,
    **kwargs,
):
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            "Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values"
        )
    if timesteps is not None:
        accepts_timesteps = "timesteps" in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom"
                f" timestep schedules. Please check whether you are using the correct scheduler."
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = "sigmas" in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom"
                f" sigmas schedules. Please check whether you are using the correct scheduler."
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps


class FlowDPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):
    """
    `FlowDPMSolverMultistepScheduler` is a fast dedicated high-order solver for diffusion ODEs.
    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic
    methods the library implements for all schedulers such as loading and saving.
    Args:
        num_train_timesteps (`int`, defaults to 1000):
            The number of diffusion steps to train the model. This determines the resolution of the diffusion process.
        solver_order (`int`, defaults to 2):
            The DPMSolver order which can be `1`, `2`, or `3`. It is recommended to use `solver_order=2` for guided
            sampling, and `solver_order=3` for unconditional sampling. This affects the number of model outputs stored
            and used in multistep updates.
        prediction_type (`str`, defaults to "flow_prediction"):
            Prediction type of the scheduler function; must be `flow_prediction` for this scheduler, which predicts
            the flow of the diffusion process.
        shift (`float`, *optional*, defaults to 1.0):
            A factor used to adjust the sigmas in the noise schedule. It modifies the step sizes during the sampling
            process.
        use_dynamic_shifting (`bool`, defaults to `False`):
            Whether to apply dynamic shifting to the timesteps based on image resolution. If `True`, the shifting is
            applied on the fly.
        thresholding (`bool`, defaults to `False`):
            Whether to use the "dynamic thresholding" method. This method adjusts the predicted sample to prevent
            saturation and improve photorealism.
        dynamic_thresholding_ratio (`float`, defaults to 0.995):
            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.
        sample_max_value (`float`, defaults to 1.0):
            The threshold value for dynamic thresholding. Valid only when `thresholding=True` and
            `algorithm_type="dpmsolver++"`.
        algorithm_type (`str`, defaults to `dpmsolver++`):
            Algorithm type for the solver; can be `dpmsolver`, `dpmsolver++`, `sde-dpmsolver` or `sde-dpmsolver++`. The
            `dpmsolver` type implements the algorithms in the [DPMSolver](https://huggingface.co/papers/2206.00927)
            paper, and the `dpmsolver++` type implements the algorithms in the
            [DPMSolver++](https://huggingface.co/papers/2211.01095) paper. It is recommended to use `dpmsolver++` or
            `sde-dpmsolver++` with `solver_order=2` for guided sampling like in Stable Diffusion.
        solver_type (`str`, defaults to `midpoint`):
            Solver type for the second-order solver; can be `midpoint` or `heun`. The solver type slightly affects the
            sample quality, especially for a small number of steps. It is recommended to use `midpoint` solvers.
        lower_order_final (`bool`, defaults to `True`):
            Whether to use lower-order solvers in the final steps. Only valid for < 15 inference steps. This can
            stabilize the sampling of DPMSolver for steps < 15, especially for steps <= 10.
        euler_at_final (`bool`, defaults to `False`):
            Whether to use Euler's method in the final step. It is a trade-off between numerical stability and detail
            richness. This can stabilize the sampling of the SDE variant of DPMSolver for small number of inference
            steps, but sometimes may result in blurring.
        final_sigmas_type (`str`, *optional*, defaults to "zero"):
            The final `sigma` value for the noise schedule during the sampling process. If `"sigma_min"`, the final
            sigma is the same as the last sigma in the training schedule. If `zero`, the final sigma is set to 0.
        lambda_min_clipped (`float`, defaults to `-inf`):
            Clipping threshold for the minimum value of `lambda(t)` for numerical stability. This is critical for the
            cosine (`squaredcos_cap_v2`) noise schedule.
        variance_type (`str`, *optional*):
            Set to "learned" or "learned_range" for diffusion models that predict variance. If set, the model's output
            contains the predicted Gaussian variance.
    """

    _compatibles = [e.name for e in KarrasDiffusionSchedulers]
    order = 1

    @register_to_config
    def __init__(
        self,
        num_train_timesteps: int = 1000,
        solver_order: int = 2,
        prediction_type: str = "flow_prediction",
        shift: Optional[float] = 1.0,
        use_dynamic_shifting=False,
        thresholding: bool = False,
        dynamic_thresholding_ratio: float = 0.995,
        sample_max_value: float = 1.0,
        algorithm_type: str = "dpmsolver++",
        solver_type: str = "midpoint",
        lower_order_final: bool = True,
        euler_at_final: bool = False,
        final_sigmas_type: Optional[str] = "zero",  # "zero", "sigma_min"
        lambda_min_clipped: float = -float("inf"),
        variance_type: Optional[str] = None,
        invert_sigmas: bool = False,
    ):
        if algorithm_type in ["dpmsolver", "sde-dpmsolver"]:
            deprecation_message = f"algorithm_type {algorithm_type} is deprecated and will be removed in a future version. Choose from `dpmsolver++` or `sde-dpmsolver++` instead"
            deprecate(
                "algorithm_types dpmsolver and sde-dpmsolver",
                "1.0.0",
                deprecation_message,
            )

        # settings for DPM-Solver
        if algorithm_type not in [
            "dpmsolver",
            "dpmsolver++",
            "sde-dpmsolver",
            "sde-dpmsolver++",
        ]:
            if algorithm_type == "deis":
                self.register_to_config(algorithm_type="dpmsolver++")
            else:
                raise NotImplementedError(
                    f"{algorithm_type} is not implemented for {self.__class__}"
                )

        if solver_type not in ["midpoint", "heun"]:
            if solver_type in ["logrho", "bh1", "bh2"]:
                self.register_to_config(solver_type="midpoint")
            else:
                raise NotImplementedError(
                    f"{solver_type} is not implemented for {self.__class__}"
                )

        if (
            algorithm_type not in ["dpmsolver++", "sde-dpmsolver++"]
            and final_sigmas_type == "zero"
        ):
            raise ValueError(
                f"`final_sigmas_type` {final_sigmas_type} is not supported for `algorithm_type` {algorithm_type}. Please choose `sigma_min` instead."
            )

        # setable values
        self.num_inference_steps = None
        alphas = np.linspace(1, 1 / num_train_timesteps, num_train_timesteps)[
            ::-1
        ].copy()
        sigmas = 1.0 - alphas
        sigmas = torch.from_numpy(sigmas).to(dtype=torch.float32)

        if not use_dynamic_shifting:
            # when use_dynamic_shifting is True, we apply the timestep shifting on the fly based on the image resolution
            sigmas = shift * sigmas / (1 + (shift - 1) * sigmas)  # pyright: ignore

        self.sigmas = sigmas
        self.timesteps = sigmas * num_train_timesteps

        self.model_outputs = [None] * solver_order
        self.lower_order_nums = 0
        self._step_index = None
        self._begin_index = None

        # self.sigmas = self.sigmas.to(
        #     "cpu")  # to avoid too much CPU/GPU communication
        self.sigma_min = self.sigmas[-1].item()
        self.sigma_max = self.sigmas[0].item()

    @property
    def step_index(self):
        """
        The index counter for current timestep. It will increase 1 after each scheduler step.
        """
        return self._step_index

    @property
    def begin_index(self):
        """
        The index for the first timestep. It should be set from pipeline with `set_begin_index` method.
        """
        return self._begin_index

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.set_begin_index
    def set_begin_index(self, begin_index: int = 0):
        """
        Sets the begin index for the scheduler. This function should be run from pipeline before the inference.
        Args:
            begin_index (`int`):
                The begin index for the scheduler.
        """
        self._begin_index = begin_index

    # Modified from diffusers.schedulers.scheduling_flow_match_euler_discrete.FlowMatchEulerDiscreteScheduler.set_timesteps
    def set_timesteps(
        self,
        num_inference_steps: Union[int, None] = None,
        device: Union[str, torch.device] = None,  # type: ignore
        sigmas: Optional[List[float]] = None,
        mu: Optional[Union[float, None]] = None,
        shift: Optional[Union[float, None]] = None,
    ):
        """
        Sets the discrete timesteps used for the diffusion chain (to be run before inference).
        Args:
            num_inference_steps (`int`):
                Total number of the spacing of the time steps.
            device (`str` or `torch.device`, *optional*):
                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        """

        if self.config.use_dynamic_shifting and mu is None:  # type: ignore
            raise ValueError(
                " you have to pass a value for `mu` when `use_dynamic_shifting` is set to be `True`"
            )

        if sigmas is None:
            sigmas = np.linspace(
                self.sigma_max, self.sigma_min, num_inference_steps + 1  # type: ignore
            ).copy()[
                :-1
            ]  # pyright: ignore

        if self.config.use_dynamic_shifting:  # type: ignore
            sigmas = self.time_shift(mu, 1.0, sigmas)  # pyright: ignore
        else:
            if shift is None:
                shift = self.config.shift  # type: ignore
            sigmas = shift * sigmas / (1 + (shift - 1) * sigmas)  # pyright: ignore

        if self.config.final_sigmas_type == "sigma_min":  # type: ignore
            sigma_last = ((1 - self.alphas_cumprod[0]) / self.alphas_cumprod[0]) ** 0.5
        elif self.config.final_sigmas_type == "zero":  # type: ignore
            sigma_last = 0
        else:
            raise ValueError(
                f"`final_sigmas_type` must be one of 'zero', or 'sigma_min', but got {self.config.final_sigmas_type}"  # type: ignore
            )

        timesteps = sigmas * self.config.num_train_timesteps  # type: ignore
        sigmas = np.concatenate([sigmas, [sigma_last]]).astype(  # type: ignore
            np.float32
        )  # pyright: ignore

        self.sigmas = torch.from_numpy(sigmas)
        self.timesteps = torch.from_numpy(timesteps).to(
            device=device, dtype=torch.int64
        )

        self.num_inference_steps = len(timesteps)

        self.model_outputs = [
            None,
        ] * self.config.solver_order  # type: ignore
        self.lower_order_nums = 0

        self._step_index = None
        self._begin_index = None
        # self.sigmas = self.sigmas.to(
        #     "cpu")  # to avoid too much CPU/GPU communication

    # Copied from diffusers.schedulers.scheduling_ddpm.DDPMScheduler._threshold_sample
    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:
        """
        "Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the
        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by
        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing
        pixels from saturation at each step. We find that dynamic thresholding results in significantly better
        photorealism as well as better image-text alignment, especially when using very large guidance weights."
        https://arxiv.org/abs/2205.11487
        """
        dtype = sample.dtype
        batch_size, channels, *remaining_dims = sample.shape

        if dtype not in (torch.float32, torch.float64):
            sample = (
                sample.float()
            )  # upcast for quantile calculation, and clamp not implemented for cpu half

        # Flatten sample for doing quantile calculation along each image
        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))  # type: ignore

        abs_sample = sample.abs()  # "a certain percentile absolute pixel value"

        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)  # type: ignore
        s = torch.clamp(
            s, min=1, max=self.config.sample_max_value  # type: ignore
        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]
        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0
        sample = (
            torch.clamp(sample, -s, s) / s
        )  # "we threshold xt0 to the range [-s, s] and then divide by s"

        sample = sample.reshape(batch_size, channels, *remaining_dims)
        sample = sample.to(dtype)

        return sample

    # Copied from diffusers.schedulers.scheduling_flow_match_euler_discrete.FlowMatchEulerDiscreteScheduler._sigma_to_t
    def _sigma_to_t(self, sigma):
        return sigma * self.config.num_train_timesteps  # type: ignore

    def _sigma_to_alpha_sigma_t(self, sigma):
        return 1 - sigma, sigma

    # Copied from diffusers.schedulers.scheduling_flow_match_euler_discrete.set_timesteps
    def time_shift(self, mu: float, sigma: float, t: torch.Tensor):
        return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.convert_model_output
    def convert_model_output(
        self,
        model_output: torch.Tensor,
        *args,
        sample: torch.Tensor = None,  # type: ignore
        **kwargs,
    ) -> torch.Tensor:  # type: ignore
        """
        Convert the model output to the corresponding type the DPMSolver/DPMSolver++ algorithm needs. DPM-Solver is
        designed to discretize an integral of the noise prediction model, and DPM-Solver++ is designed to discretize an
        integral of the data prediction model.
        <Tip>
        The algorithm and model type are decoupled. You can use either DPMSolver or DPMSolver++ for both noise
        prediction and data prediction models.
        </Tip>
        Args:
            model_output (`torch.Tensor`):
                The direct output from the learned diffusion model.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
        Returns:
            `torch.Tensor`:
                The converted model output.
        """
        timestep = args[0] if len(args) > 0 else kwargs.pop("timestep", None)
        if sample is None:
            if len(args) > 1:
                sample = args[1]
            else:
                raise ValueError("missing `sample` as a required keyward argument")
        if timestep is not None:
            deprecate(
                "timesteps",
                "1.0.0",
                "Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        # DPM-Solver++ needs to solve an integral of the data prediction model.
        if self.config.algorithm_type in ["dpmsolver++", "sde-dpmsolver++"]:  # type: ignore
            if self.config.prediction_type == "flow_prediction":  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                x0_pred = sample - sigma_t * model_output
            else:
                raise ValueError(
                    f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`,"  # type: ignore
                    " `v_prediction`, or `flow_prediction` for the FlowDPMSolverMultistepScheduler."
                )

            if self.config.thresholding:  # type: ignore
                x0_pred = self._threshold_sample(x0_pred)

            return x0_pred

        # DPM-Solver needs to solve an integral of the noise prediction model.
        elif self.config.algorithm_type in ["dpmsolver", "sde-dpmsolver"]:  # type: ignore
            if self.config.prediction_type == "flow_prediction":  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                epsilon = sample - (1 - sigma_t) * model_output
            else:
                raise ValueError(
                    f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`,"  # type: ignore
                    " `v_prediction` or `flow_prediction` for the FlowDPMSolverMultistepScheduler."
                )

            if self.config.thresholding:  # type: ignore
                sigma_t = self.sigmas[self.step_index]  # type: ignore
                x0_pred = sample - sigma_t * model_output
                x0_pred = self._threshold_sample(x0_pred)
                epsilon = model_output + x0_pred

            return epsilon

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.dpm_solver_first_order_update
    def dpm_solver_first_order_update(
        self,
        model_output: torch.Tensor,
        *args,
        sample: torch.Tensor = None,  # type: ignore
        noise: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        """
        One step for the first-order DPMSolver (equivalent to DDIM).
        Args:
            model_output (`torch.Tensor`):
                The direct output from the learned diffusion model.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
        Returns:
            `torch.Tensor`:
                The sample tensor at the previous timestep.
        """
        timestep = args[0] if len(args) > 0 else kwargs.pop("timestep", None)
        prev_timestep = args[1] if len(args) > 1 else kwargs.pop("prev_timestep", None)
        if sample is None:
            if len(args) > 2:
                sample = args[2]
            else:
                raise ValueError(" missing `sample` as a required keyward argument")
        if timestep is not None:
            deprecate(
                "timesteps",
                "1.0.0",
                "Passing `timesteps` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        if prev_timestep is not None:
            deprecate(
                "prev_timestep",
                "1.0.0",
                "Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        sigma_t, sigma_s = (
            self.sigmas[self.step_index + 1],  # type: ignore
            self.sigmas[self.step_index],  # type: ignore
        )  # pyright: ignore
        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)
        alpha_s, sigma_s = self._sigma_to_alpha_sigma_t(sigma_s)
        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)
        lambda_s = torch.log(alpha_s) - torch.log(sigma_s)

        h = lambda_t - lambda_s
        if self.config.algorithm_type == "dpmsolver++":  # type: ignore
            x_t = (sigma_t / sigma_s) * sample - (
                alpha_t * (torch.exp(-h) - 1.0)
            ) * model_output
        elif self.config.algorithm_type == "dpmsolver":  # type: ignore
            x_t = (alpha_t / alpha_s) * sample - (
                sigma_t * (torch.exp(h) - 1.0)
            ) * model_output
        elif self.config.algorithm_type == "sde-dpmsolver++":  # type: ignore
            assert noise is not None
            x_t = (
                (sigma_t / sigma_s * torch.exp(-h)) * sample
                + (alpha_t * (1 - torch.exp(-2.0 * h))) * model_output
                + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise
            )
        elif self.config.algorithm_type == "sde-dpmsolver":  # type: ignore
            assert noise is not None
            x_t = (
                (alpha_t / alpha_s) * sample
                - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * model_output
                + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise
            )
        return x_t  # pyright: ignore

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update
    def multistep_dpm_solver_second_order_update(
        self,
        model_output_list: List[torch.Tensor],
        *args,
        sample: torch.Tensor = None,  # type: ignore
        noise: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        """
        One step for the second-order multistep DPMSolver.
        Args:
            model_output_list (`List[torch.Tensor]`):
                The direct outputs from learned diffusion model at current and latter timesteps.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
        Returns:
            `torch.Tensor`:
                The sample tensor at the previous timestep.
        """
        timestep_list = args[0] if len(args) > 0 else kwargs.pop("timestep_list", None)
        prev_timestep = args[1] if len(args) > 1 else kwargs.pop("prev_timestep", None)
        if sample is None:
            if len(args) > 2:
                sample = args[2]
            else:
                raise ValueError(" missing `sample` as a required keyward argument")
        if timestep_list is not None:
            deprecate(
                "timestep_list",
                "1.0.0",
                "Passing `timestep_list` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        if prev_timestep is not None:
            deprecate(
                "prev_timestep",
                "1.0.0",
                "Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        sigma_t, sigma_s0, sigma_s1 = (
            self.sigmas[self.step_index + 1],  # pyright: ignore
            self.sigmas[self.step_index],  # type: ignore
            self.sigmas[self.step_index - 1],  # pyright: ignore
        )

        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)
        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)
        alpha_s1, sigma_s1 = self._sigma_to_alpha_sigma_t(sigma_s1)

        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)
        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)
        lambda_s1 = torch.log(alpha_s1) - torch.log(sigma_s1)

        m0, m1 = model_output_list[-1], model_output_list[-2]

        h, h_0 = lambda_t - lambda_s0, lambda_s0 - lambda_s1
        r0 = h_0 / h
        D0, D1 = m0, (1.0 / r0) * (m0 - m1)
        if self.config.algorithm_type == "dpmsolver++":  # type: ignore
            # See https://arxiv.org/abs/2211.01095 for detailed derivations
            if self.config.solver_type == "midpoint":  # type: ignore
                x_t = (
                    (sigma_t / sigma_s0) * sample
                    - (alpha_t * (torch.exp(-h) - 1.0)) * D0
                    - 0.5 * (alpha_t * (torch.exp(-h) - 1.0)) * D1
                )
            elif self.config.solver_type == "heun":  # type: ignore
                x_t = (
                    (sigma_t / sigma_s0) * sample
                    - (alpha_t * (torch.exp(-h) - 1.0)) * D0
                    + (alpha_t * ((torch.exp(-h) - 1.0) / h + 1.0)) * D1
                )
        elif self.config.algorithm_type == "dpmsolver":  # type: ignore
            # See https://arxiv.org/abs/2206.00927 for detailed derivations
            if self.config.solver_type == "midpoint":  # type: ignore
                x_t = (
                    (alpha_t / alpha_s0) * sample
                    - (sigma_t * (torch.exp(h) - 1.0)) * D0
                    - 0.5 * (sigma_t * (torch.exp(h) - 1.0)) * D1
                )
            elif self.config.solver_type == "heun":  # type: ignore
                x_t = (
                    (alpha_t / alpha_s0) * sample
                    - (sigma_t * (torch.exp(h) - 1.0)) * D0
                    - (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1
                )
        elif self.config.algorithm_type == "sde-dpmsolver++":  # type: ignore
            assert noise is not None
            if self.config.solver_type == "midpoint":  # type: ignore
                x_t = (
                    (sigma_t / sigma_s0 * torch.exp(-h)) * sample
                    + (alpha_t * (1 - torch.exp(-2.0 * h))) * D0
                    + 0.5 * (alpha_t * (1 - torch.exp(-2.0 * h))) * D1
                    + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise
                )
            elif self.config.solver_type == "heun":  # type: ignore
                x_t = (
                    (sigma_t / sigma_s0 * torch.exp(-h)) * sample
                    + (alpha_t * (1 - torch.exp(-2.0 * h))) * D0
                    + (alpha_t * ((1.0 - torch.exp(-2.0 * h)) / (-2.0 * h) + 1.0)) * D1
                    + sigma_t * torch.sqrt(1.0 - torch.exp(-2 * h)) * noise
                )
        elif self.config.algorithm_type == "sde-dpmsolver":  # type: ignore
            assert noise is not None
            if self.config.solver_type == "midpoint":  # type: ignore
                x_t = (
                    (alpha_t / alpha_s0) * sample
                    - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * D0
                    - (sigma_t * (torch.exp(h) - 1.0)) * D1
                    + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise
                )
            elif self.config.solver_type == "heun":  # type: ignore
                x_t = (
                    (alpha_t / alpha_s0) * sample
                    - 2.0 * (sigma_t * (torch.exp(h) - 1.0)) * D0
                    - 2.0 * (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1
                    + sigma_t * torch.sqrt(torch.exp(2 * h) - 1.0) * noise
                )
        return x_t  # pyright: ignore

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update
    def multistep_dpm_solver_third_order_update(
        self,
        model_output_list: List[torch.Tensor],
        *args,
        sample: torch.Tensor = None,  # type: ignore
        **kwargs,
    ) -> torch.Tensor:
        """
        One step for the third-order multistep DPMSolver.
        Args:
            model_output_list (`List[torch.Tensor]`):
                The direct outputs from learned diffusion model at current and latter timesteps.
            sample (`torch.Tensor`):
                A current instance of a sample created by diffusion process.
        Returns:
            `torch.Tensor`:
                The sample tensor at the previous timestep.
        """

        timestep_list = args[0] if len(args) > 0 else kwargs.pop("timestep_list", None)
        prev_timestep = args[1] if len(args) > 1 else kwargs.pop("prev_timestep", None)
        if sample is None:
            if len(args) > 2:
                sample = args[2]
            else:
                raise ValueError(" missing`sample` as a required keyward argument")
        if timestep_list is not None:
            deprecate(
                "timestep_list",
                "1.0.0",
                "Passing `timestep_list` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        if prev_timestep is not None:
            deprecate(
                "prev_timestep",
                "1.0.0",
                "Passing `prev_timestep` is deprecated and has no effect as model output conversion is now handled via an internal counter `self.step_index`",
            )

        sigma_t, sigma_s0, sigma_s1, sigma_s2 = (
            self.sigmas[self.step_index + 1],  # pyright: ignore
            self.sigmas[self.step_index],  # type: ignore
            self.sigmas[self.step_index - 1],  # pyright: ignore
            self.sigmas[self.step_index - 2],  # pyright: ignore
        )

        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t)
        alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0)
        alpha_s1, sigma_s1 = self._sigma_to_alpha_sigma_t(sigma_s1)
        alpha_s2, sigma_s2 = self._sigma_to_alpha_sigma_t(sigma_s2)

        lambda_t = torch.log(alpha_t) - torch.log(sigma_t)
        lambda_s0 = torch.log(alpha_s0) - torch.log(sigma_s0)
        lambda_s1 = torch.log(alpha_s1) - torch.log(sigma_s1)
        lambda_s2 = torch.log(alpha_s2) - torch.log(sigma_s2)

        m0, m1, m2 = model_output_list[-1], model_output_list[-2], model_output_list[-3]

        h, h_0, h_1 = lambda_t - lambda_s0, lambda_s0 - lambda_s1, lambda_s1 - lambda_s2
        r0, r1 = h_0 / h, h_1 / h
        D0 = m0
        D1_0, D1_1 = (1.0 / r0) * (m0 - m1), (1.0 / r1) * (m1 - m2)
        D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)
        D2 = (1.0 / (r0 + r1)) * (D1_0 - D1_1)
        if self.config.algorithm_type == "dpmsolver++":  # type: ignore
            # See https://arxiv.org/abs/2206.00927 for detailed derivations
            x_t = (
                (sigma_t / sigma_s0) * sample
                - (alpha_t * (torch.exp(-h) - 1.0)) * D0
                + (alpha_t * ((torch.exp(-h) - 1.0) / h + 1.0)) * D1
                - (alpha_t * ((torch.exp(-h) - 1.0 + h) / h**2 - 0.5)) * D2
            )
        elif self.config.algorithm_type == "dpmsolver":  # type: ignore
            # See https://arxiv.org/abs/2206.00927 for detailed derivations
            x_t = (
                (alpha_t / alpha_s0) * sample
                - (sigma_t * (torch.exp(h) - 1.0)) * D0
                - (sigma_t * ((torch.exp(h) - 1.0) / h - 1.0)) * D1
                - (sigma_t * ((torch.exp(h) - 1.0 - h) / h**2 - 0.5)) * D2
            )
        return x_t  # pyright: ignore

    def index_for_timestep(self, timestep, schedule_timesteps=None):
        if schedule_timesteps is None:
            schedule_timesteps = self.timesteps

        indices = (schedule_timesteps == timestep).nonzero()

        # The sigma index that is taken for the **very** first `step`
        # is always the second index (or the last index if there is only 1)
        # This way we can ensure we don't accidentally skip a sigma in
        # case we start in the middle of the denoising schedule (e.g. for image-to-image)
        pos = 1 if len(indices) > 1 else 0

        return indices[pos].item()

    def _init_step_index(self, timestep):
        """
        Initialize the step_index counter for the scheduler.
        """

        if self.begin_index is None:
            if isinstance(timestep, torch.Tensor):
                timestep = timestep.to(self.timesteps.device)
            self._step_index = self.index_for_timestep(timestep)
        else:
            self._step_index = self._begin_index

    # Modified from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.step
    def step(
        self,
        model_output: torch.Tensor,
        timestep: Union[int, torch.Tensor],
        sample: torch.Tensor,
        generator=None,
        variance_noise: Optional[torch.Tensor] = None,
        return_dict: bool = True,
    ) -> Union[SchedulerOutput, Tuple]:
        """
        Predict the sample from the previous timestep by reversing the SDE. This function propagates the sample with
        the multistep DPMSolver.
        Args:
            model_output (`torch.Tensor`):
                The direct output from learned diffusion model.
            timestep (`int`):
                The current discrete timestep in the diffusion chain.
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
            generator (`torch.Generator`, *optional*):
                A random number generator.
            variance_noise (`torch.Tensor`):
                Alternative to generating noise with `generator` by directly providing the noise for the variance
                itself. Useful for methods such as [`LEdits++`].
            return_dict (`bool`):
                Whether or not to return a [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`.
        Returns:
            [`~schedulers.scheduling_utils.SchedulerOutput`] or `tuple`:
                If return_dict is `True`, [`~schedulers.scheduling_utils.SchedulerOutput`] is returned, otherwise a
                tuple is returned where the first element is the sample tensor.
        """
        if self.num_inference_steps is None:
            raise ValueError(
                "Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler"
            )

        if self.step_index is None:
            self._init_step_index(timestep)

        # Improve numerical stability for small number of steps
        lower_order_final = (self.step_index == len(self.timesteps) - 1) and (
            self.config.euler_at_final  # type: ignore
            or (self.config.lower_order_final and len(self.timesteps) < 15)  # type: ignore
            or self.config.final_sigmas_type == "zero"  # type: ignore
        )
        lower_order_second = (
            (self.step_index == len(self.timesteps) - 2)
            and self.config.lower_order_final  # type: ignore
            and len(self.timesteps) < 15
        )

        model_output = self.convert_model_output(model_output, sample=sample)
        for i in range(self.config.solver_order - 1):  # type: ignore
            self.model_outputs[i] = self.model_outputs[i + 1]
        self.model_outputs[-1] = model_output  # type: ignore

        # Upcast to avoid precision issues when computing prev_sample
        sample = sample.to(torch.float32)
        if (
            self.config.algorithm_type in ["sde-dpmsolver", "sde-dpmsolver++"]  # type: ignore
            and variance_noise is None
        ):
            noise = randn_tensor(
                model_output.shape,
                generator=generator,
                device=model_output.device,
                dtype=torch.float32,
            )
        elif self.config.algorithm_type in ["sde-dpmsolver", "sde-dpmsolver++"]:  # type: ignore
            noise = variance_noise.to(  # type: ignore
                device=model_output.device, dtype=torch.float32
            )  # pyright: ignore
        else:
            noise = None

        if (
            self.config.solver_order == 1  # type: ignore
            or self.lower_order_nums < 1
            or lower_order_final
        ):
            prev_sample = self.dpm_solver_first_order_update(
                model_output, sample=sample, noise=noise
            )
        elif (
            self.config.solver_order == 2  # type: ignore
            or self.lower_order_nums < 2
            or lower_order_second
        ):
            prev_sample = self.multistep_dpm_solver_second_order_update(
                self.model_outputs, sample=sample, noise=noise  # type: ignore
            )
        else:
            prev_sample = self.multistep_dpm_solver_third_order_update(
                self.model_outputs, sample=sample  # type: ignore
            )

        if self.lower_order_nums < self.config.solver_order:  # type: ignore
            self.lower_order_nums += 1

        # Cast sample back to expected dtype
        prev_sample = prev_sample.to(model_output.dtype)

        # upon completion increase step index by one
        self._step_index += 1  # pyright: ignore

        if not return_dict:
            return (prev_sample,)

        return SchedulerOutput(prev_sample=prev_sample)

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.scale_model_input
    def scale_model_input(self, sample: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """
        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
        current timestep.
        Args:
            sample (`torch.Tensor`):
                The input sample.
        Returns:
            `torch.Tensor`:
                A scaled input sample.
        """
        return sample

    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.scale_model_input
    def add_noise(
        self,
        original_samples: torch.Tensor,
        noise: torch.Tensor,
        timesteps: torch.IntTensor,
    ) -> torch.Tensor:
        # Make sure sigmas and timesteps have the same device and dtype as original_samples
        sigmas = self.sigmas.to(
            device=original_samples.device, dtype=original_samples.dtype
        )
        if original_samples.device.type == "mps" and torch.is_floating_point(timesteps):
            # mps does not support float64
            schedule_timesteps = self.timesteps.to(
                original_samples.device, dtype=torch.float32
            )
            timesteps = timesteps.to(original_samples.device, dtype=torch.float32)  # type: ignore
        else:
            schedule_timesteps = self.timesteps.to(original_samples.device)
            timesteps = timesteps.to(original_samples.device)  # type: ignore

        # begin_index is None when the scheduler is used for training or pipeline does not implement set_begin_index
        if self.begin_index is None:
            step_indices = [
                self.index_for_timestep(t, schedule_timesteps) for t in timesteps
            ]
        elif self.step_index is not None:
            # add_noise is called after first denoising step (for inpainting)
            step_indices = [self.step_index] * timesteps.shape[0]
        else:
            # add noise is called before first denoising step to create initial latent(img2img)
            step_indices = [self.begin_index] * timesteps.shape[0]

        sigma = sigmas[step_indices].flatten()  # type: ignore
        while len(sigma.shape) < len(original_samples.shape):
            sigma = sigma.unsqueeze(-1)

        alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)
        noisy_samples = alpha_t * original_samples + sigma_t * noise
        return noisy_samples

    def __len__(self):
        return self.config.num_train_timesteps  # type: ignore
</file>

<file path="wan/utils/utils.py">
## Based on: https://github.com/kohya-ss/musubi-tuner/blob/main/src/musubi_tuner/wan/utils/utils.py (Apache)

# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import argparse
import binascii
import os
import os.path as osp

import imageio
import torch
import torchvision


def rand_name(length=8, suffix=""):
    name = binascii.b2a_hex(os.urandom(length)).decode("utf-8")
    if suffix:
        if not suffix.startswith("."):
            suffix = "." + suffix
        name += suffix
    return name


def cache_video(
    tensor,
    save_file=None,
    fps=30,
    suffix=".mp4",
    nrow=8,
    normalize=True,
    value_range=(-1, 1),
    retry=5,
):
    # cache file
    cache_file = (
        osp.join("/tmp", rand_name(suffix=suffix)) if save_file is None else save_file
    )

    # save to cache
    error = None
    for _ in range(retry):
        try:
            # preprocess
            tensor = tensor.clamp(min(value_range), max(value_range))
            tensor = torch.stack(
                [
                    torchvision.utils.make_grid(
                        u, nrow=nrow, normalize=normalize, value_range=value_range
                    )
                    for u in tensor.unbind(2)
                ],
                dim=1,
            ).permute(1, 2, 3, 0)
            tensor = (tensor * 255).type(torch.uint8).cpu()

            # write video
            writer = imageio.get_writer(cache_file, fps=fps, codec="libx264", quality=8)
            for frame in tensor.numpy():
                writer.append_data(frame)
            writer.close()
            return cache_file
        except Exception as e:
            error = e
            continue
    else:
        print(f"cache_video failed, error: {error}", flush=True)
        return None


def cache_image(
    tensor, save_file, nrow=8, normalize=True, value_range=(-1, 1), retry=5
):
    # cache file
    suffix = osp.splitext(save_file)[1]
    if suffix.lower() not in [".jpg", ".jpeg", ".png", ".tiff", ".gif", ".webp"]:
        suffix = ".png"

    # save to cache
    error = None
    for _ in range(retry):
        try:
            tensor = tensor.clamp(min(value_range), max(value_range))
            torchvision.utils.save_image(
                tensor,
                save_file,
                nrow=nrow,
                normalize=normalize,
                value_range=value_range,
            )
            return save_file
        except Exception as e:
            error = e
            continue


def str2bool(v):
    """
    Convert a string to a boolean.

    Supported true values: 'yes', 'true', 't', 'y', '1'
    Supported false values: 'no', 'false', 'f', 'n', '0'

    Args:
        v (str): String to convert.

    Returns:
        bool: Converted boolean value.

    Raises:
        argparse.ArgumentTypeError: If the value cannot be converted to boolean.
    """
    if isinstance(v, bool):
        return v
    v_lower = v.lower()
    if v_lower in ("yes", "true", "t", "y", "1"):
        return True
    elif v_lower in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError("Boolean value expected (True/False)")
</file>

</files>
